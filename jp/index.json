[{"categories":"그래프이론","contents":"定義 1 ネットワーク $\\left( V, E \\right)$ で二つのノード $s,t \\in V$ をつなぐ最短距離のパスの数を $\\sigma_{st} = \\sigma_{ts}$ とし、特に $s,t$ をつなぐパスの中で別のノード $v \\in V$ を含むパスの数を $\\sigma_{st} (v)$ と表示しよう。次のように定義される $C_{S} : V \\to \\mathbb{Z}$ をノード $v$ のストレス中心性Stress Centralityという。 $$ C_{S} (v) := \\sum_{s \\ne v \\ne t \\in V} \\sigma_{st} (v) $$\n説明 $\\sigma_{st} (v)$ $\\sigma_{st}$ の定義をよく読めば、$s,t$の間の最短距離$d(s,t) = d(t,s)$ではなくその最短距離になるパスの数で、全ての$v \\in V$に対して$\\sigma_{vv} = 1$であり、グラフの距離関数 $d$ に関して$\\sigma_{st} (v)$は次のようになる。 $$ \\sigma_{st} (v) = \\begin{cases} 0 \u0026amp; , \\text{if } d \\left( s , t \\right) \u0026lt; d \\left( s , v \\right) + d \\left( v , t \\right) \\\\ \\sigma_{sv} \\cdot \\sigma_{vt} \u0026amp; , \\text{otherwise} \\end{cases} $$\n直感的な意味 ストレス中心性は1953年にShimbelによって紹介された、最も古い中心性の一つであり、数式 $$ C_{S} (v) = \\sum_{s \\ne v \\ne t \\in V} \\sigma_{st} (v) $$ を見れば、ノード $v \\in V$ が全ての二つのノード $s, t \\in V$ の間を仲介することによって、どれだけ最短距離のパスを多く作り出しているかを示すと考えることができる。自然界の多くの現象\u0026hellip;例えば水滴が表面積を最小限にし、動作の作用が最小になるように、二つのノードをつなぐ方法の中では最も短かったり早かったりするパスに多くの関心があり、このように最短距離のパスに頻繁に属する点には多くのストレスがかかるだろう。この直感的な意味から、$C_{S} (v)$をストレス中心性と呼ぶことはかなり意味がある。\n後に、ストレス中心性を補完する尺度として媒介中心性が紹介された。\n一緒に見る ネットワークの様々な中心性 🔒(24/02/08) 次数中心性 🔒(24/02/16) 媒介中心性 🔒(24/02/12) ストレス中心性 🔒(24/02/20) 近接中心性 🔒(24/02/24) 固有ベクトル中心性 Brandes. (2001). A Faster Algorithm for Betweenness Centrality. https://doi.org/10.1080/0022250X.2001.9990249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2522,"permalink":"https://freshrimpsushi.github.io/jp/posts/2522/","tags":null,"title":"ネットワーク理論におけるストレス中心性"},{"categories":"행렬대수","contents":"定義 自然言語において、スパースSparse（希薄な、まばらな）とは、値が$0$である場合、ほぼ存在しないものとみなされることからきた言葉だ。スパーシティSparsityは、どれだけ多くの$0$から構成されているかの度合いを指す。\nスパース行列 大部分の成分が$0$である行列をスパース行列という。\n$S$-スパース 1 $\\mathbf{v} \\in \\mathbb{R}^{d}$が多くても、$S \\ll d$個の$0$でない成分がある場合、$\\mathbf{x}$は**$S$-スパース**であると言われる。ここで、整数$S \\in \\mathbb{Z}$はベクトル$\\mathbf{v}$のサポート$\\operatorname{supp} \\mathbf{v} = \\left\\{ k : \\mathbf{v}_{k} \\ne 0 \\right\\}$に対して次の不等式を満たす。 $$ \\operatorname{card} \\left( \\operatorname{supp} \\mathbf{v} \\right) \\le S $$ ここで、$\\operatorname{card}$はカーディナリティを意味する。\n説明 語彙的には希薄行列や疎行列があるが、通常、どちらも使わず、英語の発音そのままに[スパース・マトリックス]と読むことが多い。スパース行列は、その名の通り、行列に「意味ある」情報がスパース（希薄）である行列として定義されるが、「大部分が」という言葉から推測できるように、数学的に厳密な定義ではない。具体的な$S$が与えられ、それが「十分にスパース」であると表現されると、その定義ははっきりする。\n最適化理論 最適化理論では、スパース回帰は回帰問題において解のスパーシティを最大化すること、つまり$S$を最小限にする最適化問題を指す。\nグラフ理論 グラフ理論では通常、データとして与えられるランダムネットワークの隣接行列はスパース行列とみなされる。\nデータ構造 実用的な計算の領域では、スパース行列はデータ構造に近い。例えば、最後の成分だけが$\\sqrt{2}$で、他はすべて$0$の$X,Y \\in \\mathbb{R}^{10^{6} \\times 10^{6}}$を考えてみる。この行列を保存するためには、浮動小数点で埋められた$2 \\cdot 10^{12}$スロットの配列が必要だが、その情報がスパース行列であることが確実であれば、使われない$0$をすべて保存する必要はなく、最後のスロットだけがあれば十分である。これは、単に保存が便利なだけでなく、演算でも効率良く行える。これら2つの行列は非常に大きいが、その積は簡単に $$ X Y = \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 2 \\end{bmatrix} $$ のように計算できる。この乗算をシュトラッセンのアルゴリズムで計算したとしても、それが賢明とは言えないだろう。「コンピュータでスパース行列を計算する」とは、これらの行列を少ない容量で読み書きでき、$0$がある部分を省略して非常に速く計算できることを意味する2。\nNeedell, D., Vershynin, R. Uniform Uncertainty Principle and Signal Recovery via Regularized Orthogonal Matching Pursuit. Found Comput Math 9, 317–334 (2009). https://doi.org/10.1007/s10208-008-9031-3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://ko.wikipedia.org/wiki/%ED%9D%AC%EC%86%8C%ED%96%89%EB%A0%AC\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2513,"permalink":"https://freshrimpsushi.github.io/jp/posts/2513/","tags":null,"title":"スパース行列"},{"categories":"측도론","contents":"定義1 関数 $f : \\mathbb{R} \\to \\mathbb{R} (\\text{または } \\mathbb{C})$ が与えられたとする。$f$が任意の有限個の互いに素な区間 $(a_{i}, b_{i}) \\subset [a,b]$に対しても以下の条件を満たす場合、$[a, b]$ 上で 絶対連続absolutely continuousと言われる。\n$$ \\forall \\epsilon \\gt 0 \\quad \\exist \\delta \\gt 0 \\text{ such that } \\sum\\limits_{i=1}^{N} (b_{i} - a_{i}) \\lt \\delta \\implies \\sum\\limits_{i=1}^{N} \\left| f(b_{j}) - f(a_{j}) \\right| \\lt \\epsilon $$\n説明 定義により、絶対連続であれば一様連続でもある。\n性質 $f$が微分可能であり、導関数 $f\u0026rsquo;$ が有界であれば、$f$は絶対連続である。\n参照 実数関数の絶対連続性 測度の絶対連続性 符号付き測度の絶対連続性 Gerald B. Folland, Real Analysis: Modern Techniques and Their Applications (2nd Edition, 1999), p105\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3542,"permalink":"https://freshrimpsushi.github.io/jp/posts/3542/","tags":null,"title":"絶対連続実関数"},{"categories":"줄리아","contents":"概要 InfiniteArrays.jlは無限のサイズを持つ配列を使えるようにするパッケージ1で、実際にはレイジー配列と多くの関連がある。レイジー評価とは、配列に計算すべきものがなんであるかは知っているけど、本当に必要になるまでは計算を先延ばしにする方法のことだ。もちろん、コンピューターは無限を理解できないけど、この方法を使ってコンピューターでも無限配列を実装したんだ。\nコード ∞ julia\u0026gt; using InfiniteArrays\rjulia\u0026gt; 3141592 \u0026lt; ∞\rtrue\rjulia\u0026gt; Inf == ∞\rtrue\rjulia\u0026gt; Inf === ∞\rfalse InfiniteArrays.jlを読み込むと、まず ∞ 記号で無限を表せるようになる。無理にパッケージを使わなくても Infで無限を表せるけど、こっちの方が直感的に使えるようになるんだ。大小関係の比較では同じ無限の大きさを持つけど、ポインターとして見た場合は違っていて区別できる。\nℵ₀ julia\u0026gt; x = zeros(Int64, ∞);\rjulia\u0026gt; length(x)\rℵ₀ 0でいっぱいの無限配列を作ると、これは無限可算集合になり、その大きさはアレフゼロ $\\aleph_{0}$だ。\n普通に使える julia\u0026gt; x[2] = 3; x[94124843] = 7; x\rℵ₀-element LazyArrays.CachedArray{Int64, 1, Vector{Int64}, Zeros{Int64, 1, Tuple{InfiniteArrays.OneToInf{Int64}}}} with indices OneToInf():\r0\r3\r0\r0\r0\r0\r0\r0\r0\r⋮\rjulia\u0026gt; sum(x)\r10 無限配列だとしても、インターフェイスが大きく変わる訳ではない。普段扱ってる配列と同様に扱えば、考えている通りに動くよ。\n全コード using InfiniteArrays\r3141592 \u0026lt; ∞\rInf == ∞\rInf === ∞\rx = zeros(Int64, ∞);\rlength(x)\rx[2] = 3; x[94124843] = 7; x\rsum(x) 環境 OS: Windows julia: v1.7.3 https://github.com/JuliaArrays/InfiniteArrays.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2511,"permalink":"https://freshrimpsushi.github.io/jp/posts/2511/","tags":null,"title":"ジュリアで無限配列を使用する方法"},{"categories":"줄리아","contents":"概要 MAT.jlは MATLABで使用されるデータ保存形式である*.matファイルを読み書きするライブラリだ1。 Juliaがそうであるように、このパッケージは既存のプログラミング言語や習慣を捨てさせるのではなく、できるだけ馴染みのある環境を提供することでユーザーを獲得する戦略を示している。\nJuliaの速さと利便性は大きな利点だが、MATLABは研究目的の視覚化に独特の利点を持っている。既にMATLABで図を描く作業に習熟している場合、\u0026lsquo;Juliaへの完全な移行\u0026rsquo;は大きな犠牲を伴うため、魅力的ではない。MAT.jlの存在は「そんな心配するな、計算はJuliaで速くこなしてから、図はMATLABで描き直せばいいんだ」という誘惑そのものだ。 逆に、\u0026lsquo;MATLABで既に作業をして構築したものが多いが、何か限界を感じてJuliaに移りたい状況\u0026rsquo;でも役立つ。 mat形式より進化したJulia独自の保存方式は、JLD2.jlパッケージを参照すればいい。\nコード X = rand(0:9, 8, 3)\rusing MAT\rmatwrite(\u0026#34;example.mat\u0026#34;, Dict(\u0026#34;Y\u0026#34; =\u0026gt; X))\rmatfile = matopen(\u0026#34;elpmaxe.mat\u0026#34;)\rA = read(matfile, \u0026#34;A\u0026#34;)\rclose(matfile) Julia → MATLAB MATLAB → Julia 環境 OS: Windows julia: v1.7.3 MAT v0.10.3 MATLAB: R2022b https://github.com/JuliaIO/MAT.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2509,"permalink":"https://freshrimpsushi.github.io/jp/posts/2509/","tags":null,"title":"ジュリアでmatファイルを読み書きする方法"},{"categories":"줄리아","contents":"概要 UnicodePlots.jlはジュリア REPLでユニコード文字を使って図を出力するライブラリ1で、プログラムが進行する中で軽量でありながら高品質の視覚化を可能にする。\nコード using UnicodePlots\rp1 = lineplot(100 |\u0026gt; randn |\u0026gt; cumsum)\rp1 = lineplot!(p1, 100 |\u0026gt; randn |\u0026gt; cumsum); p1\rUnicodePlots.heatmap(cumsum(abs.(randn(100,100)), dims=2)) 上の例のコードを実行した結果は次の通りです。\n環境 OS: Windows julia: v1.7.3 UnicodePlots v3.0.4 https://github.com/JuliaPlots/UnicodePlots.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2507,"permalink":"https://freshrimpsushi.github.io/jp/posts/2507/","tags":null,"title":"ジュリアコンソールでシンプルなグラフィックを出力する方法"},{"categories":"줄리아","contents":"方法 コンソールでCtrl + Lを押すと、コンソールが一見してクリアされるが、一部の環境では本当にリセットされるわけではなく、ウィンドウが上にスクロールされたように見える場合もある。きれいに消去したり、キーボード入力に頼らないためには、ASCII文字\\033cを出力するといい12。\nprint(\u0026#34;\\033c\u0026#34;) また、\\007を出力すると通知音が鳴る。3 簡易的なシミュレーションの終了など、音で知りたい場合に意外と便利だ。\nprintln(\u0026#34;\\007\u0026#34;) 環境 OS: Windows julia: v1.7.3 https://stackoverflow.com/questions/26548687/julia-how-to-clear-console\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://stackoverflow.com/questions/47503734/what-does-printf-033c-mean\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://discourse.julialang.org/t/how-to-play-a-sound-or-tone-when-a-program-ends/41239/13\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2505,"permalink":"https://freshrimpsushi.github.io/jp/posts/2505/","tags":null,"title":"ジュリアでコンソールを初期化する方法"},{"categories":"줄리아","contents":"概要 1 Juliaでは、dropmissing()関数を使って簡単に欠損値を削除できる。\nコード julia\u0026gt; df = DataFrame(x = [\u0026#34;i\u0026#34;, missing, \u0026#34;k\u0026#34;, \u0026#34;j\u0026#34;], y = [1, 2, 3, missing])\r4×2 DataFrame\rRow │ x y │ String? Int64? ─────┼──────────────────\r1 │ i 1\r2 │ missing 2\r3 │ k 3\r4 │ j missing 上記のように欠損値missingがあるデータフレームが与えられているとしよう。\njulia\u0026gt; dropmissing(df, :x)\r3×2 DataFrame\rRow │ x y │ String Int64? ─────┼─────────────────\r1 │ i 1\r2 │ k 3\r3 │ j missing julia\u0026gt; dropmissing(df, :y)\r3×2 DataFrame\rRow │ x y │ String? Int64 ─────┼────────────────\r1 │ i 1\r2 │ missing 2\r3 │ k 3 欠損値を削除したい列のシンボルを引数に入れればいい。\njulia\u0026gt; dropmissing(df)\r2×2 DataFrame\rRow │ x y │ String Int64\r─────┼───────────────\r1 │ i 1\r2 │ k 3 データフレーム全体から欠損値をすべて削除したい場合は、列を何も入力しなければいい。\n全コード using DataFrames\rdf = DataFrame(x = [\u0026#34;i\u0026#34;, missing, \u0026#34;k\u0026#34;, \u0026#34;j\u0026#34;], y = [1, 2, 3, missing])\rdropmissing(df, :x)\rdropmissing(df, :y)\rdropmissing(df) 環境 OS: Windows julia: v1.7.3 https://discourse.julialang.org/t/how-to-remove-rows-containing-missing-from-dataframe/12234/7\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2503,"permalink":"https://freshrimpsushi.github.io/jp/posts/2503/","tags":null,"title":"ジュリアでデータフレームの欠損値を削除する方法"},{"categories":"통계적분석","contents":"概要 空間統計分析では、空間過程がアイソトロピックで、セミバリオグラムが$\\gamma \\left( \\left\\| \\mathbf{h} \\right\\| \\right) = \\gamma (d)$を満たす場合、$\\gamma$は複雑な行列の形ではなく、1次元スカラー関数、即ち$\\gamma : \\mathbb{R} \\to \\mathbb{R}$として表現できる。これはポイントリファレンスデータ$Y(s), Y(s + d)$間の相関関係が折れ線グラフで描けることを意味する。\nモデル 1 (バリオグラムの等方性の投稿に続いて)\nバリオグラムのグラフは、データの特性に応じていくつかの型に現れる。\nこれらをどう解釈するかはともかく、このような図を描く方法は、x軸をデータ$Y(s), Y(s+d)$間の距離$d$、y軸を$\\gamma (d)$にすれば良い。そもそもこのように図で表されること自体、$\\gamma$を「バリオグラム」と呼ぶのが自然な命名だと確認できる。\n実際のデータでは、特定の長さ$d$に正確に対応するデータペアが多くないかもしれないため、一定の区分をパーティションとして分けて、経験的に求めることができる。上の図はジュリアでバリオグラムを描いた例で、$h = d$に従ってセミバリオグラムだけでなく、そのクラスの頻度も表示されている2。\n数式 バリオグラムをフィッティングするためのモデルとして、いくつかの関数が知られている。\nここでは、いくつかの重要なモデルについて簡単にコメントしていく：\nLinear：距離に比例して影響力が決まるモデルで、一見意味があるように見えるが、コバリオグラムとの解釈が難しいため、実際には使用されない。 Spherical：一定の距離以上で影響力が完全に消えるモデルで、多くのデータで合理的な選択となる。 Exponential：距離が遠くなるにつれて影響が指数的に減少する、最もシンプルで納得しやすいモデル 。学部生レベルのプロジェクトでは、これだけで十分。 Matérn：y切片に相当する$\\tau^{2}$とグラフのスケールを決める$\\sigma^{2}$のほか、形状自体に影響を与える$\\phi$から$\\nu$までが含まれ、上記のモデルの中では最も多くのデータに使用できる。Exponentialが簡単で無難なら、Matérnは最も強力で無難に多く使われるモデルと言える。数式で現れる$K_{\\nu}$は第1種変形ベッセル関数。 これで、数式をもとにセミバリオグラムをどう読むか見ていこう。その前に、次の数式を覚えておくと良いが、$\\gamma$と$C$はトレードオフの関係にあり、$C$が共分散を意味するので、$\\gamma$の値が高いということは、データ間の関係が低下するという意味になる。 $$ \\text{Var} Y = \\gamma ( \\mathbf{h} ) + C ( \\mathbf{h} ) $$\n一般的に、バリオグラムは$t$が大きくなるにつれて$\\gamma (t)$も大きくなり、ある程度からはさらに増加しない形を描くことが多い。直感的に、これは距離が離れるにつれてデータ間の関連性が低下し、ある距離を超えると特に関係がなくなることを表している。\nナゲット $$ \\text{Nugget} := \\gamma \\left( 0^{+} \\right) = \\lim_{t \\to 0+} \\gamma (t) = \\tau^{2} $$ 既知のモデルでは、$\\tau^{2}$に該当する値として\n理論的には$\\text{Var} Y = \\gamma ( \\mathbf{h} ) + C ( \\mathbf{h} )$なので、$\\mathbf{h} = 0$の時$C ( 0 ) = \\text{Var} Y$で、$\\gamma ( 0 ) = 0$であるべきだが 実際のデータを扱うと、正確にデータ$\\left| \\mathbf{h} \\right| = 0$の時は何の意味もなく、非常に近い点でも少しの差が出る。 このように、理論とは異なり生じるy切片をナゲットと呼ぶ。\nシル $$ \\text{Sill} := \\lim_{t \\to \\infty} \\gamma (t) = \\tau^{2} + \\sigma^{2} $$ 既知のモデルでは、$\\tau^{2} + \\sigma^{2}$に該当する値として、$\\gamma (t)$のシルと呼ばれる。モデルによっては理論的に収束しないかもしれないが、適切な許容値$0.05$を設けて「十分に天井に達したと思えば」その部分をシルと呼ぶことができる。特に、シルとナゲットの間の高さ$\\sigma$を部分シルと呼ぶ。\nレンジ $\\gamma (t)$が初めてシルに触れる地点までをレンジと呼ぶ。シルを求める際、許容値を設けた場合は特に有効レンジとも呼ばれる。\nBanerjee. (2015). Hierarchical Modeling and Analysis for Spatial Data(2nd Edition): p24~29.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://juliaearth.github.io/GeoStats.jl/stable/variography/empirical.html#Variograms\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2502,"permalink":"https://freshrimpsushi.github.io/jp/posts/2502/","tags":null,"title":"セミバリオグラムのモデル"},{"categories":"머신러닝","contents":"概要1 2 勾配降下法で使用されるアダプティブラーニングレートと、これを適用したモデルであるAdaGrad、RMSProp、Adamについて説明する。\n説明 勾配降下法で学習率learning rateは、パラメータが収束する速度、メソッドの成功の有無などを決定する重要なパラメータである。通常 $\\alpha$、$\\eta$と表記され、パラメータをアップデートする際、どれだけ勾配を反映するかを決める因子である。\n学習率による最適化の様子：大きな学習率(左)、小さな学習率(右)\r$$ \\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\alpha \\nabla L (\\boldsymbol{\\theta}_{i}) $$\n基本的な勾配降下法では $\\alpha$ は定数として説明されているが、この場合 勾配はベクトルなので、全ての変数（パラメータ）に対して同じ学習率が適用される。\n$$ \\alpha \\nabla L (\\boldsymbol{\\theta}) = \\alpha \\begin{bmatrix} \\dfrac{\\partial L}{\\partial \\theta_{1}} \u0026amp; \\dfrac{\\partial L}{\\partial \\theta_{2}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial L}{\\partial \\theta_{k}} \\end{bmatrix} = \\begin{bmatrix} \\alpha\\dfrac{\\partial L}{\\partial \\theta_{1}} \u0026amp; \\alpha\\dfrac{\\partial L}{\\partial \\theta_{2}} \u0026amp; \\cdots \u0026amp; \\alpha\\dfrac{\\partial L}{\\partial \\theta_{k}} \\end{bmatrix} $$\nしたがって、学習率を $\\boldsymbol{\\alpha} = (\\alpha_{1}, \\alpha_{2}, \\dots, \\alpha_{k})$ のようなベクトルとして考え、勾配項を以下の式のように一般化することができる。\n$$ \\boldsymbol{\\alpha} \\odot \\nabla L (\\boldsymbol{\\theta}) = \\begin{bmatrix} \\alpha_{1}\\dfrac{\\partial L}{\\partial \\theta_{1}} \u0026amp; \\alpha_{2}\\dfrac{\\partial L}{\\partial \\theta_{2}} \u0026amp; \\cdots \u0026amp; \\alpha_{k}\\dfrac{\\partial L}{\\partial \\theta_{k}} \\end{bmatrix} $$\nここで $\\odot$ は行列のアダマール積（要素毎の積）を表す。このようにパラメータ毎に異なる適用される学習率 $\\boldsymbol{\\alpha}$ を アダプティブラーニングレートadaptive learning rateと呼ぶ。以下の技術はアダプティブラーニングレートを勾配に依存して決定するため、$\\boldsymbol{\\alpha}$ は以下のような関数と見なすことができる。\n$$ \\boldsymbol{\\alpha} (\\nabla L(\\boldsymbol{\\theta})) = \\begin{bmatrix} \\alpha_{1}(\\nabla L(\\boldsymbol{\\theta})) \u0026amp; \\alpha_{2}(\\nabla L(\\boldsymbol{\\theta})) \u0026amp; \\dots \u0026amp; \\alpha_{k}(\\nabla L(\\boldsymbol{\\theta})) \\end{bmatrix} $$\n以下ではAdaGrad、RMSProp、Adamを紹介する。ここで重要な事実は、モーメンタム技術を含むこれらのオプティマイザー間に絶対的優位性はないということである。分野によって、作業によって最適なオプティマイザーは異なるので、単純に「何が最も良いか」についての判断や質問は良くない。自分が属する分野で主に使用されているものが何かを把握することが役に立ち、それがない場合やよくわからない場合は、SGD+モーメンタムまたはAdamを使用することが無難である。\nAdaGrad AdaGradは論文\u0026quot;(Duchi et al., 2011)Adaptive subgradient methods for online learning and stochastic optimization\u0026quot;で紹介されたアダプティブラーニングレート技術です。この名前はadaptive gradientの略で、[エイダグラード]または[アダグラード]と読みます。AdaGradでは、各パラメータに対する学習率を勾配に反比例するように設定します。ベクトル $\\mathbf{r}$ を次のように定義します。\n$$ \\mathbf{r} = (\\nabla L) \\odot (\\nabla L) = \\begin{bmatrix} \\left( \\dfrac{\\partial L}{\\partial \\theta_{1}} \\right)^{2} \u0026amp; \\left( \\dfrac{\\partial L}{\\partial \\theta_{2}} \\right)^{2} \u0026amp; \\cdots \u0026amp; \\left( \\dfrac{\\partial L}{\\partial \\theta_{k}} \\right)^{2} \\end{bmatrix} $$\n全体の学習率global learning rate $\\epsilon$、任意の小さい数 $\\delta$ に対して、アダプティブラーニングレート $\\boldsymbol{\\alpha}$ は次のようになります。\n$$ \\boldsymbol{\\alpha} = \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}} $$\n式からわかるように、勾配の成分が大きい変数には学習率を小さくし、勾配の成分が小さい変数には学習率を大きく適用します。$\\delta$ は分母が $0$ や非常に小さい数になるのを防ぐためのもので、通常は $10^{-5} \\sim 10^{-7}$ の値を使用することが多いです。また、学習率は反復ごとに累積されます。$i$ 番目の反復での勾配を $\\nabla L _{i} = \\nabla L (\\boldsymbol{\\theta}_{i})$ とすると、\n$$ \\begin{align} \\mathbf{r}_{i} \u0026amp;= (\\nabla L_{i}) \\odot (\\nabla L_{i}) \\nonumber \\\\ \\boldsymbol{\\alpha}_{i} \u0026amp;= \\boldsymbol{\\alpha}_{i-1} + \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{i}}} = \\sum_{j=1}^{i} \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{j}}} \\\\ \\boldsymbol{\\theta}_{i+1} \u0026amp;= \\boldsymbol{\\theta}_{i} - \\boldsymbol{\\alpha}_{i} \\odot \\nabla L_{i} \\nonumber \\end{align} $$\nアルゴリズム: AdaGrad 入力 全体の学習率 $\\epsilon$、小さな正数 $\\delta$、エポック $N$ 1. パラメータ $\\boldsymbol{\\theta}$ を任意の値で初期化する。 2. 学習率を $\\boldsymbol{\\alpha} = \\mathbf{0}$ で初期化する。 3. for $i = 1, \\cdots, N$ do 4. $\\mathbf{r} \\leftarrow \\nabla L(\\boldsymbol{\\theta}) \\odot \\nabla L(\\boldsymbol{\\theta})$ # 勾配を計算し、各成分を二乗する 5. $\\boldsymbol{\\alpha} \\leftarrow \\boldsymbol{\\alpha} + \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}}$ # アダプティブ学習率更新 6. $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\boldsymbol{\\alpha} \\odot \\nabla L(\\boldsymbol{\\theta})$ # パラメータ更新 7. end for RMSProp RMSPropは Root Mean Square Propagationの略で、ジェフリー・ヒントンGeoffrey Hintonの講義 Neural networks for machine learningで提案されたアダプティブラーニングレート技術です。基本的にAdaGradの変形であり、追加される項が指数的に減少するように$(1)$の加算を加重和に変えただけです。$\\rho \\in (0,1)$に対して、\n$$ \\boldsymbol{\\alpha}_{i} = \\rho \\boldsymbol{\\alpha}_{i-1} + (1-\\rho) \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{i}}} = (1-\\rho) \\sum_{j=1}^{i} \\rho^{i-j} \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{j}}} $$\n通常は $\\rho = 0.9, 0.99$ のような大きな値が使用されます。\nアルゴリズム: RMSProp 入力 全体の学習率 $\\epsilon$、小さな正数 $\\delta$、減衰率 $\\rho$、エポック $N$ 1. パラメータ $\\boldsymbol{\\theta}$ を任意の値で初期化する。 2. 学習率を $\\boldsymbol{\\alpha} = \\mathbf{0}$ で初期化する。 3. for $i = 1, \\dots, N$ do 4. $\\mathbf{r} \\leftarrow \\nabla L(\\boldsymbol{\\theta}) \\odot \\nabla L(\\boldsymbol{\\theta})$ # 勾配を計算し、各成分を二乗する 5. $\\boldsymbol{\\alpha} \\leftarrow \\rho \\boldsymbol{\\alpha} + (1-\\rho) \\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}}}$ # アダプティブ学習率更新 6. $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\boldsymbol{\\alpha} \\odot \\nabla L(\\boldsymbol{\\theta})$ # パラメータ更新 7. end for Adam Adam\u0026ldquo;adaptive moments\u0026quot;から派生は論文\u0026rdquo;(Kingma and Ba, 2014)Adam: A method for stochastic optimization\u0026quot;で紹介されたオプティマイザです。アダプティブラーニングレートとモーメンタム技術を組み合わせたもので、RMSProp + モーメンタムと見ることができます。RMSPropとモーメンタムを理解していれば、Adamを理解するのは難しくありません。RMSProp、モーメンタム、Adamをそれぞれ比較すると、以下のようになります。$\\nabla L_{i} = \\nabla L(\\boldsymbol{\\theta}_{i})$とすると、\nMomentum\r$\\mathbf{p}_{i} = \\beta_{1}\\mathbf{p}_{i-1} + \\nabla L_{i} \\quad (\\mathbf{p}_{0}=\\mathbf{0}) \\\\\r\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\alpha \\mathbf{p}_{i}$\rRMSProp\r$\\mathbf{r}_{i} = \\nabla L_{i} \\odot \\nabla L_{i} \\\\\r\\boldsymbol{\\alpha}_{i} = \\beta_{2} \\boldsymbol{\\alpha}_{i-1} + (1-\\beta_{2})\\dfrac{\\epsilon}{\\delta + \\sqrt{\\mathbf{r}_{i}}} \\quad (\\boldsymbol{\\alpha}_{0}=\\mathbf{0})\\\\\r\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\boldsymbol{\\alpha}_{i} \\odot \\nabla L_{i}$\rAdam\r$\\mathbf{p}_{i} = \\beta_{1}\\mathbf{p}_{i-1} + (1-\\beta_{1}) \\nabla L_{i-1} \\quad (\\mathbf{p}_{0}=\\mathbf{0}) \\\\\\\\[0.5em]\r\\hat{\\mathbf{p}}_{i} = \\dfrac{\\mathbf{p}_{i}}{1-(\\beta_{1})^{i}} \\\\\\\\[0.5em]\r\\mathbf{r}_{i} = \\beta_{2} \\mathbf{r}_{i-1} + (1-\\beta_{2}) \\nabla L_{i} \\odot \\nabla L_{i} \\\\\\\\[0.5em]\r\\hat{\\mathbf{r}}_{i} = \\dfrac{\\mathbf{r}}{1-(\\beta_{2})^{i}} \\\\\\\\[0.5em]\r\\hat{\\boldsymbol{\\alpha}}_{i} = \\dfrac{\\epsilon}{\\delta + \\sqrt{\\hat{\\mathbf{r}}_{i}}} \\\\\\\\[0.5em]\r\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\hat{\\boldsymbol{\\alpha}_{i}} \\odot \\hat{\\mathbf{p}_{i}}\r$\r$\\hat{\\mathbf{p}}_{i}$および$\\hat{\\mathbf{r}}_{i}$を計算する際に$1 - \\beta^{i}$で割る理由は、$\\mathbf{p}_{i}$および$\\mathbf{r}_{i}$が加重和であるため、これを加重平均に変換するためです。\nアルゴリズム: Adam\r入力 全体の学習率 $\\epsilon$ (推奨値は $0.001$), エポック $N$ 小さな定数 $\\delta$ (推奨値は $10^{-8}$) 減衰率 $\\beta\\_{1}, \\beta\\_{2}$ (推奨値はそれぞれ $0.9$ と $0.999$) 1. パラメータ $\\boldsymbol{\\theta}$ を任意の値で初期化する。 2. 学習率を $\\boldsymbol{\\alpha} = \\mathbf{0}$ で初期化する。 3. モーメンタムを $\\mathbf{p} = \\mathbf{0}$ で初期化する。 4. for $i = 1, \\dots, N$ do 5. $\\mathbf{p} \\leftarrow \\beta\\_{1}\\mathbf{p} + (1-\\beta\\_{1}) \\nabla L$ # 加重和でモーメンタム更新 6. $\\hat{\\mathbf{p}} \\leftarrow \\dfrac{\\mathbf{p}}{1-(\\beta\\_{1})^{i}}$ # 和を加重平均に補正 7. $\\mathbf{r} \\leftarrow \\beta\\_{2} \\mathbf{r} + (1-\\beta\\_{2}) \\nabla L \\odot \\nabla L$ # 加重和で勾配の二乗ベクトル更新 8. $\\hat{\\mathbf{r}} \\leftarrow \\dfrac{\\mathbf{r}}{1-(\\beta\\_{2})^{i}}$ # 和を加重平均に補正 9. $\\hat{\\boldsymbol{\\alpha}} \\leftarrow \\dfrac{\\epsilon}{\\delta + \\sqrt{\\hat{\\mathbf{r}}}}$ # アダプティブ学習率更新 10. $\\boldsymbol{\\theta} = \\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\alpha}} \\odot \\hat{\\mathbf{p}}$ # パラメータ更新 11. end for Ian Goodfellow, Deep Learning, 8.5 アダプティブラーニングレートを用いたアルゴリズム\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nオ・イルソク, 機械学習(MACHINE LEARNING) (2017), ch 5.4 アダプティブラーニングレート\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3529,"permalink":"https://freshrimpsushi.github.io/jp/posts/3529/","tags":null,"title":"適応的な学習率: AdaGrad, RMSProp, Adam"},{"categories":"줄리아","contents":"概要 Juliaで環境変数を参照する方法を説明する1。\nコード Base.ENV\rBase.ENV[\u0026#34;JULIA_NUM_THREADS\u0026#34;] 見るように、別のパッケージをロードする必要はなく、Base.ENVを通じて直接アクセスできる。辞書として読まれるため、求める環境変数の名前をキーとして置くと、その環境変数を文字列で得る。上のコード2行を実行した結果は以下の通りだ。\njulia\u0026gt; Base.ENV\rBase.EnvDict with 62 entries:\r\u0026#34;ALLUSERSPROFILE\u0026#34; =\u0026gt; \u0026#34;C:\\\\ProgramData\u0026#34;\r\u0026#34;APPDATA\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\AppData\\\\Roaming\u0026#34;\r\u0026#34;CHROME_CRASHPAD_PIPE_NAME\u0026#34; =\u0026gt; \u0026#34;\\\\\\\\.\\\\pipe\\\\crashpad_14984_WLSYYXMTXMJWXZQG\u0026#34;\r\u0026#34;COMMONPROGRAMFILES\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files\\\\Common Files\u0026#34;\r\u0026#34;COMMONPROGRAMFILES(X86)\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files (x86)\\\\Common Files\u0026#34;\r\u0026#34;COMMONPROGRAMW6432\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files\\\\Common Files\u0026#34;\r\u0026#34;COMPUTERNAME\u0026#34; =\u0026gt; \u0026#34;SICKRIGHT\u0026#34;\r\u0026#34;COMSPEC\u0026#34; =\u0026gt; \u0026#34;C:\\\\WINDOWS\\\\system32\\\\cmd.exe\u0026#34;\r\u0026#34;CUDA_PATH\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.5\u0026#34;\r\u0026#34;CUDA_PATH_V11_5\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.5\u0026#34;\r\u0026#34;DRIVERDATA\u0026#34; =\u0026gt; \u0026#34;C:\\\\Windows\\\\System32\\\\Drivers\\\\DriverData\u0026#34;\r\u0026#34;GOPATH\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\go\u0026#34;\r\u0026#34;HOMEDRIVE\u0026#34; =\u0026gt; \u0026#34;C:\u0026#34;\r\u0026#34;HOMEPATH\u0026#34; =\u0026gt; \u0026#34;\\\\Users\\\\rmsms\u0026#34;\r\u0026#34;JULIA_NUM_THREADS\u0026#34; =\u0026gt; \u0026#34;16\u0026#34;\r\u0026#34;LOCALAPPDATA\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\AppData\\\\Local\u0026#34;\r\u0026#34;LOGONSERVER\u0026#34; =\u0026gt; \u0026#34;\\\\\\\\SICKRIGHT\u0026#34;\r\u0026#34;NAVER\u0026#34; =\u0026gt; \u0026#34;e=2.718281\u0026#34;\r\u0026#34;NUMBER_OF_PROCESSORS\u0026#34; =\u0026gt; \u0026#34;16\u0026#34;\r\u0026#34;NVCUDASAMPLES11_5_ROOT\u0026#34; =\u0026gt; \u0026#34;C:\\\\ProgramData\\\\NVIDIA Corporation\\\\CUDA Samples\\\\v11.5\u0026#34;\r\u0026#34;NVCUDASAMPLES_ROOT\u0026#34; =\u0026gt; \u0026#34;C:\\\\ProgramData\\\\NVIDIA Corporation\\\\CUDA Samples\\\\v11.5\u0026#34;\r\u0026#34;NVTOOLSEXT_PATH\u0026#34; =\u0026gt; \u0026#34;C:\\\\Program Files\\\\NVIDIA Corporation\\\\NvToolsExt\\\\\u0026#34;\r\u0026#34;ONEDRIVE\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\OneDrive - knu.ac.kr\u0026#34;\r\u0026#34;ONEDRIVECOMMERCIAL\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\OneDrive - knu.ac.kr\u0026#34;\r\u0026#34;ONEDRIVECONSUMER\u0026#34; =\u0026gt; \u0026#34;C:\\\\Users\\\\rmsms\\\\OneDrive\u0026#34;\r\u0026#34;OPENBLAS_MAIN_FREE\u0026#34; =\u0026gt; \u0026#34;1\u0026#34;\r\u0026#34;OPENBLAS_NUM_THREADS\u0026#34; =\u0026gt; \u0026#34;8\u0026#34;\r⋮ =\u0026gt; ⋮\rjulia\u0026gt; Base.ENV[\u0026#34;JULIA_NUM_THREADS\u0026#34;]\r\u0026#34;16\u0026#34; 環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/base/base/#Base.ENV\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2499,"permalink":"https://freshrimpsushi.github.io/jp/posts/2499/","tags":null,"title":"ジュリアで環境変数を参照する方法"},{"categories":"머신러닝","contents":"概要1 2 勾配降下法におけるモーメンタム技術は、パラメーターを更新する際に以前の勾配もすべて使用することである。これが本質であり、これに尽きる。しかし、奇妙な更新式や物理学の運動量が動機となったとか、質量を$1$に設定し初期速度を$0$にするといった説明は理解を難しくするだけである。本稿では、モーメンタム技術をできるだけシンプルに説明する。\nビルドアップ パラメーターを$\\boldsymbol{\\theta}$、損失関数を$L$とするとき、標準的な勾配降下法は、以下のように反復的にパラメーターを更新する方法である。\n$$ \\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\alpha \\nabla L_{i} $$\nここで$L_{i} = L(\\boldsymbol{\\theta}_{i})$は、$i$番目の反復で計算された損失関数を意味する。モーメンタム技術とは、これに単に前の反復で計算された損失関数の勾配$\\nabla L_{i-1}$を加えることに過ぎない。\n$$ \\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\alpha \\nabla L_{i} - \\alpha \\nabla L_{i-1} - \\cdots - \\alpha \\nabla L_{0} $$\nここで、反復が進むにつれて勾配の影響を減らし、勾配の合計が発散するのを防ぐために係数$\\beta \\in (0,1)$を追加すると、次のようになる。\n$$ \\begin{align} \\boldsymbol{\\theta}_{i+1} \u0026amp;= \\boldsymbol{\\theta}_{i} - \\alpha \\nabla L_{i} - \\beta \\alpha \\nabla L_{i-1} - \\cdots - \\beta^{i}\\alpha \\nabla L_{0} \\nonumber \\\\ \u0026amp;= \\boldsymbol{\\theta}_{i} - \\alpha\\sum_{j=0}^{i} \\beta^{j} \\nabla L_{i-j} \\end{align} $$\n定義 $(1)$のようにパラメーターを更新することをモーメンタム技術momentum methodと呼び、追加される項$\\alpha\\sum\\limits_{j=0}^{i} \\beta^{j} \\nabla L_{i-j}$をモーメンタムmomentumと呼ぶ。\n説明 上記の定義によれば、モーメンタム技術は一般化された勾配降下法であり、むしろ勾配降下法はモーメンタム技術の$\\beta = 0$の特別なケースに過ぎないと見ることができる。$\\beta$が$1$に近いほど以前の勾配を多く反映し、$0$に近いほど少なく反映する。\n勾配降下法は、現在の傾きが最も大きい方向へパラメーターを更新するために貪欲アルゴリズムである。モーメンタム技術は勾配降下法の貪欲な部分を少し和らげ、現在最善の選択ではないが長期的にはより有効な選択をすることができるようにする。また、勾配の方向が急激に変わるのを防ぐことができる。\n当然ながら、パラメーターを更新する際の勾配の大きさが勾配降下法より大きいため、収束速度が速いという利点がある。また、経験的に局所的最小値local minimaから比較的脱出しやすいことが知られており、坂を転がり下るボールが十分な速さであれば、下り坂の途中にある小さな坂も越えて通り過ぎることができると説明される。\nここで重要な事実は、適応的学習率技術を含むこれらのオプティマイザー間に絶対的な優位性はないということである。分野や作業によって最適なオプティマイザーが異なるため、「何が最も良いか」という判断や質問は適切ではない。自分が所属する分野で主に使用されているものが何かを知ることが役立ち、それがないか分からなければSGD+モーメンタムまたはAdamを使用するのが無難である。\nネステロフのモーメンタム モーメンタム技術を再検討すると、次のパラメーター$\\boldsymbol{\\theta}_{i+1}$を得るために、現在のパラメーター$\\boldsymbol{\\theta}_{i}$に現在のパラメーターで計算された勾配$\\alpha \\nabla L(\\boldsymbol{\\theta}_{i})$を蓄積しながら加えていく。\nネステロフのモーメンタムNesterov momentumまたはネステロフ加速勾配Nesterov accelerated gradient, NAGと呼ばれる技術は、「現在のパラメーターに前の勾配を加えた値」で勾配を求め、これを現在のパラメーターに加えて次のパラメーターを求める。言葉ではやや複雑だが、モーメンタム技術を理解していれば、以下のアルゴリズムを見ることでネステロフのモーメンタムを理解するのが簡単になるかもしれない。\nアルゴリズム モーメンタム項を$\\mathbf{p}$と表す。\nアルゴリズム: モーメンタム技術 入力 学習率 $\\alpha$, モーメンタムパラメーター $\\beta$, エポック $N$ 1. パラメーター $\\boldsymbol{\\theta}$を任意の値で初期化する。 2. モーメンタムを $\\mathbf{p} = \\mathbf{0}$で初期化する。 3. for $k = 1, \\cdots, N$ do 4. $\\mathbf{p} \\leftarrow \\beta \\mathbf{p} - \\alpha \\nabla L(\\boldsymbol{\\theta})$ # 勾配を計算しモーメンタムを更新 5. $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\mathbf{p}$ # パラメーターを更新 6. end for アルゴリズム: ネステロフのモーメンタム 入力 学習率 $\\alpha$, モーメンタムパラメーター $\\beta$, エポック $N$ 1. パラメーター $\\boldsymbol{\\theta}$を任意の値で初期化する。 2. モーメンタムを $\\mathbf{p} = \\mathbf{0}$で初期化する。 3. for $k = 1, \\cdots, N$ do 4. $\\mathbf{p} \\leftarrow \\beta \\mathbf{p} - \\alpha \\nabla L(\\boldsymbol{\\theta} + \\beta \\mathbf{p})$ # 勾配を計算しモーメンタムを更新 5. $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\mathbf{p}$ # パラメーターを更新 6. end for 両方の方法について初めの数回の計算を見ると、以下のようになる。簡単に$\\mathbf{p}_i = \\alpha \\nabla L_i$、および$\\mathbf{p}^i = \\alpha \\nabla L(\\boldsymbol{\\theta}_i - \\beta^{1}\\mathbf{p}^{i-1} - \\beta^{2}\\mathbf{p}^{i-2} - \\cdots - \\beta^{i}\\mathbf{p}^{0})$（このとき$\\mathbf{p}^{0} = \\mathbf{p}_0$）と表記すると、\nモーメンタム ネステロフのモーメンタム $\\boldsymbol{\\theta}\\_{0} =$ 初期値\r$\\boldsymbol{\\theta}\\_{0} =$ 初期値\r$\\boldsymbol{\\theta}_{1} = \\boldsymbol{\\theta}_{0} - \\alpha \\nabla L_{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{0} - \\mathbf{p}_{0}$\r$\\boldsymbol{\\theta}_{1} = \\boldsymbol{\\theta}_{0} - \\alpha \\nabla L_{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{0} - \\mathbf{p}^{0}$\r$\\boldsymbol{\\theta}_{2} = \\boldsymbol{\\theta}_{1} - \\alpha\\nabla L_{1} - \\beta \\mathbf{p}_{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{1} - \\mathbf{p}_{1} - \\beta \\mathbf{p}_{0}$\r$\\boldsymbol{\\theta}_{2} = \\boldsymbol{\\theta}_{1} - \\alpha \\nabla L(\\boldsymbol{\\theta}_{1} - \\beta \\mathbf{p}^{0}) - \\beta \\mathbf{p}^{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{1} - \\mathbf{p}^{1} - \\beta \\mathbf{p}^{0}$\r$\\boldsymbol{\\theta}_{3} = \\boldsymbol{\\theta}_{2} - \\mathbf{p}_{2} - \\beta \\mathbf{p}_{1} - \\beta^{2} \\mathbf{p}_{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{2} - \\sum\\limits_{j=0}^{2}\\beta^{j}\\mathbf{p}_{2-j}$\r$\\boldsymbol{\\theta}_{3} = \\boldsymbol{\\theta}_{2} - \\alpha \\nabla L(\\boldsymbol{\\theta}_{2} - \\beta \\mathbf{p}^{1} - \\beta^{2}\\mathbf{p}^{0}) - \\beta \\mathbf{p}^{1} - \\beta^{2}\\mathbf{p}^{0} \\\\ \\quad\\ = \\boldsymbol{\\theta}_{2} - \\mathbf{p}^{2} - \\beta \\mathbf{p}^{1} - \\beta^{2} \\mathbf{p}^{0}$\r$$\\vdots$$\r$$\\vdots$$\r$\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\sum\\limits_{j=0}^{i}\\beta^{j}\\mathbf{p}_{i-j}$\r$\\boldsymbol{\\theta}_{i+1} = \\boldsymbol{\\theta}_{i} - \\sum\\limits_{j=0}^{i}\\beta^{j}\\mathbf{p}^{i-j}$\rイアン・グッドフェロー, ディープラーニング, 第8.3.2節 モーメンタム, 第8.3.3節 ネステロフのモーメンタム\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nオ・イルソク, 機械学習(MACHINE LEARNING) (2017), 第5.3節 モーメンタム\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3528,"permalink":"https://freshrimpsushi.github.io/jp/posts/3528/","tags":null,"title":"勾配降下における運動量法"},{"categories":"통계적분석","contents":"定義 1 ユークリッド空間の固定された部分集合 $D \\subset \\mathbb{R}^{r}$ で、確率変数 $Y(s) : \\Omega \\to \\mathbb{R}^{1}$ の集合である空間過程 $\\left\\{ Y(s) \\right\\}_{s \\in D}$ と方向ベクトル $\\mathbf{h} \\in \\mathbb{R}^{r}$ を考える。具体的に $n \\in \\mathbb{N}$ 個のサイトを $\\left\\{ s_{1} , \\cdots , s_{n} \\right\\} \\subset D$ のように表し、$Y(s)$ は全ての $s \\in D$ に対して分散が存在すると仮定する。次のように定義される $2 \\gamma ( \\mathbf{h} )$ をバリオグラムと呼ぶ。 $$ 2 \\gamma ( \\mathbf{h} ) := E \\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right]^{2} $$ 特にバリオグラムの半分 $\\gamma ( \\mathbf{h} )$ をセミバリオグラムと呼ぶ。\n説明 定常的空間過程の定義:\n全ての $s \\in D$ で $\\mu (s)$ が定数関数 $\\mu (s) := \\mu$ であり、全ての $\\mathbf{h}$ に対して共分散がある関数 $C$ を通じて $s$ に無関係に $\\mathbf{h}$ のみの関数 $C : \\mathbb{R}^{r} \\to \\mathbb{R}$ として表される場合、$\\left\\{ Y(s) \\right\\}$ が弱定常性を持つと言われる。 $$ \\text{Cov} \\left( Y (s) , Y \\left( s + \\mathbf{h} \\right) \\right) = C \\left( \\mathbf{h} \\right) $$ ここで、$C$ を共分散関数またはコバリオグラムと呼ぶ。 $\\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right]$ の平均が $0$ であり分散が唯一 $\\mathbf{h}$ にのみ依存する場合、$\\left\\{ Y(s) \\right\\}$ が固有の定常性を持つと言われる。 $$ \\begin{align*} E \\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right] =\u0026amp; 0 \\\\ \\text{Var} \\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right] =\u0026amp; 2 \\gamma ( \\mathbf{h} ) \\end{align*} $$ 固有の定常性 定義から見ると、バリオグラム $2 \\gamma ( \\mathbf{h} ) = E \\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right]^{2}$ は実際には $s$ にも依存する関数であるが、通常、与えられた空間過程が固有に定常的であると仮定される。逆に、固有の定常性自体が $s$ に依存しないという条件を持っているため、これら二つは切り離して考えることができない。\n弱定常性 弱定常性の定義で出てくる $C \\left( \\mathbf{h} \\right)$ を共分散関数と呼ぶことは自然であり、$\\gamma$ がなくても単独で定義され得るにも関わらず、特にコバリオグラムと呼ぶ理由は次のような関係があるためである。\n定理 弱定常的空間過程 $\\left\\{ Y (s) \\right\\}_{s \\in D}$ に対して、セミバリオグラム $\\gamma \\left( \\mathbf{h} \\right)$ とコバリオグラム $C \\left( \\mathbf{h} \\right)$ は次を満たす。 $$ \\text{Var} Y = \\gamma \\left( \\mathbf{h} \\right) + C \\left( \\mathbf{h} \\right) $$\n証明 空間過程 $\\left\\{ Y \\right\\}$ の弱定常性に従って、全ての $\\mathbf{h} \\in \\mathbb{R}^{r}$ に対して $\\text{Cov} \\left( Y (s) , Y \\left( s + \\mathbf{h} \\right) \\right) = C \\left( \\mathbf{h} \\right)$ で方向ベクトルにゼロベクトルを代入してみると次を得る。 $$ C \\left( \\mathbf{0} \\right) = \\text{Cov} \\left( Y (s) , Y (s) \\right) = \\text{Var} Y (s) $$\n定常性の包含関係: 強定常的空間過程は弱定常的空間過程であり、弱定常的空間過程は固有である。 $$ \\text{Strong} \\implies \\text{Weak} \\implies \\text{Intrinsic} $$\n一方、弱定常的空間過程が固有定常的であるため、全ての $\\mathbf{h} \\in \\mathbb{R}^{r}$ に対して $$ \\text{Var} \\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right] = 2 \\gamma ( \\mathbf{h} ) $$ が成り立つ。これを逆に解くと、 $$ \\begin{align*} \u0026amp; 2 \\gamma \\left( \\mathbf{h} \\right) \\\\ =\u0026amp; \\text{Var} \\left[ Y \\left( s + \\mathbf{h} \\right) - Y (s) \\right] \\\\ =\u0026amp; \\text{Var} \\left[ Y \\left( s + \\mathbf{h} \\right) \\right] + \\text{Var} \\left[ Y (s) \\right] - 2 \\text{Cov} \\left[ Y \\left( s + \\mathbf{h} \\right) , Y (s) \\right] \\\\ =\u0026amp; \\text{Cov} \\left[ Y \\left( s + \\mathbf{h} \\right) , Y \\left( s + \\mathbf{h} \\right) \\right] + \\text{Cov} \\left[ Y (s) , Y (s) \\right] - 2 \\text{Cov} \\left[ Y \\left( s + \\mathbf{h} \\right) , Y (s) \\right] \\\\ =\u0026amp; C ( \\mathbf{0} ) + C ( \\mathbf{0} ) - 2 C ( \\mathbf{h} ) \\\\ =\u0026amp; 2 \\left[ C ( \\mathbf{0} ) - C ( \\mathbf{h} ) \\right] \\\\ =\u0026amp; 2 \\left[ \\text{Var} Y - C ( \\mathbf{h} ) \\right] \\end{align*} $$ したがって、次の等式を得る。 $$ \\gamma \\left( \\mathbf{h} \\right) = \\text{Var} Y - C \\left( \\mathbf{h} \\right) $$\n■\n併せて見る 等方性バリオグラム: バリオグラムが方向に依存せず、距離のみに依存する場合、バリオグラムは等方性であると言われる。 セミバリオグラムのモデル: セミバリオグラムが等方性である場合、x軸を$d := \\left\\| \\mathbf{h} \\right\\|$としy軸を$\\gamma (h)$として描いた散布図を特定のモデルにフィッティングして、距離に対する分散がどのようになるか感覚を掴むことができる。このグラフィカルな検証から$2 \\gamma$と$C$がバリオ'グラム'と呼ばれている。 経験的バリオグラム$\\gamma^{\\ast}$: 実際のデータでは、$\\mathbf{h}$と正確に一致する観測値が多くない可能性がある。分析に先立って、データが一定の前提を満たしているか$\\gamma^{\\ast}$で見ることが勧められる。 Banerjee. (2015). Hierarchical Modeling and Analysis for Spatial Data(2nd Edition): p24.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2498,"permalink":"https://freshrimpsushi.github.io/jp/posts/2498/","tags":null,"title":"バリオグラムの定義"},{"categories":"줄리아","contents":"概要 ジュリアでもプログラムの進行状況を知らせてくれるグラスバーを手軽に使うことができる。\nコード ProgressMeter.jl 「ProgressMeter.jl」パッケージの「@showprogress」マクロを「for」ループに置けばよい1。\nusing ProgressMeter\rchi2 = []\r@showprogress for n in 1:20000\rpush!(chi2, sum(randn(n) .^ 2))\rend 下の「ProgressBars.jl」に比べるとマクロを使うのでコードがより簡潔である。\nProgressBars.jl 「ProgressBars.jl」パッケージの「ProgressBar()」関数で「for」ループの反復子Iteratorを包めば良い2。\nusing ProgressBars\rchi2 = []\rfor n in ProgressBar(1:20000)\rpush!(chi2, sum(randn(n) .^ 2))\rend 実際の作業内容はどうであれ構わないが、プログラムの進行状況は次のようにきれいに出力される。 当然だが、「for」ループ文で正確に何回目の繰り返しになっているかだけ分かるので、1回の繰り返し当たりの平均遂行時間を知らせるだけで、正確な所要時間を予測することはできない。\n環境 OS: Windows julia: v1.7.3 https://github.com/timholy/ProgressMeter.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/cloud-oak/ProgressBars.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2497,"permalink":"https://freshrimpsushi.github.io/jp/posts/2497/","tags":null,"title":"ジュリアでプログレスバーの使い方"},{"categories":"통계적분석","contents":"定義 1 Euclidean 空間の固定された部分集合 $D \\subset \\mathbb{R}^{r}$における確率変数 $Y(s) : \\Omega \\to \\mathbb{R}^{1}$の集合である空間過程 $\\left\\{ Y(s) \\right\\}_{s \\in D}$と方向ベクトル $\\mathbf{h} \\in \\mathbb{R}^{r}$を考えよう。具体的に $n \\in \\mathbb{N}$ 個のサイトを $\\left\\{ s_{1} , \\cdots , s_{n} \\right\\} \\subset D$のように表し、$Y(s)$ が全ての $s \\in D$に対して分散が存在すると仮定する。\n全ての $\\left\\{ s_{1} , \\cdots , s_{n} \\right\\}$ と全ての $\\mathbf{h}$ に対して以下の二つのランダムベクトルの分布が同じならば、$\\left\\{ Y(s) \\right\\}$ は 強い定常性を持つと言われる。 $$ \\left( Y \\left( s_{1} \\right) , \\cdots , Y \\left( s_{n} \\right) \\right) \\\\ \\left( Y \\left( s_{1} + \\mathbf{h} \\right) , \\cdots , Y \\left( s_{n} + \\mathbf{h} \\right) \\right) $$ 全ての $s \\in D$で $\\mu (s)$ が定数関数 $\\mu (s) := \\mu$ であり、かつ $s , s + \\mathbf{h}$ 両方が $D$ に属している場合、全ての $\\mathbf{h}$ に対する共分散がある関数 $C$ によって $s$ に無関係に $\\mathbf{h}$ のみの関数 $C : \\mathbb{R}^{r} \\to \\mathbb{R}$ として表されるとき、$\\left\\{ Y(s) \\right\\}$ は 弱い定常性を持つとされる。 $$ \\text{Cov} \\left( Y (s) , Y \\left( s + \\mathbf{h} \\right) \\right) = C \\left( \\mathbf{h} \\right) $$ ここで、$C$を共分散関数あるいはバリオグラムと呼ぶ。特に $\\left\\| \\mathbf{h} \\right\\| \\to \\infty$ の時 $C \\left( \\mathbf{h} \\right) \\to 0$ であれば、$\\left\\{ Y(s) \\right\\}$ は エルゴード的であると言う。 $\\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right]$ の平均が $0$ であり分散が唯一 $\\mathbf{h}$ に依存する場合、$\\left\\{ Y(s) \\right\\}$ は 内在的定常性を持つと言われる。 $$ \\begin{align*} E \\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right] =\u0026amp; 0 \\\\ \\text{Var} \\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right] =\u0026amp; 2 \\gamma ( \\mathbf{h} ) \\end{align*} $$ $2 \\gamma \\left( \\mathbf{h} \\right)$ をバリオグラムと呼ぶ。 定理 強い定常性を持つ空間過程は弱い定常性を持っており、弱い定常性過程は内在的である。 $$ \\text{Strong} \\implies \\text{Weak} \\implies \\text{Intrinsic} $$ また、全ての $\\left\\{ s_{1} , \\cdots , s_{n} \\right\\}$ に対するランダムベクトル $\\left( Y \\left( s_{1} \\right) , \\cdots , Y \\left( s_{n} \\right) \\right)$ が多変量正規分布に従う場合、$\\left\\{ Y(s) \\right\\}$ はガウス型である。弱い定常性空間過程が強い定常性になるための必要十分条件は、空間過程がガウス型であることである。 $$ \\text{Strong} \\overset{\\text{gaussian}}{\\impliedby} \\text{Weak} $$\n説明 定常性が必要な理由 時系列分析の定常性が様々なモデルの仮定になったのと同様に、空間過程の定常性も空間データの分析に先立って当然満たされなければならない性質を指す。定常性を仮定できない場合、多くの場合で分析自体が無意味になる。\n強い定常性は文字通り定常性そのものだ。問題は、理論的にこれが本当の定常性であるとしても、現実の中でその例を見つけるのが難しいかもしれないことであり、弱い定常性という緩やかな条件に退く必要がある。 弱い定常性は、全てのサイトでの分布は知らなくても、少なくとも平均が一定でその共分散が相対的な距離と方向 $\\mathbf{h}$ にのみ依存すればよいという妥協である。 内在的定常性の内在的という表現は統計学だけを勉強していた人には新しいかもしれないが、次の定義と似ている観点から、観測された二点間の差が単に $\\mathbf{h}$ にのみ依存するという点で'内在的'と呼ばれるに値する。 内在的関数の定義: 微分幾何学で、(単位法線 $\\mathbf{n}$には依存せずに) 第一基本形式の係数 $g_{ij}$にのみ依存する関数を内在的と呼ぶ。\nエルゴード性 エルゴード（Ergodic）の発音は韓国語で[얼가딕]に近いが、それはさておき。\n空間過程がエルゴード的であること、すなわち $$ \\lim_{\\left\\| \\mathbf{h} \\right\\| \\to \\infty} C \\left( \\mathbf{h} \\right) = 0 $$ とは、方向がどうであれ、二つのサイト間の距離が離れるにつれて、その相関関係が減少するという仮定である。これはかなり合理的な仮定である。全てのデータがエルゴード的なわけではないが、直感的には、$C \\left( \\mathbf{h} \\right)$が周期性を持つか、非常に特異な例でない限り、距離が離れるほど関係が弱くなることは一般的である。$C \\left( \\mathbf{h} \\right) \\searrow \\varepsilon$程度は、限界的な意味で期待しても良さそうだ。\n確率過程でのエルゴードは通常、時間 $t$ に依存するものと見なされ、長い時間が経った後（$t \\to \\infty$）に特定の状態が最初の状態に戻るという概念に似た形で接近している。同様に、空間過程では時間ではなく、遠くの距離（$\\left\\| \\mathbf{h} \\right\\| \\to \\infty$）で互いに相関関係が減少するという形で扱われている。もちろん、多くの分野でエルゴード性を時間と初期状態と関連づけて説明するのは事実だが、全く強引な命名ではない。\nBanerjee. (2003). Hierarchical Modeling and Analysis for Spatial Data: p23~24.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2496,"permalink":"https://freshrimpsushi.github.io/jp/posts/2496/","tags":null,"title":"空間過程の定常性"},{"categories":"줄리아","contents":"概要 R言語の合計sum()や平均mean()には、関数自体がその欠損値を無視するオプションを持っているが、ジュリアではそのようなオプションがない代わりに関数型プログラミングFunctional Programming的な方法を積極的に使っている。\nコード julia\u0026gt; data = [0,1,2,3,0]\r5-element Vector{Int64}:\r0\r1\r2\r3\r0\rjulia\u0026gt; sum(data) / length(data)\r1.2\rjulia\u0026gt; sum(data) / sum(!iszero, data)\r2.0 上は$0$まで含んで全ての標本数で割った1.2、下は!iszeroという関数を引数に与えて$0$でない値だけをカウントして得た標本数で割った2.0を得た。Rよりも強力だと言える部分は、関数自体のオプションに頼らずにisnan(), isinf(), ismissing()などの多数の例外処理関数を同じ方法で使用でき、カスタムが自由であることだ。\nパフォーマンスに大きな違いはないが、is~系の関数のリターンが必ずブーリアンであるなら、分母のsum()がcount()に変わっても問題ない。\n環境 OS: Windows julia: v1.7.0 ","id":2495,"permalink":"https://freshrimpsushi.github.io/jp/posts/2495/","tags":null,"title":"ジュリアで0または欠損値を除外した平均値の計算方法"},{"categories":"통계적분석","contents":"定義 1 特に$r \u0026gt; 1$の時、ユークリッド空間の固定された部分集合$D \\in \\mathbb{R}^{r}$に対して、以下の$p$-変量ランダムベクトルの集合$Y(s) : \\Omega \\to \\mathbb{R}^{p}$を空間過程とも呼ぶ。 $$ \\left\\{ Y(s) : s \\in D \\right\\} $$ 特に空間過程が有限集合で、次のようにベクトルで表現される時は、ランダムフィールドと呼ぶ。 $$ \\left( Y \\left( s_{1} \\right) , \\cdots , Y \\left( s_{n} \\right) \\right) $$\n説明 特に空間データの中でポイント参照データを扱う時には、$Y(s)$は$s$に対して連続的にサンプリングできると仮定するが、実際に得られる実現の$D = \\left\\{ s_{1} , \\cdots , s_{n} \\right\\}$は有限集合だろう。\n大学の確率過程論の授業では、通常$r = p = 1$と$[ 0 , \\infty ) \\subset \\mathbb{R}$に関する次のような確率過程だけを学ぶ。 $$ \\left\\{ Y_{t} : t \\in [ 0 , \\infty ) \\right\\} $$ こうして時系列データに対する背景のような感じで確率過程に触れてきたなら、空間過程の定義は多少戸惑うかもしれない。しかし、一般的な確率過程の定義はただ「ランダムエレメントの集合」で十分なので、$\\left\\{ Y(s) \\right\\} _{s \\in D}$を確率過程と呼ばない理由はない。\n空間過程を時間過程の一般化と呼ぶよりは、最初から彼らは区別されていなかったと言った方が正しい。よく分からなければ、時系列を扱う時の時間だけの1次元軸$\\mathbb{R}^{1}$もちゃんとしたユークリッド空間だと思い出すといい。よく考えてみれば、時間$t \\in \\mathbb{R}$の流れに沿う確率'過程'自体も、日常用語とそんなに通じなかったから、空間'過程'という表現にあまり違和感を持たなくてもいい。\nBanerjee. (2003). Hierarchical Modeling and Analysis for Spatial Data: p23.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2494,"permalink":"https://freshrimpsushi.github.io/jp/posts/2494/","tags":null,"title":"空間プロセス"},{"categories":"전자기학","contents":"질문 電磁気学は文字通り電場 $\\mathbf{E}$と磁場 $\\mathbf{B}$について学ぶ学問です。電磁気学を学ぶ中で一度は次のような疑問を持ったことでしょう。\nなぜ磁場の記号として$\\mathbf{B}$を使用するのか？\n電場が$\\mathbf{E}$であるのはElectric fieldから来ているからとしても、磁場はMagnetic fieldなのになぜ$\\mathbf{B}$なのでしょうか？一見不自然に感じる記号ですが、これは実際に大きな理由なく定められたものです。\r回答 マックスウェルの記法1 2 マックスウェルはマックスウェル方程式を通じて古典電磁気学3を完成させ、「電磁気学の父」と呼ばれています。ニュートン、ライプニッツ、オイラーなど、数学/科学で顕著な成果を上げた人々は、名前と業績だけでなく、彼らの記法までもが後世に残ることになります。これはマックスウェルにも当てはまり、磁場を$\\mathbf{B}$、補助場を$\\mathbf{H}$と記すことも、マックスウェルがそう記したために自然と続いていると考えられています。\nマックスウェルが電磁気学で登場する様々なベクトルの記号に使用した文字4は以下の通りです。5 マックスウェルはこれらのベクトルをAからJまでのアルファベットで表記しましたが、C, D, Fなど記号に相応しいものがある場合はそれを用い、残りはマックスウェルの裁量で定められたようです。\n記法\r意味\rマックスウェル\r現在\r$\\frak{A}$$(A)$\r$\\mathbf{A}$\rその点の電磁気モーメンタム\r現在ではベクトルポテンシャルと呼ばれている。\r$\\frak{B}$$(B)$\r$\\mathbf{B}$\r磁気誘導\r現在では磁場と呼ばれている。\r$\\frak{C}$$(C)$\r$I$\r（全）電流、電流\r$\\frak{D}$$(D)$\r$\\mathbf{D}$\r電気'D'isplacement、変位場\r$\\frak{E}$$(E)$\r$\\mathcal{E}$\r電気'E'motive intensity\r現在では起電力electromotive force, emfと言われている。\r$\\frak{F}$$(F)$\r$\\mathbf{F}$\r機械的'F'orce\r現在ではローレンツ力と呼ばれている。\r$\\frak{G}$$(G)$\r点の速度\r$\\frak{H}$$(H)$\r$\\mathbf{H}$\r磁気力\r現在ではH-フィールドH-field、補助場auxiliary field、磁場強度magnetic field intensityなどと呼ばれている。\n$\\frak{I}$$(I)$\r$\\mathbf{M}$\r磁化'I'ntensity\r現在では磁化密度と呼ばれる物理量のようだ。\r$\\frak{J}$$(J)$\r$\\mathbf{J}$\r導電性の電流、導電電流\rほとんどの記号は今もそのまま使用されており、電流は現在、currentのintensityの頭文字を取って$I$と表記されています。\nまた、この関連で検索すると'ビオ・サバールの法則ではBiotの名前から取った'という主張も見つかりますが、私の意見ではそうではありません。まず'磁場の記号はなぜ$\\mathbf{B}$なのか?'という質問は'現在、磁場の記号としてなぜ$\\mathbf{B}$を使用するのか?'という質問と同じであり、これに対する答えは'マックスウェルがそう使用したから'が妥当だと思います。それならば、'マックスウェルが磁場の記号として$\\mathbf{B}$を使用したのはBiotの名前から取ったからではないか?'と考えることもできます。しかし、この回答には明確な根拠があるわけではないようです。もしそうだとしてもBiotの名前から$\\mathbf{B}$であるというよりは、'上記のベクトルの中で記号$\\mathbf{B}$と最も適合するのは磁気誘導、つまりビオ・サバールの法則と関連があるものだ'という説明の方が適切ではないでしょうか？（正直、Biot、bi-polar field、borealから取ったというのは無理矢理感があると思います）\nBとHのうち磁場はどちら？ 一方で$\\mathbf{B}$と$\\mathbf{H}$のうち、どちらを磁場magnetic fieldと呼ぶべきかについての議論もあります。$\\mathbf{H}$は媒質に関係なく実験で制御可能な値です。そのため、一般に工学関連の分野（例えば電気技師の教科書）では$\\mathbf{H}$を磁場と呼び、物理学関連の分野では$\\mathbf{B}$を磁場と呼ぶことが一般的です。しかし、ウィキの磁場の記事でも説明されているように、ローレンツ力を媒介するのが$\\mathbf{B}$であるため、$\\mathbf{E}$を電場と呼ぶのと同様に$\\mathbf{B}$を磁場と呼ぶのが一貫性があり、妥当だと考えられます。\n$$ \\text{Lorentz force}: \\mathbf{F}=Q\\left[ \\mathbf{E} + (\\mathbf{v}\\times\\mathbf{B}) \\right] $$\nhttps://www.johndcook.com/blog/2012/02/12/why-magnetic-field-b/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.cantorsparadise.com/why-the-symbol-for-magnetic-field-is-b-e40658e17ece\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n量子力学的現象を考慮しない\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nフラクトゥールFraktur書体である。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMaxwell, James Clerk. A treatise on electricity and magnetism. Vol. 2. Oxford: Clarendon Press, 1873. page 257\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3523,"permalink":"https://freshrimpsushi.github.io/jp/posts/3523/","tags":null,"title":"磁場の記号にBを使う理由"},{"categories":"줄리아","contents":"概要 Juliaの回帰分析を行うためのGLM.jlパッケージを簡単に紹介する1。この説明では、Rのインターフェースとどれくらい似ているかを強調するため、詳細な説明は省略する。\nコード ジュリア using GLM, RDatasets\rfaithful = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;faithful\u0026#34;)\rout1 = lm(@formula(Waiting ~ Eruptions), faithful) 上記のコードを実行した結果は以下の通りである。\njulia\u0026gt; out1 = lm(@formula(Waiting ~ Eruptions), faithful)\rStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}}}}, Matrix{Float64}}\rWaiting ~ 1 + Eruptions\rCoefficients:\r───────────────────────────────────────────────────────────────────────\rCoef. Std. Error t Pr(\u0026gt;|t|) Lower 95% Upper 95%\r───────────────────────────────────────────────────────────────────────\r(Intercept) 33.4744 1.15487 28.99 \u0026lt;1e-84 31.2007 35.7481\rEruptions 10.7296 0.314753 34.09 \u0026lt;1e-99 10.11 11.3493\r─────────────────────────────────────────────────────────────────────── Rでの回帰分析の結果と比較してみてください。\nRとの比較 out1\u0026lt;-lm(waiting~eruptions,data=faithful); summary(out1) out1 = lm(@formula(Waiting ~ Eruptions), faithful) 上はRのコードで、下はJuliaのコードである。変数を入力するために@formulaマクロを使用し、Rの慣習をほぼ完璧に再現できていることがわかる。\n環境 OS: Windows julia: v1.7.0 GLM v1.8.0 一緒に見る Rで回帰分析を行う方法 https://juliastats.org/GLM.jl/v0.11/index.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2493,"permalink":"https://freshrimpsushi.github.io/jp/posts/2493/","tags":null,"title":"ジュリアで回帰分析を行う方法"},{"categories":"통계적분석","contents":"説明 1 空間データSpatial Dataとは、文字通り空間に関する情報を含むデータであり、空間統計学Spatial Statisticsは、実際の辞書の意味での「空間」としてユークリッド空間 $\\mathbb{R}^{r}$ を分析する統計学の一分野だ。時系列分析が時間軸 $t$ に沿って変化するデータを分析するのに対し、空間データ分析は与えられた $D \\subset \\mathbb{R}^{r}$、(通常は $r = 2$ の時) 位置に応じて変化するデータを分析する。\n考えてみれば、データを説明する軸が $r \u0026gt; 1$ 個に増えるだけで時系列データと比べてその種類は多様だ。空間データは根本的に次の三つの主要なタイプに分類することができる。\nポイント参照データ ポイント参照データPoint-referenced Dataは、固定された $D \\subset \\mathbb{R}^{r}$ の位置で点 $s \\in Y$ が連続的に変化すると仮定し、座標が与えられたほとんどのデータをランダムベクトル $Y(s)$ として表現することができる。上の例では、PM2.5監視所で測定された濃度を座標に応じて地図に表示している。\nポイント参照データは 地質統計データGeostatistical Dataとも呼ばれる。\nエリアデータ エリアデータAreal Dataはポイント参照データと同様に $D \\subset \\mathbb{R}^{r}$ は固定されているが、その内部で有限のパーティションに分けられる点が異なる。ポイント参照データは万能のように見えるかもしれないが、市、郡、区、町、地区など、座標ではなく人間社会によって分けられた行政区画を表現する。上の例では、座標ではなく不規則な形irregular shapeで分割されたブロックごとに貧困度を表示している。\nエリアデータは、$D$ のパーティションが規則的な形状、つまり例と異なり、きちんと均一に切られている場合、ラティスLatticeデータとも呼ばれる。\nポイントパターンデータ ポイントパターンデータPoint Pattern Dataは、前の二つとは異なり、$D \\subset \\mathbb{R}^{r}$ 自体がランダムなデータを指す。特に全ての $s \\in D$ において $Y(s) = 1$ の時、ポイントパターンデータは各位置で事故が起こった事実のみを伝えることになる。上の例はサバイバーシップバイアスを説明する際によく引用される図であり、第二次世界大戦時に帰還したアメリカの戦闘機のどの部分が損傷したかを示している。2 この場合、被弾部位はPM2.5監視所や行政区画のように決まっているわけではなく、その位置自体が変わり、損傷がポイントパターンデータのイベントだ。\nBanerjee. (2003). Hierarchical Modeling and Analysis for Spatial Data: p16~18.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.andrewahn.co/silicon-valley/survivorship-bias/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2492,"permalink":"https://freshrimpsushi.github.io/jp/posts/2492/","tags":null,"title":"空間データ分析とは？"},{"categories":"머신러닝","contents":"概要 モンテカルロ積分は、与えられた関数の積分を計算するのが困難な場合に使用される数値的近似方法の一つである。次のような状況を想定しよう。与えられた $[0, 1]$または一般的に $[0, 1]^{n}$で積分可能な関数 $f$に対して、私たちは $f(x)$の式を知っているが、その積分を計算するのは簡単ではない。しかし、私たちは $f$の積分 $I[f]$を計算したい。\n$$ \\begin{equation} I[f] = \\int_{[0,1]} f(x) dx \\end{equation} $$\n定義 モンテカルロ積分Monte Carlo integrationとは、与えられた $[0, 1]$ 上での分布に基づきサンプル $\\left\\{ x_{i} \\right\\}$を抽出し、$f$の積分を次のように推定estimateする方法である。\n$$ I[f] \\approx I_{n}[f] := \\dfrac{1}{n}\\sum_{i=1}^{n} f(x_{i}) $$\n区分求積法との違い 区分求積法のアイデアは、区間 $[0,1]$を $n$等分し、点 $\\left\\{ x_{i} = \\frac{i-1}{n} \\right\\}_{i=1}^{n}$を得て、これらの点での関数値を全て加算することである。\n$$ \\text{区分求積法}[f] = \\dfrac{1}{n}\\sum_{i=1}^{n} f(x_{i}) $$\n式の見た目だけではモンテカルロ積分と区分求積法は異なるもののないように見えるが、その意味は全く異なる。区分求積法での $\\left\\{ x_{i} \\right\\}$は区間 $[0, 1]$を $n$等分して得た点であるのに対し、モンテカルロ積分では $x$が従う分布 $p(x)$から抽出された $n$個のサンプルを意味する。したがって、区分求積法で得られた値は単純に $f$が描くグラフの下の面積を意味するが、モンテカルロ積分で得られた値は $f$の期待値である。\n性質 式 $(1)$が持つ統計的な意味は「$I[f]$は $X$が一様分布に従うときの $f(X)$の期待値と同じである」ということである。\n$$ X \\sim U(0,1) \\implies I[f] = \\int_{[0,1]} f(x) dx = E\\left[ f(X) \\right] $$\n期待値 確率変数 $X$が一様分布に従うとしよう。$I_{n}[f]$は $I[f]$の不偏推定量である。\n$$ E\\left[ I_{n}[f] \\right] = I[f] $$\n証明 $$ \\begin{align*} E\\left[ I_{n}[f] \\right] \u0026amp;= E\\left[ \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} f(X_{i}) \\right] \\\\ \u0026amp;= \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} E\\left[ f(X_{i}) \\right] \\qquad \\text{by linearity of $E$} \\\\ \u0026amp;= \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} I\\left[ f \\right] \\\\ \u0026amp;= I\\left[ f \\right] \\end{align*} $$\n■\n分散 証明 分散の性質\n[a] $\\Var (aX) = a^{2} \\Var (X)$\n[b] $X, Y$が独立ならば、$\\Var (X + Y) = \\Var(X) + \\Var(Y)$\n$f(X)$の分散を $\\sigma^{2}$としよう。すると分散の性質により、\n$$ \\begin{align*} \\Var \\left[ I_{n}[f] \\right] \u0026amp;= \\Var \\left[ \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} f(X_{i}) \\right] \\\\ \u0026amp;= \\dfrac{1}{n^{2}} \\Var \\left[ \\sum\\limits_{i=1}^{n} f(X_{i}) \\right] \\\\ \u0026amp;= \\dfrac{1}{n^{2}} \\sum\\limits_{i=1}^{n} \\Var \\left[ f(X_{i}) \\right] \\\\ \u0026amp;= \\dfrac{1}{n^{2}} \\sum\\limits_{i=1}^{n} \\sigma^{2} \\\\ \u0026amp;= \\dfrac{\\sigma^{2}}{n} \\end{align*} $$\n■\n一般化 ここで $p(x) \\ge 0$で $\\int_{[0,1]} p = 1$となる関数 $p$について、積分 $I[fp]$を考えよう。\n$$ I[fp] = \\int_{[0, 1]}f(x)p(x) dx $$\nこれは確率密度関数が $p$である確率変数 $X$について、$f(X)$の期待値と同じである。この値を近似する方法として、次の二つの方法が考えられる。\nサンプル $\\left\\{ x_{i} \\right\\}_{i=1}^{n}$を一様分布から抽出し、$I[fp]$を次のように近似する。 $$ X_{i} \\sim U(0,1) \\qquad I[fp] \\approx \\dfrac{1}{n}\\sum\\limits_{i}f(x_{i})p(x_{i}) $$\nサンプル $\\left\\{ x_{i} \\right\\}_{i=1}^{n}$を $p(x)$から抽出し、$I[fp]$を次のように近似する。 $$ X_{i} \\sim p(x) \\qquad I[fp] = I_{p}[f] \\approx \\dfrac{1}{n}\\sum\\limits_{i}f(x_{i}) $$\n言い換えれば、1.は $f(x)p(x)$を一様分布でサンプリングして平均を求めたものであり、2.は $f(x)$を $p(x)$でサンプリングして平均を求めたものである。これらのうち分散がより小さいのは1.である。$I = I[fp] = I[fp]$と簡単に記しよう。\n1.の場合 $$ \\begin{align*} \\sigma_{1}^{2} = \\Var [fp] \u0026amp;= E \\left[ (fp - I)^{2} \\right] \\\\ \u0026amp;= \\int (fp - I)^{2} dx \\\\ \u0026amp;= \\int (fp)^{2} dx - 2I\\int fp dx + I^{2}\\int dx\\\\ \u0026amp;= \\int (fp)^{2} dx - 2I^{2} + I^{2}\\\\ \u0026amp;= \\int (fp)^{2} dx - I^{2}\\\\ \\end{align*} $$\n2.の場合 $$ \\begin{align*} \\sigma_{2}^{2} = \\Var [f] \u0026amp;= E_{p} \\left[ (f - I)^{2} \\right] \\\\ \u0026amp;= \\int (f - I)^{2}p dx \\\\ \u0026amp;= \\int f^{2}p dx - 2I\\int fp dx + I^{2}\\int pdx\\\\ \u0026amp;= \\int f^{2}p dx - 2I^{2} + I^{2}\\\\ \u0026amp;= \\int f^{2}p dx - I^{2}\\\\ \\end{align*} $$\nしかし $0 \\le p \\le 1$であるため、$f^{2}p \\ge f^{2}p^{2}$である。したがって\n$$ \\sigma_{1}^{2} \\le \\sigma_{2}^{2} $$\n","id":3515,"permalink":"https://freshrimpsushi.github.io/jp/posts/3515/","tags":null,"title":"モンテカルロ積分"},{"categories":"선형대수","contents":"定義1 二つのベクトル$\\mathbf{x}, \\mathbf{u} \\in \\mathbb{R}^{n}$が次のようであるとする。\n$$ \\mathbf{x}=\\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n} \\end{bmatrix},\\quad \\mathbf{u}^{T} = \\begin{bmatrix} u_{1} \u0026amp; u_{2} \u0026amp; \\cdots \u0026amp; u_{n} \\end{bmatrix} $$\n実数である定数$a_{ij} \\in \\mathbb{R} (1\\le i,j \\le n)$に対して、次のように定義される関数$A : \\mathbb{R}^{n} \\times \\mathbb{R}^{n} \\to \\mathbb{R}$を二重線形形式bilinear formという。\n$$ A(\\mathbf{u},\\mathbf{x}):=\\sum \\limits_{i,k=1}^{n} a_{ik}u_{i}x_{k} $$\n二重線形形式で、定数$a_{ij} (1\\le i,j \\le n)$が複素数であり、$a_{ij}=\\overline{a_{ji}}$を満たす場合、エルミート形式Hermite formという。\n$$ A(\\mathbf{u},\\mathbf{x})=\\sum \\limits _{i,k=1} ^{n} a_{ik}u_{i}x_{k} = \\mathbf{u}^{\\ast} A \\mathbf{x} $$\n説明 端的に言えば、二重線形形式の文脈でエルミート行列とは、行列$A$がエルミート行列である場合である。\n定数の行列を次のように記す。\n$$ \\quad A=\\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp;\\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp;\\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{bmatrix} $$\nすると、二重線形形式は行列の積として表され、これを行列$A$に対応する二重線形形式ともいう。\n$$ A(\\mathbf{u},\\mathbf{x})=\\sum \\limits_{i,k=1}^{n} a_{ik}u_{i}x_{k}= \\mathbf{u}^{T}A\\mathbf{x} $$\nもし一次連立方程式が\n$$ \\begin{cases} a_{11}x_{1}+a_{12}x_{2}+\\cdots +a_{1n}x_{n}\u0026amp;=y_{1} \\\\ a_{21}x_{1}+a_{22}x_{2}+\\cdots +a_{2n}x_{n}\u0026amp;=y_{2} \\\\ \u0026amp;\\vdots \\\\ a_{n1}x_{1}+a_{n2}x_{2}+\\cdots +a_{nn}x_{n}\u0026amp;=y_{n} \\end{cases} $$\nとして与えられた場合、各方程式に$u_{i}$を掛けてすべて足し合わせることで、下記のような二重線形形式を得ることができる。$I$は単位行列である。\n$$ A(\\mathbf{u},\\mathbf{x})=\\sum \\limits_{i,k=1}^{n} a_{ik}u_{i}x_{k}=\\sum \\limits_{i=1}^{n}u_{i}y_{i}=I(\\mathbf{u}, \\mathbf{y}) $$\n二次形式は、二重線形形式で$\\mathbf{u} = \\mathbf{x}$である特別な場合である。\n参照 一次形式 二次形式 二重線形形式 エルミート形式 Howard Anton, Elementary Linear Algebra: Aplications Version (12th Edition, 2019), p416-417\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3513,"permalink":"https://freshrimpsushi.github.io/jp/posts/3513/","tags":null,"title":"双線形形式とエルミート形式"},{"categories":"선형대수","contents":"定義 $V$を$n$次元のベクトル空間と呼ぶ。与えられた定数$a_{ij} \\in \\mathbb{R}(\\text{or } \\mathbb{C})$について、以下の2次斉次関数$A : V \\to \\mathbb{R}(\\text{or } \\mathbb{C})$を二次形式という。\n$$ A(\\mathbf{x}) := \\sum\\limits_{i,j=1}^{n} a_{ij}x_{i}x_{j},\\qquad (a_{ij} = a_{ji}) $$\nこの時、$\\mathbf{x} = \\begin{bmatrix} x_{1} \u0026amp; \\cdots \u0026amp; x_{n} \\end{bmatrix}^{T}$である。$i \\ne j$に対する$a_{ij}x_{i}x_{j}$を混合項という。\n説明 定義によると、$A(\\lambda \\mathbf{x}) = \\lambda^{2} A(\\mathbf{x})$が成り立つ。\n行列形 $A$を$n\\times n$対称行列$A = \\begin{bmatrix} a_{ij} \\end{bmatrix}$とする。行列$A$に対する二次形式を、Quadraticの頭文字を取って、$Q_{A}(\\mathbf{x})$と表記し、$A$に関連する二次形式と呼ぶ。\n$$ Q_{A}(\\mathbf{x}) = \\mathbf{x}^{T}A\\mathbf{x} =\\begin{bmatrix} x_{1} \u0026amp; \\cdots \u0026amp;x_{n} \\end{bmatrix} \\begin{bmatrix} a_{11} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{n1} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ \\vdots \\\\ x_{n} \\end{bmatrix}=\\sum \\limits _{i=1} ^{n}\\sum \\limits _{j=1} ^{n}a_{ij}x_{i}x_{j} $$\n例えば、$\\mathbb{R}^{2}$上の二次形式は以下の通り。\n$$ \\begin{align*} \u0026amp; a_{11}^{\\ }x_{1}^{2} + a_{22}^{\\ }x_{2}^{2} + a_{12}^{\\ }x_{1}^{\\ }x_{2}^{\\ } + a_{21}^{\\ }x_{2}^{\\ }x_{1}^{\\ } \\\\ =\u0026amp;\\ a_{11}^{\\ }x_{1}^{2} + a_{22}^{\\ }x_{2}^{2} + 2a_{12}^{\\ }x_{1}^{\\ }x_{2}^{\\ } \\end{align*} $$\n$\\mathbb{R}^{3}$上の二次形式は以下の通り。\n$$ a_{11}^{\\ }x_{1}^{2} + a_{22}^{\\ }x_{2}^{2} + a_{33}^{\\ }x_{3}^{2} + 2a_{12}^{\\ }x_{1}^{\\ }x_{2}^{\\ } + 2a_{13}^{\\ }x_{1}^{\\ }x_{3}^{\\ } + 2a_{23}^{\\ }x_{2}^{\\ }x_{3}^{\\ } $$\n繰り返しを避けるために、混合項を上記のように組み合わせて記述することが一般的である。二次形式は行列の内積の性質によって以下のように表すことができる。実数、複素数に対してそれぞれ以下のようだ。\n$$ \\begin{align*} Q_{A}(\\mathbf{x}) \u0026amp;= \\mathbf{x}^{T} A \\mathbf{x} = \\mathbf{x} \\cdot A\\mathbf{x} = A\\mathbf{x} \\cdot \\mathbf{x} = \\left\u0026lt; A\\mathbf{x}, \\mathbf{x}\\right\u0026gt; = \\left\u0026lt; \\mathbf{x}, A \\mathbf{x} \\right\u0026gt; \\\\ Q_{A}(\\mathbf{x}) \u0026amp;= \\mathbf{x}^{\\ast} A \\mathbf{x} = \\mathbf{x} \\cdot A\\mathbf{x} = A\\mathbf{x} \\cdot \\mathbf{x} = \\left\u0026lt; A\\mathbf{x}, \\mathbf{x}\\right\u0026gt; = \\left\u0026lt; \\mathbf{x}, A \\mathbf{x} \\right\u0026gt; \\end{align*} $$\n$A$が対角行列の場合、$a_{ij}=0 (i \\ne j)$が成り立つため、二次形式$Q_{A}(\\mathbf{x})$は混合項を持たない。\n$$ Q_{A}(\\mathbf{x}) = \\mathbf{x}^{T}A\\mathbf{x} =\\begin{bmatrix} x_{1} \u0026amp; \\cdots \u0026amp;x_{n} \\end{bmatrix} \\begin{bmatrix} a_{11} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; a_{nn} \\end{bmatrix}\\begin{bmatrix} x_{1} \\\\ \\vdots \\\\ x_{n} \\end{bmatrix}=\\sum \\limits _{i=1}^{n} a_{ii}x_{i}^{2} $$\n関連項目 一次形式 二次形式 双線形形式 エルミート形式 ","id":3512,"permalink":"https://freshrimpsushi.github.io/jp/posts/3512/","tags":null,"title":"二次形式"},{"categories":"확률분포론","contents":"定義 $n \\in \\mathbb{N}$ とカテゴリ $k \\in \\mathbb{N}$ の数の確率変数で構成されるランダムベクトルを $\\left( X_{1} , \\cdots , X_{k} \\right)$ と示そう。 $$ \\sum_{i=1}^{k} X_{i} = n \\qquad \\\u0026amp; \\qquad \\sum_{i=1}^{k} p_{i} = 1 $$ これを満たす $\\mathbf{p} = \\left( p_{1} , \\cdots , p_{k} \\right) \\in [0,1]^{k}$ について、以下の確率質量関数を持つ多変量確率分布 $M_{k} \\left( n, \\mathbf{p} \\right)$ を多項分布Multinomial Distributionと呼ぶ。 $$ p \\left( x_{1} , \\cdots , x_{k} \\right) = {{ n! } \\over { x_{1} ! \\cdots x_{k}! }} p_{1}^{x_{1}} \\cdots p_{k}^{x_{k}} \\qquad , x_{1} , \\cdots , x_{k} \\in \\mathbb{N}_{0} $$\n$[0,1]^{k} = [0,1] \\times \\cdots \\times [0,1]$ は$k$-セルだ。 $\\mathbb{N}_{0} = \\left\\{ 0 \\right\\} \\cup \\mathbb{N}$ は自然数と $0$ を含む集合だ。 説明 定義をそのまま解釈すると、$\\left( X_{1} , \\cdots , X_{k} \\right)$ は確率質量関数 $$ \\begin{align*} p \\left( x_{1} , \\cdots , x_{k} \\right) =\u0026amp; P \\left( X_{1} = x_{1} , \\cdots , X_{k} = x_{k} \\right) \\\\ =\u0026amp; {{ n! } \\over { x_{1} ! \\cdots x_{k}! }} p_{1}^{x_{1}} \\cdots p_{k}^{x_{k}} \\end{align*} $$ を持つので、$n$ 個の要素がそれぞれ $k$ 個のカテゴリーの中で $i$ 番目のカテゴリーに入る確率が $p_{i}$ の時、実際に各自のカテゴリーにどれだけの要素があるかを示すランダムベクトルだ。特に $k = 2$ の時は、二項分布の一般化そのものになる。\n基本特性 平均と共分散 [1]: $\\mathbf{X} := \\left( X_{1} , \\cdots , X_{k} \\right) \\sim M_{k} \\left( n, \\mathbf{p} \\right)$ の場合、$i$番目の成分 $X_{i}$ の期待値は $$ E \\left( X_{i} \\right) = n p_{i} $$ で、共分散行列は以下のようになる。 $$ \\text{Cov} \\left( \\mathbf{X} \\right) = n \\begin{bmatrix} p_{1} \\left( 1 - p_{1} \\right) \u0026amp; - p_{1} p_{2} \u0026amp; \\cdots \u0026amp; - p_{1} p_{k} \\\\ - p_{2} p_{1} \u0026amp; p_{2} \\left( 1 - p_{2} \\right) \u0026amp; \\cdots \u0026amp; - p_{2} p_{2} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ - p_{k} p_{1} \u0026amp; - p_{k} p_{2} \u0026amp; \\cdots \u0026amp; p_{k} \\left( 1 - p_{k} \\right) \\end{bmatrix} $$ 定理 バンドリングの性質 $i \\ne j$ に対して、$X_{i} + X_{j}$ は二項分布 $\\text{Bin} \\left( n , p_{i} + p_{j} \\right)$ に従う。 $$ X_{i} + X_{j} \\sim \\text{Bin} \\left( n , p_{i} + p_{j} \\right) $$ これをバンドリングの性質Lumping Propertyと呼ぶ。\n証明 平均 各成分 $X_{i}$ だけを見れば、結局 $p_{i}$ の確率でカテゴリ $i$ に入るか入らないかの二項分布なので、$X_{i} \\sim \\text{Bin} \\left( n , p_{i} \\right)$ で、その期待値は $E \\left( X_{i} \\right) = n p_{i}$ である。\n■\n共分散 バンドリングの性質を使って直接導出する。\n■\nバンドリングの性質 1 $n = 1$ の場合、つまり一回の試行をした時、$X_{i} + X_{j}$ はその試行の結果が $i$ 番目か $j$ 番目のカテゴリーに属する時正確に $1$ であり、他の場合は $0$ のベルヌーイ分布 $\\text{Bin} \\left( 1, p_{i} + p_{j} \\right)$ に従う。\n二項分布の加算: 確率変数 $X_{1} , \\cdots , X_{n}$ が相互独立とする。二項分布の場合、$X_i \\sim \\text{Bin} ( n_{i}, p)$ が真なら $$ \\displaystyle \\sum_{i=1}^{m} X_{i} \\sim \\text{Bin} \\left( \\sum_{i=1}^{m} n_{i} , p \\right) $$\n一方で $n$ 回の試行はそれぞれ独立に行われるので、二項分布の加算に従って、次を得る。 $$ X_{i} + X_{j} \\sim \\text{Bin} \\left( \\sum_{j=1}^{n} 1 , p_{i} + p_{j} \\right) = \\text{Bin} \\left( n , p_{i} + p_{j} \\right) $$\n■\nhttps://math.stackexchange.com/a/1678138/459895\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2480,"permalink":"https://freshrimpsushi.github.io/jp/posts/2480/","tags":null,"title":"多項分布"},{"categories":"확률분포론","contents":"ビルドアップ 変形ベッセル関数 $$ J_{\\nu}(x) = \\sum \\limits_{n=0}^{\\infty} \\frac{(-1)^{n} }{\\Gamma (n+1) \\Gamma (n+\\nu+1)} \\left(\\frac{x}{2} \\right)^{2n+\\nu} $$\n第一種ベッセル関数 $J_{\\nu}$ に対して次のように定義される $I_{\\nu}$ を変形第一種ベッセル関数という1。\n$$ \\begin{align*} I_{\\nu} (z) :=\u0026amp; i^{-\\nu} J_{\\nu} \\left( iz \\right) \\\\ =\u0026amp; \\left( {{ z } \\over { 2 }} \\right)^{\\nu} \\sum_{k=0}^{\\infty} {{ {{ z } \\over { 2 }}^{2k} } \\over { k! \\Gamma \\left( \\nu + k + 1 \\right) }} \\\\ =\u0026amp; {{ \\left( {{ z } \\over { 2 }} \\right)^{\\nu} } \\over { \\sqrt{\\pi} \\Gamma \\left( \\nu + {{ 1 } \\over { 2 }} \\right) }} \\int_{-1}^{1} e^{zt} \\left( 1 - t^{2} \\right)^{\\nu - {{ 1 } \\over { 2 }}} dt \\end{align*} $$\n方向統計 一方、方向統計学Statisticalとは、通常のユークリッド空間ではなく、ある多様体上の確率分布や統計的推測などを研究する分野だ。たとえば地球のような球体を代表するスフィアや、$2 \\pi$-モジュラスで表されるトーラスなどに置かれたデータを扱うが、すぐに空間情報統計学(球面)や分⾚間の角度(トーラス)に応用できるなど、その未来は明るい部門だ。だが、ここに登場する分布は、次のような奇妙な確率密度関数を持っている。 $$ f_{p} \\left( \\mathbf{x} ; \\mu , \\kappa \\right) := \\left( {{ \\kappa } \\over { 2 }} \\right)^{p/2 - 1} {{ 1 } \\over { \\Gamma \\left( p/2 \\right) I_{p/2 - 1} \\left( \\kappa \\right) }} \\exp \\left( \\kappa \\mu^{T} \\mathbf{x} \\right) \\qquad , \\mathbf{x} \\in S^{p-1} $$ ここで最初の複雑な因子は、修正第一種ベッセル関数 $I_{\\nu}$ を含むが、$\\int_{S^{p-1}} f d \\mathbf{x} = 1$ になるように定数で正規化Normalizeしてくれるものだ。このように複雑な関数が使用される理由は単純だ。\n解答 第一種ベッセル関数の導出: $\\nu \\in \\mathbb{R}$ に対して、下のような形の微分方程式を$\\nu$次ベッセル方程式 という。 $$ \\begin{align*} \u0026amp;\u0026amp; x^{2} y^{\\prime \\prime} +xy^{\\prime}+(x^{2}-\\nu^{2})y \u0026amp;= 0 \\\\ \\text{or} \u0026amp;\u0026amp; y^{\\prime \\prime}+\\frac{1}{x} y^{\\prime} + \\left( 1-\\frac{\\nu^{2}}{x^{2}} \\right)y \u0026amp;= 0 \\end{align*} $$ ベッセル方程式は、球面座標系で波動方程式を解く時に登場する微分方程式だ。 係数は定数ではなく、独立変数 $x$ に依存する。フロベニウスの方法で解を求めることができ、級数解 は次のようになる。 $$ \\begin{align*} J_{\\nu}(x) \u0026amp;= \\sum \\limits_{n=0}^{\\infty} \\frac{(-1)^{n} }{\\Gamma (n+1) \\Gamma (n+\\nu+1)} \\left(\\frac{x}{2} \\right)^{2n+\\nu} \\\\ J_{-\\nu}(x) \u0026amp; =\\sum \\limits_{n=0}^{\\infty}\\frac{(-1)^{n}}{\\Gamma (n+1)\\Gamma (n-\\nu+1)} \\left( \\frac{x}{2} \\right)^{2n-\\nu} \\end{align*} $$\nベッセル関数の導出を考えると、ベッセル方程式 という微分方程式の一つやその解など、難しい表現が登場する。しかし、方向統計学に関する上記の引用で重要な文はただ一つだ。\n\u0026ldquo;ベッセル方程式は、球面座標系で波動方程式を解く時に登場する微分方程式だ。\u0026rdquo;\n数理物理学で波動方程式について語るかもしれないが、我々に必要なのは$\\int_{S^{p-1}} f d \\mathbf{x} = 1$だけだ。問題は、通常のユークリッド空間と違って球面上の確率密度関数の値が「中心から離れて$0$に近づく」という現象がないため、積分自体が簡単ではないことだ。正規分布の確率密度関数が円$S^{1}$を包む形を想像してみてほしい。\n上の図で$\\tau = 1$の時を見ると、正規分布の果てしない尻尾は、無限に$S^{1}$を回りながら、無限に$0$ではない厚さを加えている。2これは円周率の二倍である$2 \\pi$を周期として繰り返し、これがまさに「球面上の波」と同じ形を持つ理由であり、ベッセル関数が使用できる理由だ。\nSungkyu Jung. \u0026ldquo;Geodesic projection of the von Mises–Fisher distribution for projection pursuit of directional data.\u0026rdquo; Electron. J. Statist. 15 (1) 984 - 1033, 2021. https://doi.org/10.1214/21-EJS1807\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStraub, J. (2017). Bayesian Inference with the von-Mises-Fisher Distribution in 3D. https://www.semanticscholar.org/paper/Bayesian-Inference-with-the-von-Mises-Fisher-in-3-D-Straub/26d5bb31153df418388b6eb242b2d8842c039c2d#extracted\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2478,"permalink":"https://freshrimpsushi.github.io/jp/posts/2478/","tags":null,"title":"第一種変形ベッセル関数が方向統計学に登場する理由"},{"categories":"복소해석","contents":"定義 1 複素数集合 $\\mathbb{C}$ の空集合じゃない部分集合 $A,B \\subset \\mathbb{C}$ に対して、$f : A \\to B$ を 複素値関数Complex Valued Functionと呼ぶ。一方で、$A, B \\subset \\mathbb{R}$ のとき、複素関数と区別する意味で、$f : A \\to B$ を 実値関数Real Valued Functionとも呼ぶ。\n説明 上の定義は、実は何も意味がない。いろいろ書いて何を言いたいんだと思うかもしれないけど、細かいことを言い出したらキリがないほど多くの部分でつまずく定義なので、定義として価値がない。\nReal, Complex Valued Functionという表現は、厳密に言えば「関数の値」が実数か複素数かを区別しており、従って定義域がどうであれ関係ない定義を立てるのが妥当だと思われる。 共域が$\\mathbb{C}^{n}$だからと言って、それを複素ベクトル関数とわざわざ呼ばない。当然、$f : \\mathbb{C} \\to \\mathbb{C}$を複素スカラー関数と呼ぶこともない。 定義域が実数で共域が複素数だからと言って、またその逆であるからといって、その関数を複素関数でありながら実関数と呼ぶことはない。 実際に、実関数は中学高校から接するほとんどの関数を指すように見えるが、数学科では多くの講座や教科書が測度論に当たる内容を単に実解析と呼んでいて、混乱を招くことがある。 要するに、複素関数と実関数という表現自体が厳密な定義に基づいて使われているわけではなく、文脈によって慣例に従う性質が強いということだ。例えば、$f : \\mathbb{C} \\to \\mathbb{C}$は誰もが複素関数と呼ぶし、$f : \\mathbb{R} \\to \\mathbb{C}$も複素関数と呼んでいいけど「関数の値が複素数の関数」と区別する「傾向」がある。$f : \\mathbb{C}^{n} \\to \\mathbb{C}^{n}$は誰でも複素関数と呼ぶが、$f : \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$を実関数と呼ぶことはほとんどないんだが、主に$\\mathbb{C}^{n}$は何かの関数や定理の一般化として意味があるが、$\\mathbb{R}^{n}$はベクトル関数としての意味が強いからだ。\nOsborne (1999). Complex variables and their applications: p22.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2473,"permalink":"https://freshrimpsushi.github.io/jp/posts/2473/","tags":null,"title":"複素関数の定義"},{"categories":"줄리아","contents":"コード Plots.jlは基本的にグリッド、目盛り、軸、カラーバーなどを全て出力するけど、これらをなくしてすっきりと描きたい場合は、次のオプションを追加すればいい。\ncolorbar=:none：カラーバーを消す。 showaxis = false：軸と目盛りを消す。 grid=false：背景のグリッドを消す。 ticks=false：背景のグリッドと目盛りを消す。 framestyle=:none：背景のグリッドと軸を消す。 using Plots\rsurface(L, title=\u0026#34;default\u0026#34;)\rsurface(L, title=\u0026#34;colorbar=:none\u0026#34;, colorbar=:none)\rsurface(L, title=\u0026#34;showaxis=false\u0026#34;, showaxis=false)\rsurface(L, title=\u0026#34;grid=false\u0026#34;, grid=false)\rsurface(L, title=\u0026#34;ticks=false\u0026#34;, ticks=false)\rsurface(L, title=\u0026#34;framestyle=:none\u0026#34;, framestyle=:none)\rsurface(L, title=\u0026#34;all off\u0026#34;, ticks=false, framestyle=:none, colorbar=:none) 環境 OS: Windows11 Version: Julia v1.8.3, Plots v1.38.6 ","id":3501,"permalink":"https://freshrimpsushi.github.io/jp/posts/3501/","tags":null,"title":"ジュリアで軸、目盛りなどをすべて無くしてきれいに出力する方法"},{"categories":"줄리아","contents":"概要 PythonやMATLABで使うmeshgrid()のような直接的な関数はない。グリッド上での関数値だけを求めたいなら、格子を作らないもっと簡単な方法がある。\nコード 2次元 列ベクトルと行ベクトルを掛けるのは、列ベクトルと行ベクトルのクロネッカー積を取るのと同じ結果を出す。\nU(t,x) = sin(π*x)*exp(- π^2 * t)\rx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\r# Fig. 1\rX = x * fill!(similar(t), 1)\rT = fill!(similar(x), 1) * t\ru1 = U.(T,X)\rheatmap(t\u0026#39;, x, u1, title=\u0026#34;Fig 1\u0026#34;) クロネッカー積を使えば、\nusing LinearAlgebra\rX = kron(x, ones(size(t)))\rT = kron(ones(size(x)), t)\ru2 = U.(T,X)\rheatmap(t\u0026#39;, x, u2, title=\u0026#34;Fig 2\u0026#34;)\rjulia\u0026gt; u1 == u2\rtrue 3次元1 U(x,y,t) = exp(-x^2) * exp(-2y^2) * exp(- π^2 * t)\rx = LinRange(-1., 1, 100)\ry = LinRange(-1., 1, 100)\rt = LinRange(0.,0.35, 50)\rX = getindex.(Iterators.product(x, y, t), 1)\rY = getindex.(Iterators.product(x, y, t), 2)\rT = getindex.(Iterators.product(x, y, t), 3)\ru3 = U.(X,Y,T)\ranim = @animate for i ∈ 1:50\rsurface(u3[:,:,i], zlims=(0,1), clim=(-1,1), title=\u0026#34;Anim. 1\u0026#34;)\rend 完全なコード using Plots\rcd = @__DIR__\rU(t,x) = sin(π*x)*exp(- π^2 * t)\rx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\r# Fig. 1\rX = x * fill!(similar(t), 1)\rT = fill!(similar(x), 1) * t\ru1 = U.(T,X)\rheatmap(t\u0026#39;, x, u1, title=\u0026#34;Fig 1\u0026#34;)\rsavefig(cd*\u0026#34;/fig1.png\u0026#34;)\r# kron\rusing LinearAlgebra\rX = kron(x, ones(size(t)))\rT = kron(ones(size(x)), t)\ru2 = U.(T,X)\rheatmap(t\u0026#39;, x, u2, title=\u0026#34;Fig 2\u0026#34;)\rsavefig(cd*\u0026#34;/fig2.png\u0026#34;)\ru1 == u2\r# 3d\rU(x,y,t) = (1/4) * exp(-x^2) * exp(-2y^2) * exp(- π^2 * t)\rx = LinRange(-2., 2, 100)\ry = LinRange(-2., 2, 100)\rt = LinRange(0.,0.35, 50)\rX = getindex.(Iterators.product(x, y, t), 1)\rY = getindex.(Iterators.product(x, y, t), 2)\rT = getindex.(Iterators.product(x, y, t), 3)\ru3 = U.(X,Y,T)\ranim = @animate for i ∈ 1:50\rsurface(u3[:,:,i], zlims=(0,0.5), clim=(0,0.3), title=\u0026#34;Anim. 1\u0026#34;)\rend\rgif(anim, cd*\u0026#34;/anim1.gif\u0026#34;, fps=10) 環境 OS: Windows11 バージョン: Julia v1.8.3、Plots v1.38.6 https://discourse.julialang.org/t/meshgrid-function-in-julia/48679/26\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3500,"permalink":"https://freshrimpsushi.github.io/jp/posts/3500/","tags":null,"title":"ジュリアでメッシュグリッドを作成する方法"},{"categories":"수리통계학","contents":"定義 データ $\\mathbf{x} = \\left\\{ x_{1} , \\cdots , x_{n} \\right\\}$ と ベクトル $\\mathbf{w} = \\left( w_{1} , \\cdots , w_{n} \\right) \\in \\mathbb{R}^{n}$ に対して、以下を加重平均Weighted Meanという。 $$ {{ \\sum_{k=1}^{n} w_{k} x_{k} } \\over { \\sum_{k=1}^{n} w_{k} }} = {{ w_{1} x_{1} + \\cdots + w_{n} x_{n} } \\over { w_{1} + \\cdots + w_{n} }} $$ その一方で、$\\mathbf{w}$ は加重ベクトルあるいは単に重みとも呼び、英語ではただのウェイトWeightという。\n説明 加重平均は、数理統計学や一般的な数学の分野で頻繁に言及される統計量で、全ての重みが同一の算術平均の一般化と見なすことができる。$\\mathbf{w} = \\left( a , \\cdots , a \\right) \\ne \\mathbf{0}$ の場合、以下のように広く使われる標本平均になる。 $$ {{ a x_{1} + \\cdots + a x_{n} } \\over { a + \\cdots + a }} = {{ x_{1} + \\cdots + x_{n} } \\over { n }} $$ $\\mathbf{x}$ が多次元に拡張されると、幾何学的には重複を許した複数の点の重心と見なすことができ、物理学では次のように各粒子の質量を重みとして持つ加重平均として質量中心を定義できる。 $$ \\mathbf{r}_{cm}=\\frac{m_{1}\\mathbf{r}_{1}+m_{2}\\mathbf{r}_{2}+\\cdots + m_{n}\\mathbf{r}_{n}}{m_{1}+ m_{2}+ \\cdots+ m_{n}}=\\frac{\\sum m_{i}\\mathbf{r}_{i}}{m} $$ 統計学というホームグラウンドでは、あまりにも当たり前で馴染み深い概念で、特別な説明もなく突然登場することが多い。例えば、様々な集団から得られた標本の合同分散 $s_{p}^{2}$ は、次のようになる。 $$ s_{p}^{2} = {{ \\left( n_{1} - 1 \\right) s_{1}^{2} + \\cdots + \\left( n_{m} - 1 \\right) s_{m}^{2} } \\over { \\left( n_{1} - 1 \\right) + \\cdots + \\left( n_{m} - 1 \\right) }} = {{ \\sum_{i=1}^{m} \\left( n_{i} - 1 \\right) s_{i}^{2} } \\over { \\sum_{i=1}^{m} \\left( n_{i} - 1 \\right) }} $$\n指数加重平均 時系列データ $\\left\\{ x_{t} \\right\\}_{t=1}^{n}$について、次の値を$\\left\\{ x_{t} \\right\\}_{t=1}^{n}$の指数加重平均exponentially weighted averageという。$\\beta \\in (0,1)$について、\n$$ \\begin{align*} \\dfrac{\\beta^{n-1}x_{1} + \\beta^{n-2}x_{2} + \\cdots + \\beta^{0}x_{n}}{\\beta^{n-1} + \\beta^{n-2} + \\cdots + \\beta^{0}} \u0026amp;= (1 - \\beta) \\dfrac{\\beta^{n-1}x_{1} + \\beta^{n-2}x_{2} + \\cdots + \\beta^{0}x_{n}}{1 - \\beta^{n}} \\\\ \u0026amp;= \\dfrac{ (1 - \\beta) \\sum\\limits_{t=1}^{n}\\beta^{n-t}x_{t} }{1 - \\beta^{n}} \\end{align*} $$\n最初の等号は等比数列の和の公式によって成り立つ。これは、過去のデータほど重みを指数的に減少させて加えることを意味する。次のように再帰的に定義されることもある。\n$$ \\begin{align*} y_{0} \u0026amp;= 0 \\\\ y_{t} \u0026amp;= \\beta y_{t-1} + (1-\\beta) x_{t} = (1-\\beta) \\sum\\limits_{j=1}^{t} \\beta^{t-j} x_{j} \\end{align*} $$\nこの場合、加重和であるため、$(1 - \\beta^{t})$で割ると加重平均になる。\n$$ \\hat{y}_{t} = \\dfrac{y_{t}}{1 - \\beta^{t}} $$\n","id":2470,"permalink":"https://freshrimpsushi.github.io/jp/posts/2470/","tags":null,"title":"加重平均の定義"},{"categories":"줄리아","contents":"概要 Juliaで多変数関数をブロードキャストする方法を紹介する。Pythonなどで行うように、meshgridを作成する方法もあるし、各次元ごとにベクトルを作成して簡単に計算することもできる。\n2変数関数 $$ u(t,x) = \\sin(\\pi x) e^{-\\pi^{2}t} $$\n上のような関数を$(t,x) \\in [0, 0.35] \\times [-1,1]$でプロットしたい場合、次のように関数値を計算できる。\nx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\ru1 = @. sin(π*x)*exp(- π^2 * t)\rheatmap(t\u0026#39;, x, u1, xlabel=\u0026#34;t\u0026#34;, ylabel=\u0026#34;x\u0026#34;, title=\u0026#34;Fig. 1\u0026#34;) 関数自体を定義し、次のように2次元グリッドを作成して同じ結果を得ることができる。\nU(t,x) = sin(π*x)*exp(- π^2 * t)\rx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\rX = x * fill!(similar(t), 1)\rT = fill!(similar(x), 1) * t\ru2 = U.(T,X)\rheatmap(t\u0026#39;, x, u2, xlabel=\u0026#34;t\u0026#34;, ylabel=\u0026#34;x\u0026#34;, title=\u0026#34;Fig. 2\u0026#34;) 3変数関数 $$ u(x,y,t) = e^{-x^{2} - 2y^{2}}e^{-\\pi^{2}t} $$\n時空間ドメイン$(x,y,t) \\in [-1,1] \\times [-1,1] \\times [0, 0.35]$で$u$の関数値を得たい場合、各変数の次元にのみサイズがあるようにベクトルを作成してブロードキャストすればいい。\n3次元メッシュを作成してブロードキャストしたい場合は、ここを参照。\njulia\u0026gt; x = reshape(LinRange(-1., 1, 100), (100,1,1))\r100×1×1 reshape(::LinRange{Float64, Int64}, 100, 1, 1) with eltype Float64:\rjulia\u0026gt; y = reshape(LinRange(-1., 1, 100), (1,100,1))\r1×100×1 reshape(::LinRange{Float64, Int64}, 1, 100, 1) with eltype Float64:\rjulia\u0026gt; t = reshape(LinRange(0.,0.35, 200), (1,1,200))\r1×1×200 reshape(::LinRange{Float64, Int64}, 1, 1, 200) with eltype Float64:\rjulia\u0026gt; u3 = @. exp(-x^2) * exp(-2y^2) * exp(- π^2 * t)\r100×100×200 Array{Float64, 3}:\ranim = @animate for i ∈ 1:200\rsurface(u3[:,:,i], zlims=(0,1), clim=(-1,1))\rend コード詳細 using Plots\rcd = @__DIR__\r# Fig. 1\rx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\ru1 = @. sin(π*x)*exp(- π^2 * t)\rheatmap(t\u0026#39;, x, u1, xlabel=\u0026#34;t\u0026#34;, ylabel=\u0026#34;x\u0026#34;, title=\u0026#34;Fig. 1\u0026#34;)\rsavefig(cd*\u0026#34;/fig1.png\u0026#34;)\r# Fig. 2\rU(t,x) = sin(π*x)*exp(- π^2 * t)\rx = LinRange(-1., 1, 100)\rt = LinRange(0., 0.35, 200)\u0026#39;\rX = x * fill!(similar(t), 1)\rT = fill!(similar(x), 1) * t\ru2 = U.(T,X)\rheatmap(t\u0026#39;, x, u2, xlabel=\u0026#34;t\u0026#34;, ylabel=\u0026#34;x\u0026#34;, title=\u0026#34;Fig. 2\u0026#34;)\rsavefig(cd*\u0026#34;/fig2.png\u0026#34;)\r# gif 1\rx = reshape(LinRange(-1., 1, 100), (100,1,1))\ry = reshape(LinRange(-1., 1, 100), (1,100,1))\rt = reshape(LinRange(0.,0.35, 200), (1,1,200))\ru3 = @. exp(-x^2) * exp(-2y^2) * exp(- π^2 * t)\ranim = @animate for i ∈ 1:200\rsurface(u3[:,:,i], zlims=(0,1), clim=(-1,1), title=\u0026#34;Anim. 1\u0026#34;)\rend\rgif(anim, cd*\u0026#34;/anim1.gif\u0026#34;, fps=30) 環境 OS: Windows11 Version: Julia v1.8.3, Plots v1.38.6 ","id":3499,"permalink":"https://freshrimpsushi.github.io/jp/posts/3499/","tags":null,"title":"ジュリアにおける多変数関数のブロードキャス팅"},{"categories":"복소해석","contents":"概要 複素数の集合$\\mathbb{C}$を位相空間として扱うための定義を紹介する。位相空間とは言っても、ほとんどが距離空間での定義を複素集合に特化させたものだ。解析入門を一生懸命勉強したなら、そんなに難しくなく受け入れられる。\n定義 1 $\\alpha \\in \\mathbb{C}$で、$\\delta \u0026gt; 0$で、$S \\subset \\mathbb{C}$とする。\n集合の開閉 以下のような集合を$\\alpha$のオープン近傍またはオープンボールという。 $$ B \\left( \\alpha ; \\delta \\right) := \\left\\{ z \\in \\mathbb{C} : \\left| z - \\alpha \\right| \u0026lt; \\delta \\right\\} $$ アスタリスクAsterisk$\\ast$が上付き文字としてついている場合は、中心の$\\alpha$を除外したものである。例えば$B^{\\ast} \\left( \\alpha ; \\delta \\right)$は以下のように定義され、空孔ボールと呼ばれる。 $$ B^{\\ast} \\left( \\alpha ; \\delta \\right) := \\left\\{ z \\in \\mathbb{C} : 0 \u0026lt; \\left| z - \\alpha \\right| \u0026lt; \\delta \\right\\} $$ $\\alpha$のオープンボールのどれかが$S$に含まれる場合、$\\alpha$を$S$の内点という。 $$ \\exist \\delta : B \\left( \\alpha , \\delta \\right) \\subset S $$ $\\alpha$の全ての空孔オープンボールが$S$と互いに素ではない場合、$\\alpha$を$S$の集積点という。 $$ \\forall \\delta : B^{\\ast} \\left( \\alpha , \\delta \\right) \\cap S \\ne \\emptyset $$ $S$の全ての点が$S$の内点である場合、$S$は開いていると言い、$S$が全ての集積点を含む場合、閉じていると言う。 有界とコンパクト $S \\subset \\mathbb{C}$の全ての元$z \\in S$に対して$\\left| z \\right| \\le M$を満たす正数$M \u0026gt; 0$が存在する場合、$S$はバウンデッドと言う。 クローズドでバウンデッドならコンパクトと言う。 複素領域 $S \\subset \\mathbb{C}$の全ての2点が経路で構成される線分で繋がることができるなら、$S$は**(多角)連結**集合と言う。 空集合でなくオープンな連結集合$\\mathscr{R} \\subset \\mathbb{C}$を領域と言い、特に複素空間での領域を意味する場合、複素領域と強調する。 追加定義 上の段落では、特に複素解析だけでなく、数学で普遍的に必要な部分のみを要約した。当然、次のような定義や記号も必要な時がある。\n以下のような集合を$\\alpha$のクローズド近傍またはクローズドボールという。 $$ B \\left[ \\alpha ; \\delta \\right] := \\left\\{ z \\in \\mathbb{C} : \\left| z - \\alpha \\right| \\le \\delta \\right\\} $$ $\\alpha$の全てのオープン近傍が$S$と$S^{c}$の点を含む場合、$\\alpha$を境界点という。$\\alpha$が内点でも境界点でもない場合、外点と言う。 $S$の全ての集積点の集合を$S$の閉包と言い、$\\overline{S}$のように表す。 $\\mathbb{C} \\setminus S$が連結集合なら、連結集合$S$を単純連結と言う。 参照 複素数集合$\\mathbb{C}$は体の公理に従うだけでなく、複素数のモジュラス$\\left| \\cdot \\right|$が与えられることによってノルム空間でもあり、距離空間である。したがって、距離空間に既に馴染みがあれば、複素空間として新たに学ぶべきことは何もない。\n距離空間でのボールと開集合閉集合 距離空間での近傍、集積点、開閉 距離空間での内部閉包境界 距離空間でのコンパクト ハイネ・ボレルの定理の証明: 元来、コンパクトを定義するためには、もっと複雑な議論が必要だが、複素解析では、バウンデッドで閉集合ならコンパクトと同等とする定義でも問題ない。 位相数学での経路連結性: 実際に定義で紹介された連結性は、経路連結性により近い。経路連結ならば連結であり、位相数学での一般的な連結性の定義を理解するためには、位相的な思考がかなり根付いていなければならないため、その代わりに「線分で繋がれる」という幾何学的な直感を借りたものである。 距離空間での連結集合 Osborne (1999). Complex variables and their applications: p10~12.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2467,"permalink":"https://freshrimpsushi.github.io/jp/posts/2467/","tags":null,"title":"複素空間の位相空間学"},{"categories":"함수","contents":"定義 関数 $c : X \\to Y$ が全ての $x_{1} , x_{2} \\in X$ に対して次を満足するなら、定数関数Constant Functionと言う。 $$ c \\left( x_{1} \\right) = c \\left( x_{2} \\right) $$\n説明 普通、定数関数を関数として最初に「認識」する出発点は、定数関数の微分法を学ぶ時だ。 $$ \\lim_{h \\to 0} {{ c \\left( x + h \\right) - c \\left( x \\right) } \\over { h }} = 0 $$ それまでの教育課程では、関数や数が何であるか理解するのが難しく、優等生でない場合は、項を「文字」と「数字」に区別するようなナンセンスな視点で数式を見ることもあった（著者もそうだった）。しかし、両辺を微分することで、その文字でない―多項関数でない部分をどう扱うべきかを考え始める。その直後に、不定積分を扱いながら $$ \\int f(x) dx = F(x) + c $$ のように「ある定数 $c$」を記述し、定数という概念に慣れる。面白いことに、冗談でも、数学で重要と言えない「定数関数」がある分野で普遍的に現れるタイミングがある。\n連続性 $X, Y$ が位相空間なら、関数の連続性について論じることができる。定数関数は、どのような空間でも自明に連続関数であり、通常、$f : X \\to \\mathbb{Z}$ のような連続関数は「整数である関数値が連続的に変わることはできないので、$f$ は他ならぬ定数関数である」と言う形で登場する。\nモノドロミー定理の証明 ","id":2465,"permalink":"https://freshrimpsushi.github.io/jp/posts/2465/","tags":null,"title":"定数関数の定義"},{"categories":"함수","contents":"定義 1 2つの多項式関数 $P_{1}(z), P_{2}(z) : \\mathbb{C} \\to \\mathbb{C}$ について、$P_{2} (z) \\ne 0$ である全ての $z \\in \\mathbb{C}$ を $\\left( P_{1} / P_{2} \\right) (z)$ に対応させる次の関数 $Q$ を有理関数または代数的分数という。 $$ Q (z) := {{ P_{1} (z) } \\over { P_{2} (z) }} \\qquad \\text{where } P_{2} (z) \\ne 0 $$\nOsborne (1999). Complex variables and their applications: p24.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2463,"permalink":"https://freshrimpsushi.github.io/jp/posts/2463/","tags":null,"title":"有理関数の定義"},{"categories":"푸리에해석","contents":"概要1 離散フーリエ変換DFTは、数式的な定義に従って計算すると、$\\mathcal{O}(N^{2})$の時間計算量を持ちますが、以下で説明するアルゴリズムに従って計算すると、時間計算量が$\\mathcal{O}(N\\log_{2}N)$に低減します。この高速フーリエ変換を使用して離散フーリエ変換を高速に実行することができます。これは高速フーリエ変換fast Fourier transform, FFTと呼ばれています。\n構築 2つの数字を掛けて、それを別の数字に加える操作を$1$回の演算operationと呼びましょう。すると、$\\sum\\limits_{i=0}^{n-1}a_{n}b_{n}$の値を求めるには$n$回の演算が必要です。\n$$ \\begin{align*} \\sum\\limits_{n=0}^{0} a_{n}b_{b} \u0026amp;= a_{0}b_{0} = \\overbrace{0 {\\color{red} +} a_{0} {\\color{red} \\times} b_{0}}^{\\color{red} 1 \\text{ operation}} \\\\ \\sum\\limits_{n=0}^{1} a_{n}b_{b} \u0026amp;= a_{0}b_{0} + a_{1}b_{1} = \\overbrace{\\big( \\overbrace{0 {\\color{red} +} a_{0} {\\color{red} \\times} b_{0}}^{\\color{red} 1 \\text{ operation}} \\big) {\\color{#5882FA} + } a_{1} {\\color{#5882FA} \\times} b_{1}}^{\\color{#5882FA}2 \\text{ operations}} \\\\ \\sum\\limits_{n=0}^{2} a_{n}b_{b} \u0026amp;= a_{0}b_{0} + a_{1}b_{1} + a_{2}b_{2} = \\overbrace{\\bigg( \\overbrace{\\big( \\overbrace{0 {\\color{red} +} a_{0} {\\color{red} \\times} b_{0}}^{\\color{red} 1 \\text{ operation}} \\big) {\\color{#5882FA} + } a_{1} {\\color{#5882FA} \\times} b_{1}}^{\\color{#5882FA}2 \\text{ operations}} \\bigg) {\\color{#FE9A2E} + } a_{2} {\\color{#FE9A2E} \\times} b_{2}}^{\\color{#FE9A2E}3 \\text{ operations}} \\\\ \\end{align*} $$\nさて、離散フーリエ変換の定義を思い出してみましょう。\n線型変換 $\\mathcal{F}_{N} : \\mathbb{C}^{N} \\to \\mathbb{C}^{N}$を離散フーリエ変換と呼びます。\n$$ \\mathcal{F}_{N}(\\mathbf{a}) = \\hat{\\mathbf{a}} = \\begin{bmatrix} \\hat{a}_{0} \\\\ \\hat{a}_{1} \\\\ \\dots \\\\ \\hat{a}_{N-1} \\end{bmatrix} ,\\quad \\hat{a}_{m} = \\sum_{n=0}^{N-1}e^{-i2\\pi mn /N} a_{n}\\quad (0\\le m \u0026lt; N) \\tag{1} $$\nこのとき、$\\mathbf{a} = \\begin{bmatrix} a_{0}\u0026amp; a_{1}\u0026amp; \\dots\u0026amp; a_{N-1} \\end{bmatrix}^{T}$です。\n$\\hat{a}_{m}$を計算するには$N$回の演算が必要で、$\\hat{\\mathbf{a}}$を計算するにはこれを$N$回繰り返す必要があるため、離散フーリエ変換を計算するには合計で$N^{2}$回の演算が必要です。つまり、$\\mathcal{O}(N^{2})$の時間計算量を持っています。これは、コンピュータ計算の観点からフーリエ変換がかなりのコストを要することを意味します。\nアルゴリズム データの長さ$N$を合成数$N = N_{1}N_{2}$としましょう。そして、インデックス$m, n$を次のように定義します。\n$$ m = m^{\\prime}N_{1} + m^{\\prime \\prime},\\quad n = n^{\\prime}N_{2} + n^{\\prime \\prime} $$\nすると、$0 \\le m^{\\prime}, n^{\\prime \\prime} \\le N_{2}-1$および$0 \\le m^{\\prime \\prime}, n^{\\prime} \\le N_{1}-1$です。$(1)$の指数部分を次のように表現できます。\n$$ \\begin{align*} e^{-i2\\pi mn /N} \u0026amp;= e^{-i2\\pi (m^{\\prime}N_{1} + m^{\\prime \\prime})(n^{\\prime}N_{2} + n^{\\prime \\prime})/(N_{1}N_{2})} \\\\ \u0026amp;= e^{-i2\\pi (m^{\\prime}n^{\\prime}N_{1}N_{2} + m^{\\prime}n^{\\prime \\prime}N_{1} + m^{\\prime \\prime}n^{\\prime}N_{2} + m^{\\prime \\prime}n^{\\prime \\prime})/(N_{1}N_{2})} \\\\ \u0026amp;= e^{-i2\\pi m^{\\prime}n^{\\prime}} e^{-i2\\pi \\big( (m^{\\prime}n^{\\prime \\prime}/N_{2}) + (m^{\\prime \\prime}n^{\\prime}/N_{1}) + (m^{\\prime \\prime}n^{\\prime \\prime}/N) \\big)} \\\\ \u0026amp;= e^{-i2\\pi \\big( (m^{\\prime}n^{\\prime \\prime}/N_{2}) + (m^{\\prime \\prime}n^{\\prime}/N_{1}) + (m^{\\prime \\prime}n^{\\prime \\prime}/N) \\big)} \\\\ \\end{align*} $$\nこれを$(1)$に代入すると、\n$$ \\begin{align*} \\hat{a}_{m} \u0026amp;= \\sum_{n=0}^{N-1}e^{-i2\\pi mn /N} a_{n} \\\\ \u0026amp;= \\sum_{n^{\\prime \\prime}=0}^{N_{2}-1} \\sum_{n^{\\prime}=0}^{N_{1}-1} e^{-i2\\pi \\big( (m^{\\prime}n^{\\prime \\prime}/N_{2}) + (m^{\\prime \\prime}n^{\\prime}/N_{1}) + (m^{\\prime \\prime}n^{\\prime \\prime}/N) \\big)}a_{n^{\\prime}N_{2}+n^{\\prime \\prime}} \\\\ \u0026amp;= \\sum_{n^{\\prime \\prime}=0}^{N_{2}-1} \\left[ \\sum_{n^{\\prime}=0}^{N_{1}-1} e^{-i2\\pi m^{\\prime \\prime}n^{\\prime}/N_{1}}a_{n^{\\prime}N_{2}+n^{\\prime \\prime}} \\right] e^{-i2\\pi [ (m^{\\prime}n^{\\prime \\prime}/N_{2}) + (m^{\\prime \\prime}n^{\\prime \\prime}/N) ] } \\end{align*} $$\n上記の式に従うと、各括弧内の$\\left[ \\sum_{n^{\\prime}=0}^{N_{1}-1} \\right]$を計算するのに$N_{1}$回の演算、括弧の外の$\\sum_{n^{\\prime \\prime}=0}^{N_{2}-1}$を計算するのに$N_{2}$回の演算が必要です。したがって、$\\hat{a}_{m}$を計算するには合計で$(N_{1} + N_{2})$回の演算が必要です。$\\hat{\\mathbf{a}}$を得るにはこれを$N$回繰り返す必要があるため、合計で$N(N_{1} + N_{2})$のコストがかかり、$N^{2}$よりも減少することが確認できます。\n括弧内を注意深く見ると、$N_{1}$が再び合成数の場合、同じロジックを適用できることがわかるでしょう。したがって、データの長さが$N = N_{1} N_{2} \\cdots N_{k}$といった合成数の場合、時間計算量は次のように減少します。\n$$ \\mathcal{O}(N^{2}) \\searrow \\mathcal{O}\\big( N(N_{1} + N_{2} + \\cdots + N_{k}) \\big) $$\nここで、$N$を$2$のべき乗$N = 2^{k}$と仮定してみましょう。すると、$\\log_{2}N = k$であり、$N^{2} = 2^{k}$から$2^{k}(2k)$だけ減少するため、時間計算量は次のように減少します。\n$$ \\mathcal{O}(N^{2}) \\searrow \\mathcal{O}(2N \\log_{2}N) $$\n補足 これは1965年にCooleyとTukey2によって提案されたため、Cooley-Tukeyアルゴリズムとも呼ばれています。ただし、彼らが最初に発明したわけではありません。ガウスも同様のアルゴリズムを研究しましたが、正しく発表しなかったため、この事実は後に明らかになりました3。\nGerald B. Folland, Fourier Analysis and Its Applications (1992), p252-253\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. W. Cooley and J. W. Tukey, An algorithm for the machine calculation of complex Fourier series, Mathematics of Computation 19 (1965), 297-301.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. T. Heideman, D. H. Johnson, and C. S. Burms, Gauss and the history of the fast Fourier transform, Archive for the History of the Exact Sciences 34 (1985), 264-277.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3492,"permalink":"https://freshrimpsushi.github.io/jp/posts/3492/","tags":null,"title":"高速フーリエ変換アルゴリズム"},{"categories":"수리통계학","contents":"定義 1 ある推定量Estimator $T$ について、$T$ の標準偏差の推定値Estimateを標準誤差Standard Errorと言う。 $$ \\text{s.e.} \\left( T \\right) := \\sqrt{ \\widehat{ \\text{Var} \\left( T \\right) } } $$\n説明 定義で統計量ではなくて、正確に推定量と言われた理由がある。僕が当てたいパラメータ $\\theta$ と「合ってるかどうか」を議論する時じゃなければ、標準誤差は意味をなさないから、式で$\\theta$ が一度も登場しないにしても、わざわざ推定量について定義するんだ。だから$T$ の候補は明らかに標本平均 $\\overline{X}$ や回帰係数 $\\beta_{k}$ などで、その信頼区間が気になるから$\\text{s.e.} \\left( T \\right)$ が必要になるんだ。\n普通は、$\\overline{X} = \\sum_{k=1}^{n} X_{k}$ の標準誤差 $S / \\sqrt{n}$ をこのような定義から学んで、これだけが標準誤差だと思うことが多いけど、実際にはそれも定義ではなくて、計算を通じて得られる公式なんだ。できるだけ省略せずに計算してみよう。 $$ \\begin{align*} \\text{s.e.} \\left( \\overline{X} \\right) =\u0026amp; \\sqrt{ \\widehat{ \\text{Var} \\left( \\overline{X} \\right) } } \\\\ =\u0026amp; \\sqrt{ \\widehat{ \\text{Var} \\left( {{ 1 } \\over { n }} \\sum_{k=1}^{n} X_{k} \\right) } } \\\\ =\u0026amp; \\sqrt{ \\widehat{ {{ 1 } \\over { n^{2} }} \\text{Var} \\left( \\sum_{k=1}^{n} X_{k} \\right) } } \\\\ \\overset{\\text{iid}}{=} \u0026amp; \\sqrt{ \\widehat{ {{ 1 } \\over { n^{2} }} \\sum_{k=1}^{n} \\text{Var} \\left( X_{k} \\right) } } \\\\ =\u0026amp; \\sqrt{ {{ 1 } \\over { n^{2} }} \\sum_{k=1}^{n} \\widehat{ \\text{Var} \\left( X_{k} \\right) } } \\\\ =\u0026amp; \\sqrt{ {{ 1 } \\over { n^{2} }} \\sum_{k=1}^{n} S^{2} } \\\\ =\u0026amp; \\sqrt{ {{ 1 } \\over { n^{2} }} n S^{2} } \\\\ =\u0026amp; \\sqrt{ {{ 1 } \\over { n }} S^{2} } \\\\ =\u0026amp; {{ 1 } \\over { \\sqrt{n} }} S \\end{align*} $$ 見ての通り、推定量Estimatorと推定値Estimateが違っていて、こんな簡単な例でもかなり混乱する。そこに、実際に標準誤差を使う多くの場面で、標本分散を自由度で割ってルートを取る形を多用するから、その形を標準誤差そのものと間違えやすい。しかし、そのような直感がよく通じるとしても、標準誤差はそのような方法で決まるものではなく、上に示したような数式的な展開で導出するのが正しいんだ。\nHadi. (2006). Regression Analysis by Example(4th Edition): p33.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2462,"permalink":"https://freshrimpsushi.github.io/jp/posts/2462/","tags":null,"title":"標準誤差の一般的な定義"},{"categories":"줄리아","contents":"概要 これを実現するには、Dates モジュールの canonicalize() 関数を使用する1。\nコード using Dates\rtic = DateTime(2022,3,7,7,1,11)\rtoc = now()\rDates.canonicalize(toc-tic) 上のコードを実行した結果は次のとおりである。\njulia\u0026gt; using Dates\rjulia\u0026gt; tic = DateTime(2022,3,7,7,1,11)\r2022-03-07T07:01:11\rjulia\u0026gt; toc = now()\r2022-07-19T22:26:22.070\rjulia\u0026gt; Dates.canonicalize(toc-tic)\r19 weeks, 1 day, 15 hours, 25 minutes, 11 seconds, 70 milliseconds 小さい単位の倍数として正確に、週単位まで自動で計算して出力されることが確認できる。\n環境 OS: Windows julia: v1.7.0 https://discourse.julialang.org/t/get-difference-between-two-dates-in-seconds/11641/4\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2461,"permalink":"https://freshrimpsushi.github.io/jp/posts/2461/","tags":null,"title":"ジュリアで2つの時刻の差を秒単位で計算する方法"},{"categories":"줄리아","contents":"要約 Plots.jlで軸と目盛りの色を指定する関連キーワードは以下の通りである。\nキーワード名 機能 guidefontcolor 軸名の色を指定 foreground_color_border, fgcolor_border 軸の色を指定 foreground_color_axis, fgcolor_axis 目盛りの色を指定 foreground_color_text, fgcolor_text 目盛りの値の色を指定 キーワード名の前にx_やy_を付けると、その軸にのみ適用される。\nコード1 軸名 軸名の色を指定するキーワードはguidefontcolorである。軸名は、xlabel, ylabelで指定できる。\nx = randn(10, 3)\rplot(plot(x, guidefontcolor = :red),\rplot(x, x_guidefontcolor = :red),\rplot(x, y_guidefontcolor = :red),\rxlabel = \u0026#34;x label\u0026#34;,\rylabel = \u0026#34;y label\u0026#34;,\r) 軸 軸の色を指定するキーワードはforeground_color_borderである。\nplot(plot(x, foreground_color_border = :red),\rplot(x, x_foreground_color_border = :red),\rplot(x, y_foreground_color_border = :red),\rxlabel = \u0026#34;x label\u0026#34;,\rylabel = \u0026#34;y label\u0026#34;,\r) 目盛り 目盛りの色を指定するキーワードはforeground_color_axisである。これをfalseにすると、目盛りだけを消すことができる。\nplot(plot(x, foreground_color_axis = :red),\rplot(x, x_foreground_color_axis = :red),\rplot(x, y_foreground_color_axis = false),\rxlabel = \u0026#34;x label\u0026#34;,\rylabel = \u0026#34;y label\u0026#34;,\r) 目盛りの値 目盛りの値の色を指定するキーワードはforeground_color_textである。これをfalseにすると、目盛りの値だけを消すことができる。\nplot(plot(x, foreground_color_text = :red),\rplot(x, x_foreground_color_text = :red),\rplot(x, y_foreground_color_text = flase),\rxlabel = \u0026#34;x label\u0026#34;,\rylabel = \u0026#34;y label\u0026#34;,\r) 環境 OS：Windows11 バージョン：Julia 1.9.4, Plots v1.39.0 関連項目 色の使い方 パレットの使い方 カラーグラデーションの使い方 色処理をするためのパッケージ Colors.jl RGBコードの使い方 RGB(1, 0, 0) HEXコードの使い方 \u0026quot;#000000\u0026quot; グラフ要素の色を指定する方法 サブプロットごとにグラフの色を指定する方法 軸、軸名、目盛り、目盛りの値の色を指定する方法 背景色の指定方法 https://docs.juliaplots.org/stable/generated/attributes_axis/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3490,"permalink":"https://freshrimpsushi.github.io/jp/posts/3490/","tags":null,"title":"Julia Plotsで軸、軸名、目盛り、目盛り値の色を指定する方法"},{"categories":"머신러닝","contents":"概要 Flux、PyTorch、TensorFlowで同じ機能をするコードを整理します。\nJulia-MATLAB-Python-R チートシート Fluxについて次のような環境とします。\nusing Flux PyTorchについて次のような環境とします。\nimport torch\rimport torch.nn as nn\rimport torch.nn.functional as F TensorFlowについて次のような環境とします。\nimport tensorflow as tf\rfrom tensorflow import keras 1次元テンソル 줄리아Julia\r파이토치PyTorch\r텐서플로우TensorFlow\r列ベクトルcolumn vector\r[1 4 -1 2] [1;4;-1;2] ","id":3489,"permalink":"https://freshrimpsushi.github.io/jp/posts/3489/","tags":null,"title":"Flux-PyTorch-TensorFlowチートシート"},{"categories":"줄리아","contents":"概要 Juliaで2次元配列と行列の間を切り替えるヒントを紹介する1。おそらくJulia 1.7以下の環境では、最もJuliaらしく、シンプルで、速く、美しい方法だろう。\nコード ここで紹介された方法だけではなく、行列と2次元配列の間を行き来する方法は数えきれないほどある。ただコードを書くだけでなく、目標そのものが難しくないから、Julia独特の構文がどのように使用されたのかも考えながら読む方がいい。\n行列から2次元配列へ julia\u0026gt; M = rand(0:9, 3, 10)\r3×10 Matrix{Int64}:\r2 4 0 1 8 0 9 2 5 7\r5 2 1 5 4 3 7 2 7 3\r7 8 1 9 0 3 2 4 1 3 上のような行列を2次元配列に変えてみよう。\njulia\u0026gt; [eachrow(M)...]\r3-element Vector{SubArray{Int64, 1, Matrix{Int64}, Tuple{Int64, Base.Slice{Base.OneTo{Int64}}}, true}}:\r[2, 4, 0, 1, 8, 0, 9, 2, 5, 7]\r[5, 2, 1, 5, 4, 3, 7, 2, 7, 3]\r[7, 8, 1, 9, 0, 3, 2, 4, 1, 3]\rjulia\u0026gt; [eachcol(M)...]\r10-element Vector{SubArray{Int64, 1, Matrix{Int64}, Tuple{Base.Slice{Base.OneTo{Int64}}, Int64}, true}}:\r[2, 5, 7]\r[4, 2, 8]\r[0, 1, 1]\r[1, 5, 9]\r[8, 4, 0]\r[0, 3, 3]\r[9, 7, 2]\r[2, 2, 4]\r[5, 7, 1]\r[7, 3, 3] eachrow()とeachcol()は行列の行と列を一行ずつ抽出するジェネレータを返す2、そしてスプラットオペレータを通じてこれらを可変配列として扱い3、角括弧[]の中に入れることで、自然に配列になる。\n2次元配列から行列へ julia\u0026gt; A = [rand(0:9,3) for _ in 1:10]\r10-element Vector{Vector{Int64}}:\r[5, 4, 9]\r[9, 7, 6]\r[9, 9, 6]\r[5, 9, 0]\r[0, 2, 8]\r[3, 9, 5]\r[1, 6, 0]\r[5, 7, 7]\r[1, 3, 5]\r[5, 4, 1] 上のような2次元配列を行列にしてみよう。\njulia\u0026gt; hcat(A...)\r3×10 Matrix{Int64}:\r5 9 9 5 0 3 1 5 1 5\r4 7 9 9 2 9 6 7 3 4\r9 6 6 0 8 5 0 7 5 1\rjulia\u0026gt; hcat(A...)\u0026#39;\r10×3 adjoint(::Matrix{Int64}) with eltype Int64:\r5 4 9\r9 7 6\r9 9 6\r5 9 0\r0 2 8\r3 9 5\r1 6 0\r5 7 7\r1 3 5\r5 4 1\rjulia\u0026gt; vcat(A...)\r30-element Vector{Int64}:\r5\r4\r9\r9\r7\r6\r9\r9\r⋮\r7\r1\r3\r5\r5\r4\r1 配列を統合するhcat()関数を使えばいい。4 基本的にhcat()とvcat()はフォールド関数であり、可変引数関数であるため、2次元配列の要素である1次元配列を直接引数としてスプラットオペレータを通じて渡さなければならない。\n全体のコード # matrix to 2d array\rM = rand(0:9, 3, 10)\r[eachrow(M)...]\r[eachcol(M)...]\r# 2d array to matrix\rA = [rand(0:9,3) for _ in 1:10]\rhcat(A...)\rhcat(A...)\u0026#39;\rvcat(A...) 環境 OS: Windows julia: v1.7.0 https://discourse.julialang.org/t/plots-efficient-scatter-plot-of-a-2xn-array/31803/6\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/arrays/#Base.eachcol\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/base/#\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/arrays/#Base.cat\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2459,"permalink":"https://freshrimpsushi.github.io/jp/posts/2459/","tags":null,"title":"ジュリアで2次元配列と行列の間の変換方法"},{"categories":"통계적분석","contents":"定義 1 $$ Y = \\beta_{0} + \\beta_{1} X_{1} + \\cdots + \\beta_{p} X_{p} + \\varepsilon $$ 多重回帰分析で、$p$個の独立変数$X_{1} , \\cdots , X_{p}$に対して上のような線形モデルを設定するとき、$\\beta_{0} , \\beta_{1} , \\cdots , \\beta_{p}$を回帰係数という。$Y$は従属変数を、$\\varepsilon$はランダムに分布したエラーを意味する。\n公式 $$ \\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; x_{11} \u0026amp; \\cdots \u0026amp; x_{p1} \\\\ 1 \u0026amp; x_{12} \u0026amp; \\cdots \u0026amp; x_{p2} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 1 \u0026amp; x_{1n} \u0026amp; \\cdots \u0026amp; x_{pn} \\end{bmatrix} \\begin{bmatrix} \\beta_{0} \\\\ \\beta_{1} \\\\ \\vdots \\\\ \\beta_{p} \\end{bmatrix} + \\begin{bmatrix} \\varepsilon_{1} \\\\ \\varepsilon_{2} \\\\ \\vdots \\\\ \\varepsilon_{n} \\end{bmatrix} $$ $n$個のデータが与えられ、$p \u0026lt; n$とするとき、線形多重回帰モデルを計画行列で表すと上のようになり、簡単に$Y = X \\beta + \\varepsilon$と表そう。$\\beta$に対する最小二乗の推定量ベクトル$\\hat{\\beta}$は以下の通りだ。 $$ \\hat{\\beta} = \\begin{bmatrix} \\hat{\\beta}_{0} \\\\ \\hat{\\beta}_{1} \\\\ \\vdots \\\\ \\hat{\\beta}_{p} \\end{bmatrix} = \\left( X^{T} X \\right)^{-1} X^{T} Y $$ それだけでなく、$\\hat{\\beta}$は$\\beta$の最良不偏推定量であるから、最良線形不偏推定量（Best Linear Unbiased Estimator, BLUE）とも呼ばれる。\n導出 2 3 私たちの目標は $$ \\left\\| \\varepsilon \\right\\|_{2}^{2} = \\sum_{k=0}^{n} \\varepsilon_{k} = \\begin{bmatrix} \\varepsilon_{0} \u0026amp; \\varepsilon_{1} \u0026amp; \\cdots \u0026amp; \\varepsilon_{n} \\end{bmatrix} \\begin{bmatrix} \\varepsilon_{0} \\\\ \\varepsilon_{1} \\\\ \\vdots \\\\ \\varepsilon_{n} \\end{bmatrix} = \\varepsilon^{T} \\varepsilon $$ これを最小化することだ。$\\varepsilon = Y - X \\beta$であるから、$\\varepsilon^{T} \\varepsilon = \\left( Y - X \\beta \\right)^{T} \\left( Y - X \\beta \\right)$を最小化する$\\beta$を見つければいい。両辺を$\\beta$で微分すると $$ \\begin{align*} {{ d } \\over { d \\beta }} \\varepsilon^{T} \\varepsilon =\u0026amp; - 2 X^{T} \\left( Y - X \\beta \\right) \\\\ = \u0026amp; - 2 X^{T} \\left( Y - X \\beta \\right) \\\\ = \u0026amp; - 2 X^{T} Y + 2 X^{T} X \\beta \\end{align*} $$ が$0$となるような$\\hat{\\beta}$は、以下の形になる。 $$ \\hat{\\beta} = \\argmin_{\\beta} \\varepsilon^{T} \\varepsilon = \\left( X^{T} X \\right)^{-1} X^{T} Y $$ 一方で、$\\hat{\\beta}$は$\\beta$に対する不偏推定量であることが容易に分かり、最小二乗法を通じて導出されたので、これより分散が小さい$\\beta$の不偏推定量は存在せず、最良不偏推定量である。\n■\n導出過程で$\\beta$に対する微分が気に入らない場合は、行列代数でのアプローチが代替案としてある。行列代数での最小二乗法では $$ X^{\\ast} Y = X^{\\ast} X \\hat{\\beta} $$ を満たす$\\hat{\\beta}$が最小二乗解となり、$X \\in \\mathbb{R}^{n \\times p}$であるから、$X^{\\ast} = X^{T}$であり、結果として$\\hat{\\beta} = \\left( X^{T} X \\right)^{-1} X^{T} Y$を得る。\n参照 単純回帰係数の推定量の導出 回帰係数ベクトルの多変量正規性 Hadi. (2006). 回帰分析の例(第4版): p53.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHadi. (2006). 回帰分析の例(第4版): p82~84.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.stat.purdue.edu/~boli/stat512/lectures/topic3.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2458,"permalink":"https://freshrimpsushi.github.io/jp/posts/2458/","tags":null,"title":"回帰係数の定義と推定量の公式導出"},{"categories":"줄리아","contents":"概要 SMTPClient.jlパッケージを使って、SMTPSimple Mail Transfer Protocolでナヴァーからメールを送る方法を紹介する1。長時間かかるシミュレーションが終わったらカカオメールにレポートを送るようにして、研究のスピードを上げるのに使っている。\nこのようにジョルディが個人トークで知らせてくれるから、自分でサーバーを確認しなくても、シミュレーションがいつ終わるか分かる。\nコード どの言語で実装しても、まず最初に以下のようにナヴァーメールでSMTPを「使用する」に設定する必要がある。\nジュリア using Dates\rtic = now()\rfor t in 1:1000\rprintln(t)\rend\rtoc = now()\rusing SMTPClient\ropt = SendOptions(\risSSL = true,\rusername = \u0026#34;네이버아이디\u0026#34;,\rpasswd = \u0026#34;비밀번호\u0026#34;)\r#Provide the message body as RFC5322 within an IO\rbody = IOBuffer(\r\u0026#34;Date: ▷eq1◁tic\\r\\n\u0026#34; *\r\u0026#34;▷eq2◁(Dates.canonicalize(toc - tic))\u0026#34; *\r\u0026#34;\\r\\n\u0026#34;)\rurl = \u0026#34;smtps://smtp.naver.com:465\u0026#34;\rrcpt = [\u0026#34;\u0026lt;수신자@kakao.com\u0026gt;\u0026#34;]\rfrom = \u0026#34;\u0026lt;발신자@naver.com\u0026gt;\u0026#34;\rresp = send(url, rcpt, from, body, opt) 上の例では、最も重要な部分はurl = \u0026quot;smtps://smtp.naver.com:465\u0026quot;だ。ナヴァーでなくても、どのサーバーを使うにしても、ここを適切に変える必要がある。送信時刻の場合はDatesモジュールのnow()を使って、メールを送る時点に固定したが、これが実際の時計と合わないと、10分ほど遅れて送られる問題を経験した。\nパイソン ジュリアを試す前に、まずはパイソンで試したコード。不思議なことに、SSLを使ってポートを456にしてもうまくいかなかったが、SSLを切って587にしたらうまくいった。参考にしたブログ2ではグーグルを基準に説明していたが、以下のコードはナヴァーを基準にうまく動作することを確認した。\nimport smtplib\rfrom email.mime.text import MIMEText\rsendEmail = \u0026#34;발신자@naver.com\u0026#34;\rrecvEmail = \u0026#34;수신자@kakao.com\u0026#34;\rpassword = \u0026#34;비밀번호\u0026#34;\rsmtpName = \u0026#34;smtp.naver.com\u0026#34; #smtp 서버 주소\rsmtpPort = 587 #smtp 포트 번호\rtext = \u0026#34;매일 내용\u0026#34;\rmsg = MIMEText(text) #MIMEText(text , _charset = \u0026#34;utf8\u0026#34;)\rmsg[\u0026#39;Subject\u0026#39;] = \u0026#34;시뮬레이션 종료\u0026#34;\rmsg[\u0026#39;From\u0026#39;] = sendEmail\rmsg[\u0026#39;To\u0026#39;] = recvEmail\rprint(msg.as_string())\rs=smtplib.SMTP( smtpName , smtpPort ) #메일 서버 연결\rs.starttls() #TLS 보안 처리\rs.login( sendEmail , password ) #로그인\rs.sendmail( sendEmail, recvEmail, msg.as_string() ) #메일 전송, 문자열로 변환하여 보냅니다.\rs.close() #smtp 서버 연결을 종료합니다. 環境 OS: Windows julia: v1.7.0 https://github.com/aviks/SMTPClient.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://gosmcom.tistory.com/72\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2457,"permalink":"https://freshrimpsushi.github.io/jp/posts/2457/","tags":null,"title":"ジュリアでネイバーからメールを送る方法"},{"categories":"데이터과학","contents":"用語 ある統計量を計算する際、その値を変えることができる独立したデータの数を自由度Degree of Freedomと呼ぶ1。\n説明 自由度を説明するのが難しい理由 新入生になって統計学を勉強してみると、この「自由度」というのがなんなのか、本当に腹が立つ。まず難しいし、頻繁に出てくるのを置いておいて、どの教科書でもその定義をはっきりさせることができないからだ。このポストも具体的に自由度を定義していないし、ただ「用語」として紹介しているだけで、「計算する時」とか「値を変えることができる」など、厳密な数学的な表現とは言えない表現を使っている。\n問題は、それが理解できるということだ。みんなが面倒くさがっているわけではなく、実際の自由度という概念自体が勉強して「理解する」よりも、経験が積み重なるうちに「体得」する感じが強いからだ。2〜3年生くらいになると、自由度が何なのか大体の感じがつかめてきて、大学院に行く頃には普通に説明もそこそこできるが、定義を暗誦することはやはり難しい。\nまず自由度という表現自体が与える「良い感情」が問題だ。それがファッションであれ、オープンワールドゲームであれ、民主主義であれ、自由度は高いほど、大きいほど良いものと考えられている。さらに新入生が最初に接する自由度は、通常「サンプルの数が$n$だから、そこから$1$を引いた$(n-1)$だけの自由度を持つ」というように計算される。深く考えずに聞いてみれば、サンプルの数も少ないより多い方が良さそうだから、統計学の自由度さえも何か「良し悪しを持つ数値」という認識を持たれるかもしれない。しかし、正確に数式で扱い、探求する文脈で、自由度は単なる数値に過ぎない。\nまた、どんな文脈でもなく、あまりにも突然、さらには頻繁に登場するのも問題だ。分散分析ANOVAや回帰分析を学ぼうとすると、突然$n-1$だとか$n-p-1$だとか、「どう計算されたかの説明があまりにも不足している」自由度がわんさか出てくる。その上で数理統計学を学んでいると、今度は突然t-分布やカイ二乗分布などが自由度だと言い出す。更にF-分布には自由度が二つあるとか言われるが、その意味が正確には把握されずに、なんだか知っているような不思議な気持ちになれる。これが大体2〜3年生の時なんだけど、この頃になって自由度についてわざわざ質問するのも恥ずかしいし、全く知らないわけではないから、なんとなく乗り切ってしまうのが普通だ。\n実際にそれらの数字が必要だと理解することは置いておいても、「自由度」と呼ぶこと自体が一見無意味に見えるまである。それでは、なぜ自由度という言葉が必要かに共感してみよう。\n極端な例：自由度という概念がなかったら？ ある役に立たなそうな概念を説明する際の良い方法の一つは、その概念がなかった場合にどのような「反則」が許されるかを説明することだ。統計量がどうのこうのという数式的な説明は置いておいて、ただ面白い想像をしてみよう。次のようなサンプル$A$が与えられたとしよう。 $$ A = \\left\\{ 13, 7, 17, 3 \\right\\} $$ この場合、サンプルの数は$n = 4$だ。しかし、後輩が自分がサンプルを「発展」させたと言って持ってきたサンプル$B$を見てみよう。 $$ B = \\left\\{ 13, 7, 17, 3 , 14, 8, 18, 4 \\right\\} $$ 後輩はこのサンプルの数が$8$個で、$A$に比べてなんと二倍も多いと言った。ここで止まらず、「自分は好きなだけサンプルを増やせるし、$n \\to \\infty$レベルまで繰り返し可能だから、大きなサンプルで使える全ての統計手法を適用できる」と主張する。しかし、一目見てもこのサンプルは粗雑に偽造されたもので、その方法は単に既存のデータに$1$を足してサンプルの数を増やしただけだ。\nhttp://www.animatedsoftware.com/statglos/sgdegree.htm\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2456,"permalink":"https://freshrimpsushi.github.io/jp/posts/2456/","tags":null,"title":"統計学における自由度"},{"categories":"보조정리","contents":"定義 $$ [a,b] := \\left\\{ x \\in \\mathbb{R} : a \\le x \\le b \\right\\} \\subset \\mathbb{R} $$\n二つの実数$a \\le b$に対して、上のような集合を区間Intervalと言う。 特に、両端$a,b$を含む場合、ブラケット[]を使って$\\left[ a,b \\right]$と書き、クローズドClosedされたと言う。 両端$a,b$を含まない場合、カッコ()を使って$\\left( a,b \\right)$と書き、オープンOpenされたと言う。 2つの端のうち一方のみを含まない場合、クロープンClopenと言い、$a$のみを含む場合は$[a,b)$と書き、$b$のみを含む場合は$(a,b]$と書く。 端が一方しかない場合、つまり無限大の場合は次のような表現を使う。 $$ \\begin{align*} (-\\infty, b) \u0026amp;:= \\left\\{ x \\in \\mathbb{R} : x \\lt b \\right\\} \\subset \\mathbb{R} \\\\ (a, \\infty) \u0026amp;:= \\left\\{ x \\in \\mathbb{R} : a \\lt x \\right\\} \\subset \\mathbb{R} \\end{align*} $$ 説明 区間は、1次元ユークリッド空間$\\mathbb{R}^{1}$で連結性を持つ部分集合として、我々が最もよく知る集合の一つで、理解しやすく、親しみやすく、どんな勉強をしても長く見ることになる。\n数値解析 数値解析のような分野では、$a,b$のようにちょうど二つだけではなく、順序なしに混ざった多くの点を扱う時がある。だから、多くの点の集合$S := \\left\\{ x_{1} , \\cdots , x_{n} \\right\\}$を含む最小の区間を次のように示すことがある。 $$ \\mathscr{H} \\left\\{ x_{1} , \\cdots , x_{n} \\right\\} := \\left[ \\min S , \\max S \\right] $$ 区間を使った演算に興味を持つ分野では、区間算術Interval Arithmeticという分野があると言われている1。\nプログラミング言語 コードを書いていてふと気になって探した面白い記事を見たことがある。2 要するに、プログラミングではなぜ$(0,n]$や$[1,n]$ではなくクロープンインターバル$[0,n)$を使うのかということだが、数学者の観点から同意する内容だけを簡単にまとめて書いてみたいと思った。\nn = 10\rfor i in 0:n\rprint(n) PythonやMATLABなど多くのプログラミング言語では、類似したコードがよく使われていて、大抵の場合、そのコードの実行結果は次のようになる。 (ちなみにこのコードはPythonでもMATLABでもない、仮想の言語だ。)\n0123456789 これが何を意味するかというと、0:nだけ制御するとしたら、それをクロープンインターバル$[0,n)$と考えるということだ。このような考え方、慣習の利点は、ほとんどの言語でインデックスを$0$から使い$i = 0, 1, \\cdots , n-1$まで回すと、その\u0026rsquo;繰り返し回数自体\u0026rsquo;が見事に$n$になるということだ。このような直感的な表記を使うと、誤りが大幅に減り、実用的な習慣になる。\nそして、C言語などでは同じ表現をfor(i=0; i\u0026lt;10; i++)のように書かなければならないが、1から始めて正確に10で終わるfor(i=1; i\u0026lt;=10; i++)と比較すると、オペレータ自体が\u0026lt;から\u0026lt;=へとより汚くなり、実際Cで使う配列にアクセスする時は0も含まれなければならないので、ちょうどこのような繰り返しはバカバカしくfor(i=0; i\u0026lt;=(10-1); i++)のように書かなければならないかもしれない。\nつまり、コーディングを学ぶ人を混乱させたり、1つずれることは、みんなをいじめるためではなく、なんとなくそれなりの理由があるかもしれないということだ。\nhttps://en.wikipedia.org/wiki/Interval_arithmetic\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://nanite.tistory.com/56\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2453,"permalink":"https://freshrimpsushi.github.io/jp/posts/2453/","tags":null,"title":"数学における区間の定義"},{"categories":"데이터과학","contents":"定義 1 量的データが与えられたとしよう。\n全体から$p \\%$より大きく、$(100-p) \\%$より小さい値を**$p$-パーセンタイル**$p$番目のパーセンタイルと言う。 $100$-パーセンタイルと$0$-パーセンタイル（データで最も大きな値と小さな値）をそれぞれ最大値, 最小値と言う。 最大値と最小値の差をデータの範囲範囲$R$と言う。 $25$-パーセンタイルを第1四分位数$Q_{1}$と言い、$75$-パーセンタイルを第3四分位数$Q_{3}$と言う。 $\\left( Q_{3} - Q_{1} \\right)$を四分位範囲四分位範囲$\\text{IQR}$と言う。 最小値、第1四分位数、中央値、第3四分位数、最大値の5つの統計量を五数要約Fiveと言う。 $$ \\min \\qquad Q_{1} \\qquad \\text{median} \\qquad Q_{3} \\qquad \\max $$ 経験上、以下の範囲を超えたデータを外れ値Outlierとも呼ぶ。 $$ \\left[ Q_{1} - 1.5 \\text{IQR} , Q_{3} + 1.5 \\text{IQR} \\right] $$ この区間の下限をロワーフェンス下限、上限をアッパーフェンス上限と言う。 説明 第2四分位数 $50$-パーセンタイル、つまり第2四分位数は中央値そのものであるため、五数要約を語る上で別途定義する必要はない。これらの要約は十分なデータがある時に、その数字だけでデータの分布を大まかに推測できるように助けてくれ、どんなデータを見ても最初に確認すべきものだ。\n外れ値 外れ値Outlierは文字通り外OutにあるものLierという意味で、一般的なデータの範囲から外れていたためにそう呼ばれる。$Q_{1} - 1.5 \\text{IQR}$はかなり小さい値で、$Q_{3} + 1.5 \\text{IQR}$はかなり大きな値だが、それらが期待される範囲から外れているため外れ値と呼ばれる。これは「経験的」や「一般的なデータ」という表現を使用しているため、数学的に厳密な定義ではないことに注意しよう。\nMendenhall. (2012). 確率統計入門 (13版): p76, 60, 78~80.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2452,"permalink":"https://freshrimpsushi.github.io/jp/posts/2452/","tags":null,"title":"パーセンタイルと外れ値"},{"categories":"보조정리","contents":"定義 幾何学的な定義 平面で与えられた一点から距離$r \u0026gt; 0$だけ離れた点の集合を円Circleと定義する。 円の周$l$と直径$2r$の比を円周率$\\pi$と定義する。 $$ \\pi := {{ l } \\over { 2r }} $$ 解析学的定義 1 $$ E (z) := \\sum_{k=0}^{\\infty} {{ z^{k} } \\over { k! }} $$ 複素関数$E : \\mathbb{C} \\to \\mathbb{C}$を上記のように指数関数の級数展開として定義し、それによりコサイン関数に類似した次の関数$C$を定義しよう。 $$ C(x) := {{ E (ix) + E(-ix) } \\over { 2 }} $$ $C(x)$の根、すなわち$C(x) = 0$を満たす解の中で最も小さい正数を$x_{0}$とするとき、その倍数を円周率$\\pi$と定義する。 $$ \\pi := 2 x_{0} $$\n説明 このポストでは、幾何学的な（簡単な）定義と解析学的な（難しい）定義を紹介したが、大学3年生以上の数学の学生なら、解析学的な定義を見て微笑むことができるだろう。\n人類の歴史において、円周率は非常に重要な定数として、遅くとも車輪が発明された時点で、その具体的な値が実用的に使用されるようになった。特に、効率的で精密な近似値としては $$ {{ 22 } \\over { 7 }} = 3.142857 \\cdots \\approx \\pi $$ といった数値も知られていた。この値は、いわゆるゆとり教育時代ゆとり世代の20世紀末の日本の教育水準をはるかに超えるほど正確であった。（教育をゆったりと行うという名目で円周率を$3$と教えていた時期だった） 2\n関連項目 円周率が無理数であることの証明 Walter Rudin, Principles of Mathmatical Analysis (3rd Edition, 1976): p178~183.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.joongang.co.kr/article/2572535\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2451,"permalink":"https://freshrimpsushi.github.io/jp/posts/2451/","tags":null,"title":"円周率の定義"},{"categories":"위상데이터분석","contents":"概要 ZomorodianとCarlssonの論文「Computing Persistent Homology」で紹介されたアルゴリズムの擬似コードを説明し、実装する。1 抽象的な単体複体で作られたフィルター付き複体を受け取り、$\\mathcal{P}$-インターバルをリターンし、コンピュータで扱いにくい永続的なモジュールの構築を省略し、行列のリダクションによって永続的なホモロジーを計算する。さらに、実際の実装では行列演算さえも使用しない。\n導出 Zomorodianのアルゴリズムの導出: アルゴリズムの理論的な内容を完全に無視すると、擬似コードをいくら見てもアルゴリズムを理解することはできないだろう。完全に理解する程度でなくても、なぜ突然行列がなくなったのか、なぜマーキングのようなものが必要なのか、少なくともその程度は理解できるように勉強してから実装に取り組むべきである。 アルゴリズム アルゴリズムが実行される前にフィルター付き複体をデータとして受け取ったとする。\nデータを保存し、アルゴリズムから得られた情報を記録するためのディクショナリやテーブル$T$を上記のように作成する。例えばJuliaのデータフレームでは、データ内の数字をepsilon、アルファベットが記された部分をsimplexに転写し、マーキングの有無を保存するブーリアンカラムmarked、チェインコンプレックスのチェインを保存するslot、計算過程で出てくる整数を保存するカラムJを追加する。\nJuliaは配列が$0$ではなく$1$から始まる言語であり、実装の便宜上インデックスを気にしない部分があるため、上のスクリーンショットのように数字が$1~2$ずつすべて異なる場合があるが、これは全く重要ではないので気にしないでほしい。 注意すべきは、slotが集合でありながらもチェインとして両方の表現を自由に行き来することである。例えば、$\\mathbb{Z}_{2}$上での計算で $$ a + (a - b) = 2 a - b = b \\pmod{2} $$ のような計算が行われるが、これは次のような集合演算 $$ \\left\\{ a \\right\\} \\cup \\left\\{ a, -b \\right\\} = \\left\\{ b \\right\\} $$ とも同じことである。代数的な演算での要素$0$は集合では「ないもの」として扱われ、それをそのまま受け入れなければならない。厳密で慎重な表記を好む人には不快かもしれないが、そこまで無理なことではないので、そのまま受け入れよう。 また、元の論文では一般的なフィールド$F$上でアルゴリズムを導出しており、すなわちすべての$q \\in F$の逆元$q^{-1} \\in F$が存在して $$ d = d - q^{-1} T[i] $$ のような計算を行うが、この実装ではバイナリフィールド$\\mathbb{Z}_{2}$で十分なので、$q^{-1}$を別途計算せずに$A \\Delta B := \\left( A \\cup B \\right) \\setminus \\left( A \\cap B \\right)$と定義された$\\Delta$に対して次のように代替した。 $$ d = d \\Delta T[i] $$ $\\deg$という表現が（プログラムの）関数としても出てくるし、インデックスとしても出てくるし、多項関数の次数としても出てくるし、非常に頻繁に出てくるので、$T$ではdegではなくepsilonと表記した。実際、トポロジカルデータアナリシスの単純なレベルでは、通常このカラムの値である半径$\\varepsilon \u0026gt; 0$が大きくなるにつれてフィルター付き複体を構成することになるためである。 $T$はシンプレックスの次元に従って完全にソートされていると仮定し、それに従ってepsilonも部分順序を持つことを期待する。 擬似コード $\\left\\{ L_{k} \\right\\}$ = COMPUTEINTERVALS$(K)$\nInput: フィルター付き複体$K$を受け取る。フィルター付き複体には、少なくともどのタイミング$\\deg \\in \\mathbb{Z}$にどのシンプレックス$\\sigma$が追加されたかについての情報が必要である。 Output: $k = 0, \\cdots , \\dim K$に対する$\\mathcal{P}$-インターバルの集合$L_{k}$の集合$\\left\\{ L_{k} \\right\\}_{k=0}^{\\dim K}$を得る。 Side Effect: データが記録されたテーブル$T$のmarkedを変更する。 $d$ = REMOVEPIVOTROWS$(\\sigma)$\nInput: $k$次元のシンプレックス$\\sigma$を受け取る。 Output: $(k-1)$次元のチェイン、つまり$k$次元のシンプレックス同士を演算したある$\\mathsf{C}_{k-1}$の要素を得る。 $i$ = maxindex$d$\nInput: チェイン$d$を受け取る。 Output: テーブル$T$でチェイン$d$に含まれるすべてのsimplexの中で最大のインデックス$i$を返す。例えばmaxindex(abc)の場合はabの$5$、bcの$6$、acの$9$の中で最大の$9$を返す必要がある。 $k$ = dim$d$\nInput: $k$次元のチェイン$d$を受け取る。 Output: 整数$k$を返す。 $k$ = dim$\\sigma$\nInput: $k$次元のシンプレックス$\\sigma$を受け取る。 Output: 整数$k$を返す。 $k$ = deg$(\\sigma)$\nInput: シンプレックス$\\sigma$を受け取る。 Output: テーブル$T$でシンプレックス$\\sigma$に対応する整数epsilonを返す。例えばdeg(cd)の場合はcdのepsilonが$2$なので$2$を返す必要がある。 キーワード\nMarkはMark$\\sigma$の形で書かれ、該当するシンプレックス$\\sigma$のmarkedをtrueに変更する。 StoreはStore$j$ and $d$ in $T[i]$の形で書かれ、$T[i]$のJに整数$j$、slotにチェイン$d$を保存する。 RemoveはRemove$x$ in $d$の形で書かれ、チェイン$d$にある$x$項を削除する。 $\\sigma^{i}$はテーブル$T$で$i$番目にあるsimplexであり、$m$は$T$の長さである。\nfunction COMPUTEINTERVALS$(K)$\n# 初期化\nfor $k \\in 0:\\dim K$\n$L_{k} := \\emptyset$\nend for\nfor $j \\in 0:(m-1)$\n$d$ = REMOVEPIVOTROWS$\\left( \\sigma^{j} \\right)$\nif $d = \\emptyset$\n# $d$が空であることは、（ピボットではない）ゼロ列の候補である\nmark $\\sigma^{j}$\nelse\n# $d$の次元を計算する必要があるため、すべての項のmaxでなければならない\n# $d$は$\\sigma^{j}$より一次元低いチェインであり、$i \u0026lt; j$しかあり得ない\n$i$ = maxindex$d$\n$k$ = dim$d$\n$L_{k}$ = $L_{k} \\cup \\left\\{ \\left( \\deg \\sigma^{i}, \\deg \\sigma^{j} \\right) \\right\\}$\nend if\nend for\nfor $j \\in 0:(m-1)$\n# まだマークされていない場合は、明らかにゼロ列である\nif $\\sigma^{j}$ ismarked and $T[j]$ isempty\n$k$ = dim$d$\n# $H_{k-1}$から$\\sum^{\\hat{e}_{i}} F[t]$に該当、無限大処理\n$L_{k}$ = $L_{k} \\cup \\left\\{ \\left( \\deg \\sigma^{i}, \\infty \\right) \\right\\}$\nend if\nend for\nreturn $\\left\\{ L_{k} \\right\\}$\nend function\nfunction REMOVEPIVOTROWS$(\\sigma)$\n$k$ = $\\dim \\sigma$\n# $\\partial abc = ab - bc + ca$とすると∂(\u0026quot;abc\u0026quot;) = [\u0026quot;ab\u0026quot;, \u0026quot;bc\u0026quot;, \u0026quot;ca\u0026quot;]\n$d$ = $\\partial_{k} \\sigma$\nRemove not marked $(k-1)$-dimensional simplex in $d$\nwhile $d \\ne \\emptyset$\n$i$ = maxindex$d$\nif $T[i]$ isempty\nbreak\nend if\n# $\\mathbb{Z}_{2}$なので、symdiff（対称差）で代用\n$d$ = $d \\Delta T[i]$\nend while\nreturn $d$\nend function\n実装 与えられた例でのアルゴリズムの実行結果は次のようになるべきである。 $$ \\begin{align*} L_{0} =\u0026amp; \\left\\{ \\left( 0, \\infty \\right) , \\left( 0,1 \\right) , \\left( 1,1 \\right) , \\left( 1,2 \\right) \\right\\} \\\\ L_{1} =\u0026amp; \\left\\{ \\left( 2,5 \\right) , \\left( 3,4 \\right) \\right\\} \\end{align*} $$\nJuliaで実装した結果は次のようになる。インデックスが正確に異なる部分を除けば、正しく実装されていることが確認できる。\n全体のコード 読めばわかるが、元の論文のノーテーションをほぼそのままにしてコードを書いた。例えば $$ L_{k} = L_{k} \\cup \\left\\{ \\left( \\deg \\sigma^{i}, \\deg \\sigma^{j} \\right) \\right\\} $$ のJulia風のコードはpush!(L_[k], (deg(σⁱ), deg(σʲ)))であるが、論文とほぼ同じように見えるようにL_[k] = L_[k] ∪ [(deg(σⁱ), deg(σʲ))]として実装した。\nusing DataFrames\rdata = DataFrame([\r0 \u0026#34;a\u0026#34;\r0 \u0026#34;b\u0026#34;\r1 \u0026#34;c\u0026#34;\r1 \u0026#34;d\u0026#34;\r1 \u0026#34;ab\u0026#34;\r1 \u0026#34;bc\u0026#34;\r2 \u0026#34;cd\u0026#34;\r2 \u0026#34;ad\u0026#34;\r3 \u0026#34;ac\u0026#34;\r4 \u0026#34;abc\u0026#34;\r5 \u0026#34;acd\u0026#34;\r], [\u0026#34;epsilon\u0026#34;, \u0026#34;simplex\u0026#34;])\rT = copy(data)\rT[!, :\u0026#34;marked\u0026#34;] .= false\rT[!, :\u0026#34;slot\u0026#34;] .= [[]]\rT[!, :\u0026#34;J\u0026#34;] .= 0\rdimK = 2\rm = nrow(T)\rtest_σ = \u0026#34;abc\u0026#34;\rdim(σ) = length(σ)\rfunction deg(σ)\rreturn T.epsilon[findfirst(T.simplex .== σ)]\rend\rdeg(test_σ)\rfunction ∂(σ)\rk = dim(σ)\rreturn [σ[(1:k)[Not(t)]] for t = 1:k]\rend\r∂(test_σ)\rfunction maxindex(chain)\rreturn (T.simplex .∈ Ref(chain)) |\u0026gt; findall |\u0026gt; maximum\rend\rmaxindex(∂(test_σ))\rfunction REMOVEPIVOTROWS(σ)\rk = dim(σ); d = ∂(σ)\rd = d[d .∈ Ref(T[T.marked,:simplex])] # Remove unmarked terms in ▷eq029◁\rwhile !(d |\u0026gt; isempty)\ri = maxindex(d)\rif T[i,:slot] |\u0026gt; isempty break end\rd = symdiff(d, T[i,:slot])\r# print(\u0026#34;d in empty\u0026#34;)\rend\rreturn d\rend\rREMOVEPIVOTROWS(test_σ)\rL_ = [[] for k = 0:dimK]\rfor j0 = 0:(m-1)\rj = j0+1\rσʲ = T[j,:simplex]\rd = REMOVEPIVOTROWS(σʲ)\rif d |\u0026gt; isempty\rT[j,:marked] = true\relse\ri = maxindex(d); k = dim(σʲ)\rσⁱ = T[i,:simplex]\rT[i,[:J,:slot]] = j0,d\rL_[k] = L_[k] ∪ [(deg(σⁱ), deg(σʲ))]\rend\rend\rfor j0 = 0:(m-1)\rj = j0+1\rσʲ = T[j,:simplex]\rif (T[j,:marked]) \u0026amp;\u0026amp; (T[j,:slot] |\u0026gt; isempty) \u0026amp;\u0026amp; (T[j,:J] |\u0026gt; iszero)\rk = dim(σʲ); L_[k] = L_[k] ∪ [(deg(σʲ), Inf)]\rprint(\u0026#34;j: $j\u0026#34;)\rend\rend Zomorodian. (2005). Computing Persistent Homology: ch4\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2449,"permalink":"https://freshrimpsushi.github.io/jp/posts/2449/","tags":null,"title":"ジョモロジアンのアルゴリズムの実装"},{"categories":"데이터과학","contents":"定義 1 $n$個の量的データが与えられたとしよう。\n標本平均$\\overline{x}$とデータの差$\\left( \\overline{x} - x_{i} \\right)$を偏差Deviationという。 偏差の二乗の和を$n-1$で割った値$s^{2}$を標本分散Variance of a Sampleと呼ぶ。 $$ s^{2} := {{ \\sum \\left( x_{i} - \\overline{x} \\right)^{2} } \\over { n-1 }} $$ 標本分散の平方根$s = \\sqrt{s^{2}}$を標準偏差Standard Deviationと言う。 説明 分散度はデータがどれだけ広がっているかを示す量で、可変性Variabilityや拡散Dispersionとも呼ばれる。分散はその分散度の尺度Measure of Variabilityとして、平均に次いで重要な統計量である。\n参照 統計学を初めて学ぶと、なぜわざわざ二乗をして、$n$ではなく$n-1$で割るのかなど、面倒に感じる点が多い。統計学を専攻し、学年が上がるにつれて（決して簡単ではない）数理的理論を学んでそれらの質問に答えることができるようになる。新入生であれば、とりあえずそのまま受け入れてもいい。\n標本分散を$n-1$で割る理由 代表値の数理的性質：平均は分散を最小化する性質を持っている。 統計学における分散の数理的定義 Mendenhall. (2012). Introduction to Probability and Statistics (13th Edition): p60~63。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2448,"permalink":"https://freshrimpsushi.github.io/jp/posts/2448/","tags":null,"title":"基礎統計学における分散の定義"},{"categories":"위상데이터분석","contents":"概要 ZomorodianとCarlssonの論文「Computing Persistent Homology」で紹介されたアルゴリズムの導出プロセスを説明する1。抽象的なシンプレクシャルコンプレックスで作られたフィルタードコンプレックスを受け取り、$\\mathcal{P}$-インターバルを返す。計算機で扱いにくいパーシステントモジュールの構築を省略し、行列のリダクションにより持続的ホモロジーを計算する。\n導出 Part 0. 事前調査\nアルゴリズムの本格的な導出に先立ち、上の図で描写されるパーシステンスコンプレックスが数式的にどのような形式であるかをまず検討する。このプロセスをしっかりと固めておかないと、論文を読むのが非常に苦痛になるだろう。\nまず、下部にある数字を$\\deg$とし、これが$0$から$5$まで増加しながら次のようにフィルタードコンプレックスを形成する。 $$ \\left\\{ a,b \\right\\} = K_{0} \\subset K_{1} \\subset K_{2} \\subset K_{3} \\subset K_{4} \\subset \\left( K_{4} \\cup \\left\\{ acd \\right\\} \\right) = K_{5} $$ $\\deg$とは関係なく、$K$は$2$-シンプレックスとして、ホモロジーを考慮する文脈で次のようなチェインコンプレックスを形成する。 $$ \\mathsf{C}_{2} \\overset{\\partial_{2}}{\\longrightarrow} \\mathsf{C}_{1} \\overset{\\partial_{1}}{\\longrightarrow} \\mathsf{C}_{0} $$ アルゴリズムの目標は、このような$\\partial_{2}$と$\\partial_{1}$がデータに与える代数的トポロジカル情報、たとえばベッチ数$\\beta_{k}$などが、どの$\\deg$で現れていつ$\\deg$で消えるかを次のように計算することである。 $$ \\begin{align*} L_{0} =\u0026amp; \\left\\{ \\left( 0, \\infty \\right) , \\left( 0,1 \\right) , \\left( 1,1 \\right) , \\left( 1,2 \\right) \\right\\} \\\\ L_{1} =\u0026amp; \\left\\{ \\left( 2,5 \\right) , \\left( 3,4 \\right) \\right\\} \\end{align*} $$\n$L_{0}$は$\\beta_{0}$に該当する情報、つまりコンポーネントがいつ現れて消えるかを示す$\\mathcal{P}$-インターバルで構成され、$L_{1}$は$\\beta_{1}$に該当する情報、つまり空間で「穴」と呼べるものがいつ現れて消える$\\mathcal{P}$-インターバルで構成されている。\nPart 1. $\\partial_{1}$\n論文では、著者たちはその計算が全てのフィールドで可能であると主張しているが、簡単にグレード付きモジュールである$\\mathbb{Z}_{2} [t]$-モジュールでどのような計算が行われるかを見てみよう。これから$\\mathsf{C}_{k}$の同次基底を$\\left\\{ e_{j} \\right\\}$、$\\mathsf{C}_{k-1}$の同次基底を$\\left\\{ \\hat{e}_{i} \\right\\}$と表記することにする。ここで同次とは、$\\mathsf{C}_{k}$をグレード付きモジュールと見たとき、項が一つしかないという意味で受け取ってもよく、つまり$t^{2} + t$のようではなく$t^{4}$のような単項形式であると考えても構わないということである。\n$$ \\deg M_{k} (i,j) = \\deg e_{j} - \\deg \\hat{e}_{i} $$ ホモロジー代数にある程度慣れていれば、今度は$\\partial_{k}$に対応する境界行列$M_{k}$を上のテーブルと方程式に合わせて構成し、そのスミス標準形$\\tilde{M}_{1}$を求めに行く感じがするだろう。まず$k=1$の場合を考えてみると、先に行列の基底が同次であると言ったので、次のように唯一の$M_{1}$を得ることができる。\nこのように基底を持って行列を構成することは、$\\partial_{k}$の一つの役割が$t^{n}$を掛けること（群作用を取ることでグレード付きモジュールで次数が上がること）の逆を行うことだと見ると理解できる。感覚を掴むために、直接計算してみよう。 $$ \\begin{align*} \\deg M_{1} (2,5) =\u0026amp; \\deg ac - \\deg c = 3 - 1 = 2 = \\deg t^{2} \\\\ \\deg M_{1} (4,5) =\u0026amp; \\deg ac - \\deg a = 3 - 0 = 3 = \\deg t^{3} \\\\ \\deg M_{1} (2,2) =\u0026amp; \\deg bc - \\deg c = 1 - 1 = 0 = \\deg t^{0} = \\deg 1 \\end{align*} $$\n先に言ったように、今度はこのエシュロン形、特にカラムエシュロン形を作\nると次のようになる。\n大学で学んだ線形代数を思い出してみると、各カラムで一番上にありながら$0$でない、図のように四角で囲んだ部分のようなものをピボットと呼んでいた。ここで次の2つの補助定理を紹介する。\n(1): カラムエシュロン形の対角成分はスミス標準形の対角成分と同じである。 (2): $\\tilde{M}_{k}$の$i$行のピボットが$\\tilde{M}_{k} (i,j) = t^{n}$であればホモロジーグループ$H_{k-1}$の$\\sum^{\\deg \\hat{e}_{i}} F[t] / t^{n}$に該当し、それ以外は$H_{k-1}$の$\\sum^{\\deg \\hat{e}_{i}} F[t]$に該当する。これは$L_{k-1}$が$\\left( \\deg \\hat{e}_{i} , \\deg \\hat{e}_{i} + n \\right)$と$\\left( \\deg \\hat{e}_{i} , \\infty \\right)$で構成されることと同値である。 つまり、\n補助定理(1)により、持続的ホモロジーを計算する際は行操作が必要なく、列操作だけで良いことになる。 補助定理(2)により、$L_{k-1}$は$\\left( \\deg \\hat{e}_{i} , \\deg \\hat{e}_{i} + n \\right)$と$\\left( \\deg \\hat{e}_{i} , \\infty \\right)$で構成される。 最初の行のピボットが$t^{1}$であり、$\\deg d = 1$であるため、$(1,1+1)$を得る。 2番目の行のピボットが$t^{0}$であり、$\\deg c = 1$であるため、$(1,1+0)$を得る。 3番目の行のピボットが$t^{1}$であり、$\\deg b = 0$であるため、$(0,0+1)$を得る。 4番目の行にピボットがなく、$\\deg a = 0$であるため、$(0,\\infty)$を得る。 これは、アルゴリズムを導出する前に言及した$L_{0}$と完全に一致する。 $$ L_{0} = \\left\\{ \\left( 0, \\infty \\right) , \\left( 0,1 \\right) , \\left( 1,1 \\right) , \\left( 1,2 \\right) \\right\\} $$\nPart 2. $\\partial_{2}$\n$L_{1}$を得るための$\\partial_{2}$の行列形$M_{2}$は上のようである。しかし、次の補助定理で計算を減らし、簡単に進めることができる。\n(3): $\\mathsf{C}_{k+1}$の標準基底と$\\mathsf{Z}_{k}$に対する$\\partial_{k+1}$を表現するためには、$\\tilde{M}_{k}$に対応する行を$M_{k+1}$から単に削除しても良い。 言葉は少し難しく聞こえるが、現在の具体的な状況では、$\\tilde{M}_{1}$の$1$-シンプレックス$ab,bc,cd,ad,ac$の中で、$cd,bc,ab$のピボットだけが残っているので、これを単に$M_{2}$から削除しても良いということである。直感的に考えると、これらがすでに$k$次元で使用されたので、$k+1$では見る必要もないという程度に受け取っても構わない。こうしてカラムエシュロン形$\\tilde{M}_{2}$を直接構築する過程を省略し、その3つの行を削除してみると、次のように下が切り取られた$\\check{M}_{2}$を得る。\n$$ \\begin{align*} z_{2} =\u0026amp; ac - bc - ab \\\\ z_{1} =\u0026amp; ad - bc - cd - ab \\end{align*} $$\n再び補助定理(2)に従って計算してみよう。\n最初の行のピボットが$t^{1}$であり、 $$ \\deg z_{2} = \\deg \\left( ac - bc - ab \\right) = \\max \\deg \\left\\{ ac , bc , ab \\right\\} = 3 $$ であるため、$(3,3+1)$を得る。 2番目の行のピボットが$t^{3}$であり、 $$ \\deg z_{1} = \\deg \\left( ad - bc - cd - ab \\right) = \\max \\deg \\left\\{ ad , bc , cd , ab \\right\\} = 2 $$ であるため、$(2,2+3)$を得る。 これは、アルゴリズムを導出する前に言及した$L_{1}$と完全に一致する。 $$ L_{1} = \\left\\{ \\left( 2,5 \\right) , \\left( 3,4 \\right) \\right\\} $$\nこのようなプロセスをコンプレックス$K$の次元$\\dim K$まで繰り返すと、求めていたアルゴリズムを得ることができる。行列の左右の大きさは$\\partial_{k}$に従い、その成分は$\\deg$に従って充填されると考えると少し混乱しなくなるだろう。\n■\n一方、補助定理(1)で列操作だけで十分だということは、これまでの導出で見たように行列表現に固執する理由がないということでもある。また、補助定理(3)により、「過去にすでに計算が終わった」部分に対して大胆に行を捨てるような効率的なプロシージャが含まれており、これにはピボットでないカラムを「マーキング」する能力などが必要である。結果として、実際のアルゴリズムの擬似コードPseudo Codeは、行列をそのまま使用するのではなく、もう少し高度なデータ型、ディクショナリーやデータフレームなどで説明されることになる。これは実際に体験すると非常に戸惑いやすく難しい。\n実装 Zomorodianのアルゴリズム実装: 科学界で働くなら誰でも読みやすいJulia言語を通じて、論文の擬似コードをほぼ文学的に翻訳した実装を紹介する。 Zomorodian. (2005). Computing Persistent Homology: ch4\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2447,"permalink":"https://freshrimpsushi.github.io/jp/posts/2447/","tags":null,"title":"ジョモロジアンのアルゴリズム誘導"},{"categories":"데이터과학","contents":"定義 1 質的データが与えられた場合、最も度数が高いカテゴリーを最頻値Modeという。量的データの場合、最も度数が高い階級を最頻階級Modal Classという。\n説明 文字通り「最も頻繁な値」という意味だ。最頻値はその一つを除いて他の情報をすべて捨てることになり、質的データでなければその意味が大きく薄れ、実際のデータを扱う時にそれほど重要ではない。確率分布で見た場合、確率密度関数で最大値になる点が最頻値になる。\n一緒に見る 統計学の三つの代表値：最頻値、中央値、平均 平均 中央値 最頻値 代表値の数理的性質: 最頻値はハミングロスHamming Lossを最小化する性質を持つ。 Mendenhall. (2012). Introduction to Probability and Statistics (13th Edition): p57.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2446,"permalink":"https://freshrimpsushi.github.io/jp/posts/2446/","tags":null,"title":"基礎統計学における最頻値の定義"},{"categories":"데이터과학","contents":"定義 1 $n$個の量的データが大きさ順に与えられたとき、全データの中央に位置する値を中央値またはメディアンMedian $m$という。$n$が奇数なら$m := x_{(n+1)/2}$を用い、$n$が偶数なら次を満たすすべての$m$が中央値である。 $$ x_{1} \\le \\cdots \\le x_{ \\lceil {{ n+1 } \\over { 2 }} \\rceil } \\le m \\le x_{ \\lceil {{ n+1 } \\over { 2 }} \\rceil + 1} \\le \\cdots \\le x_{n} $$\nここで、$\\lceil \\cdot \\rceil : \\mathbb{R} \\to \\mathbb{Z}$は天井関数だ。\n説明 中央値はデータの中心を示す尺度Measure of Centerとして、平均値と比較して外れ値Outlierに対して敏感ではない特性があり、唯一性が保証されない。定義で言及したように、標本の数が偶数の場合は無限に多くの中央値が存在するが、数学的な概念で無限に多いだけで、実際には以下のように単一で定める。 $$ m := \\left( x_{\\lceil {{ n+1 } \\over { 2 }} \\rceil} + x_{ \\lceil {{ n+1 } \\over { 2 }} \\rceil + 1} \\right) / 2 $$\n例えば、与えられたデータが $$ 1,2,5,8,9 $$ の場合、標本の数が奇数なので真ん中に位置する$m = 5$が中央値であり、 $$ 1,2,2,4,7,81 $$ の場合は$2 \\le m \\le 4$全てが中央値だが、厳密には$m = (2+4)/2 = 3$として扱う。ここで、$81$のような大きな外れ値のせいで平均は$16.16$に跳ね上がるが、中央値はその影響を受けないことが確認できる。\n関連項目 統計学の三つの代表値: 最頻値、中央値、平均値 平均値 中央値 最頻値 代表値の数理的性質: 中央値は偏差の和を最小化する性質を持っている。 Mendenhall. (2012). Introduction to Probability and Statistics (13th Edition): p55.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2444,"permalink":"https://freshrimpsushi.github.io/jp/posts/2444/","tags":null,"title":"基礎統計学における中央値の定義"},{"categories":"확률론","contents":"概要 測度論と確率論を学んだ人向けの定義と概念の要約資料です。迅速な復習と定義の参照のために作成されました。\n測度論 代数 $X \\ne \\varnothing$の部分集合たちのコレクション $\\mathcal{A}$が 有限 合集合と補集合に対して閉じている時、これを代数と言います。\n可算合集合に対して閉じている代数を$\\sigma$-代数と言います。\nNote:\n定義により $\\mathcal{A}$はまた交集合に対しても閉じています $\\big( \\because E_{1} \\cap E_{2} = \\left( E_{1} \\cup E_{2} \\right)^{c} \\in \\mathcal{A}$ for $E_{1}, E_{2} \\in \\mathcal{A} \\big)$ $\\mathcal{A}$は空集合 $\\varnothing$と全集合 $X$を含みます。 $\\big( \\because E \\in \\mathcal{A}$ $\\implies$ $\\varnothing = E \\cap E^{c} \\in \\mathcal{A} \\text{ and } X = E \\cup E^{c} \\in \\mathcal{A} \\big)$ $X$が位相空間なら、$X$の開集合たちのコレクションから作られる$\\sigma$-代数を$X$上のボレル $\\sigma$-代数と言い、$\\mathcal{B}_{X}$と表記します。\nボレル $\\sigma$-代数は全ての開集合を含む最も小さい唯一の$\\sigma$-代数です。 $\\mathcal{E}$を$X$上の$\\sigma$-代数としましょう。順序対 $(X, \\mathcal{E})$を可測空間と言い、$E \\in \\mathcal{E}$を可測集合と言います。\n特に言及がない限り、以下では固定された可測空間 $(X, \\mathcal{E})$について扱います。\n可測関数 全ての実数 $\\alpha \\in \\mathbb{R}$に対して、次を満たす関数 $f : X \\to \\mathbb{R}$を($\\mathcal{E}$-)可測と言います。 $$ \\left\\{ x \\in X : f(x) \\gt \\alpha \\right\\} \\in \\mathcal{E}\\qquad \\forall \\alpha \\in \\mathbb{R}. $$\n一般化 $(X, \\mathcal{E})$、$(Y, \\mathcal{F})$を可測空間とします。関数 $f : X \\to Y$が次を満たす時、これを$(\\mathcal{E}, \\mathcal{F})$-可測と言います。 $$ f^{-1}(F) = \\left\\{ x \\in X : f(x) \\in F \\right\\} \\in \\mathcal{E}\\qquad \\forall F \\in \\mathcal{F}. $$\nNote: $\\mathcal{E}$-可測関数は上の定義で$(Y, \\mathcal{F}) = (\\mathbb{R}, \\mathcal{B}_{\\mathbb{R}})$の場合と同じです。\n測度 $\\mathcal{E}$ (または $(X, \\mathcal{E})$、$X$)上の測度とは、次を満たす関数 $\\mu : \\mathcal{E} \\to [0, \\infty]$です。\nNull empty set: $\\mu (\\varnothing) = 0$。 Countable additivity: $\\left\\{ E_{j} \\right\\}$が$\\mathcal{E}$の互いに素な集合たちなら、$\\displaystyle \\mu \\left( \\bigcup\\limits_{j} E_{j} \\right) = \\sum\\limits_{j} \\mu (E_{j})$。 三つ組 $(X, \\mathcal{E}, \\mu)$を測度空間と言います。特に言及がない限り以下では固定された測度空間 $(X, \\mathcal{E}, \\mu)$について扱います。\nボレル測度とは、定義域がボレル $\\sigma$-代数$\\mathcal{B}_{\\mathbb{R}}$の測度を言います： $$ \\mu : \\mathcal{B}_{\\mathbb{R}} \\to [0, \\infty] $$\n$(X, \\mathcal{E})$、$(Y, \\mathcal{F})$上の二つの測度 $\\mu$、$\\nu$に対して、次を満たす$\\mathcal{E} \\times \\mathcal{F}$上の唯一の測度 $\\mu \\times \\nu$を$\\mu$と$\\nu$の積測度と言います。 $$ \\mu \\times \\nu (E \\times F) = \\mu (E) \\nu (F)\\qquad \\text{ for all rectangles } E \\times F. $$\n積分 実関数 $f$が有限な関数値を持つ時、これを単純と言います。\n単純可測関数 $\\varphi$は次のような形で表されます。 $$ \\begin{equation} \\varphi = \\sum\\limits_{j=1}^{n} a_{j}\\chi_{E_{j}}, \\text{ where } E_{j} = \\varphi^{-1}(\\left\\{ a_{j} \\right\\}) \\text{ and } \\operatorname{range} (\\varphi) = \\left\\{ a_{1}, \\dots, a_{n} \\right\\}. \\end{equation} $$ ここで $\\chi_{E_{j}}$は$E_{j}$の特性関数です。これを$\\varphi$のstandard representationと言います。\n$\\varphi$がstandard representation $(1)$を持つ単純可測関数の時、測度 $\\mu$に対する**$\\varphi$の積分**を次のように定義します。 $$ \\int \\varphi d\\mu := \\sum\\limits_{j=1}^{n} a_{j}\\mu (E_{j}). $$ Notation: $$ \\int \\varphi d\\mu = \\int \\varphi = \\int \\varphi(x) d\\mu (x), \\qquad \\int = \\int_{X}. $$\n$f$が$(X, \\mathcal{E})$上の可測関数の時、$\\mu$に対する**$f$の積分**を次のように定義します。 $$ \\int f d\\mu := \\sup \\left\\{ \\int \\varphi d\\mu : 0 \\le \\varphi \\le f, \\varphi \\text{ is simple and measurable} \\right\\}. $$\n$f : X \\to \\mathbb{R}$の正の部分と負の部分をそれぞれ次のように定義します。 $$ f^{+}(x) := \\max \\left( f(x), 0 \\right)),\\qquad f^{-1}(x) := \\min \\left(-f(x), 0 \\right)). $$ もし二つの積分$\\displaystyle \\int f^{+}$、$\\displaystyle \\int f^{-}$が有限なら、$f$が積分可能と言います。また$\\left| f \\right| = f^{+} - f^{-}$が成立します。\n積分可能な実関数たちの集合はベクトル空間であり、積分はこのベクトル空間上の線形汎関数です。このベクトル空間を次のように表記します。 $$ L = L(X, \\mathcal{E}, \\mu) = L(X, \\mu) = L(X) = L(\\mu), \\qquad L = L^{1} $$\n$L^{p}$空間\n測度空間$(X, \\mathcal{E}, \\mu)$と$0 \\lt p \\lt \\infty$に対して、$L^{p}$を次のように定義します。 $$ L^{p}(X, \\mathcal{E}, \\mu) := \\left\\{ f : X \\to \\mathbb{R} \\left| f \\text{ is measurable and } \\left( \\int \\left| f \\right|^{p} d\\mu \\right)^{1/p} \\lt \\infty \\right. \\right\\}. $$\n確率論 表記法と用語 $$ \\begin{array}{lll} \\text{Analysts\u0026rsquo; Term} \u0026amp;\u0026amp; \\text{Probabilists\u0026rsquo; Term} \\\\ \\hline \\text{Measure space } (X, \\mathcal{E}, \\mu) \\text{ such that } \\mu (X) = 1 \u0026amp;\u0026amp; \\text{Probability space } (\\Omega, \\mathcal{F}, P) \\\\ \\text{Measure } \\mu : \\mathcal{E} \\to \\mathbb{R} \\text{ such that } \\mu (X) = 1 \u0026amp;\u0026amp; \\text{Probability } P : \\mathcal{F} \\to \\mathbb{R} \\\\ (\\sigma\\text{-)algebra $\\mathcal{E}$ on $X$} \u0026amp;\u0026amp; (\\sigma\\text{-)field $\\mathcal{F}$ on $\\Omega$} \\\\ \\text{Mesurable set } E \\in \\mathcal{E} \u0026amp;\u0026amp; \\text{Event } E \\in \\mathcal{F} \\\\ \\text{Measurable real-valued function } f : X \\to \\mathbb{R} \u0026amp;\u0026amp; \\text{Random variable } X : \\Omega \\to \\mathbb{R} \\\\ \\text{Integral of } f, {\\displaystyle \\int f d\\mu} \u0026amp;\u0026amp; \\text{Expextation of } f, E(X) \\\\ f \\text{ is } L^{p} \u0026amp;\u0026amp; X \\text{ has finite $p$th moment} \\\\ \\text{Almost everywhere, a.e.} \u0026amp;\u0026amp; \\text{Almost surely, a.s.} \\end{array} $$\n$$ \\begin{align*} \\left\\{ X \\gt a \\right\\} \u0026amp;:= \\left\\{ w : X(w) \\gt a \\right\\} \\\\ P\\left( X \\gt a \\right) \u0026amp;:= P\\left( \\left\\{ w : X(w) \\gt a \\right\\} \\right) \\end{align*} $$\n基礎定義 可測空間$(\\Omega, \\mathcal{F})$、$(\\mathbb{R}, \\mathcal{B}_{\\mathbb{R}})$に対して、$(\\mathcal{F}, \\mathcal{B}_{\\mathbb{R}})$-可測関数$X : \\Omega \\to \\mathbb{R}$を確率変数と言います。つまり、 $$ X^{-1}(B) \\in \\mathcal{F}\\qquad \\forall B \\in \\mathcal{B}_{\\mathbb{R}}. $$\n$(\\Omega, \\mathcal{F})$上の確率(または確率測度)とは、$P(\\Omega) = 1$を満たす測度$P : \\mathcal{F} \\to \\mathbb{R}$です。\n$X$を確率変数とする時、\n期待値: $\\displaystyle E(X) := \\int X dP$ 分散: $\\sigma^{2}(X) := E\\left[ (X - E(X))^{2} \\right] = E(X^{2}) - E(X)^{2}$ $X$の(確率)分布とは、次を満たす$\\mathbb{R}$上の確率$P_{X} : \\mathcal{B}_{\\mathbb{R}} \\to \\mathbb{R}$です： $$ P_{X}(B) := P(X^{-1}(B)). $$\n$X$の分布関数$F_{X}$は次のように定義されます： $$ F_{X}(a) := P_{X}\\left( (-\\infty, a] \\right) = P(X \\le a). $$\n確率変数の数列$\\left\\{ X_{i} \\right\\}_{i=1}^{n}$に対して、確率ベクトル$(X_{1}, \\dots, X_{n})$は次のように定義される関数を言います： $$ (X_{1}, \\dots, X_{n}) : \\Omega \\to \\mathbb{R}^{n} $$ $$ (X_{1}, \\dots, X_{n})(x) := (X_{1}(x), \\dots, X_{n}(x)). $$\nNote: $(X_{1}, \\dots, X_{n})^{-1}(B_{1} \\times \\cdots \\times B_{n})= X_{1}^{-1}(B_{1}) \\cap \\cdots \\cap X_{n}^{-1}(B_{n})$。\n$n=2$の場合を先に見ましょう。$(X, Y) : \\Omega \\to \\mathbb{R}^{2}$に対して次が成立します。 $$ (X, Y)^{-1} (a, b) = \\left\\{ x \\in \\Omega : X(x) = a \\right\\} \\cap \\left\\{ x \\in \\Omega : Y(x) = b \\right\\}. $$ 従って、全てのボレル集合$B_{1}$、$B_{2} \\in \\mathcal{B}_{\\mathbb{R}}$に対して次を得ます。 $$ (X, Y)^{-1}(B_{1} \\times B_{2}) = (X, Y)^{-1}(B_{1}, B_{2}) = X^{-1}(B_{1}) \\cap Y^{-1}(B_{2}). $$ これを任意の$\\mathbb{R}^{n}$に対して拡張すると、 $$ \\begin{equation} \\begin{aligned} (X_{1}, \\dots, X_{n})^{-1}(B_{1} \\times \\cdots \\times B_{n}) \u0026amp;= (X_{1}, \\dots, X_{n})^{-1}(B_{1}, \\dots, B_{n}) \\\\ \u0026amp;= X_{1}^{-1}(B_{1}) \\cap \\cdots \\cap X_{n}^{-1}(B_{n}). \\end{aligned} \\end{equation} $$\n$X_{1}, \\dots, X_{n}$の結合分布とは確率ベクトル$(X_{1}, \\dots, X_{n})$の確率分布で定義されます： $$ P_{(X_{1}, \\dots, X_{n})} : \\mathcal{B}_{\\mathbb{R}^{n}} \\to \\mathbb{R}, $$ $$ P_{(X_{1}, \\dots, X_{n})}(B_{1} \\times \\cdots \\times B_{n}) := P\\left( (X_{1}, \\dots, X_{n})^{-1}(B_{1} \\times \\cdots \\times B_{n}) \\right). $$\n独立 $P(E) \\gt 0$の事象$E$に対して、$\\Omega$上の確率 $$ P_{E}(F) = P(E|F) := P(E \\cap F)/P(E) $$ を$E$上の条件付き確率と言います。\nもし$P_{E}(F) = P(F)$なら、$F$を$E$と独立と言います： $$ \\text{$F$ is independent of $E$} \\iff P(E \\cap F) = P(E)P(F). $$ 次が成立する時、$\\Omega$の事象たちのコレクション$\\left\\{ E_{j} \\right\\}$が独立と言います： $$ P(E_{1} \\cap \\cdots \\cap E_{n}) = P(E_{1}) P(E_{2}) \\cdots P(E_{n}) = \\prod \\limits_{i=1}^{n} P(E_{j}). $$\n$\\Omega$上の確率変数たちのコレクション$\\left\\{ X_{j} \\right\\}$が独立ということは、全てのボレル集合$B_{j} \\in \\mathcal{B}_{\\mathbb{R}}$に対して事象たち$\\left\\{ X_{j}^{-1}(B_{j}) \\right\\}$が独立ということを言います。つまり次の式が成立することを意味します： $$ P\\left(X_{1}^{-1}(B_{1}) \\cap \\cdots \\cap X_{n}^{-1}(B_{n})\\right) = \\prod \\limits_{j=1}^{n} P(X_{j}^{-1}(B_{j})). $$\n確率分布の定義と$(2)$により、上記式の左辺から次を得ます。 $$ \\begin{align*} P\\left(X_{1}^{-1}(B_{1}) \\cap \\cdots \\cap X_{n}^{-1}(B_{n})\\right) \u0026amp;= P\\left( (X_{1}, \\dots, X_{n})^{-1}(B_{1} \\times \\cdots \\times B_{n}) \\right) \\\\ \u0026amp;= P_{(X_{1}, \\dots, X_{n})} \\left( B_{1} \\times \\cdots \\times B_{n} \\right). \\end{align*} $$ 一方、積測度と確率分布の定義により、右辺から次を得ます。 $$ \\prod \\limits_{j=1}^{n} P(X_{j}^{-1}(B_{j})) = \\prod \\limits_{j=1}^{n} P_{X_{j}}(B_{j}) = \\left( \\prod \\limits_{j=1}^{n} P_{X_{j}} \\right) \\left( B_{1} \\times \\cdots \\times B_{n} \\right). $$ 従って$\\left\\{ X_{j} \\right\\}$が独立なら、 $$ P_{(X_{1}, \\dots, X_{n})} = \\prod\\limits_{j=1}^{n}P_{X_{j}}. $$\n$\\left\\{ X_{j} \\right\\}$が独立な確率変数の集合であることは、$\\left\\{ X_{j} \\right\\}$の結合分布がそれぞれの分布の積と同じであることと同値です。\n参考文献 Robert G. Bartle, The Elements of Integration and Lebesgue Measure (1995) Gerald B. Folland, Real Analysis: Modern Techniques and Their Applications (1999) ","id":3473,"permalink":"https://freshrimpsushi.github.io/jp/posts/3473/","tags":null,"title":"측도론과 확률론 요약 정리"},{"categories":"통계적검정","contents":"定義 1 2 科学で母集団についてのある推測を統計的仮説と言い、仮説を採用または棄却する統計的意思決定プロセスを統計的仮説検定Testing of Statistical Hypothesisという。この過程には2つの競合する仮説があり、主に研究者が支持したい仮説を対立仮説Alternative Hypothesis $H_{1}$ と言い、それに反して対立仮説が真である確かな根拠がない場合に受け入れる仮説を帰無仮説Null Hypothesis $H_{0}$ と言う。仮説検定のための統計量を検定統計量Test Statisticと言う。\n説明 もしお前が統計学を専攻するなら、帰無仮説 $H_{0}$ vs 対立仮説 $H_{1}$ … という永遠に繰り返される戦いに直面するだろう。最初はすごく難しそうに見えるけど、知っていくうちに愛憎交じりあう感情が出てくるから、怖がらずに理解してみよう。\n教科書に出てくる難しい数学の話（棄却域、検定統計量、有意水準等）は一時的に置いておいて、日常的な状況で仮説検定がどのように生じ得るかを想像してみよう。例えば、架空の製薬会社Aが肝機能に役立つ、具体的には一つの肝数値であるASTを下げる新薬aを発売する場面を考えてみよう：\naが市販化されるためには、食品医薬品局のような機関のあるテストをパスしなければならず、その場合は当然aがASTを下げることを証明しなければならない。もちろんその方法は「実際にASTが下がった人を10人以上連れてくる」ような手探りではなく、統計的に意味があるべきだ。 100人の中で10人以上、または「全体の臨床試験参加者の10%以上」が超えるのはどうだろうか？先ほどよりは合理的だが、単純に11〜40が正常なAST数値を500から490程度に下げたとしても、効果があったと見なすかどうかは問題があるかもしれない。 一つの方法はaを継続的に摂取した $1$ グループと摂取していない（プラシーボ）$2$ グループに分けて、それぞれの肝数値の平均を比較することだ。$1$ グループの平均を $\\mu_{1}$ 、$2$ グループの平均を $\\mu_{2}$ とすると、製薬会社Aが望む結果はおそらく次のようになるだろう。 $$ \\mu_{1} \u0026lt; \\mu_{2} $$ 上で紹介された定義に従って、対立仮説は次のように定められる。 $$ H_{1}: \\mu_{1} \u0026lt; \\mu_{2} $$ 式だけを見てもまだ500と490のレベルを比較する問題はあるように見えるが、今は一人や二人の個人ではなく、標本集団という統計について話している。例えば同じく500 vs 490でも分散が200だとしたら、それはたまたまの偶然かもしれない。しかし、分散が2程度に小さいなら、新薬aは明らかにASTを下げたと見える。[ 注：二つの集団の平均を比較するためにその分散を使うアイデアはかなり使えそうだ。それを発展させたのがまさに分散分析ANOVAである。 ] しかしとりあえず仮説検定に戻ってみよう。対立仮説がこのように定められたら、帰無仮説は次のような反対の内容になるかもしれない。 $$ H_{0}: \\mu_{1} \\ge \\mu_{2} $$ ここで重要なのは、帰無仮説が受け入れられる条件が「対立仮説が真である確かな根拠がない」ということであり、それが帰無仮説自体が積極的に採用されるわけではないということ。帰無仮説が受け入れられるのは、それを棄却できないためであり、それが真実であると証明されたためではない。 例えば、探検家コロンブスの対立仮説が「アメリカ大陸は存在する」とした場合、コロンブスが最初の探検でアメリカを見つけられなかったからといって「アメリカ大陸は存在しない」という帰無仮説が真になるわけではない。まだ確かな根拠がないので一旦は「アメリカ大陸は存在しない」と受け入れるだけであり、証拠の欠如が欠如の証拠ではない。 幸いにも対立仮説 $H_{1}$ が統計的な根拠によって真であるとしよう。ただし、厳密に言えば、この分析を通じて明らかにされたのは、新薬aを飲んだ集団のASTが低下したということだけに注意が必要だ。臨床医や病理学者ほどのドメインDomain, 分野に対する専門性を持たない分析者が自信を持って言えるのは「どんな理由があれ、新薬の効果は確実に証明された」ということであり、新薬aがどのような原理でASTを下げたかという因果関係までを言及する根拠にはならない。 参照 仮説検定の分かりやすい定義：厳密さよりもむしろ手軽に受け入れやすい定義を紹介する。 帰無仮説と対立仮説を定める方法：その定義にどんな問題があるかを説明する。 仮説検定の難しい定義：比較的厳密な数理統計的定義を紹介する。 慶北大学校統計学科。 (2008)。エクセルを使った統計学: p199。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMendenhall. (2012)。確率と統計の導入 (第13版): p344。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2442,"permalink":"https://freshrimpsushi.github.io/jp/posts/2442/","tags":null,"title":"仮説検定の簡単な定義"},{"categories":"위상데이터분석","contents":"定義 1 $K$を単体複体とする。部分集合$L \\subset K$も単体複体であれば、$K$の部分複体という。 $$ \\emptyset = K^{0} \\subset K^{1} \\subset \\cdots \\subset K^{m} = K $$ $K$の部分複体からなるネステッドシークエンスを$K$のフィルトレーションという。一般的に、全ての$i \\ge m$に対して、$K^{i} = K^{m}$とする。このようなフィルトレーションが存在する場合、$K$をフィルタード複体と呼ぶ。\n説明 アセンディングチェーン 上記のフィルタード複体のサブ複体は、$m = 0,\\cdots,5$に関して、アセンディングチェーンに類似した構造を示している。 $$ K^{0} \\subset K^{1} \\subset K^{2} \\subset K^{3} \\subset K^{4} \\subset K^{5} $$ フィルターという表現は、最大の単体がネットにかかっていく（←左方向へ）と、徐々に小さくなるイメージを思い浮かべさせるが、数学では必ずしも縮小する方向だけを想像する必要はない。\nトポロジカルデータ分析 ヴィートリス＝リップス複体やチェック複体のような複体は、与えられた半径$\\varepsilon \u0026gt; 0$によって決まり、この$\\varepsilon$を少しずつ増加させて得られる複体を列挙すると、それがフィルタード複体となる。このようなフィルタード複体内で、トポロジカルな性質が現れたり消えたりする永続性を調べ、データの特徴を分析するのが、トポロジカルデータ分析の1つのアプローチである。\n参照 様々なフィルトレーション $$ A_{1} \\subset A_{2} \\subset \\cdots \\subset A_{n} \\subset \\cdots $$ 数学全般において、構造がネステッドシークエンスを形成する場合、それをフィルトレーションと呼ぶ。\n確率過程のフィルトレーション 複体のフィルトレーション ベクトル空間のフラグ Zomorodian. (2005). Computing Persistent Homology: 2.2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2441,"permalink":"https://freshrimpsushi.github.io/jp/posts/2441/","tags":null,"title":"複素体のフィルトレーション"},{"categories":"데이터과학","contents":"定義 1 母集団に関連する数値的に記述される尺度数値的記述尺度を母数パラメータといい、サンプルから算出されたものを統計量スタティスティックという。\n説明 統計学の定義については様々な考え方があるが、基本的には推論統計学、特に数理統計学での「母数とは何か」に関心を持つ学問とされている。この観点から、統計学はデータを通じて母集団の母数を知る方法を研究する学問だと言える。\n一方、面白いことに、統計量Statisticにsを一つ加えるとStatistics、つまり統計学そのものになる。これは、統計学が統計量に関する研究であり、特にそれが母数に関するものである時は推定量エスティメータと言われることを意味している。例えば、ほとんどの場合、サンプルを全て加算し、その数で割る統計量である標本平均 $\\overline{x}$ は母平均 $\\mu$ を知るための推定量である。\n関連項目を見る 数理統計学での統計量と推定量 ハイパーパラメータ メンデンホール. (2012).「確率と統計の紹介」 (13版): p50.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2440,"permalink":"https://freshrimpsushi.github.io/jp/posts/2440/","tags":null,"title":"基礎統計学における母数と統計量"},{"categories":"데이터과학","contents":"定義 1 $$ \\overline{x} := {{ 1 } \\over { n }} \\sum_{k=1}^{n} x_{k} $$\n$n$量的データが与えられた時、その値を全て足し$n$で割った値$\\overline{x}$を標本平均Sample Mean、算術平均Arithmetic Mean、アベレージAverageなどと呼ぶ。\n説明 一般に、平均がデータをどれだけよく要約できるか、効果的かは、わざわざ説明する必要はない。大学レベル以上の統計学を勉強している人なら、以下の疑問を理解し、平均に注意すべき時を知っていなければならない：\n平均はいつも信頼できるか？ 明らかにそうではない。インターネット上に「ノースカロライナ大学で平均年収が最も高い学部は地理学部だ」という内容の有名なジョークがある。2そのような面白いエピソードからも分かるように、平均は異常値に弱く、代表値として適切ではない場合がある。 特に危険な時は？ 標本が極端に少ない時、異常値が多い時、分布が単峰性でない時などがある。ほとんどないが、理論的に母平均が存在しないと仮定する状況も考えられる。 それでも最も重要とされているのはなぜか？ 中心極限定理のためだ。どのような分布からのランダムサンプルでも、標本平均の確率分布は正規分布に分布収束するという強力な定理で、単純だが統計学の基礎をなす統計量としてその価値がある。 統計学を勉強している人は、平均がいつその意味を失うかを理解し、データを慎重に確認する習慣をつけなければならない。言い換えれば、平均を正しく使うだけでなく、使わないタイミングを知ることも非常に重要だ。以下のツイッターは、データを無視した時に平均がどれだけ意味をなさなくなるか、少し誇張を交えて警告している：\n参照 統計学の三つの代表値：最頻値、中央値、平均 平均 中央値 最頻値 代表値の数学的性質: 平均は分散を最小化する性質を持っている。 Mendenhall. (2012). 確率と統計の導入 (13版): p54.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026#160;\u0026#x21a9;\u0026#xfe0e; ","id":2438,"permalink":"https://freshrimpsushi.github.io/jp/posts/2438/","tags":null,"title":"基礎統計学における平均の定義"},{"categories":"줄리아","contents":"正規分布\njulia\u0026gt; using Distributions\rjulia\u0026gt; d = Normal()\rNormal{Float64}(μ=0.0, σ=1.0)\rjulia\u0026gt; rand(d, 2,2)\r2×2 Matrix{Float64}:\r-0.618228 -0.729552\r-1.46898 -0.636276 一様分布\njulia\u0026gt; rand(Uniform(), 2,2)\r2×2 Matrix{Float64}:\r0.0952175 0.348995\r0.845515 0.768308\rjulia\u0026gt; rand(Uniform(1,10), 2,2)\r2×2 Matrix{Float64}:\r7.09885 1.65445\r6.14428 7.31004 コーシー分布\njulia\u0026gt; rand(Cauchy(), 2,2)\r2×2 Matrix{Float64}:\r-20.1142 0.118282\r-0.110452 -0.420331\rjulia\u0026gt; rand(Cauchy(), 2,2)\r2×2 Matrix{Float64}:\r2.96951 -0.0587456\r0.0388744 -0.422848 ","id":3463,"permalink":"https://freshrimpsushi.github.io/jp/posts/3463/","tags":null,"title":"ジュリアで与えられた分布からランダムにサンプリングする方法"},{"categories":"줄리아","contents":"説明1 Juliaでランダム抽出する関数は以下の通りです。\nrand([rng=default_rng()], [S], [dims...]) rngはRandom Number Generatorの略で、乱数抽出アルゴリズムを指定します。何を意味しているのかわからなければ、触らなくても大丈夫です。\nSは（おそらく）Setの略で、ランダム抽出をする集合を指定する変数です。Sに入力可能な変数は以下のものがあります。\nインデックスがあるオブジェクト AbstractDictまたはAbstractSet 文字列 タイプ（整数、浮動小数点のみ可能です。有理数、無理数は不可。） 抽出集合をタイプで指定した場合、整数型ならtypemin(S):type(S)の範囲から抽出します（BigIntは対応していません）。\njulia\u0026gt; typemin(Int16), typemax(Int16)\r(-32768, 32767)\rjulia\u0026gt; typemin(Int32), typemax(Int32)\r(-2147483648, 2147483647)\rjulia\u0026gt; typemin(Int64), typemax(Int64)\r(-9223372036854775808, 9223372036854775807) 浮動小数点なら$[0, 1)$の範囲から抽出します。\njulia\u0026gt; rand(Float64)\r0.4949745522302659\rjulia\u0026gt; rand(ComplexF64)\r0.8560168003603014 + 0.16478582700545064im [dims...]は抽出する配列の次元を表します。rand(S, m, n)ならば、集合Sの要素から（重複を含めて）$m \\times n$個を抽出して$m \\times n$形の配列を返します。次元を入力しなければ、実数が返されます。実数と1次元ベクトルが明確に区別されるので、注意してください。さらに、$2\\times 3$形の配列を得たいと思って次元を(2,3)のようにタプルで入力すると、Sの変数として受け取られるので、全く異なる結果が出るので注意してください。\njulia\u0026gt; rand(Float64) # 실수 추출\r0.42226201756172266\rjulia\u0026gt; rand(Float64, 1) # 성분이 실수인 1x1 배열로 추출\r1-element Vector{Float64}:\r0.7361136057571305\rjulia\u0026gt; rand(2,3) # 성분이 실수인 2x3 배열로 추출 2×3 Matrix{Float64}:\r0.648742 0.364548 0.0550352\r0.0350098 0.56055 0.83297\rjulia\u0026gt; rand((2,3)) # 2와 3중에서 추출\r3 より高度な内容は次を参考にしてください。\nランダムシードを固定する方法 重みを付けてランダム抽出する方法 分布を与えてランダム抽出する方法 コード インデックスがあるオブジェクト julia\u0026gt; rand((2,5))\r5\rjulia\u0026gt; rand(2:5)\r3\rjulia\u0026gt; rand([2,3,4,5])\r4\rjulia\u0026gt; rand([\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, 4])\r\u0026#34;x\u0026#34; ディクショナリ 抽出集合をディクショナリにすると、キー-値ペアPair自体が抽出されます。\njulia\u0026gt; d = Dict(2=\u0026gt;4, 3=\u0026gt;5, 4=\u0026gt;\u0026#34;6\u0026#34;)\rDict{Int64, Any} with 3 entries:\r4 =\u0026gt; \u0026#34;6\u0026#34;\r2 =\u0026gt; 4\r3 =\u0026gt; 5\rjulia\u0026gt; rand(d)\r4 =\u0026gt; \u0026#34;6\u0026#34;\rjulia\u0026gt; rand(d)\r2 =\u0026gt; 4 文字列 抽出集合を文字列にすると、文字列内の文字の中からランダムに一つが抽出されます。\njulia\u0026gt; str = \u0026#34;freshrimpsushi\u0026#34;\r\u0026#34;freshrimpsushi\u0026#34;\rjulia\u0026gt; rand(str)\r\u0026#39;e\u0026#39;: ASCII/Unicode U+0065 (category Ll: Letter, lowercase)\rjulia\u0026gt; rand(str)\r\u0026#39;h\u0026#39;: ASCII/Unicode U+0068 (category Ll: Letter, lowercase) タイプ julia\u0026gt; rand(Int32, 3)\r3-element Vector{Int32}:\r1552806175\r-384901411\r-1580189675\rjulia\u0026gt; rand(UInt32, 3)\r3-element Vector{UInt32}:\r0xd2f44f99\r0x166a8b9e\r0x92fe22dc\rjulia\u0026gt; rand(Float32, 3)\r3-element Vector{Float32}:\r0.59852564\r0.6247238\r0.23303497\rjulia\u0026gt; rand(ComplexF32, 3)\r3-element Vector{ComplexF32}:\r0.10872495f0 + 0.6622572f0im\r0.6408408f0 + 0.46815878f0im\r0.7766515f0 + 0.73314756f0im 環境 OS: Windows11 Version: Julia 1.9.0 https://docs.julialang.org/en/v1/stdlib/Random/#Base.rand\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3462,"permalink":"https://freshrimpsushi.github.io/jp/posts/3462/","tags":null,"title":"ジュリアでランダムに抽出する方法"},{"categories":"데이터과학","contents":"定義 1 2 難しい定義 定量データの度数分布で作成された棒グラフをヒストグラムHistogramと言う。\n簡単な定義 数字で表されたデータを一定の区間に分け、その回数を数え、その大きさを棒グラフの高さとして表示する棒グラフをヒストグラムと言う。\n説明 ヒストグラムは、科学文献に欠かせない視覚化技法であり、データに不確実性が含まれている場合に確率分布を表すために用いられる。棒グラフまでは一般的に多くの人が接するが、ヒストグラムは簡単な定義で紹介したように、最小限の補足説明が必要な場合がしばしばある。\n上のスクリーンショットでは、A列は与えられた定量データで作成されたヒストグラムを示している。通常の棒グラフはカテゴリーをはっきり分けるため棒の間を空けるが、ほとんどのヒストグラムは棒の間隔をできるだけ減らし、確率分布の視覚表現として機能する。\n注意: Bin ヒストグラムでは、一つの区間の大きさをビンBinと呼ぶ。定量データであるため、階級の大きさと似ている概念であり、どのようにビンを設定するかによってヒストグラムの見え方が大きく変わることがある。例えば上のデータで階級の数を5つから2つに減らすと、次のようになる。\nサンプルの数がたった13個でも、階級の数を大幅に減らすと最初のヒストグラムで適切に反映された9〜10に対応する高い値が、大幅に単純化されすぎて本来の確率分布をまったく表していない。直感に頼って確率分布を迅速に把握することは重要だが、過信しすぎるとそのような主観性の罠に陥ることに注意しよう。\n一緒に見る Excelでヒストグラムを描く方法 Rでヒストグラムをより細かく見る方法 Mendenhall. (2012). Introduction to Probability and Statistics (13th Edition): p25, 165.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n경북대학교 통계학과. (2008). 엑셀을 이용한 통계학: p29.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2432,"permalink":"https://freshrimpsushi.github.io/jp/posts/2432/","tags":null,"title":"量的データのヒストグラム"},{"categories":"프로그래밍","contents":"개요1 名前のある140以上のCSSカラーパレットです。\n코드 ","id":3459,"permalink":"https://freshrimpsushi.github.io/jp/posts/3459/","tags":null,"title":"CSSカラー名札"},{"categories":"데이터과학","contents":"定義 質的データの各観測値が現れる頻度を頻度という。 頻度を全データの数で割ったものを相対頻度とし、相対頻度に100を掛けた値をパーセンテージという。 複数のカテゴリーに各頻度または相対頻度がどのように分布しているかを表すことを頻度分布という。 説明 Frequencyは、物理学の文脈では一般的に周波数と訳されるが、統計学の文脈ではそのデータがどれだけ頻繁に見つかるかを示す数値としての意味を持つ。ただし、学術的な用語でない「頻度」という言葉は混乱を与えることがあるため、漢字の頻を外して単に頻度と呼ぶと考えれば良い。\n言葉は少し難しそうに書いてあるけど、実際には「何回」かを意味する。非常に簡単な例として、コインを50回投げて、表が40回、裏が10回出た場合、それぞれの頻度は40、10となり、相対頻度は0.8、0.2となる。常識的に考えて、この例ではコインの表が異常に多く出たと思われるだろう。私たちの自然な関心事は、このコインが本当に公平かどうかであり、頻度分布を把握することで、統計的な推論を進めることになる。\n参照 質的データの頻度 量的データの階級 ","id":2426,"permalink":"https://freshrimpsushi.github.io/jp/posts/2426/","tags":null,"title":"定性データの頻度"},{"categories":"데이터과학","contents":"定義 1 統計学は、データを収集し、分析し、表示し、解釈し、決定する方法の集合だ。\n記述統計学は、図表やグラフと要約尺度などを使用してデータを構成し、表示し、説明する方法で構成されている。 推測統計学は、標本から母集団についての決定をするまたは予測をする方法で構成されている。 私見 以下は教科書外の話だ。\n個人的に統計学を「確率に関する理論を積極的に使用する応用数学の一分野」と定義したい。\nこれは一見統計学の特徴に過ぎないかもしれないが、実際に専攻レベルで学ぶ統計学―特に推測統計学を支える理論は、数理統計学であり、統計的な推論Statistical Inferenceとするものは大体確率論的な議論に基づいている。 統計学と直接的な関連はないが、確率論を導入して微細世界について研究する物理理論も統計力学Statistical Mechanicsと呼ばれている。 また、2010年代に入って機械学習、特にディープラーニングが大きく発展し、非構造データに対する技術水準が急速に上がった。彼らは、古典的な統計学がうまく扱えなかった分野、つまり自然言語処理、コンピュータビジョン、強化学習などの分野で非常に良い結果を出している。残念ながら、そのような分野を統計学の一部と見なす見解はほとんど見られない。 これらの理由から、定義で言及された統計学の定義はむしろデータサイエンスData Scienceの定義と呼ぶ方が正確かもしれない。ディープラーニングが流行する前にも、従来の機械学習は非パラメトリックNonparametricな方法として統計学の一部だったが、今となっては統計学が唯一のデータサイエンスではないことを認め、そのアイデンティティを確立する時が来ている。\nだが、悲しむ必要はない。生まれながらに統計学はディープラーニングなどとは異なり、その理論的基盤がしっかりしており、実際にパフォーマンス万能主義に失望して疲れた人が増えている。データサイエンスの全てだった時代に比べると少し小さくなったかもしれないが、応用数学の中でも依然として最も大きいものは変わらない。\n慶北大学校統計学科。(2008)。エクセルを使った統計学：p2~3。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2424,"permalink":"https://freshrimpsushi.github.io/jp/posts/2424/","tags":null,"title":"統計学の定義"},{"categories":"데이터과학","contents":"定義 1 質的変数 質的Qualitativeな特性を測定した変数を質的変数という。\n食べ物が\u0026hellip; 美味しい / まあまあ / まずい 色が\u0026hellip; 赤い / 青い / 黄色い 専攻が\u0026hellip; 数学 / 統計学 / 物理学 このような質的変数は、一般的にカテゴリカルCategoricalデータとも言われる。\n量的変数 量的Quantitativeな特性を測定した変数を量的変数という。\n年齢が\u0026hellip; 20歳 / 31歳 / 11歳 身長が\u0026hellip; 170.0cm / 170.5cm / 162.1cm 年齢や視力のようにはっきりとした値をとる量的変数を離散Discrete変数、身長や体重のように連続的な値をとる量的変数を連続Continuous変数という。\n説明 定義がなんだか奇妙に思えるかもしれないが、実際に「質的」と「量的」という言葉は、元々知っていた言葉ではなく、このような学術的な用語から日常的な表現を学ぶほうがむしろ正しいかもしれない。例えば、何かの品質を評価するときに、私たちは文字通り「クオリティが高い」という表現をよく使う。しかし、「質が高い」「質が低い」という言葉自体が、「1432ほど良い」や「17%ほど良い」とどう違うのかを考えてみよう。\n質的とは、このようにある順序（良い-まあまあ-悪い）を持つことはできるが、通常数値で表すのが難しいものを指す。もちろん、カテゴリー化されている（ドイツ語-フランス語-日本語）も問題ない。 量的はその反対で、量Amountを表すものを指す。ただし、ここで離散変数と連続変数の定義は少し難解かもしれない。 はっきりとした値とは？ はっきりとした値とは、いわゆる自然数や目盛りがあるような、ある単位で間隔を置いた値を説明する表現である。もちろん、どんな本にもそんなことは書いてないだろうし、私も見たことがない。そして、書きながらもあまり良い表現ではないと認める。代わりに私がとても気に入っている表現は以下の通りである。\nカウンタブルな値を取る変数を離散変数という。その値が限定的であるか数えることができるときのみを想定する。\n問題は、このように数学的に正確な表現が、すぐに離散変数が何なのか混乱しているあなたには何の役にも立たないことである。このような表現を理解することは、離散変数が何であるかを知っている人が離散変数について学ぶのと変わらない。\n何かがCountableであるとは、インド・ヨーロッパ語族、例えば私たちに馴染み深い英語、フランス語、スペイン語などで「1つ、2つ、\u0026hellip;」と数えられるものを指す。英語でそのようなものを表す名詞があれば、それを可算名詞と呼び、数学的に言えば自然数の集合と一対一対応が存在する。\nあまり役に立たない説明かもしれない。例を見て理解してみよう。以下の数は大抵離散変数である:\n牧場にいる豚の数 年間交通事故の死者数 専門書のページ数 幼児の年齢\u0026hellip;「24ヶ月の男の子」、「1歳2ヶ月の女の子」など 1Lの水筒の数 次に、離散変数かどうか迷うかもしれない例を見てみよう:\n1Lの水筒3つに入っている水の量\u0026hellip; 水筒の数ではなく、水の量なら連続型である。 視力\u0026hellip; 通常は0.1刻みだが、もし0.5、1.0、1.5の3つのグループしかなければ、離散変数 と見なすことができ、データの構成によっては質的変数と見なす余地もある。\n分類問題と回帰問題 通常、データサイエンスでは、従属変数が質的変数か量的変数かによって、分類問題と回帰問題を区別する。\n注意事項 実際にデータを扱いながら、経験が少ない初心者が犯しやすいミスがある。質的変数と量的変数を理解していないわけではなく、単に慣れていないために起こり得るミスであり、誰もが犯す可能性のあるミスである。多くの場合、回帰分析のような難しいものを勉強する頃にこのような罠に陥り、その直感を人工的に養う機会はほとんどない。次の投稿を見ると、正確に何を意味するのかはわからないかもしれないが、それがどのような罠なのかは大まかに理解できるかもしれない。\n質的変数を含む回帰分析 エンコーディング 性別を示す際に、男性を$0$、女性を$1$とエンコーディングEncodingする場合がよくあるが、目に見える数字があるからといって、これが離散変数（量的変数）になるわけではない。\nこのようなエンコーディングは、プライバシーのためにも使用される。想像してみよう。医療データは、個人の敏感な情報を多く含み、場合によってはデータだけで個々の人を特定できるほど特徴的な変数が多い。このような場合、データを公開する際に特定の情報を単に数字で隠すこともある。例えば、精神病歴、女性の中絶の有無などがある。\nレーティング 同様にエンコーディングの場合、レーティングが存在する場合がある。例えば、高卒が$0$、大卒が$1$、博士が$2$と表される場合、これが量的変数のように見えるが、依然として質的変数である。いわゆる低学歴、高学歴などは、一般社会の通念に過ぎず、データ的にこれらの数字は特に順序を示さない。現実のさまざまな例でこの主張を続けることができるが、ただちに高卒が$1$、大卒が$0$、博士が$2$とエンコーディングされるだけで、すでに量的変数ではないことがわかるだろう。\nヘックスコード 赤と青を区別することは質的変数だが、ピンク、ローズピンク、ディープピンクを区別するデータはどうだろう？これが口紅の話であれば、依然として質的変数で十分だが、例えば布の色であり、何千もの色がある場合、これらをRGBヘックスコードで表現できる。このようなデータに接する機会はほとんどないかもしれないが、直感的に質的変数だと思っても、量的変数として表現できる可能性があることを念頭に置く必要がある。\nジェンダー データにジェンダーGenderというカテゴリーが登場することもあるが、あなたが政治的正しさPolitical Correctnessに共感するか、うんざりするかにかかわらず、データがそう提供されているならば、まずはそのまま受け入れる必要がある。\nこれは本当の話だ。上で性別の例として挙げたように、ジェンダーが$0,1,2,3, \\cdots$でエンコーディングされたデータがあり、ジェンダー問題に全く関心がなかったある先輩が、「これ、ジェンダーで2と3は何？」と戸惑っていたのを見たことがある。アメリカ社会で調査されたデータでは、よくあることだ。 ポイントは、このようなことが起こらないように、ジェンダー問題に関心を持って勉強することではなく、特定のドメインDomainに関する知識が不足している場合は、直感に頼ってデータを検討しないことである。\nなぜ私たちはこれを知 る必要があるのか？\nこれらは非常に簡単で単純なことであるため、私たちはこれらを正確に区別し理解することができなければならない。ここでの私たちとは、統計学を応用する研究者を含め、統計学専攻者や、他の分野にバックグラウンドを持ちながらもデータサイエンスに従事する可能性のある人々を指す。\nこのように私たちが説明を探し、勉強し、課題をこなし、発表に慣れていく間に、皆さんの同僚たちはそれぞれ社会に適した何かをしていたであろう。残念ながら、それらの仕事は多くの場合大変だったため、私たちほどデータに精通していない可能性が高い。\n彼らはデータに無関心であったり、無知であったりするため、ここで述べられた注意事項を守らず、これらのばかげたミスを犯している可能性がある。そして、それらについて疑わない一般の人々を想像してみよう。あなたの上司Bossも例外ではない。\n私たちはそれを防がなければならない。\nMendenhall. (2012). 『確率と統計の入門』(13版): p10.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2420,"permalink":"https://freshrimpsushi.github.io/jp/posts/2420/","tags":null,"title":"質的変数と連続変数"},{"categories":"추상대수","contents":"定義 実数の可逆な $n \\times n$ 行列の集合を $\\mathrm{GL}(n, \\mathbb{R})$ または $\\mathrm{GL}_{n}(\\mathbb{R})$ と表記し、$n$次の一般線型群general linear group of degree $n$と呼ぶ。\n$$ \\mathrm{GL}(n, \\mathbb{R}) := \\left\\{ n \\times n \\text{ invertible matrix} \\right\\} = M_{n \\times n}(\\mathbb{R}) \\setminus {\\left\\{ A \\in M_{n \\times n}(\\mathbb{R}) : \\det{A} = 0 \\right\\}} $$\n説明 可逆な行列だけを集めたので、行列の積に関して群になる。また、微分可能な構造を持つため、リー群でもある。\n","id":3450,"permalink":"https://freshrimpsushi.github.io/jp/posts/3450/","tags":null,"title":"一般リニア群"},{"categories":"위상데이터분석","contents":"定理 1 2 カバーとリフトの定義: 単位区間を$I = [0,1]$のように表す。\n$X$のオープンセット$U \\subset X$が**$p$によって均等にカバーされる**Evenly Covered by $p$とは、全ての$\\alpha \\in \\forall$に対応する全ての制限関数$p |_{\\widetilde{U}_{\\alpha}}$がホメオモルフィズムであり $$ \\alpha_{1} \\ne \\alpha_{2} \\implies \\widetilde{U}_{\\alpha_{1}} \\cap \\widetilde{U}_{\\alpha_{2}} = \\emptyset $$ を満たす、つまり互いに素な$\\widetilde{X}$のオープンセット$\\widetilde{U}_{\\alpha} \\subset \\widetilde{X}$について $$ p^{-1} \\left( U \\right) = \\bigsqcup_{\\alpha \\in \\forall} \\widetilde{U}_{\\alpha} $$ が成り立つことを意味する。 $p : \\widetilde{X} \\to X$が全射関数であり、全ての$x \\in X$に対して$p$によって均等にカバーされる$x$のオープンネイバーフッド$U_{x} \\subset X$が存在する場合、$p : \\widetilde{X} \\to X$をカバーCoveringという。 カバー$p$の定義域$\\widetilde{X}$をカバースペースCovering Space、値域$X$をベーススペースBase Spaceという。 $n \\in \\mathbb{N}$とする。$f : I^{n} \\to X$と$\\widetilde{f} : I^{n} \\to \\widetilde{X}$が次を満たす場合、$\\widetilde{f}$を$f$のリフトLiftという。 $$ f = p \\circ \\widetilde{f} $$ $1$-スフィア$S^{1}$を値域に持つカバーを$p : \\mathbb{R} \\to S^{1}$としよう。\nパスリフティング定理 連続関数$f : I \\to S^{1}$はリフト$\\widetilde{f} : I \\to \\mathbb{R}$を持つ。特に与えられた$x_{0} \\in S^{1}$と$\\widetilde{x}_{0} \\in p^{-1} \\left( x_{0} \\right)$に対して、$\\widetilde{f} \\left( 0 \\right) = \\widetilde{x}_{0}$である$\\widetilde{f}$は一意に存在する。\nホモトピー・リフティング定理 連続関数$F : I^{2} \\to S^{1}$はリフト$\\widetilde{F} : I^{2} \\to \\mathbb{R}$を持つ。特に与えられた$x_{0} \\in S^{1}$と$\\widetilde{x}_{0} \\in p^{-1} \\left( x_{0} \\right)$に対して、$\\widetilde{F} \\left( 0 , 0 \\right) = \\widetilde{x}_{0}$である$\\widetilde{F}$は一意に存在する。\n説明 リフティング定理Lifting Theoremは一般に単位円$S^{1}$の性質を研究するための補助定理として言及され、形式的にFormally見た場合、パスリフティングか、ホモトピー・リフティングかという区別はあまり意味がない。\nむしろ、ほとんどの数学者が気にすべき質問は$X \\ne S^{1}$である$f: I^{m} \\to X$に対する一般化が可能かという点であり、実際にはコンパクト空間$Y$に対する連続関数$f: Y \\times I^{m} \\to X$に対するリフティング定理まで論じることができる。ただし、このような拡張が実際には全く役に立たないため、直接学ぶには過剰だと言われている。\n証明 戦略: パスリフティング定理のみを証明する。本質的にホモトピー・リフティング定理の証明はパスリフティング定理の証明と同じである。パスリフティング定理ではコンパクト空間である$I$から区間を有限に分割して証明するように、ホモトピー・リフティング定理では同様にコンパクトな空間である$I^{2}$を有限に分割して同じ議論を繰り返す。\nPart 1. 設定\n$p : \\widetilde{X} \\to X$が全射関数であり、全ての$x \\in X$に対して$p$によって均等にカバーされる$x$のオープンネイバーフッド$U_{x} \\subset X$が存在する場合、$p : \\widetilde{X} \\to X$をカバーCoveringという。 $p : \\mathbb{R} \\to S^{1}$はカバーとされているので、全ての$x \\in S^{1}$に対して$p$によって均等にカバーされる$x$のネイバーフッド$U_{x} \\subset S^{1}$が存在する。\n$I = [0,1]$はコンパクトなので$I \\subset \\bigcup_{k=1}^{n} \\left[ a_{k-1} , a_{k} \\right]$を満たすような有限の点の集合$\\left\\{ a_{k} \\right\\}_{k=0}^{n} \\subset I$が存在し、 $$ 0 = a_{0} \u0026lt; a_{1} \u0026lt; \\cdots \u0026lt; a_{n-1} \u0026lt; a_{n} = 1 $$ その区間$\\left[ a_{k-1} , a_{k} \\right] \\subset I$に対する$f$のイメージは$S^{1}$に含まれ、特にあるオープンセット$U \\subset S^{1}$に対して以下の包含関係を満たす。 $$ f \\left( \\left[ a_{k-1} , a_{k} \\right] \\right) \\subset U \\subset S^{1} $$ このような$U$に対するカバー$p$の互いに素なプレイメージを$\\widetilde{U}_{t} := p^{-1} \\left( U_{t} \\right)$とすれば、それぞれ$t \\in \\mathbb{Z}$に対して$U$とホメオモルフィックである。\nPart 2. 帰納的構築\n任意の$x \\in S^{1}$ではなく、具体的に$x_{0} \\in S^{1}$を選び、その$p$のプレイメージの要素の一つを$\\widetilde{x}_{0} := p^{-1} \\left( x_{0} \\right) \\in \\mathbb{R}$と表す。元の設定によれば、これらの要素の集合は$\\mathbb{Z}$との間に全単射が存在するが、どれがどうであれ関係ない。\n私たちは$I$全体ではなく、$\\left[ 0, a_{k} \\right]$に対して$\\widetilde{f}_{k} (0) = \\widetilde{x}_{0}$を満たすリフト$\\widetilde{f}_{k}$を帰納的に定義して、結果的に$\\widetilde{f}$を見つけようとしている。\n$k = 0$の場合は単に$\\widetilde{f}_{0} (0) = \\widetilde{x}_{0}$とし、他に選択肢はない。 $k \\ne 0$の場合、連続関数$\\widetilde{f}_{k} : \\left[ 0 , a_{k} \\right] \\to \\mathbb{R}$が一意に定義されると仮定する。 ある一意の$\\widetilde{U} \\in \\left\\{ \\widetilde{U}_{t} \\right\\}_{t \\in \\mathbb{Z}}$に対して$\\widetilde{f} \\left( a_{k} \\right) \\in \\widetilde{U}$である。 $\\widetilde{f}_{k}$は連続であり、区間$\\left[ a_{k} , a_{k+1} \\right]$は経路連結であるため、$\\widetilde{f}_{k}$の拡張関数$\\widetilde{f}_{k+1}$がどのように定義されても、少なくとも$\\left[ a_{k} , a_{k+1} \\right]$は必ず$\\widetilde{U}$内にマッピングされなければならない。 $p$がカバーであるため、全ての$t \\in \\mathbb{Z}$に対してホメオモルフィズム$p | \\widetilde{U}_{t} : \\widetilde{U}_{t} \\to U$が存在し、それにより $$ p \\circ \\rho_{k} = f | \\left[ a_{k} , a_{k+1} \\right] $$ を満たす一意の関数$\\rho_{k} : \\left[ a_{k} , a_{k+1} \\right] \\to \\widetilde{U}$が存在する。このような関数$\\rho_{k}$の存在は、$p$の制限関数がホメオモルフィズムであること―すなわち単射であることに基づくため、$\\rho_{k} \\left( a_{k} \\right) = \\widetilde{f}_{k} \\left( a_{k} \\right)$であり、$\\rho_{k}$の連続性も保証される。 接着補題: 位相空間$X,Y$に対して、二つの閉集合$A,B \\subset X$が$A \\cup B = X$を満たし、二つの連続関数$f : A \\to Y$と$g : B \\to Y$が全ての$x \\in A \\cap B$に対して$f(x) = g(x)$であるとする。すると、以下のように定義された$h$は連続関数である。 $$ h(x) : = \\begin{cases} f(x), \u0026amp; x \\in A \\\\ g(x), \u0026amp; x \\in B \\end{cases} $$\n接着補題により、以下のような連続関数$\\widetilde{f}_{k+1} : \\left[ 0 , a_{k+1} \\right] \\to \\mathbb{R}$を一意に定義できる。 $$ \\widetilde{f}_{k+1} := \\begin{cases} \\widetilde{f}_{k} (s) \u0026amp; , \\text{if } s \\in \\left[ 0, a_{k} \\right] \\\\ \\rho_{k} (s) \u0026amp; , \\text{if } s \\in \\left[ a_{k} , a_{k+1} \\right] \\end{cases} $$ 数学的帰納法により、$S^{1}$は$t \\in \\mathbb{Z}$周回する螺旋に向かうリフトが具体的に存在する。ここで、$k = 0, 1, \\cdots , n$は$\\mathbb{R}$で上下に動くインデックスではなく、$S^{1}$を回転させながら有限に分割するインデックスであることをよく想像しなければならない。$k$が$1$ずつ増えるごとに、$\\mathbb{R}$では整数の数だけ多くの区間の集合$\\left\\{ \\widetilde{U}_{t} \\right\\}_{t \\in \\mathbb{Z}}$も同様に回転して動く。\nPart 3. ホモトピー・リフティング定理\n$0$から$n-1$までの整数を集めた集合$\\left\\{ 0, 1, \\cdots , n-1 \\right\\}$を簡単に$0:n$と書こう。 $I$がコンパクトであることに基づいて$0 = a_{0} \u0026lt; \\cdots \u0026lt; a_{n} = 1$を選べたように、$I^{2}$もコンパクトであるため、 $$ \\begin{align*} 0 = a_{0} \u0026lt; \\cdots \u0026lt; a_{n} = 1 \\\\ 0 = b_{0} \u0026lt; \\cdots \u0026lt; b_{m} = 1 \\end{align*} $$ のように正方形を格子に切る有限の二つの自然数$n , m \\in \\mathbb{N}$が存在し、それぞれの小さなマスを$i = 0:n$、$j = 0:m$に対して $$ R_{i,j} := \\left[ a_{i-1}, a_{i} \\right] \\times \\left[ b_{j-1} , b_{j} \\right] \\subset I^{2} $$ と定義すると、 $$ R_{0,0} , R_{0,1} , \\cdots , R_{0,m} , R_{1,0} \\cdots, R_{n,m} $$ のような小さな長方形のシーケンスが得られる。これに対してパスリフティング定理で行った議論を繰り返せば、ホモトピー・リフティング定理が証明される。\n■\nKosniowski. (1980). A First Course in Algebraic Topology: p137~138.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHatcher. (2002). Algebraic Topology: p29~31.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2419,"permalink":"https://freshrimpsushi.github.io/jp/posts/2419/","tags":null,"title":"代数的トポロジーにおけるリフティング定理の証明"},{"categories":"데이터과학","contents":"概要 現代社会では、データについて全く知らない知識人はいない。全く関心がない非専門家でも、「何かについての知識」や「通信するための資源」のようなデータや情報といった類義語を容易に思い浮かべることができるほど、データという概念は普遍化し、大衆化された。以下の記述はほんの少しでも、データサイエンスの観点から、データをより厳密に定義しようとする試みに過ぎない。\n定義 1 変数Variableは、時間や個人Individual、または対象Objectによって変わる特性Characteristicを指す。 変数が測定される個人や対象を 実験単位Experimental Unitといい、実験単位から実際に測定された結果を 測定値Measurementという。 測定値の集合を データDataという。 説明 データの語源 2 英語のデータは「与えられた、または認められた事実」を意味し、ラテン語で「与える」という意味の動詞「Do-」の過去分詞であるDatumから派生しており、「与えられた」という意味を持つ。Dataは、そのDatumの複数形にあたる。\n皮肉にも、このようなデータの語源は、上述のように、何か特徴であるとか、実験をしながらどうにか定義しようとしていたことよりも、より正確にデータの本質を指している。データサイエンスの世界では、データはすでに与えられたもの、またはこれから我々に与えられるべきものであり、新しい発見や創造の対象とは明らかに異なる属性を持つ。\nつまり、データはどうしようもなく、すなわち 与えられたものだ。粗野な比喩として、長持ちする電球を発明する状況を想像してみよう。平均寿命が100時間の電球Aから電球Bを改良した場合、各電球B（Object）の寿命を測定することができるだろう。この測定値を集めたものがまさに電球Bの寿命データであり、それらの数値は電球Bによって与えられたものであって、電球Aのデータ自体をどうにか変えて得たものではない。\n変数と実験？ 変数變數は、字のごとく変わる数値として考えがちで、データを簡単に説明するときにはよく数字が登場するけれど、非構造データに対する理解が深まった現代社会では、データを数字やカテゴリーに限定する必要はない。データの種類には、写真、文書、信号、株価、動画、ネットワーク構造など、人が認識できるすべてが対象である。同様に、測定値測定値も、「値」という文字を使うために数字のように見えるかもしれないが、そのように数値として考える必要はない。可能な限り、英語の表現Measurementをそのまま使うことをお勧めする。\nまた、実験単位の実験は、白衣を着た科学者たちが研究所で行うものだけを指すわけではない。基礎確率論で事象が起こることを「任意の試行」と呼ぶように、表現のための表現として受け入れても十分だ。\n母集団と標本 調査者Investigatorが関心を持つすべての測定値の集合を 母集団Populationという。 母集団の部分集合を サンプルSampleと呼ぶ。 \u0026hellip;このような定義から、現実的には多くのデータが母集団のサンプルであることが推測できる。一方で、母集団の英語表現Populationは、統計学とも密接に関係している人口という意味も持っているので注意してほしい。\n統計学のコンセプトは基本的に「母集団について知りたいが、実際には母集団をすべて調査することはできないので、サンプルを通じて母集団の特性を把握すること」、つまり、データを通じて関心ある対象の本質を推測することと言える。\n参考までに 数理統計学におけるサンプルの定義 学部2～3年生レベルで触れる数理統計学では、このポストで説明するサンプルについて数理的な定義を下し、データの別の表現である実現Realizationを紹介している。\nMendenhall. (2012). Introduction to Probability and Statistics (13th Edition): p8.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.etymonline.com/word/data\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2418,"permalink":"https://freshrimpsushi.github.io/jp/posts/2418/","tags":null,"title":"データの定義と語源"},{"categories":"위상데이터분석","contents":"定義 1 2 二つの位相空間$\\widetilde{X}, X$に対して、$p : \\widetilde{X} \\to X$が連続関数であるとしよう。任意の添字集合を$\\forall$のように示し、$\\widetilde{U}_{\\alpha} \\subset \\widetilde{X}$から$p$への制限関数を単に$p |_{\\widetilde{U}_{\\alpha}} : \\widetilde{U}_{\\alpha} \\to U$のように書こう。\n$I = [0,1]$は$0$から$1$までの単位区間だ。 $\\bigsqcup$は互いに素な集合の和集合を表す。 カバーリング $X$の開集合$U \\subset X$が**$p$によって均等にカバーされる**Evenly Covered by $p$ということは、全ての$\\alpha \\in \\forall$に対応する全ての制限関数$p |_{\\widetilde{U}_{\\alpha}}$がホメオモーフィズムであり $$ \\alpha_{1} \\ne \\alpha_{2} \\implies \\widetilde{U}_{\\alpha_{1}} \\cap \\widetilde{U}_{\\alpha_{2}} = \\emptyset $$ を満たす、つまり、 $$ p^{-1} \\left( U \\right) = \\bigsqcup_{\\alpha \\in \\forall} \\widetilde{U}_{\\alpha} $$ が$\\widetilde{X}$の互いに素な開集合$\\widetilde{U}_{\\alpha} \\subset \\widetilde{X}$について成り立つことを意味する。 $p : \\widetilde{X} \\to X$が全射関数であり、全ての$x \\in X$に対して$p$によって均等にカバーされる$x$の開近傍$U_{x} \\subset X$が存在するならば、$p : \\widetilde{X} \\to X$をカバーリングという。 カバーリング$p$の定義域$\\widetilde{X}$をカバーリングスペース、値域$X$をベーススペースという。 リフト $n \\in \\mathbb{N}$とする。$f : I^{n} \\to X$と$\\widetilde{f} : I^{n} \\to \\widetilde{X}$が次を満たすなら、$\\widetilde{f}$を$f$のリフトという。 $$ f = p \\circ \\widetilde{f} $$ 例 数学的な定義は難しすぎるので、$X = S^{1}$と$\\widetilde{X} = \\mathbb{R}$の単純な例を考えてみよう。率直に言って、定義の中のカバーリングとリフトは、この例の一般化レベルだ。\n直感的なリフト $\\widetilde{X} = \\mathbb{R}$と書いたが、図では$\\mathbb{R}^{3}$に埋め込まれた螺旋として表され、これは螺旋$h : \\mathbb{R} \\to \\mathbb{R}^{3}$について $$ s \\mapsto \\left( \\cos 2 \\pi s, \\sin 2 \\pi s , s \\right) $$ と表すのと同じだ。今、$I = [0,1]$から$\\mathbb{R}$へのパスを $$ \\widetilde{\\omega}_{n} (s) := ns $$ と定義すれば、これは$0$から始まって$n$で終わり、螺旋を$n \\in \\mathbb{Z}$周巻き付けることになる。一方、スフィア$S^{1}$は $$ \\omega_{n} (s) := \\left( \\cos 2 \\pi n s , \\sin 2 \\pi n s \\right) $$ 次元での単位円として表すことができ、自然にプロジェクションProjection$p : (x,y,z) \\mapsto (x,y)$はカバーリングとなる。直感的に見ると、$p$は解かれた螺旋を平面に送る投影であり、逆に$\\widetilde{\\omega}_{n}$は無数に重なった$\\omega_{n}$を立体空間に引き上げたもので、これをリフトと呼ぶのが適切だ。数式で表すと $$ \\omega_{n} = p \\circ \\widetilde{\\omega}_{n} $$ になる。今、定義を見直すと、今までの$I^{1}$は一つの直感的な例で、リストされた条件を全て満たせば、それらをカバーリングやリフトと呼ぶ理由はない。代数位相の文脈では、$I^{2}$でのリフト、つまりホモトピー$H : I^{2} \\to X$のリフトを考えるのがすぐに思い浮かぶ可能性がある。\n均等カバーが難しすぎる 定義で均等なカバーが非常に難しく書かれているが、直感的に考えれば実は単純な概念だ。\n$U \\subset S^{1}$の逆像は螺旋上で$\\widetilde{U}_{k}$として互いに素な集合たちの和集合として表現され、その各々は小さな断片$U$とホメオモーフィックだ。ただし、この例では運良く整数$k$に対応するように添字が与えられ、形も単純だが、実際には定義どおり添字集合$\\forall$がどれほど奇怪であるか予想もできない。だから、ほとんどの数学者が簡単で直接的な定義を好むにもかかわらず、均等なカバーの記述に関しては妥協が難しい。\nKosniowski. (1980). A First Course in Algebraic Topology: p135. 144.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHatcher. (2002). Algebraic Topology: p29.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2417,"permalink":"https://freshrimpsushi.github.io/jp/posts/2417/","tags":null,"title":"代数トポロジーにおける被覆と持ち上げ"},{"categories":"줄리아","contents":"概要 元々ジュリアでは、データを出力する時にREPLのサイズに合わせてきれいに出力されるが、時には全体のデータを楽に見たい時がある。データがfooであれば、show(stdout, \u0026quot;text/plain\u0026quot;, foo)を通じて全体のデータを出力させることができる1。\nコード julia\u0026gt; foo = rand(100,2)\r100×2 Matrix{Float64}:\r0.956438 0.663427\r0.790117 0.472821\r0.976134 0.198475\r0.727601 0.472336\r0.0469046 0.991999\r0.625807 0.26634\r0.490773 0.588481\r0.352966 0.426474\r0.585632 0.00185974\r⋮\r0.615713 0.707071\r0.891683 0.622176\r0.370576 0.937107\r0.430644 0.0439135\r0.535987 0.551992\r0.43273 0.217023\r0.345366 0.500033\r0.551719 0.0761454 元々は上のように⋮が印刷されるが、プレーンテキストで印刷すると次のように全体が出力される。\njulia\u0026gt; show(stdout, \u0026#34;text/plain\u0026#34;, foo)\r100×2 Matrix{Float64}:\r0.956438 0.663427\r0.790117 0.472821\r0.976134 0.198475\r0.727601 0.472336\r0.0469046 0.991999\r0.625807 0.26634\r0.490773 0.588481\r0.352966 0.426474\r0.585632 0.00185974\r0.13357 0.90977\r0.789999 0.137833\r0.11626 0.385958\r0.629265 0.40623\r0.111327 0.483414\r0.22717 0.0960839\r0.854027 0.690618\r0.00862816 0.426555\r0.292845 0.588308\r0.475157 0.935968\r0.936422 0.116917\r0.421748 0.335614\r0.354324 0.444122\r0.52423 0.311464\r0.306786 0.873037\r0.308008 0.70787\r0.0885757 0.558464\r0.0510476 0.840701\r0.320569 0.28571\r0.89837 0.517027\r0.218359 0.622536\r0.563148 0.488849\r0.508919 0.818068\r0.880726 0.550501\r0.555517 0.953056\r0.466298 0.29687\r0.816757 0.528656\r0.789289 0.294199\r0.51256 0.173814\r0.972556 0.11602\r0.438784 0.815105\r0.218237 0.257226\r0.0838205 0.535666\r0.287095 0.877342\r0.176927 0.942882\r0.855193 0.577759\r0.813356 0.488643\r0.407358 0.970933\r0.224252 0.455783\r0.430215 0.727\r0.0585314 0.727251\r0.77538 0.777196\r0.114963 0.610359\r0.445436 0.472755\r0.0565616 0.153393\r0.695217 0.00669471\r0.673818 0.284351\r0.308611 0.386984\r0.761394 0.32279\r0.017963 0.114759\r0.465956 0.788791\r0.970691 0.264864\r0.0953205 0.359958\r0.437556 0.283858\r0.323666 0.893141\r0.971015 0.109052\r0.117792 0.919322\r0.898883 0.947123\r0.248386 0.462831\r0.895525 0.434108\r0.526593 0.288652\r0.891208 0.848443\r0.344758 0.412774\r0.697527 0.592066\r0.531953 0.50251\r0.0565245 0.449993\r0.168528 0.783811\r0.129681 0.22014\r0.489568 0.232417\r0.875734 0.380527\r0.0207026 0.915546\r0.210948 0.476037\r0.822661 0.517793\r0.579839 0.0221691\r0.455027 0.920253\r0.932968 0.771582\r0.960643 0.841065\r0.0835567 0.943408\r0.578494 0.502968\r0.0655954 0.528926\r0.590831 0.41364\r0.840604 0.790515\r0.327964 0.269113\r0.615713 0.707071\r0.891683 0.622176\r0.370576 0.937107\r0.430644 0.0439135\r0.535987 0.551992\r0.43273 0.217023\r0.345366 0.500033\r0.551719 0.0761454 環境 OS: Windows julia: v1.7.0 https://stackoverflow.com/questions/49304329/how-to-show-all-elements-of-vectors-and-matrices-in-julia/67090474#67090474\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2416,"permalink":"https://freshrimpsushi.github.io/jp/posts/2416/","tags":null,"title":"ジュリアでデータを省略せずに出力する方法"},{"categories":"위상데이터분석","contents":"定義 1 位相空間 $X$ と単位区間 $I = [0,1]$ が与えられたとする。\n$X$ のパス $f, g : I \\to X$ に対して、$f (1) = g(0)$ の時、2つのパスの積または合成 $f \\cdot g$ を以下のように定義する。 $$ f \\cdot g (s) := \\begin{cases} f \\left( 2s \\right) \u0026amp; , \\text{if } s \\in [0, 1/2] \\\\ g \\left( 2s - 1 \\right) \u0026amp; , \\text{if } s \\in [1/2, 1] \\end{cases} $$ パス $f : I \\to X$ に対して、$\\overline{f} (s) := f (1-s)$ で定義されたパス $\\overline{f} : I \\to X$ を$f$ の逆パスと言う。 すべての $s_{1} , s_{2} \\in I$ に対して、$c_{x_{0}} \\left( s_{1} \\right) = c_{x_{0}} \\left( s_{2} \\right) = x_{0}$ であるパス $c_{x_{0}}$、つまり定数関数であるパスを定数パスと言う。 $f (0) = f(1) = x_{0} \\in X$ の時、つまり始点と終点が同じパス $f$ をループと言い、$x_{0} \\in X$ を基点と言う。 $x_{0} \\in X$ を基点とするホモトピー $f$ のすべてのホモトピークラス $\\left[ f \\right]$ の集合を$\\pi_{1} \\left( X , x_{0} \\right)$ として示す。2つのループのホモトピークラス $[f] , [g] \\in \\pi_{1} \\left( X , x_{0} \\right)$ に対する二項演算 $\\ast$ を $$ [f] \\ast [g] := \\left[ f \\cdot g \\right] $$ として定義する時、群 $\\pi_{1} \\left( \\left( X , x_{0} \\right) \\right)$ を基本群と呼ぶ。通常、$\\ast$ は述べられることさえなく$[f] [g] = [f \\cdot g]$ として書かれる。 説明 まず数学で基本という言葉が付いているものは、何であれ非常に重要なものである。初めて接する時はループがどうたらこの群をどう作るかなど抵抗感を感じるかもしれないが、ただのループの集合ではなくそのホモトピーを考えることによって、結局は位相空間の性質に興味を持つという点を考えれば、その意味を想像しやすくなる。\nパスの積 パスの積 $f \\cdot g$ は式から直接わかるように、$0 \\le s \\le 1/2$ では$f$に沿って進み、$1/2 \\le s \\le 1$ では$g$ に沿って進む。このようなパス同士の積が連続であることは、貼り付けの補題によって保証される。\n貼り付けの補題: 位相空間 $X,Y$ に対して二つの閉集合 $A,B \\subset X$ が $A \\cup B = X$ を満たし、二つの連続関数 $f : A \\to Y$ と $g : B \\to Y$ がすべての $x \\in A \\cap B$ に対して $f(x) = g(x)$ であるとする。そうすると、以下のように定義された$h$ は連続関数である。 $$ h(x) : = \\begin{cases} f(x), \u0026amp; x \\in A \\\\ g(x), \u0026amp; x \\in B \\end{cases} $$\n全く難しくない概念だが、言葉が少々気になるかもしれない。パスは本質的に関数であるため、合成と言うと合成関数と混同される可能性があり、また、積も $X$ で乗算と言えるような操作が存在する場合、混乱を招く可能性がある。しかし、心配とは裏腹に実際に勉強してみると、ホモトピークラスの操作 $[f] [g] = [f \\cdot g]$ だけが主に言及され、この時に登場する$f \\cdot g$ をわざわざ言葉で説明することはほとんどない。\n基本群の逆元と単位元 逆パスとは、パスの積において辿った跡を消していくことと言えるだろう。直感的に理解するためには、$\\overline{f}$ が単にパス $f$ と同じであるが、その方向が逆であると考える。\nそれでは、任意のパス $f$ に対して、$\\overline{f}$ との積 $f \\cdot \\overline{f}$ は始点と終点が同じであるため、ループとなる。ここで注目すべき点は、$f$ の終点であり$\\overline{f}$ の始点である$x_{1}$ を正確に通過するか、ただ単に$x_{0}$ その場に静止する定数パス$c_{x_{0}}$ であるか、正確には同じではないがホモトピックだということである。$\\pi_{1} \\left( X, x_{0} \\right)$ はホモトピークラスの集合であるため、すべての$f$ に対して $$ f \\cdot \\overline{f} \\simeq c_{x_{0}} $$ が成り立つことになる。これを見ると、$f$ が何であれそのホモトピークラス$[f]$ は、$\\overline{f}$ のホモトピークラス $\\left[ \\overline{f} \\right]$ との演算を通じて常に$\\left[ c_{x_{0}} \\right]$ となり、$c_{x_{0}}$ はその定義そのものから$\\pi_{1} \\left( X , x_{0} \\right)$ の単位元であることが分かる。\n単連結空間 これまでの議論を見ると、基本群に$x_{0}$ が本当に必要かについて疑問が湧くかもしれない。\n例えば、上の図のように、ある$x_{0}$ から始まって新しい点$x_{1} \\in X$ に到着する前のパスを「消してしまう」形のループを考えることができ、どの基点を選んでも大して変わらないように見える。\n基本群の基点の置き換え: 位相空間 $X$ が与えられたとする。$h : I \\to X$ を$x_{0}$ から$x_{1}$ までのパスとすると、$\\beta_{h} [f] := \\left[ h \\cdot f \\cdot \\overline{h} \\right]$ で定義された関数$\\beta_{h} : \\pi_{1} \\left( X , x_{1} \\right) \\to \\pi_{0} \\left( X_{1} , x_{0} \\right)$ はアイソモーフィズムであり、これを基点の置き換えと呼ぶ。\n上の定理によると、$X$ が経路連結であれば、$\\pi_{1} \\left( X , x \\right)$ は基点$x$ の選択に関係なくすべてアイソモーフィックなので、$\\pi_{1} \\left( X \\right)$ として表現されたり、さらに簡潔には$\\pi_{1} X$ とも書かれる。\n特に、経路連結性を持ちながら基本群$\\pi_{1} X$ が自明群である、つまり単位元 $e$ だけを持つ有限群とアイソモーフィックで、$\\pi_{1} X \\simeq \\left\\{ e \\right\\}$ である場合、$X$ は単連結空間とされる。\n逆に言えば、一般的に基本群はこの基点$x_{0}$ の選択によって性質が大きく異なることがあり、経路連結であっても代数的にどのような性質を持っているかを別途確認する必要がある。\nHatcher. (2002). Algebraic Topology: p26~28.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2415,"permalink":"https://freshrimpsushi.github.io/jp/posts/2415/","tags":null,"title":"代数位相幾何学における基本群"},{"categories":"머신러닝","contents":"概要1 $$ \\includegraphics[height=20em]{https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png} $$\nMNISTmodified national institute of standards and technology データベースとは、アメリカの高校生と人口調査局の職員の数字の手書き文字に関するデータセットを指す。一般に[エムニスト]と呼ばれる。\n公式ホームページ 機械学習/ディープラーニング入門の例としてよく使用されるデータセットである。NISTでは、手書きの郵便番号の自動分類のための文字認識技術の評価のため、以下のような形式で手書きデータを収集した。ここでヤン・ルカンYann LeCunが高校生と人口調査局の職員の手書きデータを取り、前処理を行い、MNISTを作成した。画像のサイズは28 x 28で、60,000枚のトレーニングセットと10,000枚のテストセットで構成されている。\n$$ \\includegraphics[height=30em]{https://www.nist.gov/sites/default/files/styles/960_x_960_limit/public/images/2019/04/27/sd19.jpg?itok=oETq77cZ} $$\n使用方法 Julia Juliaでは、機械学習データセットパッケージであるMLDatasets.jlを使用できる。基本的にはFloat32型のトレーニングセットを読み込む。オプションでこれを変更して読み込むことができる。使用できるメソッドは以下の通り。\ndataset[i]: i番目の特徴量とターゲットのタプルを返す。 dataset[:]: 全ての特徴量とターゲットのタプルを返す。 length(dataset): データの数を返す。 convert2image(dataset, i): i番目のデータをグレースケールの画像に変換する。ImageShow.jlパッケージが必要である。 julia\u0026gt; using MLDatasets\rjulia\u0026gt; train = MNIST()\rdataset MNIST:\rmetadata =\u0026gt; Dict{String, Any} with 3 entries\rsplit =\u0026gt; :train\rfeatures =\u0026gt; 28×28×60000 Array{Float32, 3}\rtargets =\u0026gt; 60000-element Vector{Int64}\rjulia\u0026gt; test = MNIST(Float64, :test)\rdataset MNIST:\rmetadata =\u0026gt; Dict{String, Any} with 3 entries\rsplit =\u0026gt; :test\rfeatures =\u0026gt; 28×28×10000 Array{Float64, 3}\rtargets =\u0026gt; 10000-element Vector{Int64}\rjulia\u0026gt; length(train), length(test)\r(60000, 10000)\rjulia\u0026gt; using Plots\rjulia\u0026gt; using ImageShow\rjulia\u0026gt; train.targets[1]\r5\rjulia\u0026gt; heatmap(convert2image(train, 1)) ラベルは整数で与えられるため、ワンホットエンコーディングを別途行う必要がある。\njulia\u0026gt; train.targets[1:5]\r5-element Vector{Int64}:\r5\r0\r4\r1\r9\rjulia\u0026gt; using Flux\rjulia\u0026gt; Flux.onehotbatch(train.targets[1:5], 0:9)\r10×5 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\r⋅ 1 ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ 1 ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ 1 ⋅ ⋅\r1 ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ ⋅\r⋅ ⋅ ⋅ ⋅ 1 Julia Fluxでワンホットエンコーディングする方法 Julia FluxでMLPを実装し、MNISTを学習する方法 環境 OS: Windows11 Version: Julia v1.8.2, MLDatasets v0.7.6, Plots v1.36.1, ImageShow v0.3.6, Flux v0.13.7 권건우·허령, 人工知能をマンガと野史で学ぶ 2, p68\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3444,"permalink":"https://freshrimpsushi.github.io/jp/posts/3444/","tags":null,"title":"MNIST Database"},{"categories":"위상데이터분석","contents":"要約 簡単な説明 どんな位相空間でも、2つの固定された点の間で定義されたホモトピーの関係は同値関係だ。\n詳しい説明 位相空間 $X$ と2つの点 $x_{0}, x_{1} \\in X$ が与えられたとする。2点間のパス$f, g : I \\to X$がホモトピックであれば、$f \\simeq g$のように表すとき、この二項関係$\\simeq$は同値関係である。また、この同値関係 $\\simeq$によって作られる同値類 $\\left\\{ g : f \\simeq g \\right\\}$は$[f]$として表される。\n説明 一見すると、この定理は空間 $X$で与えられた点 $x_{0}, x_{1}$があるとき、そのすべてのパスが2つの点だけで表されるかのように誤解されるかもしれない。しかし、それはすべてのパスに対するホモトピーが存在する場合の話で、簡単な例としてトーラスを考えると、空間の真ん中に穴があいていて、「すべてのパス」においてホモトピーが存在するわけではないことがわかる。\n幸いにも、一般的なユークリッド空間 $\\mathbb{R}^{p}$では成立し、さらに一般的には凸なベクトル空間であれば、上記のパラグラフでの推測が当てはまると考えられる。\n証明 1 $\\simeq$が反射的Reflexiveであり、対称的Symmetricであり、推移的Transitiveであることを示せば良い。反射性は$f \\simeq f$間に$\\left\\{ h_{t} = f \\right\\}$という定数ホモトピーが存在するので自明である。対称性も$h_{t}$が$f$と$g$の間に存在し、$\\left\\{ h_{1-t} \\right\\}$が$g$と$f$間のホモトピーとして存在するので自明である。推移性はもう少し難しい。パス$f : I \\to X$に対応する二変数連続関数$f$が $$ F : I \\times I \\to X $$ であり、パス$g : I \\to X$に対応する二変数連続関数$G$が $$ G : I \\times I \\to X $$ としたとき、中間のパス$h$に対応する二変数連続関数を $$ H (s,t) = \\begin{cases} F \\left( s, 2t \\right) \u0026amp; , \\text{if } t \\in [0,1/2] \\\\ G \\left( s, 2t - 1 \\right) \u0026amp; , \\text{if } t \\in [1/2,1] \\end{cases} $$ と定義すると、すぐに$f \\simeq h$と$h \\simeq g$の場合、$f \\simeq g$を可能にするホモトピーが存在することが直接確認できる。図式で表すと、下図のように$I^{2}$から定義された2つの関数の定義域を半分にしてつなげた形になる。\nこの証明で提出された$h_{t}$が関数として適切に定義されているかWell-defined、本当に連続であるかについての議論はここでは省略する。\n■\nHatcher. (2002). Algebraic Topology: p26.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2413,"permalink":"https://freshrimpsushi.github.io/jp/posts/2413/","tags":null,"title":"ホモトピー類"},{"categories":"줄리아","contents":"概要 Juliaは、MATLABレベルの線形代数をサポートしている。むしろMATLABよりも進化した、直感的で美しい構文を見ると、Juliaが作られた時点でよく設計されていたと感じられる1。\nコード julia\u0026gt; A = [ 1 0 3\r0 5 1\r3 1 9\r] 3×3 Matrix{Int64}:\r1 0 3\r0 5 1\r3 1 9 見ての通り、行列を定義する段階で既に直感的で便利だ。ここで、普通に必要とされるいくつかの関数について学ぶ。小見出しに関連記事をリンクし、別の説明は省略する。\nトレース tr() julia\u0026gt; tr(A)\r15 行列式 det() julia\u0026gt; det(A)\r-1.000000000000003 逆行列 inv() julia\u0026gt; inv(A)\r3×3 Matrix{Float64}:\r-44.0 -3.0 15.0\r-3.0 6.10623e-16 1.0\r15.0 1.0 -5.0\rjulia\u0026gt; round.(Int64, inv(A))\r3×3 Matrix{Int64}:\r-44 -3 15\r-3 0 1\r15 1 -5 対角行列と対角成分 diag(), diagm() julia\u0026gt; diag(A)\r3-element Vector{Int64}:\r1\r5\r9\rjulia\u0026gt; diagm([1,5,9])\r3×3 Matrix{Int64}:\r1 0 0\r0 5 0\r0 0 9 ノルム norm() julia\u0026gt; norm(A, 1)\r23.0 固有値 eigvals() julia\u0026gt; eigvals(A)\r3-element Vector{Float64}:\r-0.020282065792505244\r4.846013411157458\r10.174268654635046\rjulia\u0026gt; eigvecs(A)\r3×3 Matrix{Float64}:\r-0.944804 0.117887 0.305692\r-0.0640048 -0.981459 0.180669\r0.321322 0.151132 0.934832\rjulia\u0026gt; eigmax(A)\r10.174268654635046 行列分解 factorize() julia\u0026gt; factorize(A)\rBunchKaufman{Float64, Matrix{Float64}}\rD factor:\r3×3 Tridiagonal{Float64, Vector{Float64}}:\r-0.0227273 0.0 ⋅ 0.0 4.88889 0.0\r⋅ 0.0 9.0\rU factor:\r3×3 UnitUpperTriangular{Float64, Matrix{Float64}}:\r1.0 -0.0681818 0.333333\r⋅ 1.0 0.111111\r⋅ ⋅ 1.0\rpermutation:\r3-element Vector{Int64}:\r1\r2\r3\rjulia\u0026gt; svd(A)\rSVD{Float64, Float64, Matrix{Float64}}\rU factor:\r3×3 Matrix{Float64}:\r-0.305692 0.117887 -0.944804\r-0.180669 -0.981459 -0.0640048\r-0.934832 0.151132 0.321322\rsingular values:\r3-element Vector{Float64}:\r10.174268654635044\r4.846013411157461\r0.02028206579250516\rVt factor:\r3×3 Matrix{Float64}:\r-0.305692 -0.180669 -0.934832\r0.117887 -0.981459 0.151132\r0.944804 0.0640048 -0.321322 行列代数カテゴリの行列分解を参照せよ。行列の形に応じて適切な分解法を自動的に選んで分解してくれる。もちろん、条件を満たすなら、具体的な分解関数を直接使っても良い。\n行列の操作 julia\u0026gt; B = [\r1 0 1\r1 1 0\r2 1 1\r]\r3×3 Matrix{Int64}:\r1 0 1\r1 1 0\r2 1 1\rjulia\u0026gt; A + B\r3×3 Matrix{Int64}:\r2 0 4\r1 6 1\r5 2 10\rjulia\u0026gt; A - B\r3×3 Matrix{Int64}:\r0 0 2\r-1 4 1\r1 0 8\rjulia\u0026gt; A * B\r3×3 Matrix{Int64}:\r7 3 4\r7 6 1\r22 10 12\rjulia\u0026gt; A .* B\r3×3 Matrix{Int64}:\r1 0 3\r0 5 0\r6 1 9\rjulia\u0026gt; B / A\r3×3 Matrix{Float64}:\r-29.0 -2.0 10.0\r-47.0 -3.0 16.0\r-76.0 -5.0 26.0\rjulia\u0026gt; B * inv(A)\r3×3 Matrix{Float64}:\r-29.0 -2.0 10.0\r-47.0 -3.0 16.0\r-76.0 -5.0 26.0\rjulia\u0026gt; A / B\rERROR: SingularException(3) 我々が考える常識的な操作は全て通用する。割り算は、当然乗算の逆元である逆行列を掛けるのと同じで、Bのように逆行列が存在しない場合は、シンギュラー例外をレイズする。\nブロック行列 [] 他の言語と比べて、ブロック行列を非常に便利に作ることができる。\njulia\u0026gt; [A B]\r3×6 Matrix{Int64}:\r1 0 3 1 0 1\r0 5 1 1 1 0\r3 1 9 2 1 1\rjulia\u0026gt; [A;B]\r6×3 Matrix{Int64}:\r1 0 3\r0 5 1\r3 1 9\r1 0 1\r1 1 0\r2 1 1\rjulia\u0026gt; [A,B]\r2-element Vector{Matrix{Int64}}:\r[1 0 3; 0 5 1; 3 1 9]\r[1 0 1; 1 1 0; 2 1 1] 二つの行列の間にスペースを置くと横に積み上げ、セミコロンを置くと縦に積み上げる。カンマは行列を積み上げるわけではなく、一般的に配列で使用していた構文そのままで、行列の配列になる。\n全体のコード 複素行列や内積に関連する内容は省略したが、全体のコードには含まれている。\nusing LinearAlgebra\rA = [\r1 0 3\r0 5 1\r3 1 9\r]\rtr(A)\rdet(A)\rinv(A)\rround.(Int64, inv(A))\rdiag(A)\rdiagm([1,5,9])\rnorm(A, 1)\reigvals(A)\reigvecs(A)\reigmax(A)\rfactorize(A)\rsvd(A)\rB = [\r1 0 1\r1 1 0\r2 1 1\r]\rdet(B)\rrank(B)\reigvals(B)\rSymmetric(B) # |\u0026gt; issymmetric\rtranspose(B)\rB\u0026#39;\rC = [\rim im 1\r2 im 0\rim 1 2\r]\rC\u0026#39;\rB\u0026#39;B\rx = [1,2,3]\ry = [0,1,2]\rx\u0026#39;y\rA + B\rA - B\rA * B\rA .* B\rB / A\rB * inv(A)\r[A B]\r[A;B]\r[A,B]\rx\u0026#39; * y\ry * x\u0026#39; 環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2412,"permalink":"https://freshrimpsushi.github.io/jp/posts/2412/","tags":null,"title":"ジュリアで線形代数パッケージを使用する方法"},{"categories":"위상데이터분석","contents":"定義1 単位閉区間$I := [0,1]$と位相空間$X$が与えられているとする。\n固定された二点$x_{0} , x_{1} \\in X$に対して、次を満たす連続関数$p : I \\to X$を$x_{0}$から$x_{1}$へのパスまたはパスPathという。 $$ \\begin{align*} p(0) =\u0026amp; x_{0} \\\\ p(1) =\u0026amp; x_{1} \\end{align*} $$ 二つのパス$f \\equiv h_{0}$と$g \\equiv h_{1}$に対して、以下の二条件を満たすパスの集合$\\left\\{ h_{t} \\right\\}_{t \\in [0,1]}$をホモトピーHomotopyという。 (i): $t$に独立して$h_{t} (0) = x_{0}$、かつ$h_{t} (1) = x_{1}$である。 (ii): 全ての$s,t \\in I$に対して、$H(s,t) := h_{t} (s)$として定義された$H : I \\times I \\to X$が連続である。文脈によって、$H$または$h_{t}$自体をホモトピーと呼ぶこともある。 二つのパス$f$と$g$にホモトピーが存在する場合、$f$と$g$がホモトピックHomotopicであると言い、$f \\simeq g$のように表す。 説明 ホモトピーとは、簡単に言えば、与えられた二つの点を連続的に繋げる関数のことであり、二つの点を繋ぐ方法が本質的に同じならば、数学的にそれらを同じものとして扱うためのものだ。例えば、上の図で左の点と右の点を繋ぐ方法は無数にあるが、（一般に知られている）位相数学の観点から見て直線で行くか少し曲がって行くか、正直何の違いがあるだろうか？[ NOTE: ここでの「少し」という表現は、定義で言及されている$H$の「連続性」を意味する。]\n上の図を見ると、$H$は$s$に従って変化する関数$h_{t}$が単位正方形$I^{2}$内で連続的に変化することを示している。\nホモトピーの意味 前述のように、二つのパスがホモトピックであるとは、一方のパスを少し変えれば反対側にあるパスになるという意味として受け取ることができる。しかし、位相数学でこのような「事実上同じ」ということを探究するのは、「真に異なる」を見るためである。\n例えば、上記のようにトーラス上で二点を繋ぐ二つのパスを見てみよう。トーラスは中央が空いているため、青いパスと赤いパスを繋ぐホモトピーは存在せず、これらはホモトピックではない。重要なのは、「点と点」だけでなく「点と点の関係の関係」を見ることにより、トーラスと非トーラス凸形を区別Classificationする段階に達したことだ。大げさに言えば、ホモトピーを研究することは、単純に「関数の関数」のような非専門家には理解しにくい言葉遊びではなく、空間の本質を見る新しい方法論である。\n同値条件 それほど証明するほどの大した事実ではなく、ただ紹介するだけなので、実際に使うときは以下の定義がもっと便利かもしれないが、言及しておく。\n$f$と$g$がホモトピックであることは、以下の二条件を満たす連続関数$H : I \\times I \\to X$が存在することと同値である。\n(i): 全ての$t \\in I$に対して、$H(0,t) = x_{0}$であり、$H(1,t) = x_{1}$である。 (ii): 全ての$s \\in I$に対して、$H (s, 0) = f(s)$であり、$H(s,1) = g(s)$である。 ホモトピーはパスのパスだ 以下の話は少し難しいので、ややこしいと思ったらスキップしてもいい。 $$ \\begin{align*} h_{t} (s) =\u0026amp; x_{0} \\to x_{1} \u0026amp; \\text{ as } s = 0 \\to 1 \\\\ h_{t} = \u0026amp; f \\to g \u0026amp; \\text{ as } t = 0 \\to 1 \\end{align*} \\qquad \\cdots 🤔 ! $$\n公式に見れば、$X$で二点$x_{0}, x_{1}$を繋ぐパス$f,g : I \\to X$は、連続関数の空間の要素である$f,g \\in C \\left( I, X \\right)$であり、$h_{t} : I \\to C \\left( I , X \\right)$はその関数空間における二つのパス$f, g$のパス $$ h_{t} \\in C \\left( I , C \\left( I , X \\right) \\right) $$ と呼ぶことができる。わざわざこのような表現を定義で使わない理由は、ホモトピーの定義自体で$C \\left( I , X \\right)$を位相空間として考えるには自然な位相を言及するのが過ぎるからである。定義は短ければ短いほど良いものだし、ホモトピーを論じるためには、二変数関数$H$の連続性만を要求することで十分である。\n$H$の連続性ではなく、関数空間での連続性について話す場合、まず関数空間のコンパクト-オープン位相などの関数空間の位相が必要だが、これは過剰である。もちろん、文献で定義が簡単に提示されているからといって、われわれも定義だけを知っていれば良いわけではない。既にホモトピーは上で簡単に定義されたので、残りの時間でその先を少し想像してみよう。数学では、$Y, X$がどんな集合であれ、関数 $$ F : Y \\to X $$ に関心を持たざるを得ない。関数の関数に関心を持つこと、つまり、定義域が$Z$で、値域が関数空間$X^{Y}$である新しい関数 $$ H : Z \\to X^{Y} \\iff H : Z \\times Y \\to X $$ を考えることは、関数のシーケンスや内積空間など、終わりのない新しい研究テーマを生み出す。このような好奇心が自然であると同意するなら、今度は$Z$と$Y$の場に単位閉区間$I = [0,1]$を入れてみよう。 $$ H : I \\times I \\to X $$\nこれは他ならぬ、私たちが定義で見た$H$である。言い換えれば、ホモトピーはその複雑な名前とは裏腹に\n単に$I \\times I$から定義され、 明らかに関心を持つべき 連続関数の連続関数であり、 始点と終点が指定されただけの 関数の集合 に過ぎない。ホモトピーが見知らずで嫌だと感じるなら、$Y,Z$という広大な領域ではなく、$I \\times I$という小さな範囲に留まることに感謝しよう。\nHatcher. (2002). Algebraic Topology: p25.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2411,"permalink":"https://freshrimpsushi.github.io/jp/posts/2411/","tags":null,"title":"ホモトピーの定義"},{"categories":"줄리아","contents":"概要 1 Datesは、日付や時間に関連する関数をまとめたモジュールだ。一般的なプログラミングはもちろん、時系列に関する、いやそれに関係なく多くのデータを扱う上で、非常に役に立つものに違いない1。\nコード 全コード using Dates\r오늘 = DateTime(2022,3,10)\rtypeof(오늘)\rpropertynames(오늘)\r오늘.instant\rmyformat = DateFormat(\u0026#34;d-m-y\u0026#34;)\r내일 = Date(\u0026#34;11-3-2022\u0026#34;, myformat)\rDates.dayname(내일)\r일주일뒤까지 = 오늘:Day(1):DateTime(2022,3,17)\rcollect(일주일뒤까지)\rDates.Day(일주일뒤까지[end]) - Dates.Day(오늘) DateTime タイプ julia\u0026gt; 오늘 = DateTime(2022,3,10)\r2022-03-10T00:00:00\rjulia\u0026gt; typeof(오늘)\rDateTime 例えば DateTime() 関数で22年3月10日の日付を 今日 に割り当てたなら、今日 は DateTime というタイプを持つことになる。DateTime は instant というプロパティを持ち、ミリ秒単位で時間を記録している。\njulia\u0026gt; propertynames(오늘)\r(:instant,)\rjulia\u0026gt; 오늘.instant\rDates.UTInstant{Millisecond}(Millisecond(63782553600000)) フォーマット DateFormat() julia\u0026gt; myformat = DateFormat(\u0026#34;d-m-y\u0026#34;)\rdateformat\u0026#34;d-m-y\u0026#34;\rjulia\u0026gt; 내일 = Date(\u0026#34;11-3-2022\u0026#34;, myformat)\r2022-03-11 よく、東西の違いで日付が違って記述される時に使われる。\n曜日 Dates.dayname() julia\u0026gt; Dates.dayname(내일)\r\u0026#34;Friday\u0026#34; 指定された日付の曜日を返してくれる。グレゴリオ暦の不合理さのため、自分で作ると意外と難しいものがこんなものだ。\n日付のベクタ julia\u0026gt; 일주일뒤까지 = 오늘:Day(1):DateTime(2022,3,17)\rDateTime(\u0026#34;2022-03-10T00:00:00\u0026#34;):Day(1):DateTime(\u0026#34;2022-03-17T00:00:00\u0026#34;)\rjulia\u0026gt; collect(일주일뒤까지)\r8-element Vector{DateTime}:\r2022-03-10T00:00:00\r2022-03-11T00:00:00\r2022-03-12T00:00:00\r2022-03-13T00:00:00\r2022-03-14T00:00:00\r2022-03-15T00:00:00\r2022-03-16T00:00:00\r2022-03-17T00:00:00 ジュリアの日付パッケージで最も役立つ部分だと言えるだろう。上のように特定の時点間の区間をネイティブのジュリア文法そのままでベクタ化すれば、まさに想像していた通りの結果が出る。作るのは二の次で、同じ機能を持つ関数が他の言語にもあるかもしれないが、この程度に文法にうまく溶け込んで、類まれな直感性を持つことは稀だろう。\n日付の引き算 - julia\u0026gt; Dates.Day(일주일뒤까지[end]) - Dates.Day(오늘)\r7 days 当たり前のように、引き算で2つの時点の間隔を計算できる。Dates.canonicalize() を使えば、時間、分、秒単位できれいに表示できる。\n環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/stdlib/Dates/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2410,"permalink":"https://freshrimpsushi.github.io/jp/posts/2410/","tags":null,"title":"ジュリアでの日付と時刻関連関数の使用方法"},{"categories":"줄리아","contents":"概要 1 2 The Fastest Fourier Transform in the West(FFTW)は、マサチューセッツ工科大学(MIT)のMatteo FrigoとSteven G. Johnsonによって開発された、離散フーリエ変換を計算するためのソフトウェアライブラリです。FFTの実装用にAbstractFFTs.jlというパッケージがありますが、これは自体を直接使用するためではなく、FFTW.jlなどの高速フーリエ変換を実装するときに支援するために作られたものです。\nこのパッケージは主に直接使われることを意図していません。代わりに、FFTs（例えばFFTW.jlやFastTransforms.jl）を実装するパッケージの開発者がAbstractFFTsで定義された型/関数を拡張します。これにより、同じ基本的なfft(x)とplan_fft(x)インターフェイスを持つ複数のFFTパッケージが共存できます。3\n概要 フーリエ変換: fft() $2$次元配列で列ごとに変換: fft( ,[1]) $2$次元配列で行ごとに変換: fft( ,[2]) 配列の特定の次元だけを変換: fft( ,[n₁, n₂, ...]) フーリエ逆変換: ifft() $0$ 中央の周波数: fftshift() 逆変換: ifftshift() 周波数サンプリング: fftfreq(n, fs=1) コード フーリエ変換 Juliaでは、フーリエ変換の表記として$\\mathcal{F}[f]$, $\\hat{f}$が直接コードに使われます。周波数が$100$, $200$, $350$のサイン波を$1/1000$間隔でサンプリングし、これを加算しましょう。\nusing FFTW\rusing Plots\rusing LaTeXStrings\rFs = 1000 #진동수\rT = 1/1000 #샘플링 간격\rL = 1000 #신호의 길이\rx = [i for i in 0:L-1].*T #신호의 도메인\rf₁ = sin.(2π*100*x) #진동수가 100인 사인파\rf₂ = 0.5sin.(2π*200*x) #진동수가 100인 사인파\rf₃ = 2sin.(2π*350*x) #진동수가 100인 사인파\rf = f₁ + f₂ + f₃ フーリエ変換: fft() フーリエ逆変換: ifft() 定義によると、$f$のフーリエ変換$\\mathcal{F}f$は$50$, $100$, $200$でのみ非ゼロ値を持ちます。また、離散フーリエ変換の定義により、$y$軸の周りに対称な値を得ますが、基本的には周波数が$0$の値が最初の値になっています。したがって、信号の周波数と振幅を確認することが目的なら、前半だけをプロットしてもよいでしょう。\nFs = 1000 # 샘플링 주파수\rℱf = fft(f) # 푸리에 변환\rξ = Fs*[i for i in 0:L/2-1]/L #주파수 도메인(절반)\rplot(ξ, abs.(ℱf[1:Int(L/2)])*2/L, title=L\u0026#34;Fourier transform of ▷eq10◁\u0026#34;, label=\u0026#34;\u0026#34;) xlabel!(\u0026#34;frequency\u0026#34;)\rylabel!(\u0026#34;amplitude\u0026#34;)\rsavefig(\u0026#34;fft.png\u0026#34;) $0$ 中央の周波数 フーリエ変換の出力は、基本的に周波数が$0$の値を最初に置きます。周波数が$0$の値を中央にしたい場合は、fftshift()を使います。これを元に戻す場合はifftshift()を使いますが、これはifft+shiftではなく、逆変換 + fftshift、つまりfftshift()の逆操作であるため、混乱しないようにしましょう。\np1 = plot(ξ, abs.(ℱf), title=L\u0026#34;▷eq11◁\u0026#34;, label=\u0026#34;\u0026#34;, yticks=[], xticks=[0, 100, 200, 350, 500, 1000]) p2 = plot(ξ.-500, abs.(fftshift(ℱf)), title=L\u0026#34;▷eq22◁\u0026#34;, label=\u0026#34;\u0026#34;, yticks=[], xticks=[-500,-350,-200,-100,0,100,200,350,500]) plot(p1, p2, size=(800,400))\rsavefig(\u0026#34;fftshift.png\u0026#34;) 高次元フーリエ変換 2次元フーリエ変換の値と比較するため、まず$x = [1\\ 2\\ 3\\ 4]^{T}$のフーリエ変換値を計算しておきましょう。\njulia\u0026gt; x = [1.0; 2; 3; 4]\r4-element Vector{Float64}:\r1.0\r2.0\r3.0\r4.0\rjulia\u0026gt; fft(x)\r4-element Vector{ComplexF64}:\r10.0 + 0.0im\r-2.0 + 2.0im\r-2.0 + 0.0im\r-2.0 - 2.0im fft()は2次元配列を入力として受け取ると自動的に2次元フーリエ変換を返します。または、fft(, [1,2])は、第一および第二次元での変換を計算するという意味で、同じ結果を返します。\njulia\u0026gt; y = [x x x x]\r4×4 Matrix{Float64}:\r1.0 1.0 1.0 1.0\r2.0 2.0 2.0 2.0\r3.0 3.0 3.0 3.0\r4.0 4.0 4.0 4.0\rjulia\u0026gt; fft(y)\r4×4 Matrix{ComplexF64}:\r40.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0+8.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0-8.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\rjulia\u0026gt; fft(y, [1,2])\r4×4 Matrix{ComplexF64}:\r40.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0+8.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r-8.0-8.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im したがって、各列で変換を行いたい場合は、fft(, [1])を使い、各行で変換を行いたい場合は、fft(, [2])を使います。\njulia\u0026gt; fft(y, [1])\r4×4 Matrix{ComplexF64}:\r10.0+0.0im 10.0+0.0im 10.0+0.0im 10.0+0.0im\r-2.0+2.0im -2.0+2.0im -2.0+2.0im -2.0+2.0im\r-2.0+0.0im -2.0+0.0im -2.0+0.0im -2.0+0.0im\r-2.0-2.0im -2.0-2.0im -2.0-2.0im -2.0-2.0im\rjulia\u0026gt; fft(y, [2])\r4×4 Matrix{ComplexF64}:\r4.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r8.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r12.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im\r16.0+0.0im 0.0+0.0im 0.0+0.0im 0.0+0.0im 周波数サンプリング fftfreq(n, fs=1) 長さが$n$で、間隔が$fs/n$の周波数ドメインを返します。既に説明したように、フーリエ変換は最初に周波数$0$の値を置くため、fftfreq()でサンプリングした周波数の最初の値は$0$です。前半は正の周波数、後半は負の周波数です。したがって、fftshift()を使うと、インデックスに応じて昇順に並べ替えられます。\njulia\u0026gt; fftfreq(4, 1)\r4-element Frequencies{Float64}:\r0.0\r0.25\r-0.5\r-0.25\rjulia\u0026gt; fftfreq(5, 1)\r5-element Frequencies{Float64}:\r0.0\r0.2\r0.4\r-0.4\r-0.2\rjulia\u0026gt; fftshift(fftfreq(4, 1))\r-0.5:0.25:0.25 環境 OS: Windows11 Version: Julia 1.8.2, FFTW 1.5.0 http://www.fftw.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/JuliaMath/FFTW.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/JuliaMath/AbstractFFTs.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3440,"permalink":"https://freshrimpsushi.github.io/jp/posts/3440/","tags":null,"title":"ジュリアで高速フーリエ変換（FFT）を使用する方法"},{"categories":"위상데이터분석","contents":"定義 1 任意の集合 $X$ が与えられたとする。\n$X$ の冪集合 $2^{X}$ の有限な部分集合の中で次を満たす複体 $A \\subset 2^{X}$ を**（抽象的な）シンプレクス**Abstract Simplicial Complexという。 $$ \\alpha \\in A \\land \\beta \\subset \\alpha \\implies \\beta \\in A $$ 複体 $A$ の要素 $\\alpha \\in A$ をシンプレックスSimplicesと呼ぶ。 シンプレックス $\\alpha$ の次元Dimension $\\dim$ は、$\\alpha$ の基数から $1$ を引いた値で定義される。 $$ \\dim \\alpha := | \\alpha | - 1 $$ 複体 $A$ の次元は、$A$ のすべてのシンプレックスの次元の中の最大値として定義される。 $$ \\dim A := \\max_{\\alpha \\in A} \\left( \\dim \\alpha \\right) $$ シンプレックス $\\alpha$ の空集合ではない真部分集合 $\\beta \\subsetneq \\alpha$ を$\\alpha$ の面Faceと呼ぶ。 次のように計算される $A$ のすべてのシンプレックスの和集合 $V(A)$ を$A$ の頂点集合Vertex Setと言う。 $$ V(A) := \\bigcup_{\\alpha \\in A} \\alpha $$ 複体の部分集合 $B \\subset A$ が複体である場合、部分複体Subcomplexと言う。 次を満たす全単射 $b : V(A) \\to V(B)$ が存在する場合、二つの複体 $A, B$ が同型Isomorphicであるという。 $$ \\alpha \\in A \\iff b (\\alpha) \\in B $$ (幾何学的な)シンプレクス複体 $K$ について、それの構成を全て無視して頂点間の関係のみを保持して得られた（抽象）シンプレクス複体 $A$ を$K$ の頂点スキーマVertex Schemeと言い、この時$K$ を$A$ の幾何学的実現Geometric Realizationと言う。 説明 抽象的なシンプレクス複体はその名の通り、幾何学的な意味を省いたシンプレクス複体の抽象化である。数学者の視点から見れば、凸包などの条件はただの面倒くさい制約に過ぎない。\n例えば、$X = \\mathbb{N}$ を考えた場合 $$ \\begin{align*} T :=\u0026amp; \\left\\{ \\left\\{ 1 \\right\\}, \\left\\{ 2 \\right\\} , \\left\\{ 3 \\right\\}, \\left\\{ 4 \\right\\} , \\right. \\\\ \u0026amp; \\left\\{ 1,2 \\right\\}, \\left\\{ 2,3 \\right\\}, \\left\\{ 3,4 \\right\\}, \\left\\{ 4,1 \\right\\}, \\left\\{ 2,4 \\right\\} \\\\ \u0026amp; \\left. \\left\\{ 1,2,4 \\right\\} , \\left\\{ 2,3,4 \\right\\} \\right\\} \\end{align*} $$ 抽象的なシンプレクス複体のすべての条件を満足し、この場合、ユークリッド空間 $\\mathbb{R}$ などの幾何学的な意味を全く気にする必要はない。$T$ は$0$次元のシンプレックス$4$ 個、$1$次元のシンプレックス$5$ 個、$2$次元のシンプレックス$2$個を持ち、その複体自体は$2$次元であり、頂点集合として$V(T) = \\left\\{ 1,2,3,4 \\right\\}$ を持つ。一方、幾何学的シンプレクス複体 $G$ が与えられた場合、$T$ を$G$ の頂点スキーマとして考えても問題ない。\nEdelsbrunner, Harer. (2010). Computational Topology An Introduction: p63~64.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2409,"permalink":"https://freshrimpsushi.github.io/jp/posts/2409/","tags":null,"title":"抽象単体複合体の定義"},{"categories":"줄리아","contents":"概要 機械学習のような分野では、計算速度の向上やメモリの節約などのために、64ビットの実数ではなく32ビットの実数が配列のデータ型として使われます。そのため、PyTorchではテンソルを作ると、基本的にテンソルのデータ型は32ビットの浮動小数点数になってます。Juliaの機械学習パッケージにはFlux.jlがあり、これで実装された人工ニューラルネットワークは、Juliaの基本配列を入力として受け取ります。テンソルのような別のデータ構造を使わないという点は利点と言えますが、データ型を手動でFloat32に設定しなければならない面倒くささもあります。以下で、デフォルトのデータ型を変更する方法を紹介します。\nコード1 ChangePrecision.jl @changeprecision マクロを使うと、begin ... endで囲まれたコード内でデフォルトのデータ型が変わります。\njulia\u0026gt; Pkg.add(\u0026#34;ChangePrecision\u0026#34;)\rjulia\u0026gt; using ChangePrecision\rjulia\u0026gt; rand(3)\r3-element Vector{Float64}:\r0.580516564576538\r0.33915094423556424\r0.3612907828959878\rjulia\u0026gt; @changeprecision Float32 begin\rrand(3)\rend\r3-element Vector{Float32}:\r0.0459705\r0.0033969283\r0.579983 環境 OS: Windows10 バージョン: Julia 1.8.2, ChangePrecision 1.0.0 https://stackoverflow.com/questions/68068823/how-to-change-default-float-to-float32-in-a-local-julia-environment\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3439,"permalink":"https://freshrimpsushi.github.io/jp/posts/3439/","tags":null,"title":"ジュリアで基本データ型を変更する方法"},{"categories":"머신러닝","contents":"定理 インプット集合Input Set $X \\ne \\emptyset$ と正定値カーネル $k: X \\times X \\to \\mathbb{R}$ が与えられているとする。学習データセットTraining Datasetを $$ D := \\left\\{ \\left( x_{i} , y_{i} \\right) \\right\\}_{i=1}^{m} \\subset X \\times \\mathbb{R} $$ とし、再生カーネルヒルベルト空間 $H_{k}$ のクラス $$ \\mathcal{F} := \\left\\{ f \\in \\mathbb{R}^{X} : f \\left( \\cdot \\right) = \\sum_{i=1}^{\\infty} \\beta_{i} k \\left( \\cdot , z_{i} \\right) \\land \\beta_{i} \\in \\mathbb{R} \\land z_{i} \\in X \\land \\left\\| f \\right\\| \u0026lt; \\infty \\right\\} \\subset H_{k} $$ を上記のように設定する。任意の目的関数 $c : \\left( D \\times \\mathbb{R} \\right) ^{m} \\to \\overline{\\mathbb{R}}$ と単調増加関数であるレギュライザーRegulizer $g : \\mathbb{R} \\to [0,\\infty)$ に対して、以下のように正則化された目的汎関数Regulized Objective Functional $L : \\mathcal{F} \\to \\overline{\\mathbb{R}}$ が定義されているとする。 $$ L (f) := c \\left( \\left( x_{1}, y_{1}, f \\left( x_{1} \\right) \\right), \\cdots , \\left( x_{m}, y_{m}, f \\left( x_{m} \\right) \\right) \\right) + g \\left( \\left\\| f \\right\\| \\right) $$ ここで、$H_{k}$ のノルム $\\left\\| \\cdot \\right\\|$ は、$k$ の正定値性Positive Definitenessによって次のように与えられる。 $$ \\left\\| \\sum_{i=1}^{\\infty} \\beta_{i} k \\left( \\cdot , z_{i} \\right) \\right\\|^{2} := \\sum_{i=1}^{\\infty} \\sum_{j=1}^{\\infty} \\beta_{i} \\beta_{j} k \\left( z_{i} , z_{j} \\right) \\ge 0 $$\n$\\mathbb{R}$ は実数の集合であり、$\\overline{\\mathbb{R}}$ は無限大 $\\infty$ を含む拡張実数である。 $\\mathbb{R}^{X}$ は定義域が $X$ で値域が $\\mathbb{R}$ である関数を集めた関数空間である。 レギュライザーとは、データに対する過学習を防ぐためのペナルティPenalty関数である。 汎関数とは、ざっくり言えば関数自体をインプットとして受け取る関数のことである。 ノンパラメトリックNonparametric $L (f)$ を最小化する関数 $f \\in \\mathcal{F}$ は、ある $\\left\\{ \\alpha_{i} \\right\\}_{i=1}^{m} \\subset \\mathbb{R}$ に対して次のような形で表される。 $$ f (\\cdot) = \\sum_{i=1}^{m} \\alpha_{i} k \\left( \\cdot , x_{i} \\right) $$\nセミパラメトリックSemiparametric $X$ で定義された実関数の集合が $\\left\\{ \\psi_{p} : X \\to \\mathbb{R} \\right\\}_{p=1}^{M}$ に対して行列 $\\left( \\psi_{p} \\left( x_{i} \\right) \\right)_{ip}$ のランクが $M$ であるとする。すると、$f \\in \\mathcal{F}$ と $h \\in \\span \\left\\{ \\psi_{p} \\right\\}$ に対して $$ c \\left( \\left( x_{1}, y_{1}, \\tilde{f} \\left( x_{1} \\right) \\right) , \\cdots , \\left( x_{m}, y_{m}, \\tilde{f} \\left( x_{m} \\right) \\right) \\right) + g \\left( \\left\\| f \\right\\| \\right) $$ を最小化する $\\tilde{f} = f + h$ は、ある $\\left\\{ \\alpha_{i} \\right\\}_{i=1}^{m}, \\left\\{ \\beta_{p} \\right\\}_{p=1}^{M} \\subset \\mathbb{R}$ に対して次のような形で表される。 $$ \\tilde{f} (\\cdot) = \\sum_{i=1}^{m} \\alpha_{i} k \\left( \\cdot , x_{i} \\right) + \\sum_{p=1}^{M} \\beta_{p} \\psi_{p} (\\cdot) $$\n説明 可能であれば、以下の二つのポストを先に読むことをお勧めする:\n再生カーネルヒルベルト空間 サポートベクターマシン 表現者 表現者定理Representer Theoremは、古典的な機械学習、特にサポートベクターマシンの文脈で最も重要な定理の一つとして、与えられたデータに対して私たちが近似しようとしている目的関数 $f$ が適切なカーネル $k$ に対して $$ f (\\cdot) = \\sum_{i=1}^{m} \\alpha_{i} k \\left( \\cdot , x_{i} \\right) $$ のような形で表されるという強力な定理である。ここでカーネルのインプットの一つが $x_{i}$ で固定された関数 $$ k \\left( \\cdot , x_{i} \\right) = \\phi \\left( x_{i} \\right) (\\cdot)\\in H_{k} $$ を表現者Representerと呼ぶ。これにより、表現者定理は「再生カーネルヒルベルト空間で学習データに適合したFitted任意の関数は、表現者たちの有限な線形結合で表すことができる」と要約することができる。特に、非線形回帰のためのサポートベクターマシンのカーネルトリックは、これに正確に合致している。\nこれはディープラーニングとシーベンコの定理の関係に似ている。 データサイエンスの文脈では、表現者 $\\phi \\left( x_{i} \\right) (\\cdot)$ はフィーチャーマップFeature Mapとも呼ばれ、任意のデータ $X$ を私たちがその特徴Featureを知ることができるヒルベルト空間に移し、その有限な和で私たちが求める関数を表現できるということは、これまで私たちが学んできた多くの機械学習技術がなぜ機能してきたのかを正当化する。もちろん、数学的な保証がなく\nてもそれらの技術はそれ自体で有効であるが、表現者定理があることで、それらの技術が理論的な基盤を築くことになるという点で非常に重要である。\n目的関数とレギュライザー 定理のステートメントでは、目的関数 $c$ とレギュライザー $g$ を非常に一般的に定義しているが、実際には多くの場合、$c$ はデータと $f$ の適合度を測る平均残差二乗、つまり $$ c = {{ 1 } \\over { m }} \\sum_{i=1}^{n} \\left( y_{i} - f \\left( x_{i} \\right) \\right)^{2} $$ と見なし、$g$ は二乗セミノルムペナルティ $g \\left( \\left\\| f \\right\\| \\right) = \\lambda \\left\\| f \\right\\|^{2}$ と見なしても問題ない1。\n注意すべき点は、この式の形や単語の意味だけを見て目的関数とレギュライザーを区別してはいけないということである。表現者定理の最も代表的な応用がサポートベクターマシンであるが、ソフトマージンを許容するソフトマージンSVMで扱う最小化問題は次のようである。 $$ \\begin{matrix} \\text{Minimize} \u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} + \\lambda \\sum_{k=1}^{n} \\xi_{k} \\\\ \\text{subject to} \u0026amp; \\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1 - \\xi_{k} \\\\ \u0026amp; \\xi_{k} \\ge 0 \\end{matrix} \\\\ k = 1, \\cdots , n $$\nここで、最適化自体だけを考えた場合、目的関数は実際には $$ {{ 1 } \\over { 2 \\lambda }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} + \\sum_{k=1}^{n} \\xi_{k} $$ と変わらず、この場合はデータとの乖離を考える $\\sum_{k=1}^{n} \\xi_{k}$ が $c$ であり、サポートベクターマシンの超平面 $f (\\mathbf{x}) = \\mathbf{w}^{T} + b$ から導かれる ${{ 1 } \\over { 2 \\lambda }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2}$ が $g$ として読まれるべきである。この最適化の意味を数学に置くか機械学習に置くかによって混乱することがあるが、\n数学の色が強い人はSVMを「まず線形回帰を終えてから例外を設けるもの」と見なし、$\\left\\| \\mathbf{w} \\right\\|_{2}^{2}$ を最初に最小化しようとする一方で、 データサイエンスの色が強い人はSVMを「まずデータをうまく分類し、その中で最もマージンの大きな超平面を見つけるもの」と見なし、$\\sum_{k=1}^{n} \\xi_{k}$ を最初に最小化しようとするためである。 どちらの視点も十分に共感できるものであり、表現者定理の応用がSVMだけでないことを考えると、ここでは暗記術のようなものを探そうとせず、問題に応じて能動的に考え受け入れる必要がある。\n証明 2 参考文献にあるように、ノンパラメトリック表現者定理のみを証明する。\nPart 1. $f = \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) + v$\n再生カーネルの定義: 関数 $k : X \\times X \\to \\mathbb{C}$ が以下の二つの条件を満たす場合、$H$ の再生カーネルReproducing Kernelと言う。\n(i): 表現者Representer: すべての $x \\in X$ に対して $$ k \\left( \\cdot , x \\right) \\in H $$ (ii) 再生性質Reproducing Property: すべての $x \\in X$ とすべての $f \\in H$ に対して $$ \\left\u0026lt; f , k \\left( \\cdot , x \\right) \\right\u0026gt; = f(x) = \\delta_{x} (f) $$ 特に、すべての $x_{1} , x_{2} \\in X$ に対して以下が成り立つ。 $$ k \\left( x_{1} , x_{2} \\right) = \\left\u0026lt; k \\left( \\cdot , x_{2} \\right), k \\left( \\cdot , x_{1} \\right) \\right\u0026gt; $$ 再生カーネル $k : X \\times X \\to \\mathbb{R}$ に対して $x \\mapsto k (\\cdot ,x)$ である(表現者)関数 $\\phi : X \\to \\mathbb{R}^{X}$ を定義する。$k$ は再生カーネルであるため、$x' \\in X$ で関数 $\\left( \\phi (x) \\right) (\\cdot)$ の関数値はすべての $x, x' \\in X$ に対して $$ \\left( \\phi (x) \\right) (x ') = k \\left( x' , x \\right) = \\left\u0026lt; \\phi \\left( x ' \\right) , \\phi (x) \\right\u0026gt; $$ である。ここで $\\left\u0026lt; \\cdot , \\cdot \\right\u0026gt;$ は $H_{k}$ の内積である。与えられた $\\left\\{ x_{i} \\right\\}_{i=1}^{m}$ に対して、任意の関数 $f \\in \\mathcal{F}$ は $\\span \\left\\{ \\phi \\left( x_{i} \\right) \\right\\}_{i=1}^{m}$ の部分とそれに直交するすべての $j$ に対して $$ \\left\u0026lt; v , \\phi \\left( x_{j} \\right) \\right\u0026gt; = 0 $$ を満たす $v \\in \\mathcal{F}$ とある $\\left( \\alpha_{1} , \\cdots , \\alpha_{m} \\right) \\subset \\mathbb{R}^{m}$ に対して次のように表現できる。 $$ f = \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) + v $$ ここで、 $$ \\begin{align*} L (f) :=\u0026amp; c \\left( \\left( x_{1}, y_{1}, f \\left( x_{1} \\right) \\right), \\cdots , \\left( x_{m}, y_{m}, f \\left( x_{m} \\right) \\right) \\right) + g \\left( \\left\\| f \\right\\| \\right) \\\\ =\u0026amp; c + g \\end{align*} $$ で、$c$ は $v$ に依存しないこと、$v = 0$ のとき $f$ が $L(f)$ を最小化することを議論する。\nPart 2. $c$ と $v$ は独立である\n関数 $f = f(\\cdot)$ と再生カーネル $k \\left( \\cdot , x_{j} \\right)$ の内積は再生性質により $$ \\begin{align*} =\u0026amp; \\left\u0026lt; f , k \\left( \\cdot , x_{j} \\right) \\right\u0026gt; \\\\ f \\left( x_{j} \\right) =\u0026amp; \\left\u0026lt; \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) + v , \\phi \\left( x_{j} \\right) \\right\u0026gt; \\\\ =\u0026amp; \\sum_{i=1}^{m} \\alpha_{i} \\left\u0026lt; \\phi \\left( x_{i} \\right) , \\phi \\left( x_{j} \\right) \\right\u0026gt; + 0 \\end{align*} $$ である。これは $v$ に独立であるため、$L (f) = c + g$ で学習データ $D$ と $f$ にのみ依存する $$ c = c \\left( \\left( x_{1}, y_{1}, f \\left( x_{1} \\right) \\right), \\cdots , \\left( x_{m}, y_{m}, f \\left( x_{m} \\right) \\right) \\right) $$ も $v$ に独立であることがわかる。\nPart 3. $g$ は $v = 0$ のとき最小化される\n(1): $v$ は $\\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right)$ に直交し、 (2): $g$ が単調関数であると仮定したため、 $$ \\begin{align*} \u0026amp; g \\left( \\left\\| f \\right\\| \\right) \\\\ =\u0026amp; g \\left( \\left\\| \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) + v \\right\\| \\right) \\\\ =\u0026amp; g \\left( \\sqrt{\\left\\| \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) + v \\right\\|^{2} + \\left\\| v \\right\\|^{2}} \\right) \u0026amp; \\because (1) \\\\ \\ge\u0026amp; g \\left( \\left\\| \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) \\right\\| \\right) \u0026amp; \\because (2) \\end{align*} $$ を得る。明らかに $v = 0$ のとき等式が成立し、$g$ が最小化されるためには $v=0$ でなければならない。一方、Part 2で $v$ は $c$ に影響を与えることができないことが確認されたため、$v = 0$ としても問題なく、$L = c + g$ を最小化する関数 $f$ は次のような形で表現できる。 $$ f (\\cdot) = \\sum_{i=1}^{m} \\alpha_{i} k \\left( \\cdot , x_{i} \\right) $$\n■\nWahba. (2019). Representer Theorem. https://pages.stat.wisc.edu/~wahba/ftp1/wahba.wang.2019submit.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSchölkopf. (2001). A Generalized Representer Theorem. https://link.springer.com/chapter/10.1007/3-540-44581-1_27\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2408,"permalink":"https://freshrimpsushi.github.io/jp/posts/2408/","tags":null,"title":"表現者の定理の証明"},{"categories":"머신러닝","contents":"定義 1 2 入力空間Input Space $X \\ne \\emptyset$ が定義域であり値域が複素数の集合 $\\mathbb{C}$ の写像 $f: X \\to \\mathbb{C}$ で構成される関数空間 $\\left( H , \\left\u0026lt; \\cdot , \\cdot \\right\u0026gt; \\right) \\subset \\mathbb{C}^{X}$ がヒルベルト空間であるとする。\n再生核ヒルベルト空間 固定された一つのデータDatum $x \\in X$ に対して、関数 $f \\in H$ を取り出す汎関数 $\\delta_{x} : H \\to \\mathbb{C}$ を**$x$ における(ディラックの)評価汎関数**(Dirac) Evaluation Functional at $x$という。 $$ \\delta_{x} (f) := f (x) $$ 全ての $x \\in X$ において評価汎関数 $\\delta_{x}$ が連続である場合、$H$ を再生核ヒルベルト空間RKHS, Reproducing Kernel Hilbert Spaceと呼び、$H_{k}$ と表記することもある。 関数 $k : X \\times X \\to \\mathbb{C}$ が以下の二つの条件を満たす場合、$H$ の再生核Reproducing Kernelという。 (i): 表現者Representer: 全ての $x \\in X$ に対して $$ k \\left( \\cdot , x \\right) \\in H $$ (ii) 再生性質Reproducing Property: 全ての $x \\in X$ と全ての $f \\in H$ に対して $$ \\left\u0026lt; f , k \\left( \\cdot , x \\right) \\right\u0026gt; = f(x) = \\delta_{x} (f) $$ 特に全ての $x_{1} , x_{2} \\in X$ に対して以下が成立する。 $$ k \\left( x_{1} , x_{2} \\right) = \\left\u0026lt; k \\left( \\cdot , x_{2} \\right), k \\left( \\cdot , x_{1} \\right) \\right\u0026gt; $$ 正定値カーネル 入力空間 $X \\ne \\emptyset$ からヒルベルト空間 $\\left( H , \\left\u0026lt; \\cdot , \\cdot \\right\u0026gt; \\right)$ への写像 $\\phi : X \\to H$ を特徴写像Feature Mapと呼ぶ。この文脈では、$H$ を特徴空間Featureと呼ぶこともある。 $\\left( H , \\left\u0026lt; \\cdot , \\cdot \\right\u0026gt; \\right)$ の内積 $\\left\u0026lt; \\cdot , \\cdot \\right\u0026gt; : H \\times H \\to \\mathbb{C}$ に対して、以下のように定義される関数 $k : X \\times X \\to \\mathbb{C}$ をカーネルKernelと呼ぶ。 $$ k \\left( x_{1} , x_{2} \\right) := \\left\u0026lt; \\phi \\left( x_{1} \\right) , \\phi \\left( x_{2} \\right) \\right\u0026gt; $$ $m$個のデータData $\\left\\{ x_{1} , \\cdots , x_{m} \\right\\} \\subset X$ に対して、以下のような行列 $K \\in \\mathbb{C}^{m \\times m}$ をカーネル $k$ のグラム行列Gram Matrixと呼ぶ。 $$ K := \\left( k \\left( x_{i} , x_{j} \\right) \\right)_{ij} $$ $k$ のグラム行列が正定値行列である場合、$k$ を正定値カーネルPositive Definite Kernelと呼ぶ。言い換えると、全ての $\\left\\{ c_{1} , \\cdots , c_{m} \\right\\} \\subset \\mathbb{C}$ に対して以下を満たすグラム行列を持つカーネル $k$ を正定値カーネルと呼ぶ。 $$ \\sum_{i=1}^{m} \\sum_{j=1}^{m} c_{i} \\bar{c_{j}} K_{ij} \\ge 0 $$ 説明 難しい内容だが、できるだけわかりやすく解説してみよう。\nデータ科学におけるヒルベルト空間の意味 ヒルベルト空間は内積が定義された完備空間である。通常、数学では内積とは何か特別な意味を持たせずに、単にいくつかの条件を満たす二変数スカラー関数として扱うが、機械学習の文脈では類似性の測定Measure of Similarityという概念として考えることができる。実際に、文書間の単語の頻度を比較するために使用されるコサイン類似度も内積を使用しており、別の例として三つのベクトル $$ A := \\left( 3, 0, 1 \\right) \\\\ B := \\left( 4, 1, 0 \\right) \\\\ C := \\left( 0, 2, 5 \\right) $$ がある場合、$A$ と $B$ が類似しており、$C$ とは異なると直感的に理解できる。しかし、これはまだ直感的な推論に過ぎず、内積を通して量化すると以下のようになる。 $$ A \\cdot B = 12 + 0 + 0 = 12 \\\\ A \\cdot C = 0 + 0 + 5 = 5 \\\\ B \\cdot C = 0 + 2 + 0 = 2 $$ 単に内積の絶対値が大きいか小さいかを見ただけでも、\u0026lsquo;見ればわかる\u0026rsquo;よりもはるかにデータをよく説明している。 定義で入力空間と呼んでいる $X$ には特に仮定がないことに注意する。実際のフィールドでは、どのような悪いデータを扱うか保証できない。例えば、$X$ が写真や文書データの場合、写真同士や文書同士を内積することは意味がない。 Q. $X$ が白黒写真の集合である場合、写真を行列と見なしてピクセルごとの値で内積を取れば良いのではないか？ A. それで良く、それが特徴写像 $\\phi : X \\to H$ である。この場合、$H$ は長方形 $[a,b] \\times [c,d]$ で定義された関数の空間となる。 このように考えると、カーネルの存在自体が既に\u0026rsquo;扱いにくいデータ\u0026rsquo;を私たちがよく知っている空間に持ち込むことと同じである。 上で述べた内積の意味が全て無意味であっても、内積空間ならばノルム空間であり距離空間であるため、私たちが常識的に存在すると考えるほとんどの仮定が成立する。内積 $\\left\u0026lt; \\cdot , \\cdot \\right\u0026gt;$ によって導かれるノルム $$ \\left\\| f \\right\\| := \\sqrt{ \\left\u0026lt; f , f \\right\u0026gt; } $$ があり、ノルム $\\left\\| f \\right\\|$ によってメトリック $$ d (f,g) = \\left\\| f -g \\right\\| $$ が導かれる。 データ科学の観点からノルムはデータに対する量化そのものである。例えば、白黒写真の全てのピクセルの値を合計した値をノルムとする場合、単にこれだけで写真がどれだけ明るいか暗いかを大まかに評価できる。 データ科学の観点から距離は二つのデータがどれだけ異なるかを教えてくれる。正しいか間違っているか、同じか異なるかを区別することは言うまでもなく重要である。 これらの理由をすべて抜きにしても、数式を展開していくと内積が必要になる場合がある。関連する例をここにすべて書くと非常に散漫になるので省略する。\u0026lsquo;サポートベクターマシン\u0026rsquo;の投稿のカーネルトリックの節を参照。 なぜ関数空間なのか？ これほどまでに難しくなければならないのか？ 数学というものはほとんどの応用で「私たちが探している関数」を見つけることである。\n補間は与えられたデータの間を埋める多項式を見つけることである。 統計的回帰分析はデータを最もよく説明する直線を見つける技術である。その直線は線形関数である。 ディープラーニングはそれをうまくやれないので活性化関数などを投入して非線形関数を近似する技術である。 フーリエ変換は関数を三角関数の線形結合として表す変換である。 これらの例を一つ一つ挙げていくときりがない。再び機械学習に戻って、私たちが関数空間を考える理由は、私たちが探しているものが結局のところ関数だからである。私たちは、その形が明示的Explicitではないかもしれないが、私たちが興味を持っているものを入れるInputと、\n私たちが望む結果を出すReturn関数を求めている。例えば、数字が書かれた写真を入れたときにその数字を返す、個人情報を入れたときにローンを返済できる確率を計算するなどの関数である。このような役に立つ関数が単純であるはずがなく、それらを知っている関数たちの合成のようなものを探したいと思っている。想像してみてほしい。健康診断の結果データ $x$ を受け取り、どれだけ健康かを計算してくれる関数を $f$ とすると、 $$ f( \\cdot ) = \\sum_{k=1}^{m} \\alpha_{k} \\phi_{k} \\left( x_{k} \\right)(\\cdot) $$ のように有限個の $\\phi_{k} (x) (\\cdot)$ を基底Basisとして持つ $f$ を探しているのである。特に $\\phi (x) = k (\\cdot , x)$ に対して、ある $f$ を見つけることができるという命題がまさに表現者定理である。\n表現者定理: 再生核ヒルベルト空間内で学習データに適合したFitted任意の関数は、表現者たちの有限の線形結合で表すことができる。\n要するに、機械学習（特にサポートベクターマシンの文脈）で私たちが見つけたいものが結局は関数であるため、それらが存在する関数空間について探求することは避けられない。\nもちろん、数学的な証明がなければ動かないプログラムはこの世に存在しない。必然的な学問であるとしても、全員に必須というわけではない。数学専攻でなければ、非常に難しいことが普通であり、どうしても難しいと思う場合は大まかに読み飛ばしても良い。\n評価関数の前になぜディラックの名前がついているのか？ $$ \\delta_{x_{0}} (x) = \\begin{cases} 1 \u0026amp; , \\text{if } x = x_{0} \\\\ 0 \u0026amp; , \\text{if } x \\ne x_{0} \\end{cases} $$ 元々ディラックのデルタ関数は上記のように一点でのみ値を持つ関数として知られている。正確な定義や用途はともかく、その変形は一点でのみ$0$でないという点を保持すれば、大抵ディラックの名前がつく。この意味を理解するための例として、二つの関数 $f : \\mathbb{R} \\to \\mathbb{R}$, $\\delta_{x_{0}} : \\mathbb{R} \\to \\mathbb{R}$ とその内積として $$ \\left\u0026lt; f, \\delta_{x_{0}} \\right\u0026gt; = \\sum_{x \\in \\mathbb{R}} f (x) \\delta_{x_{0}} (x) = f \\left( x_{0} \\right) $$ を想像してみる。通常関数の内積には積分を行うが、和ではないことや全ての $x \\in \\mathbb{R}$ に対して加算することが危険であることは理解しているが、最終的には概念と感覚が一致する部分があることがわかる。\nこのセンスで、$\\delta_{x_{0}} (f)$ は上記の議論を隠して単にその結果である $x_{0}$ で評価された $f \\left( x_{0} \\right)$ を、\u0026rsquo;$x_{0}$ において一点だけを得る\u0026rsquo;関数としている。\n再生性質と呼ぶ理由 再生核ヒルベルト空間の定義を読むと非常に興味深い。通常、数学で「何かの空間」と言うと、その定義自体が「何か」が存在する空間としているが、RKHSは突然「評価汎関数が全ての点で連続である」というヒルベルト空間として定義されているためである。\nリース表現定理: $\\left( H, \\left\\langle \\cdot,\\cdot \\right\\rangle \\right)$がヒルベルト空間であるとする。$H$の線形汎関数 $f \\in H^{ \\ast }$と$\\mathbf{x} \\in H$ に対して $f ( \\mathbf{x} ) = \\left\\langle \\mathbf{x} , \\mathbf{w} \\right\\rangle$および$\\| f \\|_{H^{\\ast}} = \\| \\mathbf{w} \\|_{H}$ を満たす$\\mathbf{w} \\in H$ が一意に存在する。\nムーア-アロンサジン定理Moore-Aronsajn Theorem: 正定値カーネルが存在する場合、それに対応するRKHSが一意に存在する。\nこの定義によれば、RKHSに再生核が一意に存在するという命題さえ自明ではなく、実際にはリース表現定理によってRKHSに再生核が一意に存在することが保証される。興味深いことに、逆に再生核\nに対応するRKHSも一意に存在する。\nこれで、定義にある数式を一つ一つ詳しく見てみよう。\n元々$k : X \\times X \\to \\mathbb{C}$ において、関数$k$に入れることができるのは$x_{1}, x_{2} \\in X$だが、定義で述べたように$x$を一つ固定すると、$k$は実質的に$k : y \\mapsto k (y,x)$となる$k : X \\to \\mathbb{C}$となる。関数として扱う立場からは、片方の入力を塞いだものであり、 $$ \\left\u0026lt; f , k \\left( \\cdot , x \\right) \\right\u0026gt; = f(x) $$ のような表現は、単に二つの関数$f (\\cdot) : X \\to \\mathbb{C}$と$k \\left( \\cdot , x \\right): X \\to \\mathbb{C}$を内積したものに過ぎない。「それがどうして$f$から出てきて、外にある内積が$x$とどう関連しているのか\u0026hellip;」と複雑に考える必要はない。$f(x) \\in \\mathbb{C}$も単に内積の結果であり、値域が複素数集合であるため、出てきた何らかの複素数に過ぎない。\nここで、再生再生, Reproducingという性質の命名について触れておきたい。Reproductionという単語自体が、その生成原理に従ってRe-(再び, 再) -produce(作る, 生)という意味を持ち、その最初の翻訳は繁殖/生殖、二番目の翻訳はコピー/複製、三番目の翻訳は再生である。繁殖は明らかに無意味であり、コピーと言うには元がない。\nしかし、$f(\\cdot)$と$k (\\cdot, x)$を内積したときに$f(x)$を得るということを、$f$が持っていた情報をカーネルによって「再生」したものと考えたらどうだろうか？私たちが時刻$t$に依存するYouTubeの動画$y(t)$という関数を持っていると想像してみよう。私たちは$y$そのものを見るのではなく、$t$が増加するにつれて再生される$\\left\\{ y(t) : t \\in [0,T] \\right\\}$自体を見ている。このような比喩から、カーネル$k$は$f$を関数そのものとしてではなく、関数値を再生してくれる「再生カーネル」と呼ばれる資格がある。\n特徴マップと不便な表記について カーネルと再生カーネルの定義をよく見ると、実際にはこれらは定義のために相互に必要としていないことがわかる。カーネルはカーネルであり、再生カーネルは再生カーネルであり、これらが一致するのは特徴マップFeature Mapが表現者Representerであるとき、つまり $$ \\phi (x) = k \\left( \\cdot , x \\right) $$ のときである。特徴マップはその名の通り、元のデータを私たちが扱いやすい形に変換してくれる変換であり、このような関数たちによって何らかの関数が表されるということは、その関数がデータから来る何らかの特徴Featureによって説明されるということと同じである。一つの問題は、ここまで直感的に何となく理解できたとしても、依然として$k \\left( \\cdot , x \\right)$のような表記が不便であり、特徴マップではなく内積から始まって別々に定義されるカーネルの動機Motiveに共感するのが難しいということである。\n特徴マップは$\\phi : X \\to H$であるため、その関数値は$x \\in X$に対応する何らかの関数$\\lambda : X \\to \\mathbb{C}$であり、これが通常混乱を招くものではない。$\\phi (x)$をもっと正確に書くと $$ \\left( \\phi (x) \\right) (\\cdot) = k \\left( \\cdot , x \\right) $$ であり、なぜこのようにしてまで点$\\cdot$を保持し、不便な表記を使用するのか疑問に思うかもしれない。ほとんどの人間は、このようにしなかった場合にもっと苦労する例を見ると、理解しやすくなる。前述したように、カーネルであれ再生カーネルであれ、結局私たちが一貫して関心を持っている空間は関数空間$H$であり、$H$の内積は関数の内積である。まず、ある関数$f$がデータ$\\left\\{ x_{i} \\right\\}_{i=1}^{m}$の表現者$\\phi \\left( x_{i} \\right)$たちの線形結合として表されるとすると $$ f (y) = \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) (y) = \\sum_{i=1}^{m} \\alpha_{i} \\left( \\phi \\left( x_{i} \\right) \\right) (y) $$ となり、すでにかなり複\n雑になっていることがわかる。これに新しい関数$g$とデータ$\\left\\{ x'_{j} \\right\\}_{j=1}^{n}$を考えると $$ g (y) = \\sum_{j=1}^{n} \\beta_{j} \\left( \\phi \\left( x'_{j} \\right) \\right) (y) $$ となる。一方で、$f$と$g$の内積を使わないのであれば、内積空間を考える理由がないが、$\\left\u0026lt; f,g \\right\u0026gt;$を書くと $$ \\left\u0026lt; f,g \\right\u0026gt; = \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\bar{\\alpha_{i}} \\beta_{j} \\left\u0026lt; \\phi \\left( x_{i} \\right) , \\phi \\left( x'_{j} \\right) \\right\u0026gt; $$ となり、余計なものが多くなる。内積をする前には、いずれにせよ関数空間を扱う上で$y \\in X$を実際に扱うことはほとんどなく、内積をした後には、既に知っている$\\phi$と内積を続けて書く必要がある。これを見ると、 $$ f (\\cdot) = \\sum_{i=1}^{m} \\alpha_{i} k \\left( \\cdot , x_{i} \\right) \\\\ g (\\cdot) = \\sum_{j=1}^{n} \\beta_{j} k \\left( \\cdot , x'_{j} \\right) \\\\ \\left\u0026lt; f,g \\right\u0026gt; = \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\bar{\\alpha_{i}} \\beta_{j} k \\left( x_{i} , x'_{j} \\right) $$ のような表記が面倒であるだけではないことに気がつくかもしれない。\n再生カーネルは正定値である データ$\\left\\{ x_{k} \\right\\}_{k=1}^{m}$が与えられたとすると、$k$がカーネルである場合、以下が成立する。 $$ \\begin{align*} \u0026amp; \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\bar{\\alpha_{i}} \\alpha_{j} k \\left( x_{i} , x_{j} \\right) \\\\ =\u0026amp; \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\left\u0026lt; \\alpha_{i} \\phi \\left( x_{i} \\right) , \\alpha_{j} \\phi \\left( x_{j} \\right) \\right\u0026gt; \\\\ =\u0026amp; \\left\u0026lt; \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) , \\sum_{j=1}^{m} \\alpha_{j} \\phi \\left( x_{j} \\right) \\right\u0026gt; \\\\ =\u0026amp; \\left\\| \\sum_{i=1}^{m} \\alpha_{i} \\phi \\left( x_{i} \\right) \\right\\|^{2} \\\\ \\ge \u0026amp; 0 \\end{align*} $$ 前述したように$\\phi : x \\mapsto k (\\cdot , x)$とすると、再生カーネル$k$はカーネルであるため正定値である。このようなカーネルの正定値性は、カーネルに関連するさまざまな性質で自然に現れる。\n関数解析以外のカーネル (1) 通常、数学でカーネルと言えば抽象代数のカーネル$\\ker$を指す。代数構造$Y$で$0$が定義されている場合、関数$f : X \\to Y$に対して$\\ker f := f^{-1} \\left( \\left\\{ 0 \\right\\} \\right)$を$f$のカーネルと言う。 (2) この概念が線形代数で特殊化されたものが線形変換のカーネルである。 カーネルが難しいと感じる場合、関数解析を専攻していない数学者にいきなりカーネルについて聞いても、十中八九(1)の意味で理解するだろう。あなたのバックグラウンドが数学に基づいているならば、当然(1)くらいは知っている必要があり、そうでなくても(2)くらいは知っているべきである。\n名前のついたカーネル 機械学習の文脈では、以下のようなカーネルが知られている。3 これらは一見カーネルのように見えないかもしれないが、カーネルの和と積が依然としてカーネルであるという事実を通じて導かれる。\nリニアカーネル: $$ k \\left( x_{1} , x_{2} \\right) = \\left\u0026lt; x_{1} , x_{2} \\right\u0026gt; $$ ポリノミアルカーネル: $c \\ge 0$ と $d \\in \\mathbb{N}$ に対して、 $$ k \\left( x_{1} , x_{2} \\right) = \\left( \\left\u0026lt; x_{1} , x_{2} \\right\u0026gt; + c \\right) ^{d} $$ ガウシアンカーネル: $\\sigma^{2} \u0026gt; 0$ に対して、 $$ k \\left( x_{1} , x_{2} \\right) = \\exp \\left( - {{ \\left\\| x_{1} - x_{2} \\right\\| } \\over { 2 \\sigma^{2} }} \\right) $$ シグモイドカーネル: $w, b \\in \\mathbb{C}$ に対して、 $$ k \\left( x_{1} , x_{2} \\right) = \\tanh \\left( w \\left\u0026lt; x_{1} , x_{2} \\right\u0026gt; + b \\right) $$ Sejdinovic, Gretton. (2014). What is an RKHS?: p7~11. http://www.stats.ox.ac.uk/~sejdinov/teaching/atml14/Theory_2014.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSchölkopf. (2001). A Generalized Representer Theorem. https://link.springer.com/chapter/10.1007/3-540-44581-1_27\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJakkula. (2006). Tutorial on Support Vector Machine (SVM). https://course.ccs.neu.edu/cs5100f11/resources/jakkula.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2406,"permalink":"https://freshrimpsushi.github.io/jp/posts/2406/","tags":null,"title":"機械学習における政府号カーネルと再生カーネルのヒルベルト空間"},{"categories":"머신러닝","contents":"モデル 1 簡単な定義 二値分類Binary Classificationが可能なデータを最もよく区別する直線や平面を見つける方法をサポートベクターマシンという。\n難しい定義 内積空間 $X = \\mathbb{R}^{p}$ とラベリングLabeling $Y = \\left\\{ -1, +1 \\right\\}$ に対し、$n$ 個のデータを集めた学習データセットTraining Datasetを $D = \\left\\{ \\left( \\mathbf{x}_{k} , y_{k} \\right) \\right\\}_{k=1}^{n} \\subset X \\times Y$ とし、 $$ \\begin{align*} X^{+} :=\u0026amp; \\left\\{ \\mathbf{x}_{k} \\in X : y_{k} = +1 \\right\\} \\\\ X^{-} :=\u0026amp; \\left\\{ \\mathbf{x}_{k} \\in X : y_{k} = -1 \\right\\} \\end{align*} $$\nとする。あるウェイトWeight $\\mathbf{w} \\in \\mathbb{R}^{p}$ とバイアスBias $b \\in \\mathbb{R}$ を持つ線形関数 $f \\left( \\mathbf{x} \\right) = \\mathbf{w}^{T} \\mathbf{x} + b$ によって作られる超平面を $H : \\mathbf{w}^{T} \\mathbf{x} + b = 0$ とするとき、$H$ と最も距離が近い $\\mathbf{x}^{+} \\in X^{+}$ と $\\mathbf{x}^{-} \\in X^{-}$ をサポートベクターSupport Vectorといい、これらの間の距離 $\\delta$ をマージンMarginという。これに対して $$ \\begin{align*} f \\left( \\mathbf{x}^{+} \\right) =\u0026amp; +1 \\\\ f \\left( \\mathbf{x}^{-} \\right) =\u0026amp; -1 \\end{align*} $$\nを満たしながらマージンが最大になるような $\\mathbf{w} , b$ を見つける機械学習技術をサポートベクターマシンSVM, Support Vector Machineという。\n$\\mathbb{R}$ は実数の集合であり、$\\mathbb{R}^{p}$ は$p$次元ユークリッド空間である。 $X \\times Y$ は二つの集合のデカルト積を意味する。 $\\mathbf{w}^{T}$ は $\\mathbf{w}$ の転置行列であり、$\\mathbf{w}^{T} \\mathbf{x}$ は二つのベクトル $\\mathbf{w}, \\mathbf{x}$ の内積 $\\left\u0026lt; \\mathbf{w} , \\mathbf{x} \\right\u0026gt;$ である。 説明 簡単に言えば、次の図のようにオレンジ色と空色のデータを二分する線や平面を見つけることである。平面図では赤い矢印で示されているのがサポートベクターに該当する。\n図では $2$次元なので線を見つけ、$3$次元なので平面を見つけたが、さらに大きな$p$次元になると超平面を見つけなければならず、図示するのは難しくなる。しかし、このように空間を二つに分けるという点は変わらない。学習データセットで二値分類が完了すれば、新しいデータを受け取ったときも$f$ に入れて線形分類器Linear Classifierとして使えばよい。\n当然ながら、同じデータを二値分類しても、左側が右側よりも良い。右側の場合、空色のデータに対するマージンが過度である。具体的にこれを求める方法は、いずれにせよパッケージがすべて自動で処理するため、知らなくてもよい。\n学部生レベルであれば、ここまでの簡単な定義を受け入れて図で大まかに理解するだけでも、今後実際に使用する際や用語を理解する上で大きな問題はない。これより少し難しい内容、実践的な要点の要約、Pythonの例示コードなどは、国内のウェブでもよく整理された文書がたくさんある。 2 3 4\n内積空間 ご覧の通り、SVM自体は概念的にそれほど難しくはないが、数学的な定義を引き出し数式を記述した理由は、今後具体的に、理論的に話すことが多いためである。\nユークリッド空間 $\\mathbb{R}^{p}$ はもちろんベクトル空間であり、内積空間でもあり、内積空間は距離空間であるため距離空間でもある。これを強調するのは、実際のデータの世界で内積空間というのが思ったよりも良い仮定であるためである。例えば、画像や文書、分子構造などをSVMにそのまま入れてもいいのか、頭を悩ませることになる。定義では暗黙のうちに「距離が近い」やベクトルの内積が含まれる線形関数 $f$ を使用しているが、理論に近づくほど、これらの仮定を当然とすることはできない。\nサポートベクター 元々このような幾何問題では、境界Boundary上にあるものをサポートと呼ぶが、例えば最小包含円問題でも円を決定する円周上の点をサポートとする。SVMの起源となったサポートベクターも同様で、$\\mathbf{x}^{+}, \\mathbf{x}^{-}$ は二つの集合 $X^{+}, X^{-}$ の観点から見ても、$\\delta/2$ から$X^{+}, X^{-}$ の距離に位置する境界上にある。\nサポートベクターが$H$ それぞれで一意である保証はないが、今後の議論で一意性が重要ではないため、一般性を失わずに一意であると仮定しよう。$\\left\\{ \\mathbf{x}_{k} \\right\\}_{k=1}^{n}$ のマージンにはデータが存在せず、 $$ f \\left( \\mathbf{x} \\right) \\begin{cases} \\ge +1 \u0026amp; , \\text{if } \\mathbf{x} \\in X^{+} \\\\ \\le -1 \u0026amp; , \\text{if } \\mathbf{x} \\in X^{-} \\end{cases} $$ であるため、すべての$\\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1$ に対して$H$ でなければならない。\nマージンの最大化 サポートベクターは$H$ と最も近い点であるため、$\\delta/2$ との距離$H$ はサポートベクターが$\\mathbf{w}$ 方向に垂直に離れたときの距離である。このマージンは$\\mathbf{x}^{+}$ でも$\\mathbf{x}^{-}$ でも同じであり、両方とも超平面$H$ との距離が$\\delta/2$ であることは、二つのサポートベクター間の距離が $$ \\delta \\mathbf{w} = \\mathbf{x}^{+} - \\mathbf{x}^{-} $$ として表されることを意味する。ここで$\\mathbf{x}^{+} - \\mathbf{x}^{-}$ のような演算は$X$ がベクトル空間であるという仮定に基づいて許可される。$\\delta \\mathbf{w} = \\mathbf{x}^{+} - \\mathbf{x}^{-}$ の両辺に$\\mathbf{w}$ と内積を取ると、つまり$\\mathbf{w}^{T}$ を左側に掛けると$f$ の定義に従って $$ \\begin{align*} \u0026amp; \\delta \\mathbf{w} = \\mathbf{x}^{+} - \\mathbf{x}^{-} \\\\ \\implies \u0026amp; \\delta \\mathbf{w}^{T} \\mathbf{w} = \\mathbf{w}^{T} \\mathbf{x}^{+} - \\mathbf{w}^{T} \\mathbf{x}^{-} \\\\ \\implies \u0026amp; \\delta \\left\\| \\mathbf{w} \\right\\|_{2}^{2} = \\left( \\mathbf{w}^{T} \\mathbf{x}^{+} + b \\right) - \\left( \\mathbf{w}^{T} \\mathbf{x}^{-} + b \\right) \\\\ \\implies \u0026amp; \\delta \\left\\| \\mathbf{w} \\right\\|_{2}^{2} = +1 - (-1) \\\\ \\implies \u0026amp; \\delta \\left\\| \\mathbf{w} \\right\\|_{2}^{2} = 2 \\\\ \\implies \u0026amp; \\delta = {{ 2 } \\over { \\left\\| \\mathbf{w} \\right\\|_{2}^{2} }} \\end{align*} $$ を得る。つまり、マージンを最大化することは目的関数 $\\left\\| \\mathbf{w} \\right\\|_{2}^{2} / 2$ を最小化することであり、要約するとSVMとは次のような最適化問題を解くオプティマイザーOptimizerである。 $$ \\begin{matrix} \\text{Minimize} \u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} \\\\ \\text{subject to} \u0026amp; \\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1 \\end{matrix} \\\\ k = 1, \\cdots , n $$\n派生モデル 難しい定義に従えば、SVMは直線であれ超平面であれ、いずれにしても線形関数を見つける線形回帰モデルであるが、当然ながらここで満足するわけがない。\nソフトマージンSVM 例えば、次のようなデータが入ってきたとしよう。SVMはデータが混在している中央部分のために、これを完全に二値分類することができない。\nここで、サポートベクターのマージンにデータが存在できないという制約のもとで$\\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1$ という条件を満たさなければならなかったことに注目してみよう。この不等式を$1$ より小さい値に許容すれば、完全な二値分類ではないにしても、完全に諦めるよりは良い結果をもたらすだろう。そして、この許容を各データごとに$\\xi_{k} \\ge 0$ とすると、新たな制約条件$\\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1 - \\xi_{k}$ を得る。このように条件が緩和されたマージンをソフトマージンSoft Marginという。\nもちろん、制約が少し緩和されたとはいえ、すべてを$\\xi_{l} = \\cdots = \\xi_{n} = 1$ にしてしまうとSVM自体を放棄してしまうことになる。これを防ぐためには、目的関数に$\\sum_{k} \\xi_{k}$ のような項を加えることがある。これは不可能な二値分類を可能にしたことに対する代償Penaltyである。もちろん、このような単純なペナルティはデータのスケールによっては全く意味がなかったり、逆に過敏に反応したりするため、$0 \\le \\sum_{k} \\xi_{k} \\le n$ そのままではなく、適切な正の数$\\lambda \u0026gt; 0$ を掛けて追加することにしよう。 $$ \\begin{matrix} \\text{Minimize} \u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} + \\lambda \\sum_{k=1}^{n} \\xi_{k} \\\\ \\text{subject to} \u0026amp; \\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1 - \\xi_{k} \\\\ \u0026amp; \\xi_{k} \\ge 0 \\end{matrix} \\\\ k = 1, \\cdots , n $$\nカーネルトリック 例えば、上のようなデータが与えられた場合、ソフトマージンであろうとなかろうと、SVMでは決して二値分類することができないように見える。しかし、よく見ると$0$ に近い側には空色の点が集まっており、外側にはオレンジ色の点が現れていることが明らかである。この情報を活用するために、次のように$z$ 軸を新たに作ってみよう。 $$ \\phi (x,y) := (x,y, x^{2} + y^{2}) $$\n上の図は、下の図を適切にキャプチャしたものである。下はマウスで対話可能な3D空間なので、いろいろと回して見てみてください。\n元の$\\mathbb{R}^{2}$ ではデータを二分する直線を見つけるのが難しかったが、このようにデータを説明する次元を増やした$\\mathbb{R}^{3}$ では、適切な平面でデータを分類するSVMを使用できるようになった。ここで自然に思い浮かぶ疑問は、「それでは、このように便利な変換$\\phi$ をカーネルKernelと呼び、カーネルを使用する方法をカーネルトリックKernel Trickと呼ぶのか？」ということである。半分正しく、半分間違っている。$\\phi$ にさらに一歩進んで、内積まで含まれたものがカーネルである。\n再びマージンの最大化に戻って、我々に与えられた最適化問題を再検討してみよう。 $$ \\begin{matrix} \\text{Minimize} \u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} \\\\ \\text{subject to} \u0026amp; \\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1 \\end{matrix} \\\\ k = 1, \\cdots , n $$\n制約条件$\\left| f \\left( \\mathbf{x}_{k} \\right) \\right| \\ge 1$ は見た目はすっきりしているが、実際にこの問題を解く際にはあまり役に立たない。元の学習データセットでの形に戻すと、$k = 1 , \\cdots , n$ に対して $$ \\begin{cases} f \\left( \\mathbf{x}_{k} \\right) \\ge 1 \u0026amp; , \\text{if } y_{k} = 1 \\\\ f \\left( \\mathbf{x}_{k} \\right) \\le -1 \u0026amp; , \\text{if } y_{k} = -1 \\end{cases} \\\\ \\implies \\begin{cases} y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) \\ge 1 \u0026amp; , \\text{if } y_{k} = 1 \\\\ y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) \\ge 1 \u0026amp; , \\text{if } y_{k} = -1 \\end{cases} \\\\ \\implies y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) \\ge 1 $$ でなければならない。このような制約条件自体を目的関数に反映させて、制約条件がないかのように扱う方法がラグランジュ乗数法である。$y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) - 1 \\ge 0$ に$\\alpha_{k} \\ge 0$ を掛けた項を元の目的関数から引いた$L(\\mathbf{w}, b)$ に対して、次の最適化問題を得る。 $$ \\begin{matrix} \\text{Minimize} \u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} - \\sum_{k=1}^{n} \\alpha_{k} \\left[ y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) - 1 \\right] \\\\ \\text{subject to} \u0026amp; \\alpha_{k} \\ge 0 \\end{matrix} \\\\ k = 1, \\cdots , n $$\n再び強調するが、我々の目的はこの目的関数を最小化する$\\mathbf{w}, b$ を見つけることであった。$\\mathbf{w}, b$ に対する目的関数の偏微分が$0$ となる条件は次の通りである。 $$ \\begin{align*} {{ \\partial L } \\over { \\partial \\mathbf{w} }} = 0 \\implies \u0026amp; \\mathbf{w} = \\sum_{k=1}^{n} \\alpha_{k} y_{k} \\mathbf{x}_{k} \\\\ {{ \\partial L } \\over { \\partial b }} = 0 \\implies \u0026amp; 0 = \\sum_{k=1}^{n} \\alpha_{k} y_{k} \\end{align*} $$\nこれをそのまま$L$ に代入してみると $$ \\begin{align*} \u0026amp; L(\\mathbf{w},b) \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\left\\| \\mathbf{w} \\right\\|_{2}^{2} - \\sum_{k=1}^{n} \\alpha_{k} \\left[ y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) - 1 \\right] \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\mathbf{w}^{T} \\mathbf{w} - \\sum_{k=1}^{n} \\alpha_{k} y_{k} \\left( \\mathbf{w}^{T} \\mathbf{x}_{k} + b \\right) + \\sum_{k=1}^{n} \\alpha_{k} \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\mathbf{w}^{T} \\sum_{k=1}^{n} \\alpha_{k} y_{k} \\mathbf{x}_{k} - \\sum_{k=1}^{n} \\alpha_{k} y_{k}\\mathbf{w}^{T} \\mathbf{x}_{k} - b \\sum_{k=1}^{n} \\alpha_{k} y_{k} - \\sum_{k=1}^{n} \\alpha_{k} \\\\ =\u0026amp; - {{ 1 } \\over { 2 }} \\sum_{k=1}^{n} \\alpha_{k} y_{k} \\mathbf{w}^{T} \\mathbf{x}_{k} - b \\cdot 0 + \\sum_{k=1}^{n} \\alpha_{k} \\\\ =\u0026amp; \\sum_{k=1}^{n} \\alpha_{k} - {{ 1 } \\over { 2 }} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} y_{i} a_{j} y_{j} \\mathbf{x}_{i}^{T} \\mathbf{x}_{j} \\\\ =\u0026amp; \\sum_{k=1}^{n} \\alpha_{k} - {{ 1 } \\over { 2 }} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} a_{j} y_{i} y_{j} \\mathbf{x}_{i}^{T} \\mathbf{x}_{j} \\\\ =\u0026amp; L \\left( \\alpha_{1} , \\cdots , \\alpha_{n} \\right) \\end{align*} $$\nを得る。当然ながら、具体的な$\\mathbf{w}$ と$b$ を計算するためには、学習データ$\\left\\{ \\left( \\mathbf{x}_{k}, y_{k} \\right) \\right\\}_{k=1}^{n}$ が必要である。\nここで注目すべき点は、数式で$\\mathbf{x}_{i}$ と$\\mathbf{x}_{j}$ の内積が使用されていることである。結局のところ、最終的に、我々は内積を取らなければならず、$X$ が内積空間でなければ、このように順調に進む保証はない。逆に言えば、$X$ が内積空間でなくても、変換$\\phi$ が$X$ を内積空間に送ることができれば、その目的関数が $$ \\sum_{k=1}^{n} \\alpha_{k} - {{ 1 } \\over { 2 }} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} a_{j} y_{i} y_{j} \\phi \\left( \\mathbf{x}_{i} \\right) ^{T} \\phi \\left( \\mathbf{x}_{j} \\right) $$ であるSVMを検討する価値がある。機械学習では、このように二つのベクトルに対する変換、内積まで含まれた関数 $$ K \\left( \\mathbf{x}_{i}, \\mathbf{x}_{j} \\right) := \\left\u0026lt; \\phi \\left( \\mathbf{x}_{i} \\right) , \\phi \\left( \\mathbf{x}_{j} \\right) \\right\u0026gt; $$ をカーネルKernelと呼ぶこともある。[ 注: データサイエンスでは、これと混同される別のカーネルも存在する。元々、数学全般でのカーネルは、名前は同じでも全く異なる機能の関数である。 ]\n数式的にここまでの内容を受け入れることができれば、なぜカーネルではなく変換$\\phi$ を導入することをカーネルトリックと呼び、変換後に内積空間であることが保証されることが重要なのかを理解したことになる。\n条件を満たす限り、カーネルはいくつかの種類を考えることができる。特に元のSVMも線形カーネルLinear Kernel $$ K \\left( \\mathbf{x}_{i}, \\mathbf{x}_{j} \\right) = \\left\u0026lt; \\mathbf{x}_{i}, \\mathbf{x}_{j} \\right\u0026gt;^{1} = \\mathbf{x}_{i}^{T} \\mathbf{x}_{j} $$ を使用したものと見なすことができる。\n参照 カーネルトリックの部分で数学的に簡単な内容を扱ったが、より深い理論に興味がある場合は、SVMを超えて以下の内容を学ぶことをお勧めする。\n機械学習におけるカーネルと再生カーネルヒルベルト空間 表現者定理の証明 コード 以下はカーネルトリックを実装したJuliaのコードである。\nstruct Sphere\rd::Int64\rend\rSphere(d) = Sphere(d)\rimport Base.rand\rfunction rand(Topology::Sphere, n::Int64)\rdirection = randn(Topology.d, n)\rboundary = direction ./ sqrt.(sum(abs2, direction, dims = 1))\rreturn boundary\rend\rusing Plots\rA = 0.3rand(Sphere(2), 200) + 0.1randn(2, 200)\rB = rand(Sphere(2), 200) + 0.1randn(2, 200)\rscatter(A[1,:],A[2,:], ratio = :equal, label = \u0026#34;+1\u0026#34;)\rscatter!(B[1,:],B[2,:], ratio = :equal, label = \u0026#34;-1\u0026#34;)\rpng(\u0026#34;raw.png\u0026#34;)\rPlots.plotly()\rϕ(z) = z[1]^2 + z[2]^2\rscatter(A[1,:],A[2,:],ϕ.(eachcol(A)), ms = 1, label = \u0026#34;+1\u0026#34;)\rscatter!(B[1,:],B[2,:],ϕ.(eachcol(B)), ms = 1, label = \u0026#34;-1\u0026#34;)\rsavefig(\u0026#34;kernel.html\u0026#34;) Jakkula. (2006). サポートベクターマシン（SVM）に関するチュートリアル. https://course.ccs.neu.edu/cs5100f11/resources/jakkula.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://ratsgo.github.io/machine%20learning/2017/05/23/SVM/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-2%EC%84%9C%ED%8F%AC%ED%8A%B8-%EB%B2%A1%ED%84%B0-%EB%A8%B8%EC%8B%A0-SVM\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://hleecaster.com/ml-svm-concept/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2402,"permalink":"https://freshrimpsushi.github.io/jp/posts/2402/","tags":null,"title":"サポートベクターマシン"},{"categories":"위상데이터분석","contents":"定義 1 2 簡単な定義 ユークリッド空間 $\\left( \\mathbb{R}^{d} , \\left\\| \\cdot \\right\\|_{2} \\right)$ と正数 $\\varepsilon \u0026gt; 0$ が与えられたとしよう。有限集合 $S \\subset \\mathbb{R}^{d}$ に対し、以下の2つの条件を満たす単体複体 $\\text{VR}_{\\varepsilon} (S)$ をビエトリス-リプス複体Vietoris-Rips Complexという。\n(i): $S$ を頂点の集合とする。 (ii): $k$-シンプレックス $\\left[ v_{0} , v_{1} , \\cdots, v_{k} \\right]$ が $\\text{VR}_{\\varepsilon} (S)$ に属することは、すべての $0 \\le i,j \\le k$ に対し、以下が成立することと同値である。 $$ \\left\\| v_{i} - v_{j} \\right\\|_{2} \\le 2 \\varepsilon $$ 難しい定義 距離空間 $\\left( X, d \\right)$ と正数 $\\varepsilon \u0026gt; 0$ が与えられたとしよう。有限集合 $S \\subset X$ に対し、以下のように定義された抽象単体複体 $\\text{VR}_{\\varepsilon} (S)$ をビエトリス-リプス複体Vietoris-Rips Complexという。 $$ \\text{VR}_{\\varepsilon} (S) := \\left\\{ \\sigma \\subset S : \\diam \\sigma \\le 2 \\varepsilon \\right\\} $$ ここで、$\\diam$ は直径Diameterとして、与えられた集合 $\\sigma \\subset X$ の全ての点間の距離のスープリームにより定義される。 $$ \\diam \\sigma := \\sup_{x_{1} , x_{2} \\in \\sigma} d \\left( x_{1} , x_{2} \\right) $$\n説明 ビエトリス-リプス複体Vietoris-Rips Complexは、しばしば単にVR複体またはリプス複体とも呼ばれる。\n少し異なる表現だが、簡単な定義であれ難しい定義であれ、核心はシンプレックスがなるための条件が、そこに含まれるすべての点の距離が $2 \\varepsilon$ 以下であることである。これを図で描くと、次のように $0$-シンプレックスの頂点を中心に、半径 $\\varepsilon$ の球体を考えた時、2つの球が触れ合うことを基準に $1$-シンプレックスを作り、3つの頂点が $1$-シンプレックスで繋がれば $2$-シンプレックスを作るという風にVR複体を構築できる。\nビエトリス-リプス複体とチェック複体の違い ビエトリス-リプス複体: $$\\text{VR}_{\\varepsilon} (S) := \\left\\{ \\sigma \\subset S : \\diam \\sigma \\le 2 \\varepsilon \\right\\}$$\nチェック複体: $$\\check{C}_{\\varepsilon} (S) := \\left\\{ \\sigma \\subset S : \\bigcap_{x \\in \\sigma} B_{d} \\left( x ; \\varepsilon \\right) \\ne \\emptyset \\right\\}$$\n次の図は、同じデータに対するビエトリス-リプス複体とチェック複体を比較したものである。上は $\\text{VR}_{\\varepsilon}$ 、下は $\\check{C}_{\\varepsilon}$ である。\nビエトリス-リプスは $k$-シンプレックスを含む条件として、そのFaceたる $k-1$-シンプレックスの存在だけを要求する。例えば、$1$-シンプレックスである $AB$、$BC$、$CA$ が存在するため、$2$-シンプレックスである $ABC$ が含まれる。 それに対し、チェック複体は、これらのフェースを含むだけでなく、$ABC$ 自体もその球が1点で結びつかなければならない。これは、チェック複体を構築する条件がビエトリス-リプス複体を構築するよりも厳格であることを意味し、次の観察につながる。 $$ \\check{C}_{\\varepsilon} \\subset \\text{VR}_{\\varepsilon} $$ さらに、次の事実が知られている。 $$ \\check{C}_{\\varepsilon} \\subset \\text{VR}_{\\varepsilon} \\subset \\check{C}_{2 \\varepsilon} $$ 定義から推測できることは、チェック複体がビエトリス-リプス複体に比べて少し多くの情報を持つ可能性があるという点である。$\\text{VR}_{\\varepsilon}$ は与えられた点が連結しているだけで、$k-1$-シンプレックスの存在が当然であると言っている。つまり、シンプレックスの存在とそのすべてのフェースの存在性の間には差がない。例えば, $2$-シンプレックスである $ABC$ が存在するということや、$1$-シンプレックスである $AB$、$BC$、$CA$ が存在するということは、正確に等価であり、大胆に言えば、一方を知っていれば、残りのものは無意味な情報である。一方、$\\check{C}_{\\varepsilon}$ は、$AB$、$BC$、$CA$ が存在することだけでは、$ABC$ の存在を断言できず、別途の確認が必要である。この点で、チェック複体はビエトリス-リプス複体に比べて、データをもう少し細かく見ていると期待できる。\n一方、実際の計算の観点からは、ビエトリス-リプス複体を構築する計算コストは圧倒的に安価である。確かに、$\\text{VR}_{\\varepsilon}$ が単に固定された点間の距離を計算するのに対し、$\\check{C}_{\\varepsilon}$ はその球がどこで交わるかもわからず、交わったとしてもそれが $\\varepsilon$ より長いか短いかもわからない。この問題を克服し、チェック複体を構築する方法の一つとして、Welzlアルゴリズムなどを使用して最小包含円を見つける方法が考慮されるが、その一回一回の計算はかなり高価であり、その高価な計算を可能なすべての点の組み合わせに対して繰り返さなければならない。\nRaul Rabadan. (2020). Topological Data Analysis for Genomics and Evolution: p126.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nEdelsbrunner, Harer. (2010). Computational Topology An Introduction: p74.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2401,"permalink":"https://freshrimpsushi.github.io/jp/posts/2401/","tags":null,"title":"ベトリス-リプス コンプレックスの定義"},{"categories":"위상데이터분석","contents":"概要 代数位相Algebraic Topologyにおいて、幾何学的な意味を考えずに単に定義だけを述べると、ベッチ数Betti Numberとは、単にチェインコンプレックスでのホモロジーグループのランクに過ぎない。問題は、このような説明がベッチ数の意味を知りたい人にとって全く役に立たず、その具体的な計算も難解であり、例を通して学ぶことも困難であることである。\nこの投稿では、少なくとも2つ目の質問に対する答え―ベッチ数をどのように計算するかについての整理とその詳細な証明を紹介する。以下に紹介される定理によれば、与えられたチェインコンプレックスに従ってある行列を見つけることができ、それに関する一連の計算プロセスを通じて、以下のような明示的Explicitな公式を導出することができる。 $$ \\beta_{p} = \\rank ?_{1} - \\rank ?_{2} $$\n本来、数学的な内容は数学を使わずに伝えられることが最も良い説明であるが、ベッチ数の場合は、その公式の導出過程の中でその根本的な原理を理解することができると考えられる。学部生程度では証明の難易度がかなり高く、追いかけるのが難しいかもしれないが、できるだけ省略せずに詳細に書いたので、少なくとも一度は試みることをお勧めする。\n定理 ホモロジーグループの定義:\n$n \\in \\mathbb{N}_{0}$ とする。アーベル群 $C_{n}$ と ホモモルフィズム $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ のチェイン $$ \\cdots \\longrightarrow C_{n+1} \\overset{\\partial_{n+1}}{\\longrightarrow} C_{n} \\overset{\\partial_{n}}{\\longrightarrow} C_{n-1} \\longrightarrow \\cdots \\longrightarrow C_{1} \\overset{\\partial_{1}}{\\longrightarrow} C_{0} \\overset{\\partial_{0}}{\\longrightarrow} 0 $$ これがすべての $n$ に対して $$ \\partial_{n} \\circ \\partial_{n+1} = 0 $$ を満たす場合、$\\mathcal{C} := \\left\\{ \\left( C_{n}, \\partial_{n} \\right) \\right\\}_{n=0}^{\\infty}$ をチェインコンプレックスChain Complexという。 商群 $H_{n} := \\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$ を $\\mathcal{C}$ の**$n$番目のホモロジーグループ**$n$-th Homology Groupという。 ホモモルフィズム $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ を境界Boundaryまたは微分Differentialオペレーターという。 $Z_{n} := \\ker \\partial_{n}$ の要素を**$n$-サイクル**Cycles、$B_{n} := \\text{Im} \\partial_{n+1}$ の要素を**$n$-境界**Boundaryという。 フリーチェインコンプレックスの標準基底分解 チェインコンプレックス $\\mathcal{C} := \\left\\{ \\left( C_{p}, \\partial_{p} \\right) \\right\\}$ のすべての $C_{p}$ が有限ランクのフリーグループであるとする。するとすべての $p$ と $Z_{p} := \\ker \\partial_{p}$ に対して、次を満たす部分群 $U_{p}, V_{p}, W_{p} \\subset C_{p}$ と が存在する。 $$ \\begin{align*} C_{p} =\u0026amp; U_{p} \\oplus V_{p} \\oplus W_{p} \\\\ =\u0026amp; U_{p} \\oplus Z_{p} \\end{align*} $$ $$ \\begin{align*} \\partial_{p} \\left( U_{p} \\right) \\subset \u0026amp; W_{p} \\\\ Z_{p} =\u0026amp; V_{p} \\oplus W_{p} \\end{align*} $$ もちろん、$Z_{p}$ は $\\partial_{p}$ の核であるため、$\\partial_{p} \\left( V_{p} \\right) = 0$ であり、$\\partial_{p} \\left( W_{p} \\right) = 0$ である。さらに、$U_{p}$ での $\\partial_{p}$ の制限関数 ${\\partial_{p}}_{| U_{p}} : U_{p} \\to W_{p-1}$ は、次のような形のスミス標準形を持つ。 $$ \\begin{bmatrix} b_{1} \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; \\cdots \u0026amp; b_{l} \\end{bmatrix} $$ ここで、$b_{i} \\in \\mathbb{N}$ であり、$b_{1} \\mid \\cdots \\mid b_{l}$ である。\nホモロジーグループの効率的な計算可能性 1 $H_{p} \\left( \\mathcal{C} \\right)$ のベッチ数を$\\mathcal{C}$ の$p$番目のベッチ数Betti Numberという。有限コンプレックス$K$ の$\\beta_{p}$ は次のようである。 $$ \\beta_{p} = \\rank Z_{p} - \\rank B_{p} $$ その具体的な値は、次のように$\\partial_{p}$ のスミス標準形によって計算することができる。図では、青い点線が$1$ の対角成分を、オレンジの実線が$1$ でない対角成分を示し、その他のすべての成分は$0$ である。2\nここで重要なのは、スミス標準形における$1$ の数$\\rank B_{p-1}$ と、ゼロベクトルの列の数$\\rank Z_{p}$ である。\n証明 3 Part 1. $B_{p} \\subset W_{p} \\subset Z_{p} \\subset C_{p}$\n$$ \\begin{align*} Z_{p} :=\u0026amp; \\ker \\partial_{p} \\\\ B_{p} :=\u0026amp; \\text{Im} \\partial_{p+1} \\\\ W_{p} :=\u0026amp; \\left\\{ c_{p} \\in C_{p} : \\lambda c_{p} \\in B_{p} , \\forall m \\ne 0 \\right\\} \\end{align*} $$ と置く。特に、$W_{p}$ は$C_{p}$ の部分群となり、$\\lambda = 1$ のみを考えた場合に$B_{p} = W_{p}$ であるという点で、境界Boundary$B_{p}$ の条件を弱めたものと見なすことができるため、弱い境界Weak Boundariesと呼ばれる。\n$W_{p}$ の定義から、$\\lambda \\ne 1$ を考えると $$ B_{p} \\subset W_{p} $$ $Z_{p}$ の定義から、$\\forall z_{p} \\in Z_{p}$ は$\\partial_{p} z_{p} = 0$ であり、$Z_{p} = \\ker \\partial_{p}$ は$\\partial_{p} : C_{p} \\to C_{p-1}$ であるため $$ Z_{p} \\subset C_{p} $$ $C_{p}$ はフリーグループと仮定されているため、トーションフリー、すなわち$\\forall z_{p} \\in Z_{p} \\subset C_{p}$ に対して$\\lambda z_{p} = 0$ を満たす$\\lambda \\ne 0$ が存在しない。一方、すべての$c_{p+1} \\in C_{p+1}$ に対して $$ \\partial_{p+1} c_{p+1} = \\lambda z_{p} \\in W_{p} $$ の両辺に$\\partial_{p}$ を適用すると $$ 0 = \\partial_{p} \\partial_{p+1} c_{p+1} = \\partial_{p} \\lambda z_{p} = \\lambda \\partial_{p} z_{p} $$ であるため、$\\partial_{p} z_{p} = 0$ でなければならない。これは、$\\lambda z_{p} \\in W_{p}$ ならば$\\lambda z_{p} \\in Z_{p}$ であることを意味するため $$ W_{p} \\subset Z_{p} $$ このような考察から、次の包含関係を得る。 $$ B_{p} \\subset W_{p} \\subset Z_{p} \\subset C_{p} $$\nPart 2. $W_{p} \\subset Z_{p}$ は$Z_{p}$ の直和群Direct Summandである\n$p$番目のホモロジーグループ$H_{p} \\left( \\mathcal{C} \\right) = Z_{p} / B_{p}$ の定義から $$ \\text{proj}_{1} : Z_{p} \\to H_{p} \\left( \\mathcal{C} \\right) $$ は剰余類$B_{p}$ に相当するだけのランクが下がった射影であり $H_{p} \\left( \\mathcal{C} \\right)$ のトーション部分群$T_{p} \\left( \\mathcal{C} \\right) \\subset H_{p} \\left( \\mathcal{C} \\right)$ に対して $$ \\text{proj}_{2} : H_{p} \\left( \\mathcal{C} \\right) \\to H_{p} \\left( \\mathcal{C} \\right) / T_{p} \\left( \\mathcal{C} \\right) $$ も射影である。 第1同型定理: 準同型写像$\\phi : G \\to G'$ が存在する場合 $$G / \\ker ( \\phi ) \\simeq \\phi (G)$$\nこれにより、$\\text{proj} := \\text{proj}_{1} \\circ \\text{proj}_{2}$ として定義された $$ \\text{proj} : Z_{p} \\to H_{p} \\left( \\mathcal{C} \\right) / T_{p} \\left( \\mathcal{C} \\right) $$ も射影である。$W_{p}$ の要素は$\\partial_{p+1} d_{p+1}$ のように表されるため、この射影$\\text{proj}$ の核は$W_{p}$ であり、すべての射影は全射Surjectionであるため、第1同型定理により $$ Z_{p} / W_{p} \\simeq H_{p} / T_{p} $$ が成立する。ここで、右辺の$H_{p}$ がどのようになっているかにかかわらず、トーション部分群$T_{p}$ で取り除いたため、トーションフリーであり、これにより、左辺の$Z_{p} / W_{p}$ もトーションフリーであることが保証される。したがって、$\\alpha_{1} , \\cdots , \\alpha_{k}$ が$Z_{p} / W_{p}$ の基底であり、$\\alpha'_{1} , \\cdots , \\alpha'_{l} \\in W_{p}$ が$W_{p}$ の基底であるとした場合、$\\alpha_{1} , \\cdots , \\alpha_{k}, \\alpha'_{1} , \\cdots , \\alpha'_{l}$ は$Z_{p}$ の基底となる。したがって、$Z_{p}$ は $$ Z_{p} = V_{p} \\oplus W_{p} $$ のように、$\\alpha_{1} , \\cdots , \\alpha_{k}$ を基底とする部分群$V_{p}$ と$W_{p}$ の直和として表現できる。\nPart 3. $Z_{p}, B_{p-1}, W_{p-1}$ の基底\nホモモルフィズムのスミス標準形: フリーアーベル群$G$ と$G'$ のランクがそれぞれ$n,m$ であり、$f : G \\to G'$ がホモモルフィズムである場合、次のような行列を持つホモモルフィズム$g$ が存在する。 $$ \\begin{bmatrix} d_{1} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\ddots \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; d_{r} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\end{bmatrix} \\in \\mathbb{Z}^{m \\times n} $$ ここで、$d_{1} , \\cdots, d_{r} \\in \\mathbb{N}$ であり、$d_{1} \\mid \\cdots \\mid d_{r}$、つまり$d_{k}$ は$d_{k+1}$ の約数Divisorである必要がある。\n$\\partial_{p} : C_{p} \\to C_{p-1}$ は、次のようなスミス標準形の$m \\times n$ 行列を持つ。\n$$ \\begin{matrix} \u0026amp; \\begin{matrix} e_{1} \u0026amp; \\cdots \u0026amp; e_{l} \u0026amp; e_{l} \u0026amp; \\cdots \u0026amp; e_{n} \\end{matrix} \\\\ \\begin{matrix} e'_{1} \\\\ \\vdots \\\\ e'_{l} \\\\ e'_{l} \\\\ \\vdots \\\\ e'_{m} \\end{matrix} \u0026amp; \\begin{bmatrix} d_{1} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\ddots \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; d_{r} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\end{bmatrix} \\end{matrix} $$\nこれにより、我々は直接的な計算を通じて次の3つを示すことになる:\n(1): $e_{l+1} , \\cdots , e_{n}$ は$Z_{p}$ の基底である。 (2): $b_{1} e'_{1} , \\cdots , b_{l} e'_{l}$ は$B_{p-1}$ の基底である。 (3): $e'_{1} , \\cdots , e'_{l}$ は$W_{p-1}$ の基底である。 補題\n$\\partial_{p}$ の定義により、一般的な$c_{p} \\in C_{p}$ に対して次が成立する。 $$ c_{p} = \\sum_{i=1}^{n} a_{i} e_{i} \\implies \\partial_{p} c_{p} = \\sum_{i=1}^{l} a_{i} b_{i} e'_{i} $$ (1): $b_{i} \\ne 0$ であるため、$Z_{p} = \\ker \\partial_{p}$ である必要十分条件は、$i = 1 \\cdots , l$ に対して$a_{i} = 0$ であることである。したがって、$e_{l+1} , \\cdots , e_{n}$ は$Z_{p}$ の基底である。 (2): すべての$\\partial_{p} c_{p} \\in B_{p-1}$ は$b_{1} e'_{1} , \\cdots , b_{l} e'_{l}$ の線形結合として表現され、$b_{i} \\ne 0$ であるため、$b_{1} e'_{1} , \\cdots , b_{l} e'_{l}$ は$B_{p-1}$ の基底である。 (3): $b_{i} e'_{i} = \\partial e_{i}$ であるため、まず$e'_{1}, \\cdots, e'_{l} \\in W_{p-1}$ である。逆に、$c_{p-1} \\in C_{p-1}$ を $$ c_{p-1} = \\sum_{i=1}^{m} d_{i} e'_{i} $$ と置き、$c_{p-1} \\in W_{p-1}$ と仮定すると、$W_{p-1}$ が$W_{p-1} = \\left\\{ c_{p} \\in C_{p} : \\lambda c_{p} \\in B_{p} , \\forall m \\ne 0 \\right\\}$ のように定義されていたため、$c_{p-1}$ はある$\\lambda \\ne 0$ に対して $$ \\lambda c_{p-1} = \\partial c_{p} = \\sum_{i=1}^{l} a_{i} b_{i} e'_{i} $$ の形で表現できる。係数を比較すると、$i \u0026gt; l$ に対して $$ \\lambda d_{i} = 0 \\implies d_{i} = 0 $$ を得る。したがって、$e'_{1} , \\cdots , e'_{l}$ は$W_{p-1}$ の基底である。 Part 4. \u0026lsquo;フリーチェインコンプレックスの標準基底分解\u0026rsquo;の証明\n$C_{p}$ と$C_{p-1}$ に対して、これまでの議論で登場する$e_{1} , \\cdots , e_{l}$ によって生成されるフリーグループを$U_{p}$ とすると、$Z_{p} = V_{p} \\oplus W_{p}$ であるため、$\\partial V_{p} = \\partial W_{p} = 0$ であり $$ \\begin{align*} C_{p} =\u0026amp; U_{p} \\oplus Z_{p} \\\\ =\u0026amp; U_{p} \\oplus \\left( V_{p} \\oplus W_{p} \\right) \\end{align*} $$ を得る。ここで、$W_{p}$ と$Z_{p}$ は$C_{p}$ により一意であるが、$U_{p}$ と$V_{p}$ は必ずしも一意である必要はないことに注意されたい。\nPart 5. \u0026lsquo;ホモロジーグループの効率的な計算可能性\u0026rsquo;の証明\nPart 4により、コンプレックス$K$ に対して、次の分解が存在することが保証される。 $$ \\begin{align*} C_{p} \\left( K \\right) =\u0026amp; U_{p} \\oplus V_{p} \\oplus W_{p} \\\\ Z_{p} =\u0026amp; V_{p} \\oplus W_{p} \\end{align*} $$\n直和の性質: $G = G_{1} \\oplus G_{2}$ としよう。もし$H_{1}$ が$G_{1}$ の部分群であり、$H_{2}$ が$G_{2}$ の部分群である場合、$H_{1}$ と$H_{2}$ も直和として表現でき、特に次が成立する。 $${{ G } \\over { H_{1} \\oplus H_{2} }} \\simeq {{ G_{1} } \\over { H_{1} }} \\oplus {{ G_{2} } \\over { H_{2} }}$$\n[1]: $H_{1} \\simeq G_{1}$ であり、$H_{2} \\simeq \\left\\{ 0 \\right\\}$ とすると $$ G / G_{1} \\simeq G_{2} $$ [2]: $H_{1} \\simeq \\left\\{ 0 \\right\\}$ とすると $$ {{ G } \\over { H_{2} }} \\simeq G_{1} \\oplus {{ G_{2} } \\over { H_{2} }}$$ Part 1で$B_{p} \\subset W_{p} \\subset Z_{p} \\subset C_{p}$ であったため、直和の性質により $$ \\begin{align*} H_{p} \\left( K \\right) =\u0026amp; Z_{p} / B_{p} \\\\ =\u0026amp; \\left( {{ V_{p} \\oplus W_{p} } \\over { B_{p} }} \\right) \\\\ =\u0026amp; V_{p} \\oplus \\left( {{ W_{p} } \\over { B_{p} }} \\right) \u0026amp; \\because [2] \\\\ =\u0026amp; \\left( {{ Z_{p} } \\over { W_{p} }} \\right) \\oplus \\left( {{ W_{p} } \\over { B_{p} }} \\right) \u0026amp; \\because [1] \\end{align*} $$ を得る。ここで、$H_{p} \\left( K \\right) = \\left( Z_{p} / W_{p} \\right) \\oplus \\left( W_{p} / B_{p} \\right)$ の\n$Z_{p} / W_{p}$ はフリーパートであり $W_{p} / B_{p}$ はトーションパートである。 これにより、$K$ の$p$番目のベッチ数$\\beta_{p}$ は、次のように求められる。 $$ \\begin{align*} \\beta_{p} =\u0026amp; \\rank H_{p} \\left( K \\right) \\\\ =\u0026amp; \\rank \\left[ \\left( Z_{p} / W_{p} \\right) \\oplus \\left( W_{p} / B_{p} \\right) \\right] \\\\ =\u0026amp; \\rank \\left( Z_{p} / W_{p} \\right) + \\rank \\left( W_{p} / B_{p} \\right) \\\\ =\u0026amp; \\left[ \\rank Z_{p} - \\rank W_{p} \\right] + \\left[ \\rank W_{p} - \\rank B_{p} \\right] \\\\ =\u0026amp; \\rank Z_{p} - \\rank B_{p} \\end{align*} $$\n一方、$H_{p-1}(K)$ のトーションパートと$b_{1} | \\cdots | b_{l} \\in \\mathbb{N}$ に対しては、次のようなアイソモルフィズムが存在することが分かる。 $$ W_{p-1} / B_{p-1} \\simeq \\left( {{ \\mathbb{Z} } \\over { b_{1} \\mathbb{Z} }} \\right) \\oplus \\cdots \\oplus \\left( {{ \\mathbb{Z} } \\over { b_{l} \\mathbb{Z} }} \\right) $$ ここで、$i \\le l$ に対して$b_{i} = 1$ であること、つまり$B_{p-1}$ のランクが$l$ であることは $$ \\mathbb{Z} / b_{i} \\mathbb{Z} = \\mathbb{Z} / \\mathbb{Z} = \\left\\{ 0 \\right\\} $$ であるため、$W_{p-1}$ のランクが$l$ 分だけ減少することを覚えておく。\n■\n例 トーラス $$ \\begin{align*} \\beta_{0} =\u0026amp; 1 \\\\ \\beta_{1} =\u0026amp; 2 \\\\ \\beta_{2} =\u0026amp; 1 \\end{align*} $$\nトーラスのベッチ数は上記のように知られている。このトーラスのチェインコンプレックスが上の図のように定義されている場合、例として$\\beta_{1} = 2$ のみを計算してみよう。上で導出された公式を使用せずに単に数学的に考えて計算する方法もあるが、読めば分かる通り、頭が痛くなるほど難しい。これと対照的に、「ホモロジーを効率的に計算する」ということがどれほど便利かを見てみよう。\nホモモルフィズムのスミス標準形: フリーアーベルグループ$G$ と$G'$ に対して、$a_{1} , \\cdots , a_{n}$ が$G$ の基底であり、$a_{1}' , \\cdots , a_{m}'$ が$G'$ の基底であるとする。もし関数$f : G \\to G'$ がホモモルフィズムであれば、次を満たす唯一の整数の集合$\\left\\{ \\lambda_{ij} \\right\\} \\subset \\mathbb{Z}$ が存在する。 $$ f \\left( a_{j} \\right) = \\sum_{i=1}^{m} \\lambda_{ij} a_{i}' $$ この時行列$\\left( \\lambda_{ij} \\right) \\in \\mathbb{Z}^{m \\times n}$ を($G$ と$G'$ の基底に関する)$f$ の行列という。\n$\\beta_{1} = \\rank Z_{1} - \\rank B_{1}$ であるため、少なくとも境界行列$\\left( \\partial_{1} \\right)$ と$\\left( \\partial_{2} \\right)$ を求める必要がある。すべての$a , b, c \\in C_{1} (T)$ に対して $$ \\begin{align*} \\partial_{1} (a) =\u0026amp; v - v = 0 = 0v \\\\ \\partial_{1} (b) =\u0026amp; v - v = 0 = 0v \\\\ \\partial_{1} (c) =\u0026amp; v - v = 0 = 0v \\end{align*} $$ であるため $$ \\left( \\partial_{1} \\right) = \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} \\implies Z_{1} = 3 , B_{0} = 0 $$ を得る。$Z_{p}$ は行列の右側のゼロベクターの数であり、$B_{p-1}$ は行列内の$1$ の数である。次に、$\\partial_{2}$ を考えると $$ \\begin{align*} \\partial_{2} (U) =\u0026amp; -a -b +c \\\\ \\partial_{2} (L) =\u0026amp; a + b - c \\end{align*} $$ であるため $$ \\left( \\partial_{2} \\right) = \\begin{bmatrix} -1 \u0026amp; 1 \\\\ -1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{bmatrix} \\sim \\begin{bmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\end{bmatrix} \\implies Z_{2} = 1 , B_{1} = 1 $$ を得る。これを総合すると、トーラスの$1$番目のベッチ数$\\beta_{1}$ は、次のように計算される。 $$ \\beta_{1} = \\rank Z_{1} - \\rank B_{1} = 3 - 1 = 2 $$ 当然ながら、この結果は、この投稿に紹介された定理に従って、フリーグループがどうであり、アイソモルフィズムがどうであるかといった、あらゆる数学的知識を駆使して得た値と一致することが保証されている。少し大胆に言えば、頭を使わずに指示された通りに計算すれば、ベッチ数、つまり「ホモロジー」を「計算」することができると要約できるだろう。もう少し良い言い方をすると、コンピュータを通じて位相数学を研究する道が開かれたということだ。\nMunkres. (1984). Elements of Algebraic Topology: p58.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nEdelsbrunner, Harer. (2010). Computational Topology An Introduction: p104.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMunkres. (1984). Elements of Algebraic Topology: p58~61.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2399,"permalink":"https://freshrimpsushi.github.io/jp/posts/2399/","tags":null,"title":"ホモロジーグループのベッチ数"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 $2$キュービット $\\ket{a, b} = \\ket{a} \\otimes \\ket{b}$に対して 交換 ゲートexchange gate $\\text{ex}$を次のように定義する。\n$$ \\begin{align*} \\text{ex} : (\\mathbb{C}^{2})^{\\otimes 2} \u0026amp;\\to (\\mathbb{C}^{2})^{\\otimes 2} \\\\ \\ket{a, b} \u0026amp;\\mapsto \\ket{b, a},\\quad \\forall a,b \\in \\left\\{ 0, 1 \\right\\} \\end{align*} $$\n$$ \\text{ex} (\\ket{a} \\otimes \\ket{b}) = \\ket{b} \\otimes \\ket{a} $$\n説明 交換ゲートは二つのキュービットの状態を互いに交換する。具体的な入出力は次の通りである。\n$$ \\text{ex} (\\ket{00}) = \\ket{00} \\\\[0.5em] \\text{ex} (\\ket{01}) = \\ket{10} \\\\[0.5em] \\text{ex} (\\ket{10}) = \\ket{01} \\\\[0.5em] \\text{ex} (\\ket{11}) = \\ket{11} $$\n行列表現は次のようである。\n$$ \\text{ex} = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\nキム・ヨンフン・ホ・ジェソン, 量子情報理論 (2020), p97\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3429,"permalink":"https://freshrimpsushi.github.io/jp/posts/3429/","tags":null,"title":"交換ゲート"},{"categories":"줄리아","contents":"概要 Juliaでは、複数のデバイスに計算タスクをスケジューリングする方法を紹介する1。正直、自分もよくわからない。\nコード using Distributed\rip_ = []\rfor last in [160,161,162,163,164,32,33,34,35,36,43,44,45,46,47]\rpush!(ip_, join([155,230,211,last],\u0026#39;.\u0026#39;))\rend\rsort!(ip_)\rfor ip in ip_\raddprocs([(\u0026#34;chaos@\u0026#34; * ip, 8)]; dir =\u0026#34;/home/chaos\u0026#34;, exename = \u0026#34;julia\u0026#34;) #add slave node\\\u0026#39;s workers\rprintln(\u0026#34;ip $ip\u0026#34; * \u0026#34; passed\u0026#34;)\rend\rnworkers()\r@everywhere function f(n)\rreturn n^2 - n end\rA = pmap(f,1:20000)\rX = []\r@async @distributed for i in 1:200\rprint(f(i))\rpush!(X, f(i))\rend pmapはうまくいくけど、@distributedはダメだ。\n環境 OS: Windows julia: v1.7.0 https://thomaswiemann.com/assets/teaching/Fall2021-Econ-31720/Econ_31720_discussion_6.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2398,"permalink":"https://freshrimpsushi.github.io/jp/posts/2398/","tags":null,"title":"ジュリアでの分散コンピューティングの方法"},{"categories":"줄리아","contents":"概要 Juliaでは、多次元配列を参照するためのインデックスタイプであるCatesianIndexを提供している1。もちろんCatesianという名前は、集合の積であるデカルト積から来ている。\nコード julia\u0026gt; M = rand(0:9, 4,4)\r4×4 Matrix{Int64}:\r9 3 7 0\r8 6 2 1\r3 8 4 9\r5 6 8 2 例えば、行列Mの3行4列目の要素、9にアクセスしたいとしよう。\njulia\u0026gt; pt = (3,4)\r(3, 4)\rjulia\u0026gt; M[pt]\rERROR: LoadError: ArgumentError: invalid index: (3, 4) of type Tuple{Int64, Int64}\rjulia\u0026gt; M[pt[1],pt[2]]\r9 直感的には、タプルpt = (3,4)をそのまま使えば良さそうだが、プログラミングに慣れている人なら、この方法に問題があることがわかるだろう。一般的に、このような二次元配列、特に行列を参照する時には、pt[1],pt[2]のように、二つの整数をはっきりと分けて入れなければならない。\njulia\u0026gt; pt = CartesianIndex(3,4)\rCartesianIndex(3, 4)\rjulia\u0026gt; M[pt]\r9 ありがたいことに、Juliaではこのインデックスを丸ごと渡すことができるCatesianIndexが提供されている。タプルをそのままCatesianIndexに変換して参照すれば、望んでいた結果を得ることができる。\n全コード M = rand(0:9, 4,4)\rpt = (3,4)\rM[pt]\rM[pt[1],pt[2]]\rpt = CartesianIndex(3,4)\rM[pt] 環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/base/arrays/#Base.IteratorsMD.CartesianIndex\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2394,"permalink":"https://freshrimpsushi.github.io/jp/posts/2394/","tags":null,"title":"ジュリアの多次元インデックス"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 $n \\in \\mathbb{N}$について、以下の[ユニタリ作用素]$G$を量子ゲートquantum gateまたは**$n$キュービットゲート**と呼ぶ。\n$$ G : \\left( \\mathbb{C}^{2} \\right)^{\\otimes n} \\to \\left( \\mathbb{C}^{2} \\right)^{\\otimes n} $$\n量子ゲートの合成を量子回路quantum circuitと呼ぶ。ここで$\\otimes$はベクトル空間のテンソル積である。\n説明 古典コンピュータでのゲートと回路を、量子コンピュータで定義したものである。ユニタリ作用素は可逆であるため、量子回路で構成された量子コンピュータから出力された値から、入力された値を求めることが常に可能である。\n種類 パウリゲート アダマールゲート $H$ 位相ゲート $R_{\\theta}$ 김영훈·허재성, 양자 정보 이론 (2020), p93-96\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3424,"permalink":"https://freshrimpsushi.github.io/jp/posts/3424/","tags":null,"title":"量子ゲートと量子回路"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 $\\mathbb{C}$ 上のベクトル空間 $\\mathbb{C}^{2}$の二つの単位ベクトル$\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$、$\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$をディラック記法で以下のように表記しよう。\n$$ \\ket{0} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\qquad \\ket{1} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} $$\n集合$\\left\\{ \\ket{0}, \\ket{1} \\right\\}$の要素を キュビットqubit、量子ビットと呼ぶ。\n$\\mathbb{C}^{2}$の$n$テンソル積 $\\left( \\mathbb{C}^{2} \\right)^{\\otimes n} = \\overbrace{\\mathbb{C}^{2} \\otimes \\cdots \\otimes \\mathbb{C}^{2}}^{n}$の標準基底\n$$ \\left\\{ \\ket{0} \\otimes \\cdots \\otimes \\ket{0}, \\dots, \\ket{1} \\otimes \\cdots \\otimes \\ket{1} \\right\\}$$\nの要素を $n$キュビット$n$qubitと呼ぶ。\n説明 キュビットはquantum bitの略語です。ビットbitが古典コンピューターで情報処理の最小単位であるならば、キュビットは量子コンピューターでその役割を果たしている。\n$n$キュビットは次のように簡単に表示される。$a = (a_{0}, a_{1}, \\dots, a_{n-1}) \\in \\left\\{ 0, 1 \\right\\}^{n}$を$n$ビットと呼ぶならば、\n$$ \\begin{align*} \\ket{a} \u0026amp;= \\ket{a_{0}, a_{1}, \\dots, a_{n-1}} \\\\ \u0026amp;= \\ket{a_{0} a_{1} \\dots a_{n-1}} \\\\ \u0026amp;= \\ket{a_{0}} \\otimes \\ket{a_{1}} \\otimes \\cdots \\otimes \\ket{a_{n-1}} \\end{align*} $$\n例：$(\\mathbb{C}^{2}) ^{\\otimes 2}$ 最も簡単な例として$(\\mathbb{C}^{2}) ^{\\otimes 2} = \\mathbb{C}^{2} \\otimes \\mathbb{C}^{2} \\cong \\mathbb{C}^{4}$の場合を具体的に見よう。$2$キュビットは次のように表記される。\n$$ \\ket{00} = \\ket{0,0} = \\ket{0} \\otimes \\ket{0},\\qquad \\ket{01} = \\ket{0,1} = \\ket{0} \\otimes \\ket{1} \\\\ \\ket{10} = \\ket{1,0} = \\ket{1} \\otimes \\ket{0},\\qquad \\ket{11} = \\ket{1,1} = \\ket{1} \\otimes \\ket{1} $$\nそれぞれの$2$キュビットを行列で表すと、クロネッカー積の定義に従って次の通りである。\n$$ \\begin{align*} \\ket{00} \u0026amp;= \\ket{0} \\otimes \\ket{0} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\otimes \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\\\[1em] 0 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\\\ \\ket{01} \u0026amp;= \\ket{0} \\otimes \\ket{1} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\otimes \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\\\[1em] 0 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\\\ \\ket{10} \u0026amp;= \\ket{1} \\otimes \\ket{0} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\otimes \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\\\[1em] 1 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\\\ \\ket{11} \u0026amp;= \\ket{1} \\otimes \\ket{1} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\otimes \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\\\[1em] 1 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\end{align*} $$\nしたがって$\\braket{ik | jl} = \\delta_{ij}\\delta_{kl}$である。このとき$\\delta$はクロネッカーデルタである。任意の$(\\mathbb{C}^{2}) ^{\\otimes 2}$の要素は次のようである。\n$$ \\begin{align*} \u0026amp; (\\alpha_{0}\\ket{0} + \\alpha_{1}\\ket{1}) \\otimes (\\beta_{0}\\ket{0} + \\beta_{1}\\ket{1})\\\\ \u0026amp;= \\alpha_{0}\\beta_{0} \\ket{0} \\otimes \\ket{0} + \\alpha_{0}\\beta_{1} \\ket{0} \\otimes \\ket{1} + \\alpha_{1}\\beta_{0} \\ket{1} \\otimes \\ket{0} + \\alpha_{1}\\beta_{1} \\ket{1} \\otimes \\ket{1} \\\\ \u0026amp;= \\alpha_{0}\\beta_{0} \\ket{00} + \\alpha_{0}\\beta_{1} \\ket{01} + \\alpha_{1}\\beta_{0} \\ket{10} + \\alpha_{1}\\beta_{1} \\ket{11} \\\\ \u0026amp;= \\alpha_{00}\\ket{00} + \\alpha_{01}\\ket{01} + \\alpha_{10}\\ket{10} + \\alpha_{11}\\ket{11} \\\\ \\end{align*} $$\n特に$\\left\\{ \\ket{a} \\right\\}_{a \\in \\left\\{ 0, 1 \\right\\}^{2}}$を$(\\mathbb{C}^{2}) ^{\\otimes 2}$の基底と呼ぶならば、任意の$\\ket{\\psi} \\in (\\mathbb{C}^{2}) ^{\\otimes 2}$に対して、\n$$ \\ket{\\psi} = \\sum\\limits_{a \\in \\left\\{ 0, 1 \\right\\}^{2}} \\braket{a | \\psi} \\ket {a} = \\sum\\limits_{a \\in \\left\\{ 0, 1 \\right\\}^{2}} \\psi_{a} \\ket {a} $$\n$\\ket{\\psi}, \\ket{\\xi} \\in (\\mathbb{C}^{2}) ^{\\otimes 2}$の内積は、\n$$ \\braket{\\psi | \\xi} = \\sum\\limits_{a \\in \\left\\{ 0, 1 \\right\\}^{2}} \\overline{\\psi_{a}} \\xi_{a} $$\nこのとき$\\overline{\\psi_{a}}$は$\\psi_{a}$の共役複素数である。\n参照 ビット 量子ゲート キム・ヨンフン、ホ・ジェソン、量子情報理論 (2020)、p93-95\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3423,"permalink":"https://freshrimpsushi.github.io/jp/posts/3423/","tags":null,"title":"キュービット：量子コンピュータにおける情報の基本単位"},{"categories":"줄리아","contents":"概要 Juliaでは、\u0026amp;\u0026amp;と||は論理積、論理和だけでなく、ショートサーキット評価Short-circuit Evaluationを実行する1。例えば、A \u0026amp;\u0026amp; BはAとBが両方とも真の時に真を返すが、実際にはAが偽なら、Bが真か偽かを見る必要はなく、A \u0026amp;\u0026amp; Bは偽になる。ショートサーキット評価は、その見る必要のないBを実際に見ずに済ますことだ。Bに対する計算を省略することにより、場合によっては速度が改善される。\n見てほしい 条件文を簡潔に書く方法 速度比較 M = rand(0:2, 10^4, 10^4);\rprint(first(M))\r@time if first(M) == 2 \u0026amp;\u0026amp; sum(M) \u0026lt; 1000\rprint(\u0026#34;!!\u0026#34;)\rend\r@time if sum(M) \u0026lt; 1000 \u0026amp;\u0026amp; first(M) == 2\rprint(\u0026#34;!!\u0026#34;)\rend 二つの条件文はsum(M) \u0026lt; 1000とfirst(M) == 2の順序が変えられただけで、正確に同じ作業を行う。しかし、first(M) == 2は行列Mの最初の要素が2かどうかだけをチェックし、sum(M)は全要素を走査して加算するため、相対的に計算に時間がかかる。\njulia\u0026gt; M = rand(0:2, 10^4, 10^4);\rjulia\u0026gt; print(first(M))\r0 もしMの最初の要素が上記のように0なら、sum(M) \u0026lt; 1000かどうかを確認するためにsum(M)を計算する必要はない。その速度は、単に順序を変えるだけで有意な差が出ることがある。\njulia\u0026gt; @time if first(M) == 2 \u0026amp;\u0026amp; sum(M) \u0026lt; 1000\rprint(\u0026#34;!!\u0026#34;)\rend\r0.000009 seconds\rjulia\u0026gt; @time if sum(M) \u0026lt; 1000 \u0026amp;\u0026amp; first(M) == 2\rprint(\u0026#34;!!\u0026#34;)\rend\r0.040485 seconds (1 allocation: 16 bytes) 環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/manual/control-flow/#Short-Circuit-Evaluation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2392,"permalink":"https://freshrimpsushi.github.io/jp/posts/2392/","tags":null,"title":"ジュリアのショートサーキット"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義 集合 $\\left\\{ 0, 1 \\right\\}$ の元を ビットbitと呼ぶ。集合 $\\left\\{ 0, 1 \\right\\}^{n}$ の元を $n$ビット$n$bitと呼ぶ。\n説明 ビットは binary digitの略称である。通常、「$0$または$1$の値を取りうるもの」と説明される。古典コンピュータが処理する最小の情報単位であり、コンピュータの回路では$1$は電気信号があることを、$0$は電気信号がないことを意味する。\n量子コンピュータで処理される情報の最小単位は、ビットにクォンタムを付けて quantum bit量子ビットと呼ばれる。\n関連項目 ブール関数 量子ビット ","id":3422,"permalink":"https://freshrimpsushi.github.io/jp/posts/3422/","tags":null,"title":"ビット: 古典的なコンピュータにおける情報の基本単位"},{"categories":"줄리아","contents":"概要 ジュリアの基本組み込み関数は知れば知るほど便利だ。早速、例を見て学ぼう。\nコード x = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rargmin(x)\rargmax(x)\rfindmin(x)\rfindmax(x)\rextrema(x)\rfindfirst(x .== 3)\rfindlast(x .== 3)\rfindall(x .== 3)\rfindnext(x .== 3, 5)\rfindprev(x .== 3, 5) 最適解 argmin(),argmax(),findmin(),findmax(),extrema() 最適解を見つける。\nx = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rjulia\u0026gt; argmin(x)\r9\rjulia\u0026gt; argmax(x)\r7\ry = [7, 8, 9, 9, 7, 9]\rjulia\u0026gt; argmax(y)\r3\rjulia\u0026gt; findall(y.==maximum(y))\r3-element Vector{Int64}:\r3\r4\r6 argmin(),argmax() はただの最適解、つまり値が最も大きく小さい場所のインデックスを返す。そんなインデックスが複数ある場合は最も小さいものを返す。だから、実際は argmax(x) $= \\min(\\argmax(x))$だ。本当の$\\argmax$は、maximum() と下の findall() 関数と一緒に使えばいい。\nx = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rjulia\u0026gt; findmin(x)\r(2, 9)\rjulia\u0026gt; findmax(x)\r(12, 7) findmin(),findmax() は最適解とその値まで返す。\nx = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rjulia\u0026gt; extrema(x)\r(2, 12) ちなみに extrema() はインデックスではなく、その値だけを返す。これは R の range() 関数と同じだ1。\n条件を満たした最初・最後のインデックス findfirst(), findlast() x = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rjulia\u0026gt; findfirst(x .== 3)\r1\rjulia\u0026gt; findlast(x .== 3)\r8 3が存在する最初と最後のインデックスを見つけた。配列の形が大まかに予想できる場合、これらを使ってコードの速度を向上させることができるだろう。\n条件を満たした全てのインデックス findall() x = [3, 7, 4, 5, 10, 3, 12, 3, 2, 4]\rjulia\u0026gt; findall(x .== 3)\r3-element Vector{Int64}:\r1\r6\r8 使いやすく、一般的なプログラミングで最も役に立つ関数だ。maximum(), minimum()と一緒に使えば、全ての $\\text{argmin}\n条件を満たした特定範囲のインデックス findnext(), findprev() julia\u0026gt; findnext(x .== 3, 5)\r6\rjulia\u0026gt; findprev(x .== 3, 5)\r1 前後である程度は例外として検索する必要がある時もある。例えば、配列の最初の要素と同じ要素を探す時にfindall()を使うと、その最初の要素も見つかってしまうので、煩わしいことになるかもしれない。\n環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/base/collections/#Base.extrema\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2390,"permalink":"https://freshrimpsushi.github.io/jp/posts/2390/","tags":null,"title":"ジュリアのfind関数들"},{"categories":"위상데이터분석","contents":"アルゴリズム $R$が主イデアル領域の時、全ての行列 $A \\in R^{m \\times n}$についてスミス標準形が一意に存在する。つまり、行列 $A \\in R^{m \\times n}$に対して、次を満たす$d_{1} , \\cdots , d_{r} \\in R$と可逆行列 $P \\in R^{m \\times m}$、$Q \\in R^{n \\times n}$が存在する。 $$ PAQ = \\begin{bmatrix} d_{1} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\ddots \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; d_{r} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\end{bmatrix} \\in R^{m \\times n} $$ ここで$d_{1} , \\cdots , d_{r}$は$R$で$0$であってはならず、$d_{1} \\mid \\cdots \\mid d_{r}$でなければならない。言い換えると、$d_{k} \\ne 0$は$d_{k+1}$の因数Divisorでなければならない。この時の一意性は$d_{k}$の単元倍を無視する。\n証明 1 2 戦略：行、列の交換とガウス消去法を使用するアルゴリズムで、具体的なスミス標準形を直接見つけてその存在を示す。\n全ての主イデアル領域は一意因数分解領域である。\n行列 $A$は、PID上の行列なので、UFD上の行列でもある。従って、単元や$0$でない全ての要素$A_{ij}$は、ユニークな因数分解を持ち、その中で最も少ない因数の数を持つ要素の絶対値を$\\alpha \u0026gt; 0$と表示し、ミニマルエントリーMinimal Entryと呼ぶ。\nちなみにMunkeresの教科書では$\\mathbb{Z}^{m \\times n}$で、単に最も小さい要素を選ぶのだが、整数環$\\mathbb{Z}$ではなく、一般的な主イデアル領域はユークリッド領域である保証すらないため、要素だけ見て「最小」と判断するのは難しい。一般的なPIDできれいにミニマルエントリーを定める場合、因数分解を考えるのが妥当だ。ただし、このポストでは便宜上、「小さい」や$a \u0026lt; b$のような表現が出てくるが、これは要素の大きさではなく、因数の数を比較していると理解すればいい。\n与えられた行列はガウス消去法により続けて変化するため、$\\alpha$も文脈によって続けて変化することに注意せよ。\nちなみに、このアルゴリズムは与えられた$A$に対して必ずスミス標準形を導けることを数学的に見せるためのアルゴリズムであり、特に効率的というわけではなく$P, Q$も分からない。\nステップ1.\n$A$がゼロ行列なら、そのままでスミス標準形なのでステップ4に進む。$A$がゼロ行列でないなら、以下を実行する。\n理解しやすくするために、まず行と列を交換して、$\\alpha$を左上の$(1,1)$に引き上げよう。これから与えられた行列のほとんどを気にせずに、最初の行、最初の列だけを見ることになる。 $$ A \\sim \\begin{bmatrix} \\alpha \u0026amp; A_{12} \u0026amp; \\cdots \u0026amp; \\\\ A_{21} \u0026amp; A_{22} \u0026amp; \\cdots \u0026amp; \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\end{bmatrix} $$\nステップ2. $k= 1, \\cdots , r$\n（当然ながら、計算中には$r$が分からない。プログラムが終わると自然と$r$が決まる。）\n全ての$i, j$に対して$\\alpha \\mid A_{ij}$ならば、$d_{k} \\gets \\alpha$を用いる。$\\alpha \\mid A_{ij}$なので、$\\alpha$を除く$A$の最初の行と列の全ての要素が$0$になるまでガウス消去法を繰り返すことができる。繰り返しが終わったら、$1$行と$1$列を削除してステップ1に戻る。この時、行列$A$のサイズは$(m - k) \\times (n - k)$になる。 何らかの$i, j$に対して$\\alpha \\nmid A_{ij}$ならば、ステップ3に進む。 ステップ3.\nここからの議論は、行でも列でも、一方が成り立てば他方にも適用できるので、一般性を失わずに$\\alpha \\ne 0$と$A_{12}$だけを扱おう。基本的にはステップ2で少なくとも一つの$i, j$に対して$\\alpha \\nmid A_{ij}$が保証され、$(1,2)$の下にその$A_{ij}$が来るように以下のCase1を実施することを目指す。\n主イデアル領域はベズー領域である: PIDはベズー領域なので、拡張ユークリッドの定理が成立する。つまり、全ての$a, b \\in D$に対して、次を満たす$m,n \\in D$が存在する。 $$ m a + n b = \\gcd \\left( a, b \\right) $$\nケース1. $\\alpha \\nmid A_{12}$の場合\n$\\alpha$はミニマルエントリーで、$\\alpha \\nmid A_{12}$なので、$\\gcd \\left( \\alpha, A_{12} \\right) \u0026lt; \\alpha$である。拡張ユークリッドの定理によって、 $$ m \\alpha + n A_{12} $$ $\\alpha$より小さくする$m,n \\in D$が存在する。ガウス消去法によって、$1$列に$m$を掛けた列と$2$列に$n$を掛けた列を加えて、$2$列に代入する。この新しい列の最初の要素である$A_{12}' = \\gcd \\left( \\alpha , A_{12} \\right)$は、元の$\\alpha$より小さく、計算自体から$\\alpha$の約数であるため、以前よりも小さく、新しいミニマルエントリーの候補だ。$1$列と$2$列を交換して、ステップ1に戻る（計算中に偶然、より小さいミニマルエントリーが存在する可能性もある）。 ケース2. $\\alpha \\mid A_{12}$の場合\n$k \\alpha + A_{12} = 0$を満たす何らかの$k \\in \\mathbb{Z}$が存在するので、$1$列に$k$を掛けて、$2$列から引く。すると、この新しい列の最初の要素である$A_{12} ' $は$0$となる。ステップ3を繰り返す。 ケース3. $A_{12} = 0$の場合\n$A_{12} \\ne 0$になるまで列交換を繰り返す。全ての$j \\ne 1$に対して、全て Munkres. (1984). Elements of Algebraic Topology: p55~57.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Smith_normal_form#Algorithm\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2389,"permalink":"https://freshrimpsushi.github.io/jp/posts/2389/","tags":null,"title":"スミス標準形の存在証明"},{"categories":"줄리아","contents":"概要 1 ジュリアでは、関数名の最後に感嘆符Bang!を追加することをバンク規約と呼ぶ。これらの関数は、与えられた引数を変更する特徴がある。\nコード function add_1!(x)\rx .+= 1\rreturn x\rend\rfoo = [2,5,-1]\radd_1!(foo)\rfoo 例えば、上のコードを実行すると、以下の結果が得られる。\njulia\u0026gt; foo = [2,5,-1]\r3-element Vector{Int64}:\r2\r5\r-1\rjulia\u0026gt; add_1!(foo)\r3-element Vector{Int64}:\r3\r6\r0\rjulia\u0026gt; foo\r3-element Vector{Int64}:\r3\r6\r0 配列fooは関数の外で定義され、「add_1!()」によって要素が$1$ずつ増えて返されただけでなく、引数自体が変更された。\n説明 代表的なメソッドであるpop!()は、配列の最後の要素を削除しつつ返すが、この関数が元の配列を変更できない場合、MatlabやRのように広く認知されているデータ構造を使用するのが難しく、一般的なプログラミングに慣れているユーザーにとっては非常に不便だったかもしれない。\nPythonで関数ではなくメソッドを使用したときにクラスのデータまで変更される感覚と同様に捉えればいい。ジュリアは言語設計上クラスをサポートしていないので、正確な説明ではないが、Pythonでメソッドを使っていた時が便利だった時に役立つことができる。\n環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/manual/style-guide/#bang-convention\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2388,"permalink":"https://freshrimpsushi.github.io/jp/posts/2388/","tags":null,"title":"ジュリアの感嘆符の規約"},{"categories":"위상데이터분석","contents":"定義 最小包含円 $n \u0026gt; d$ としよう。$d$次元のユークリッド空間で与えられた有限な集合$P = \\left\\{ p_{k} \\right\\}_{k=1}^{n} \\subset \\mathbb{R}^{d}$に対して、以下のような最適化問題を最小包含円問題Smallest Enclosing Disk Problemという。 $$ \\begin{matrix} \\text{Minimize} \u0026amp; r \\ge 0 \\\\ \\text{subject to} \u0026amp; \\left\\| c - p_{k} \\right\\|_{2} \\le r \\end{matrix} \\\\ c \\in \\mathbb{R}^{d} , k = 1, \\cdots , n $$\nヒント これは正確には定理と呼ぶほどのものではないが、この問題を扱う際に知っておくと良い事実である。\n$P$がアフィン独立である場合、円の境界には$P$の点が最少で$2$から最大で$d+1$個まで存在する。つまり、点が重なっていたり、3点以上が一直線上にあるような場合を除き、正確に$2 \\le m \\le d+1$個の点で最小包含円が一意に定まる。 例えば、$d = 2$次元平面では、円は正確に3つの点で一意に定まる。 一般に$n \u0026gt; d+1$個の点が与えられた場合、アルゴリズムAlgorithmではなく明示的な公式Explicit Formulaを見つけることは不可能とされている。境界上に最小包含円を一意に定める点をサポートSupportと呼ぶが、単に点を持っているだけでは誰がサポートかを知る方法がないためである。 このように境界上の点をサポートと呼ぶのは、幾何学の問題で一般的である。サポートベクターマシンでは、サポートが境界上の点を意味する。 したがって、この問題を解くアルゴリズムを開発するということは、境界上のサポートを見つけることを保証するか、それを迅速に見つける方法に関する研究と言っても過言ではない。ただし、現在使用されているアルゴリズムでは、その根本的なアイデアはほぼウェルツルのものに基づいており、その改良や変種をまとめて単にウェルツルアルゴリズムWelzl Algorithmと呼んでいる1。少なくともこの投稿で紹介されている後続の研究はすべてその系統に属している。 解法 ウェルツルアルゴリズム 2 ウェルツルアルゴリズムWelzl Algorithmは、最小包含円問題を解く再帰的なRecursive解法である。基本的には、点を一つずつ追加したり削除したりしながらサポートを見つけ、そのようにして得られた円が与えられたすべての点を包含しているかどうかを繰り返し確認する。\n点が$n \\le d+1$個の場合にそれらを包含する円を正確に見つけることは比較的簡単であるため、そのような関数が存在すると仮定する。実際の実装では、これが思ったほど単純ではないが、最小包含円問題の核心ではない。\n擬似コード 3 $(c,r)$ = welzl$\\left( P, S \\right)$\nInput: 与えられた点の集合$P = \\left\\{ p_{k} \\right\\}_{k=1}^{n} \\subset \\mathbb{R}^{d}$とサポーターの候補$S \\subset P$を受け取る。ヒントで述べたように、$\\left| S \\right| \\le d+1$である。 Output: $P$のすべての点を包含する最も小さい円の中心$c$と半径$r$のタプル$(c,r)$を得る。中心が$c$で半径が$r$の閉じた球を$D = B \\left[ c,r \\right]$と表す。 $(c,r)$ = trivial$\\left( S \\right)$\nInput: $\\left| S \\right| \\le d+1$の点の集合$S \\subset P$を受け取る。 Output: $S$のすべての点を包含する最も小さい円の中心$c$と半径$r$のタプル$(c,r)$を得る。welzlに比べて単純であるという仮定に従い、trivialの擬似コードは別途記述しない。 function welzl$\\left( P, S \\right)$\n$S := \\emptyset$\nif $P = \\emptyset$ or $\\left| S \\right| = d+1$ then\nreturn trivial$\\left( S \\right)$\nelse\nchoose $p \\in P$\n$D := $ welzl$\\left( P \\setminus \\left\\{ p \\right\\}, S \\right)$\nif $p \\in D$ then\nreturn $D$\nend if\nend if\nreturn welzl$\\left( P \\setminus \\left\\{ p \\right\\} , S \\cup \\left\\{ p \\right\\} \\right)$\nend function\nwelzlは再帰関数として記述されるため、実際のプログラムではtrivialから計算が始まることになる。welzlの擬似コードで登場するchooseは、choose$x \\in X$のように書かれ、集合$X$から要素$x$を一様ランダムに選ぶキーワードKeywordである。\n最後にwelzl$\\left( P \\setminus \\left\\{ p \\right\\} , S \\cup \\left\\{ p \\right\\} \\right)$をリターンすること自体が、$P$にある点を一つずつ取り除いて$S$に入れてみてサポートを見つけるプロセスであることが分かれば、このアルゴリズムを理解したことになる。\n説明 最小包含円の制約条件を文字通りに解釈してみよう。$\\left\\| c - p_{k} \\right\\|_{2} \\le r$を満たす$c$と$r$を見つけるということは、少なくとも与えられた点をすべて包むことができる中心Centre$c$と半径Radius$r$を見つけることを意味する。しかし、ユークリッド空間は非常に広いため、$c$がどこであっても$r$を無制限に大きくすれば、この制約条件は必ず満たせる。当然のことながら、私たちの関心事は、それをしながら$r$を最小化すること、つまり与えられた点を包む最も小さい球を見つけることである。\n単純化の問題点 包含Enclosingは、数学全般で広く使われている表現ではないが、発音そのままにインクロージングと言うにはあまりに長くて感じがこないため、任意に翻訳した。そもそもインクロージングEnclosing自体がバウンディングBoundingを圧倒しているわけではない。また、円盤Diskも内部を含む閉じた球の意味で使われたが、実際の英語表現では単にボールBallやスフィアSphereもよく使われる。単語や翻訳にこだわらないようにしよう。\n歴史 早くもライムント・ザイデルRaimund Seidelによって線形計画法に基づく解法が知られていた。\n1991年にエモ・ヴェルツルEmo Welzlが彼の論文で再帰的なRecursiveアルゴリズムを提案し、いわゆるSOTAState Of The Artを記録した。2022年現在でも、一般的な最小包含ディスク問題ではこのヴェルツルのアルゴリズムが最も優れているとされており、以降の多くの研究はこれを改良する形で行われてきた。\n1999年にはベルント・ゲルトナーBernd Gärtnerが、ヴェルツルのアルゴリズムに従いつつも、二次計画法Quadratic Programmingの応用を導入してこれを改善した。4 彼のコードはC++で書かれており、チューリッヒ連邦工科大学のウェブサイト5で実際の実装を見ることができる。\n2003年にはカスパー・フィッシャーKaspar Fischerが、ゲルトナーとの研究で線形計画法のシンプレックス法で登場するブランドのルールを導入し、高速なコードを作成した。6 2013年にはトーマス・ラーソンThomas Larssonが、近似的なものではなく、速度と堅牢性Robustnessを備えた方法を提案した。7\nここまで紹介された研究を参照すると、ヴェルツル-ゲルトナー-フィッシャーと続く大きな流れを確認することができる。\n応用 ヴェルツルアルゴリズムの代表的な応用は、チェックコンプレックスの構築である。\nEdelsbrunner, Harer. (2010). Computational Topology An Introduction: p73~75.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWelzl. (1991). Smallest enclosing disks (balls and ellipsoids). https://doi.org/10.1007/BFb0038202\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Smallest-circle_problem#Welzl's_algorithm\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGärtner. (1999). Fast and robust smallest enclosing balls. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.90.5783\u0026amp;rep=rep1\u0026amp;type=pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://people.inf.ethz.ch/gaertner/subdir/software/miniball.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKaspar Fischer. (2003). Fast Smallest-Enclosing-Ball Computation in High Dimensions. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.90.5783\u0026amp;rep=rep1\u0026amp;type=pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThomas Larsson. (2013). Fast and Robust Approximation of Smallest Enclosing Balls in Arbitrary Dimensions. https://doi.org/10.1111/cgf.12176\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2385,"permalink":"https://freshrimpsushi.github.io/jp/posts/2385/","tags":null,"title":"ベルツルアルゴリズム: 最小内包ディスク問題の解法"},{"categories":"선형대수","contents":"ビルドアップ 便宜上、複素数空間$\\mathbb{C}$について展開するが、$\\mathbb{R}$や任意のベクトル空間でも構わない。 有限集合$\\Gamma$から複素数空間への関数の集まりを$\\mathbb{C}^{\\Gamma}$のように記すことにする。\n$$ \\mathbb{C}^{\\Gamma} = \\left\\{ f : \\Gamma \\to \\mathbb{C} \\right\\} $$\n$\\Gamma = \\mathbf{n} = \\left\\{ 1, \\dots, n \\right\\}$の場合、実質的に$\\mathbb{C}^{\\mathbf{n}} = \\mathbb{C}^{n}$になり、ベクトル空間のテンソル積は次のように定義される。\n$$ \\mathbb{C}^{\\Gamma_{1}} \\otimes \\mathbb{C}^{\\Gamma_{2}} := \\mathbb{C}^{\\Gamma_{1} \\times \\Gamma_{2}} $$\n$v_{i} \\in \\mathbb{C}^{\\Gamma_{i}}$と$n_{i} = \\left| \\Gamma_{i} \\right|$とする。$\\mathbb{C}^{\\Gamma_{i}}$に対応する標準基底をそれぞれ$\\left\\{ e_{j_{i}} \\right\\}_{j_{i} \\in \\Gamma_{i}}$とする。すると、$v_{i}$は次のように表される。\n$$ \\begin{align*} v_{1} \u0026amp;: \\left\\{ 1, \\dots, n_{1} \\right\\} \\to \\mathbb{C} \u0026amp;\u0026amp;\u0026amp; v_{2} \u0026amp;: \\left\\{ 1, \\dots, n_{2} \\right\\} \\to \\mathbb{C} \\\\ v_{1} \u0026amp;= (v_{1}(1), \\dots, v_{1}(n_{1})) \\in \\mathbb{C}^{n_{1}} \u0026amp;\u0026amp;\u0026amp; v_{2} \u0026amp;= (v_{2}(1), \\dots, v_{2}(n_{2})) \\in \\mathbb{C}^{n_{2}} \\\\ \u0026amp; = \\sum \\limits_{j_{1} = 1}^{n_{1}}v_{1}(j_{1}) e_{j_{1}} \u0026amp;\u0026amp;\u0026amp;\u0026amp; = \\sum \\limits_{j_{2} = 1}^{n_{2}}v_{2}(j_{2}) e_{j_{2}} \\end{align*} $$\n$v_{1} \\otimes v_{2}$のように表されるテンソル積$\\mathbb{C}^{\\Gamma_{1}} \\otimes \\mathbb{C}^{\\Gamma_{2}}$の要素を$v_{1}$と$v_{2}$の積ベクターと言う。\n定義 $v_{1}$と$v_{2}$の積ベクター$v_{1} \\otimes v_{2}$を次のように定義する。\n$$ \\begin{align*} v_{1} \\otimes v_{2} \u0026amp;= \\left( \\sum \\limits_{j_{1} \\in \\Gamma_{1}}v_{1}(j_{1}) e_{j_{1}} \\right) \\otimes \\left( \\sum \\limits_{j_{2} \\in \\Gamma_{2}}v_{2}(j_{2}) e_{j_{2}} \\right) \\\\ \u0026amp;:= \\sum\\limits_{(j_{1}, j_{2}) \\in \\Gamma_{1} \\times \\Gamma_{2}} \\left( \\prod\\limits_{i=1}^{2} v_{i}(j_{i}) \\right) e_{j_{1}} \\otimes e_{j_{2}} \\end{align*} $$\nこの時、$v_{1} \\otimes v_{2}$はテンソル積の定義によって$\\mathbb{C}^{\\Gamma_{1}} \\otimes \\mathbb{C}^{\\Gamma_{2}}$の要素になる。\n$$ v_{1} \\otimes v_{2} := \\sum\\limits_{(j_{1}, j_{2}) \\in \\Gamma_{1} \\times \\Gamma_{2}} \\left( \\prod\\limits_{i=1}^{2} v_{i}(j_{i}) \\right) e_{j_{1}} \\otimes e_{j_{2}} \\in \\mathbb{C}^{\\Gamma_{1}} \\otimes \\mathbb{C}^{\\Gamma_{2}} $$\n説明 $\\mathbb{C}^{\\Gamma_{1}} \\otimes \\mathbb{C}^{\\Gamma_{2}}$の全ての要素が$v_{1} \\otimes v_{2}$のような積ベクターの形で表されるわけではない。例えば、以下のベクターは2つのベクターの積ベクターとして表現可能だが、$(e_{1} \\otimes e_{1}) + (e_{2} \\otimes e_{2})$は不可能である。\n$$ e_{1} \\otimes e_{1} - e_{1} \\otimes e_{2} + e_{2} \\otimes e_{1} - e_{2} \\otimes e_{2} = (e_{1} + e_{2}) \\otimes (e_{1} - e_{2}) $$\n簡単な例として、上の定義を再び具体的に解き直してみよう。$\\Gamma_{1} = \\left\\{ 1, 2 \\right\\}$、$\\Gamma_{2} = \\left\\{ 1, 2, 3 \\right\\}$とする。$v_{i} \\in \\mathbb{C}^{\\Gamma_{i}}$とする。$\\mathbb{C}^{\\Gamma_{1}} = \\mathbb{C}^{2}$に対応する標準基底を$\\left\\{ e_{j_{1}} \\right\\}_{j_{1} \\in \\Gamma_{1}}$、$\\mathbb{C}^{\\Gamma_{2}} = \\mathbb{C}^{3}$に対応する標準基底を$\\left\\{ e_{j_{2}} \\right\\}_{j_{2} \\in \\Gamma_{2}}$とする。すると、$v_{1}$、$v_{2}$は次のようになる。\n$$ \\begin{align*} v_{1} \u0026amp;: \\left\\{ 1, 2 \\right\\} \\to \\mathbb{C} \u0026amp;\u0026amp;\u0026amp; v_{2} \u0026amp;: \\left\\{ 1, 2, 3 \\right\\} \\to \\mathbb{C} \\\\ v_{1} \u0026amp;= (v_{1}(1), v_{1}(2)) \\in \\mathbb{C}^{2} \u0026amp;\u0026amp;\u0026amp; v_{2} \u0026amp;= (v_{2}(1), v_{2}(2), v_{2}(3)) \\in \\mathbb{C}^{3} \\\\ \u0026amp; = \\sum \\limits_{j_{1} = 1}^{2}v_{1}(j_{1}) e_{j_{1}} \u0026amp;\u0026amp;\u0026amp;\u0026amp; = \\sum \\limits_{j_{2} = 1}^{3}v_{2}(j_{2}) e_{j_{2}} \\end{align*} $$\nすると、$v_{1}$と$v_{2}$の積ベクターは次のようになる。\n$$ \\begin{align*} v_{1} \\otimes v_{2} \u0026amp;= (v_{1}(1), v_{1}(2)) \\otimes (v_{2}(1), v_{2}(2), v_{2}(3)) \\\\ \u0026amp;= \\left( \\sum \\limits_{j_{1} = 1}^{2} v_{1}(j_{1}) e_{j_{1}} \\right) \\otimes \\left( \\sum \\limits_{j_{2} = 1}^{3} v_{2}(j_{2}) e_{j_{2}} \\right) \\\\ \u0026amp;:= \\sum\\limits_{(j_{1}, j_{2}) \\in \\Gamma_{1} \\times \\Gamma_{2}} \\left( \\prod\\limits_{i=1}^{2} v_{i}(j_{i}) \\right) e_{j_{1}} \\otimes e_{j_{2}} \\in \\mathbb{C}^{\\Gamma_{1}} \\otimes \\mathbb{C}^{\\Gamma_{2}} \\\\ \u0026amp;= v_{1}(1)v_{2}(1)e_{1} \\otimes e_{1} + v_{1}(1)v_{2}(2)e_{1} \\otimes e_{2} + v_{1}(1)v_{2}(3)e_{1} \\otimes e_{3} \\\\ \u0026amp;\\quad + v_{1}(2)v_{2}(1)e_{1} \\otimes e_{1} + v_{1}(2)v_{2}(2)e_{1} \\otimes e_{2} + v_{1}(2)v_{2}(3)e_{1} \\otimes e_{3} \\\\ \u0026amp;= \\left( v_{1}(1)v_{2}(1), v_{1}(1)v_{2}(2), v_{1}(1)v_{2}(3), v_{1}(2)v_{2}(1), v_{1}(2)v_{2}(2), v_{1}(2)v_{2}(3) \\right) \\\\ \u0026amp;\\in \\mathbb{C}^{6} \\cong \\mathbb{C}^{\\Gamma_{1}} \\otimes \\mathbb{C}^{\\Gamma_{2}} \\end{align*} $$\n$v_{1} \\otimes v_{2}$の成分をよく見ると、これが行列と関連があることが推測できるだろう。\n座標行列 行列空間$M_{m \\times n}(\\mathbb{C})$を考える。$E_{ij}$が$(i,j)$成分だけ$1$で残りが全て$0$の$m \\times n$行列とするならば、$\\left\\{ E_{ij} \\right\\}$は$M_{m\\times n}(\\mathbb{C})$の基底になる。テンソル積$\\mathbb{C}^{m} \\otimes \\mathbb{C}^{n}$の基底ベクター$e_{i} \\otimes e_{j}$を$E_{ij}$に移す線形変換を$\\phi$としよう。\n$$ \\begin{align*} \\phi : \\mathbb{C}^{m} \\otimes \\mathbb{C}^{n} \u0026amp;\\to M_{m \\times n} (\\mathbb{C}) \\\\ e_{i} \\otimes e_{j} \u0026amp;\\mapsto E_{ij} \\end{align*} $$\nこれは基底を基底に写像するので、同型写像になる。2つのベクター$v \\in \\mathbb{C}^{m}$、$w \\in \\mathbb{C}^{n}$が次のようだとする。\n$$ v = \\sum_{i} \\alpha_{i}e_{i} = \\begin{bmatrix} \\alpha_{1} \\\\ \\vdots \\\\ \\alpha_{m} \\end{bmatrix} \\qquad w = \\sum_{j} \\beta_{j}e_{j} = \\begin{bmatrix} \\beta_{1} \\\\ \\vdots \\\\ \\beta_{n} \\end{bmatrix} $$\n積ベクター$v, w$を$\\phi$で送ると次のようになる。\n$$ \\begin{align*} \\phi ( v \\otimes w ) \u0026amp;= \\phi \\left( \\sum\\limits_{i,j} \\alpha_{i}\\beta_{j} e_{i} \\otimes e_{j} \\right) \\\\ \u0026amp;= \\sum\\limits_{i,j} \\alpha_{i}\\beta_{j} \\phi \\left( e_{i} \\otimes e_{j} \\right) \\\\ \u0026amp;= \\sum\\limits_{i,j} \\alpha_{i}\\beta_{j} E_{ij} \\\\ \u0026amp;= \\begin{bmatrix} \\alpha_{1}\\beta_{1} \u0026amp; \\cdots \u0026amp; \\alpha_{1}\\beta_{n} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\alpha_{m}\\beta_{1} \u0026amp; \\cdots \u0026amp; \\alpha_{m}\\beta_{n} \\\\ \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} \\alpha_{1} \\\\ \\vdots \\\\ \\alpha_{m} \\end{bmatrix} \\begin{bmatrix} \\beta_{1} \u0026amp; \\cdots \u0026amp; \\beta_{n} \\end{bmatrix} \\\\ \u0026amp;= vw^{T} \\end{align*} $$\nこれは各成分が$\\alpha_{i}\\beta_{j}$の行列である。したがって、$\\phi$によって積ベクター$v \\otimes w$は一つの$m \\times n$と対応する。行列$\\phi (v \\otimes w) = vw^{T}$を標準基底に対する$v \\otimes w$の座標行列と言う。これはベクターの座標ベクターと同じ概念と見なすことができる。\n一般化 有限集合$\\Gamma_{i} (1 \\le i \\le r)$、$\\Gamma = \\Gamma_{1} \\times \\cdots \\times \\Gamma_{r}$、$v_{i} \\in \\mathbb{C}^{\\Gamma_{i}}$に対して、$v_{i}$の積ベクターを次のように定義する。\n$$ \\begin{align*} v_{1} \\otimes \\cdots \\otimes v_{r} \u0026amp;= \\left( \\sum \\limits_{j_{1} \\in \\Gamma_{1}}v_{1}(j_{1}) e_{j_{1}} \\right) \\otimes \\cdots \\otimes \\left( \\sum \\limits_{j_{r} \\in \\Gamma_{r}}v_{r}(j_{r}) e_{j_{r}} \\right) \\\\ \u0026amp;:= \\sum\\limits_{(j_{1}, \\dots, j_{r}) \\in \\Gamma} \\left( \\prod\\limits_{i=1}^{r} v_{i}(j_{i}) \\right) e_{j_{1}} \\otimes \\cdots \\otimes e_{j_{r}} \\\\ \u0026amp;= \\in \\mathbb{C}^{\\Gamma_{1}} \\otimes \\cdots \\otimes \\mathbb{C}^{\\Gamma_{r}} \\end{align*} $$\n見てみよう ベクトル空間のテンソル積$V \\otimes W$\n${}$ 物理学でのテンソル：テンソルの易しい定義 微分多様体上で定義されるテンソル ","id":3415,"permalink":"https://freshrimpsushi.github.io/jp/posts/3415/","tags":null,"title":"テンソル積の積ベクトル"},{"categories":"줄리아","contents":"概要 ジュリアで、viewは配列のサブアレイを素早く参照させるデータ構造だ。実際に使う立場から見れば面倒で差がないように見えるけど、怠惰に参照されてもっと軽い配列を返す。だから、基本的なレベルで最適化されたジュリアのコードでは、@viewsというマクロを簡単に見つけることができる。\nコード 行列Mのサブマトリックスを参照してみよう。\n関数形式：view() view(A, inds...)\nAのinds...に従ったviewを返す。 しかし、この形式は一般的にコードを読みにくくするため、好まれない。次のマクロを使用すると、viewを使用しながらも基本的なジュリアの文法と大きな違いはない。\nマクロ：@view @viewマクロは、サブアレイを参照する文脈のコードにviewが適用されたかのように替えるマクロだ。\nブロック全体に適用：@views @viewsマクロは、続くブロック全体に@viewを適用する。これのおかげで、viewなしで快適に書いた関数の前に@views f(x) ... endと@viewsだけを付ければ、自動的にviewが適用される。\n全体のコード 速度比較 fcopy()とfview()は、まったく同じ機能を持つ関数だが、速度に違いがある。一見、速度は似ているように見えるが、ほとんどがコンパイル時間だ。これを除いて、単純な実行時間だけを比較すると、約4倍の差がある。\n環境 OS: Windows julia: v1.7.0 ","id":2384,"permalink":"https://freshrimpsushi.github.io/jp/posts/2384/","tags":null,"title":"ジュリアで部分配列を迅速に参照する方法"},{"categories":"선형대수","contents":"ビルドアップ1 便宜上、複素数空間$\\mathbb{C}$について展開するが、$\\mathbb{R}$や任意のベクター空間でも問題ない。 有限集合$\\Gamma$から複素数空間への関数の集まりを$\\mathbb{C}^{\\Gamma}$として表記しよう。\n$$ \\mathbb{C}^{\\Gamma} = \\left\\{ f : \\Gamma \\to \\mathbb{C} \\right\\} $$\n$\\Gamma$を$\\mathbf{n} = \\left\\{ 1, 2, \\dots, n \\right\\}$とする。各$1 \\le i \\le n$を複素数$z_{i} \\in \\mathbb{C}$に送る関数を$(z_{1}, \\dots, z_{n})$と表記すると、これは$\\mathbb{C}^{\\mathbf{n}}$に属する関数であり、$n$-複素数順序対集合$\\mathbb{C}^{n}$のベクターとも同じだ。\n$$ (z_{1}, \\dots, z_{n}) : i \\mapsto z_{i} $$\n$$ \\mathbb{C}^{n} := \\mathbb{C}^{\\mathbf{n}} = \\left\\{ (z_{1}, \\dots, z_{n}) \\vert z_{i} \\in \\mathbb{C} \\right\\} $$\nつまり、$v \\in \\mathbb{C}^{\\Gamma}$は$v : i \\mapsto z_{i}$と同じ関数と見ることもでき、$v = (z_{1}, \\dots, z_{\\left| \\Gamma \\right|})$と同じ順序対と見ることもできる。\n有限集合$\\Gamma_{1}$、$\\Gamma_{2}$に対して、二つのベクター空間$\\mathbb{C}^{\\Gamma_{1}}$と$\\mathbb{C}^{\\Gamma_{2}}$のテンソル積は、$\\Gamma_{1}$と$\\Gamma_{2}$の積空間$\\Gamma_{1} \\times \\Gamma_{2}$から作られる関数空間（ベクター空間）$\\mathbb{C}^{\\Gamma_{1} \\times \\Gamma_{2}}$として定義される。\n定義2 有限集合$\\Gamma_{1}$、$\\Gamma_{2}$に対し、二つのベクター空間$\\mathbb{C}^{\\Gamma_{1}}$と$\\mathbb{C}^{\\Gamma_{2}}$のテンソル積tensor productを以下のように定義する。\n$$ \\mathbb{C}^{\\Gamma_{1}} \\otimes \\mathbb{C}^{\\Gamma_{2}} := \\mathbb{C}^{\\Gamma_{1} \\times \\Gamma_{2}} $$\nここで、$\\Gamma_{1} \\times \\Gamma_{2}$は$\\Gamma_{1}$と$\\Gamma_{2}$の積空間だ。\n説明 簡単な例として、$\\Gamma_{1} = \\mathbf{2} = \\left\\{ 1, 2 \\right\\}$、$\\Gamma_{2} = \\mathbf{3} = \\left\\{ 1, 2, 3 \\right\\}$としよう。そして、$\\Gamma$をこれらの積空間としよう。\n$$ \\Gamma = \\Gamma_{1} \\times \\Gamma_{2} = \\left\\{ (1,1), (1,2), (1,3), (2,1), (2,2), (2,3) \\right\\} $$\nその要素をそれぞれ以下のように表記しよう。\n$$ e_{i} \\otimes e_{j} = (i, j) $$\nすると、概要での論理に従えば、$v \\in \\mathbb{C}^{\\Gamma}$は$(i,j) \\mapsto \\alpha_{ij}$と同じ関数であり、$\\left( \\alpha_{11}, a_{12}, a_{13}, a_{21}, a_{22}, a_{23} \\right)$と同じ順序対とみなすことができる。したがって、$\\mathbb{C}^{\\Gamma}$は$\\left\\{ e_{i} \\otimes e_{j} : 1 \\le i \\le 2, 1 \\le j \\le 3 \\right\\}$を基底として持つベクター空間だ。\n$$ \\begin{align*} \\mathbb{C}^{\\Gamma} \u0026amp;= \\left\\{ \\sum\\limits_{i,j} \\alpha_{i,j} e_{i} \\otimes e_{j} : \\alpha_{ij} \\in \\mathbb{C} \\right\\} \\\\ \u0026amp;= \\left\\{ \\left( \\alpha_{11}, a_{12}, a_{13}, a_{21}, a_{22}, a_{23} \\right) : \\alpha_{ij} \\in \\mathbb{C} \\right\\} \\end{align*} $$\nつまり、$\\mathbb{C}^{6}$と同型だ。\n$$ \\mathbb{C}^{\\Gamma} = \\mathbb{C}^{2} \\otimes \\mathbb{C}^{3} \\cong \\mathbb{C}^{6} $$\n$\\mathbb{C}$を積空間にまとめると、変数の位置が増え、テンソル積にまとめると、インデックスの位置が増えると考えるとわかりやすいだろう。\n$$ z_{1} \\in \\mathbb{C}\\qquad (z_{1},z_{2}) \\in \\mathbb{C} \\times \\mathbb{C}\\qquad (z_{1}, z_{2}, z_{3}) \\in \\mathbb{C}\\times \\mathbb{C} \\times \\mathbb{C} $$\n$$ (z_{1}, z_{2}) \\in \\mathbb{C}^{2} \\qquad (z_{11}, z_{12}, z_{21}, z_{22}) \\in \\mathbb{C}^{2} \\otimes \\mathbb{C}^{2} \\\\[1em] (z_{111}, z_{112}, z_{121}, z_{122}, z_{211}, z_{212}, z_{221}, z_{222}) \\in \\mathbb{C}^{2} \\otimes \\mathbb{C}^{2} \\otimes \\mathbb{C}^{2} $$\n各$e_{i} \\otimes e_{j}$は$\\mathbb{C}^{6}$の基準基底と以下のように対応する。\n$$ e_{1} \\otimes e_{1} = \\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} \\quad e_{1} \\otimes e_{2} = \\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} \\quad e_{1} \\otimes e_{3} = \\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} \\\\[2em] e_{2} \\otimes e_{1} = \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\end{bmatrix} \\quad e_{2} \\otimes e_{2} = \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\\end{bmatrix} \\quad e_{2} \\otimes e_{3} = \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\\end{bmatrix} $$\n行列のクロネッカー積で表すと次のようになる。\n$$ e_{1} \\otimes e_{1} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\otimes \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\\\[1.5em] 0 \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\quad e_{1} \\otimes e_{2} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\otimes \\begin{bmatrix} 0 \\\\ 1 \\\\ 0\\end{bmatrix} = \\begin{bmatrix} 1 \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\\\[1.5em] 0 \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\\\[2em] e_{1} \\otimes e_{3} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\otimes \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\\\[1.5em] 0 \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\quad e_{2} \\otimes e_{1} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\otimes \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\\\[1.5em] 1 \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\\\[2em] e_{2} \\otimes e_{2} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\otimes \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\\\[1.5em] 1 \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\quad e_{2} \\otimes e_{3} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\otimes \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\\\[1.5em] 1 \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix} $$\nまた、これにより次のことが成立することがわかる。\n$$ \\mathbb{C} \\otimes \\mathbb{C}^{n} \\cong \\mathbb{C}^{n} \\qquad \\mathbb{C} \\otimes \\mathbb{C} \\cong \\mathbb{C} $$\n性質 $\\mathbb{C}^{n} \\otimes \\mathbb{C}^{m}$は以下の二つの演算に関してベクター空間である。 $(x_{1} \\otimes y_{1}) + (x_{2} \\otimes y_{2}) = (x_{1} + x_{2}) \\otimes (y_{1} + y_{2})$ $\\alpha (x \\otimes y) = (\\alpha x) \\otimes y = x \\otimes (\\alpha y)$ $\\mathbb{C}^{n} \\otimes \\mathbb{C}^{m} \\cong \\mathbb{C}^{nm}$ $\\dim (\\mathbb{C}^{n} \\otimes \\mathbb{C}^{m}) = \\dim(\\mathbb{C}^{n}) \\cdot \\dim(\\mathbb{C}^{m}) = nm$ 参照 積ベクター $v \\otimes w$\n${}$ 物理学におけるテンソルとは：テンソルの簡単な定義 微分多様体上で定義されるテンソル 金英勳・許在聲, 量子情報理論 (2020), p3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n金英勳・許在聲, 量子情報理論 (2020), p31\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3414,"permalink":"https://freshrimpsushi.github.io/jp/posts/3414/","tags":null,"title":"ベクトル空間のテンソル積"},{"categories":"위상데이터분석","contents":"ビルドアップ 難しい内容ですが、できるだけ理解しやすいように、すべての計算と説明を省略せずに丁寧に残しました。ホモロジーに興味がある方は、ぜひお読みください。\n実際に、私たちが興味を持っている位相空間 $X$ があり、これが特定のシンプリシャルコンプレックスに従って$\\Delta$-コンプレックス構造を通して表現されるとしましょう。小さな例として、上の図では右側のトーラスが $X$ であり、左側がシンプリシャルコンプレックスに相当します。\nシンプレックスの定義:\nアフィン独立な $v_{0}, v_{1} , \\cdots , v_{n} \\in \\mathbb{R}^{n+1}$ の凸包を**$n$-シンプレックス** $\\Delta^{n}$ と呼び、ベクトル $v_{k}$ を頂点と呼びます。数式的には以下のようになります。 $$ \\Delta^{n} := \\left\\{ \\sum_{k} t_{k} v_{k} : v_{k} \\in \\mathbb{R}^{n+1} , t_{k} \\ge 0 , \\sum_{k} t_{k} = 1 \\right\\} $$ $\\Delta^{n}$ から一つの頂点が除かれて作られる $n-1$-シンプレックス $\\Delta^{n-1}$ を $\\Delta^{n}$ の面と呼びます。$\\Delta^{n}$ のすべての面の和集合を $\\Delta^{n}$ の境界と呼び、$\\partial \\Delta^{n}$ と表します。 シンプレックスの内部 $\\left( \\Delta^{n} \\right)^{\\circ} := \\Delta^{n} \\setminus \\partial \\Delta^{n}$ をオープンシンプレックスと呼びます。 ここで、シンプリシャルコンプレックスとはシンプレックスで構成されるコンプレックスで、具体的には以下のようなCWコンプレックスで構成されているとしましょう。\n$n$-セルの定義:\n以下のように定義された $D^{n} \\subset \\mathbb{R}^{n}$ を $n$-ユニットディスクと呼びます。 $$ D^{n} := \\left\\{ \\mathbf{x} \\in \\mathbb{R}^{n} : \\left\\| \\mathbf{x} \\right\\| \\le 1 \\right\\} $$ $D^{n} \\setminus \\partial D^{n}$ とホモトピー同値な開集合 $e^{n}$ を $n$-セルとも呼びます。 CWコンプレックスの定義:\n離散的な集合 $X^{0} \\ne \\emptyset$ を**$0$-セル**とみなします。 $n$-スケルトン $X^{n}$ は $X^{n-1}$ に$n$-セル $e_{\\alpha}^{n}$ を $\\phi_{\\alpha} : S^{n-1} \\to X^{n-1}$ で結合することによって作られます。 $X := \\bigcup_{n \\in \\mathbb{N}} X^{n}$ が弱位相を持つ位相空間になるとき、$X$ をセルコンプレックスと呼びます。 定義 1 $\\Delta$-コンプレックス構造を持つ位相空間 $X$ が与えられているとしましょう。\n$X$ のオープン $n$-シンプレックスである$n$-セル $e_{\\alpha}^{n}$ を基底を持つ自由アーベル群 $\\Delta_{n} (X)$ と表しましょう。$\\Delta_{n} (X)$ の要素を**$n$-チェインと呼び、係数 $k_{\\alpha} \\in \\mathbb{Z}$ に対して以下のような形式的和で表します。 $$ \\sum_{\\alpha} k_{\\alpha} e_{\\alpha}^{n} $$ 一方、CWコンプレックスの定義から、各 $n$-セル $e_{\\alpha}^{n}$ にはそれに対応する特性写像** $\\sigma_{\\alpha} : \\Delta^{n} \\to X$ が存在するため、単に次のように表すこともあります。 $$ \\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha} $$ 次のように定義される準同型 $\\partial_{n} : \\Delta_{n} (X) \\to \\Delta_{n-1} (X)$ を境界準同型と呼びます。ここで、$\\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} \\cdots , v_{n} \\right]$ は、$\\sigma_{\\alpha}$ の $X$ の $n-1$-シンプレックス に対する制限関数であることを意味します。 $$ \\partial _{n} \\left( \\sigma_{\\alpha} \\right) := \\sum_{i=0}^{n} \\left( -1 \\right)^{i} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} \\cdots , v_{n} \\right] $$ 3. 商群 $\\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$ を $H_{n}^{\\Delta}$ と表し、$H_{n}^{\\Delta}$ はホモロジーグループであるため、$X$ の第 $n$ シンプリシャルホモロジーグループと呼びます。\n群 $0$ は $\\left\\{ 0 \\right\\}$ で定義されたマグマです。つまり、空の代数構造です。 準同型 $\\partial^{2} = 0$ はゼロ準同型です。 $\\text{Im}$ は像です。 $\\ker$ はカーネルです。 集合でハット表記 $\\hat{v}_{i}$ は、次のように $v_{i}$ だけを除くことを意味します。 $$ \\left\\{ v_{1} , \\cdots , \\hat{v}_{i} \\cdots , v_{n} \\right\\} := \\left\\{ v_{1} , \\cdots , v_{n} \\right\\} \\setminus \\left\\{ v_{i} \\right\\} $$ 説明 定義に文字が多いので、理解する前に目に入りにくいのは普通です。血となり肉となる説明なので、丁寧に読むようにしましょう。個人的に勉強している間に苦労した部分をできるだけわかりやすく書くように努めました。\n$\\Delta_{n} (X)$ の要素をなぜチェーンと呼ぶのか？ $$ \\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha} $$ のような記法で $\\sigma_{\\alpha} : \\Delta^{n} \\to X$ を考えることで、これで $e_{\\alpha}^{n}$ が $\\Delta^{n}$ の要素なのか $X$ の要素なのかといったことはあまり考える必要がなくなりました。$n=2$ で全ての係数が $k_{\\alpha} = 1$ の場合、幾何学的に想像できる例として、以下の図の右側のような図形 $\\sum_{i=1}^{7} \\sigma_{i}$ を考えてみましょう。\nここで鎖という表現が理解できれば幸いですが、そうでなくても実際にはあまり関係ありません。とにかく後で重要なのは、それぞれの $n$-チェイン $\\Delta_{n} (X)$ でチェーンコンプレックスを構築することです。\n$\\Delta_{n} (X)$ は本当にグループなのか？ 非常に重要ですが、定義でチェインを説明するときに、形式的和という表現を使いました。これは $\\Delta_{n} (X)$ の要素を説明したに過ぎず、$\\Delta_{n} (X)$ 上で定義された二項演算ではありません。形式的和という言葉が示すように、これはあくまで形式的なものです。小学校の時に使っていた記法を借りてくれば、\n2😀 + 💎 - 3🍌\rのように、とりあえずその位置を絵などで埋めたものと考えても問題ありません。上の式は数学的には意味がありません。なぜなら、笑顔 😀 の2倍が何であり、そこに宝石 💎 を加えることが何であり、バナナ 🍌 を3つ引くことが何なのか、定義されておらず、定義するのも困難だからです。これらを扱うのが難しい状況は、正確に $\\sum_{\\alpha} k_{\\alpha} e_{\\alpha} \\simeq \\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha}$ で\n(そもそも加算を定義できない)オープンシンプレックス $e_{\\alpha}^{n}$ 対応する $\\sigma_{\\alpha}$ が関数である（関数そのものなのか関数値を指しているのかがわかりにくい） それを任意の整数倍して加算した $-3 e_{1}^{n} + 7 e_{2}^{n} \\simeq -3 \\sigma_{1} + 7 \\sigma_{2}$ の意味がわからない という問題と同じです。代数的構造どころか、この集合がどのように見えるのかすらわかりにくいですが、幸いにもこれらの問題は $\\Delta_{n} (X)$ にとっては関係がありません。もし\n$\\sigma=$2😀 + 💎 - 3🍌\rが $\\Delta_{n} (X)$ の要素、つまり $n$-チェインであるとするならば、これらの要素の逆元は、すべての係数 $k_{\\alpha} \\in \\left( \\mathbb{Z} , + \\right)$ の逆元 $-k_{\\alpha} \\in \\left( \\mathbb{Z} , + \\right)$ を係数として持つ\n$-\\sigma=$ (-2)😀 + (-1)💎 + (-(-3))🍌\rで定義するだけで十分です。これにより $\\Delta_{n} (X)$ の単位元は、任意の $\\sigma \\in \\Delta_{n} (X)$ に対して $0 := \\sigma + (-\\sigma)$ で定義され、$\\mathbb{Z}$ がアーベル群であるため、$\\Delta_{n} (X)$ もアーベル群になります。ここで、群 $\\left( \\Delta_{n} (X) , + \\right)$ の演算 $+$ は $\\left( \\mathbb{Z} , + \\right)$ の $+$ から導かれたものですが、同じものではありません。$n$-チェイン $\\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha} \\in \\Delta_{n} (X)$ で登場する $\\sum$ とも異なります。\n要約すると以下のようになります。\n最初に定義したときの $\\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha}$ で加算のように見えるものは、そもそも演算ではなく記法に過ぎませんでした。 $\\left( \\Delta_{n} (X) , + \\right)$ の $+$ は $\\left( \\mathbb{Z} , + \\right)$ の $+$ から導かれましたが、同じものではありません。 $\\left( \\Delta_{n} (X) , + \\right)$ は自由アーベル群であり、これで $\\sum_{\\alpha} k_{\\alpha} \\sigma_{\\alpha}$ も二項演算 $+$ の関数値になります。 $\\partial$ をなぜ境界と呼ぶのか？ $$ \\partial _{n} \\left( \\sigma_{\\alpha} \\right) := \\sum_{i=0}^{n} \\left( -1 \\right)^{i} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} \\cdots , v_{n} \\right] $$\n定義にある数式だけを見ても理解しにくいですが、以下の図を見ればすぐに理解できるでしょう。\n例えば $\\partial_{2}$ を考えると、次のような計算を行うことができます。 $$ \\begin{align*} \u0026amp; \\partial _{2} \\left[ v_{0} ,v_{1}, v_{2} \\right] \\\\ =\u0026amp; \\sum_{i=0}^{2} (-1)^{i} \\left[ v_{0} ,v_{1}, v_{2} \\right] \\setminus \\left[ v_{i} \\right] \\\\ =\u0026amp; (-1)^{0} \\left[ v_{1}, v_{2} \\right] + (-1)^{1} \\left[ v_{0}, v_{2} \\right] + (-1)^{2} \\left[ v_{0}, v_{1} \\right] \\\\ =\u0026amp; \\left[ v_{1}, v_{2} \\right] - \\left[ v_{0}, v_{2} \\right] + \\left[ v_{0}, v_{1} \\right] \\end{align*} $$\nホモロジーグループを学ぶレベルなら、三角形 $\\left[ v_{0} ,v_{1}, v_{2} \\right]$ の境界が $\\left[ v_{1}, v_{2} \\right], \\left[ v_{0}, v_{2} \\right], \\left[ v_{0} , v_{1} \\right]$ で構成されること自体を受け入れられない人はほとんどいないでしょう。本当に理解しにくいのは、一体 $\\left[ v_{1}, v_{2} \\right] - \\left[ v_{0}, v_{2} \\right]$ が何なのかということです。1-シンプレックスである線分同士を引くことが意味を成すのでしょうか？それをベクトルとして扱い、2-シンプレックスである三角形同士の演算はどうなるのでしょうか？\nすべて間違っています。しっかりと頭を整理してもう一度見てみましょう。$\\partial_{2} \\left[ v_{0} ,v_{1}, v_{2} \\right] \\in \\Delta_{1} (X)$ は、その幾何学的な意味を離れて、単に3つの要素 $\\left[ v_{1}, v_{2} \\right], \\left[ v_{0}, v_{2} \\right], \\left[ v_{0} , v_{1} \\right]$ の形式的和である $$ (+1) \\left[ v_{1}, v_{2} \\right] + (-1) \\left[ v_{0}, v_{2} \\right] + (+1) \\left[ v_{0}, v_{1} \\right] $$\nに過ぎません。これを順番に $$ \\begin{align*} a := \\left[ v_{1}, v_{2} \\right] \\ b:= \\left[ v_{0}, v_{2} \\right] \\ c:= \\left[ v_{0} , v_{1} \\right] \\end{align*} $$ と置くと、$\\Delta_{1} (X)$ の正体がようやく見えてきます。例えば、$1$-チェイン $x \\in \\Delta_{1} (X)$ は、ある係数 $k_{a} , k_{b} , k_{c} \\in \\mathbb{Z}$ に対して $$ x = k_{a} a + k_{b} b + k_{c} c $$ のように表される要素です。逆に $a,b,c$ の立場から自由群 $\\Delta_{1} (X) := F[\\left\\{ a,b,c \\right\\}]$ を構築する過程を考えると、$\\Delta_{1} (X)$ とは、3つの未知数で作られる群、つまり $\\mathbb{Z}^{3} \\simeq \\mathbb{Z} \\oplus \\mathbb{Z} \\oplus \\mathbb{Z}$ と同型な群に過ぎないことがわかります。\nこのような考え方の転換は、続く例を理解する上で必須です。幾何を置いて、代数的に考えましょう。\n例 $$ \\begin{align*} \\\\ \\partial_{n} :\u0026amp; \\Delta_{n} (X) \\to \\Delta_{n-1} (X) \\\\ H_{n}^{\\Delta} (X) =\u0026amp; \\ker \\partial_{n} / \\text{Im} \\partial_{n+1} \\end{align*} $$\n特に $n = 0$ の場合、$\\partial_{0} : \\Delta_{0} \\left( X \\right) \\to 0$ なので $\\ker \\partial_{0} = \\Delta_{0} \\left( X \\right)$ です。\n円 $S^{1}$ $1$-ユニットスフィア、つまり円 $X = S^{1}$を考えると、$0$-シンプレックスは頂点 $v$ 一つ、$1$-シンプレックスはエッジ $e$ 一つ、$n \\ge 2$ で $n$-シンプレックスは存在しないので、チェーンコンプレックス自体は以下のように構成されるでしょう。\n$$ \\cdots \\longrightarrow 0 \\longrightarrow \\Delta_{1}\\left( S^{1} \\right) \\overset{\\partial_{1}}{\\longrightarrow} \\Delta_{0}\\left( S^{1} \\right) \\overset{\\partial_{0}}{\\longrightarrow} 0 $$\n自由群 $\\Delta_{1}\\left( S^{1} \\right)$ は $e$ 一つで生成されるので $\\Delta_{1}\\left( S^{1} \\right) \\simeq \\mathbb{Z}$ であり、$\\Delta_{0}\\left( S^{1} \\right)$ も $v$ 一つで生成されるので $\\Delta_{0}\\left( S^{1} \\right) \\simeq \\mathbb{Z}$ です。一方 $$ \\partial e = v - v = 0 $$ なので $\\partial_{1}$ はゼロ準同型です。\n$n = 0$ の場合、$\\ker \\partial_{0} = \\Delta_{0} \\left( S^{1} \\right)$ であり、$\\partial_{1}$ がゼロ準同型なのでその像は $\\left\\{ 0 \\right\\}$ となり、以下が得られます。 $$ \\begin{align*} H_{0}^{\\Delta} \\left( S^{1} \\right) =\u0026amp; \\ker \\partial_{0} / \\text{Im} \\partial_{1} \\\\ \\simeq\u0026amp; \\Delta_{0} \\left( S^{1} \\right) / \\left\\{ 0 \\right\\} \\\\ \\simeq\u0026amp; \\mathbb{Z} \\end{align*} $$\n$n = 1$ の場合、$\\partial_{2}$ の定義域が $0$ なので $\\text{Im} \\partial_{2} = \\left\\{ 0 \\right\\}$ であり、$\\partial_{1}$ がゼロ準同型なので $\\ker \\partial_{1}$ はその定義域である $\\Delta_{1} \\left( S^{1} \\right)$ 自体となり、以下が得られます。 $$ \\begin{align*} H_{1}^{\\Delta} \\left( S^{1} \\right) =\u0026amp; \\ker \\partial_{1} / \\text{Im} \\partial_{2} \\\\ \\simeq\u0026amp; \\Delta_{1} \\left( S^{1} \\right) / \\left\\{ 0 \\right\\} \\\\ \\simeq\u0026amp; \\mathbb{Z} \\end{align*} $$\n$n \\ge 2$ に対しては、$H_{n}^{\\Delta} \\left( S_{1} \\right) \\simeq 0$ なので、以下のように要約できます。 $$ H_{n}^{\\Delta} \\left( S_{1} \\right) \\simeq \\begin{cases} \\mathbb{Z} \u0026amp; , \\text{if } n = 0, 1 \\\\ 0 \u0026amp; , \\text{if } n \\ge 2 \\end{cases} $$\nトーラス $T^{2}$ 上の図のようなトーラス $T^{2}$を考えると、$0$-シンプレックスは頂点 $v$ 一つ、$1$-シンプレックスはエッジ $a$、$b$、$c$ 三つ、$2$-シンプレックスは $U$、$L$ 二つ、$n \\ge 3$ で $n$-シンプレックスは存在しないので、チェーンコンプレックス自体は以下のように構成されるでしょう。\n$$ \\cdots \\longrightarrow 0 \\longrightarrow \\Delta_{2}\\left( T \\right) \\overset{\\partial_{2}}{\\longrightarrow} \\Delta_{1}\\left( T \\right) \\overset{\\partial_{1}}{\\longrightarrow} \\Delta_{0}\\left( T \\right) \\overset{\\partial_{0}}{\\longrightarrow} 0 $$\nこれにより、自由群 $\\Delta_{n} \\left( T \\right)$ は $$ \\Delta_{n} \\left( T \\right) \\simeq \\begin{cases} \\mathbb{Z}^{1} \u0026amp; , \\text{if } n = 0 \\\\ \\mathbb{Z}^{3} \u0026amp; , \\text{if } n = 1 \\\\ \\mathbb{Z}^{2} \u0026amp; , \\text{if } n = 2 \\\\ 0 \u0026amp; , \\text{if } n \\ge 3 \\end{cases} $$\nとなります。一方、エッジ $a$、$b$、$c$ の両端点は $v$ に接続されているので $$ \\begin{align*} \\partial a =\u0026amp; v - v = 0 \\\\ \\partial b =\u0026amp; v - v = 0 \\\\ \\partial c =\u0026amp; v - v = 0 \\end{align*} $$ であり、円の場合と同様に $\\partial_{1}$ はゼロ準同型です。\n$n = 0$ の場合、円の場合と同様に以下が成立します。 $$ \\begin{align*} H_{0}^{\\Delta} \\left( T \\right) =\u0026amp; \\ker \\partial_{0} / \\text{Im} \\partial_{1} \\\\ \\simeq\u0026amp; \\Delta_{0} \\left( T \\right) / \\left\\{ 0 \\right\\} \\\\ \\simeq\u0026amp; \\mathbb{Z} \\end{align*} $$\n$n = 1$ の場合、$\\partial_{1}$ がゼロ準同型なので $\\ker \\partial_{1}$ はその定義域である $\\Delta_{1} \\left( T \\right)$ 自体です。一方で $\\partial_{2} : \\Delta_{2}\\left( T \\right) \\to \\Delta_{1}\\left( T \\right)$ について $$ \\partial_{2} U = a + b - c = \\partial_{2} L $$ であり、$\\left\\{ a, b, a + b - c \\right\\}$ は $\\Delta_{1}\\left( T \\right)$ の基底なので $H_{1}^{\\Delta}$ は$a$ と $b$ で生成される自由群と同型です。つまり、以下が成立します。 $$ H_{1}^{\\Delta} \\left( T \\right) \\simeq \\mathbb{Z} \\oplus \\mathbb{Z} $$\n$n = 2$ の場合、$\\partial_{3}$ の定義域が $0$ なので $\\text{Im} \\partial_{3} = \\left\\{ 0 \\right\\}$ であり、$\\partial_{2} : \\Delta_{2}\\left( T \\right) \\to \\Delta_{1}\\left( T \\right)$ では $\\Delta_{2}\\left( T \\right) \\simeq \\mathbb{Z}^{2}$ で $\\Delta_{1}\\left( T \\right) \\simeq \\mathbb{Z}^{3}$ なので $\\ker \\partial_{2} \\simeq \\mathbb{Z}^{3-2}$ です。これを整理すると、以下が得られます。 $$ \\begin{align*} H_{2}^{\\Delta} \\left( T \\right) =\u0026amp; \\ker \\partial_{2} / \\text{Im} \\partial_{3} \\\\ \\simeq\u0026amp; \\mathbb{Z}^{3-2} / \\left\\{ 0 \\right\\} \\\\ \\simeq\u0026amp; \\mathbb{Z} \\end{align*} $$\n$n \\ge 3$ に対しては、$H_{n}^{\\Delta} \\left( T \\right) \\simeq 0$ なので、以下のように要約できます。 $$ H_{n}^{\\Delta} \\left( T \\right) \\simeq \\begin{cases} \\mathbb{Z} \u0026amp; , \\text{if } n = 0 \\\\ \\mathbb{Z} \\oplus \\mathbb{Z} \u0026amp; , \\text{if } n = 1 \\\\ \\mathbb{Z} \u0026amp; , \\text{if } n = 2 \\\\ 0 \u0026amp; , \\text{if } n \\ge 3 \\end{cases} $$\n定理 $H_{n}^{\\Delta}$ はホモロジーグループである ホモロジーグループの定義:\n$n \\in \\mathbb{N}_{0}$ とします。アーベル群 $C_{n}$ と準同型 $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ のチェーン $$ \\cdots \\longrightarrow C_{n+1} \\overset{\\partial_{n+1}}{\\longrightarrow} C_{n} \\overset{\\partial_{n}}{\\longrightarrow} C_{n-1} \\longrightarrow \\cdots \\longrightarrow C_{1} \\overset{\\partial_{1}}{\\longrightarrow} C_{0} \\overset{\\partial_{0}}{\\longrightarrow} 0 $$ がすべての $n$ に対して $$ \\partial_{n} \\circ \\partial_{n+1} = 0 $$ を満たす場合、$\\mathcal{C} := \\left\\{ \\left( C_{n}, \\partial_{n} \\right) \\right\\}_{n=0}^{\\infty}$ をチェーンコンプレックスと呼びます。 商群 $H_{n} := \\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$ を $\\mathcal{C}$ の第 $n$ ホモロジーグループと呼びます。 準同型 $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ を境界または微分オペレータと呼びます。 $$ \\cdots \\longrightarrow \\Delta_{n+1} \\overset{\\partial_{n+1}}{\\longrightarrow} \\Delta_{n} \\overset{\\partial_{n}}{\\longrightarrow} \\Delta_{n-1} \\longrightarrow \\cdots $$\nチェーンコンプレックス $\\left\\{ \\left( \\Delta_{n} (X) , \\partial_{n} \\right) \\right\\}_{n=0}^{\\infty}$ に対して $H_{n}^{\\Delta} := \\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$ はホモロジーグループです。つまり、すべての $n \\in \\mathbb{N}$ に対して $\\partial_{n} \\circ \\partial_{n+1}$ はゼロ準同型です。\n証明 $\\sigma \\in \\Delta_{n}$ に $\\partial_{n-1} \\circ \\partial_{n}$ を適用してみると、以下が得られます。 $$ \\begin{align*} \u0026amp; \\left( \\partial_{n-1} \\circ \\partial_{n} \\right) \\left( \\sigma \\right) \\\\ =\u0026amp; \\partial_{n-1} \\left( \\partial_{n} \\left( \\sigma \\right) \\right) \\\\ =\u0026amp; \\partial_{n-1} \\left( \\sum_{i=0}^{n} \\left( -1 \\right)^{i} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} , \\cdots , v_{n} \\right] \\right) \\\\ =\u0026amp; \\sum_{j \u0026lt; i} \\left( -1 \\right)^{i} \\left( -1 \\right)^{j} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} , \\cdots , \\hat{v}_{j} , \\cdots , v_{n} \\right] \\\\ \u0026amp; + \\left( -1 \\right) \\sum_{j \u0026gt;i} \\left( -1 \\right)^{i} \\left( -1 \\right)^{j} \\sigma_{\\alpha} | \\left[ v_{1} , \\cdots , \\hat{v}_{i} , \\cdots , \\hat{v}_{j} , \\cdots , v_{n} \\right] \\\\ =\u0026amp; 0 \\end{align*} $$\n実際、このような証明は、一般的に証明するよりも、帰納的な例を示すことがより役立ちます。 $$ \\begin{align*} \u0026amp; \\partial_{1} \\left( \\partial_{2} \\left[ v_{0}, v_{1} , v_{2} \\right] \\right) \\\\ =\u0026amp; \\partial_{1} \\left( \\left[ v_{1} , v_{2} \\right] - \\left[ v_{0}, v_{2} \\right] + \\left[ v_{0}, v_{1} \\right] \\right) \\\\ =\u0026amp; \\partial_{1} \\left[ v_{1} , v_{2} \\right] - \\partial_{1} \\left[ v_{0}, v_{2} \\right] + \\partial_{1} \\left[ v_{0}, v_{1} \\right] \\\\ =\u0026amp; \\left[ v_{2} \\right] - \\left[ v_{1} \\right] - \\left( \\left[ v_{2} \\right] - \\left[ v_{0} \\right] \\right) + \\left[ v_{1} \\right] - \\left[ v_{0} \\right] \\\\ =\u0026amp; 0 \\end{align*} $$\n■\nHatcher. (2002). Algebraic Topology: p104~106.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2383,"permalink":"https://freshrimpsushi.github.io/jp/posts/2383/","tags":null,"title":"シンプリシアルホモロジーグループの定義"},{"categories":"줄리아","contents":"概要 ブロードキャスティングは Juliaで最も重要な概念の一つであり、ベクトル化されたコードを書く際に非常に便利な文法だ1。二項演算の前に.を置いたり、関数の後に.を置くことで使用する。これは点ごとに関数を適用するという意味であり、その目的にぴったりの表現だ。\nプログラミング的にブロードキャスティングは、マップとリデュースのマップを使いやすくしたものと見ることができる。\nコード 二項演算 二項演算には.を付けて使用する。例えば、行列$A \\in \\mathbb{Z}_{9}^{3 \\times 4}$の全要素にスカラ$a \\in \\mathbb{R}$を足すコードは以下の通りだ。\njulia\u0026gt; A = rand(0:9, 3,4)\r3×4 Matrix{Int64}:\r5 6 3 3\r7 4 8 8\r0 2 2 7\rjulia\u0026gt; a = rand()\r0.23234165065465284\rjulia\u0026gt; A .+ a\r3×4 Matrix{Float64}:\r5.23234 6.23234 3.23234 3.23234\r7.23234 4.23234 8.23234 8.23234\r0.232342 2.23234 2.23234 7.23234 一般関数 julia\u0026gt; f(x) = x^2 - 1\rf (generic function with 3 methods)\rjulia\u0026gt; f(a)\r-0.9460173573710713 例えば関数$f : \\mathbb{R} \\to \\mathbb{R}$を考えてみよう。これはスカラ関数なので、$a \\in \\mathbb{R}$に対して上記のようにうまく計算される。\njulia\u0026gt; f(A)\rERROR: LoadError: DimensionMismatch しかし、行列$A$を入れてみるとLoadErrorが発生する。考えてみれば、行列の平方、特に$A \\in \\mathbb{Z}_{9}^{3 \\times 4}$のような直方体の行列の平方とは何か、という問題から始まる。そのため、$f(x) = x^{2} - 1$のような関数に無闇に入れることはできない。しかし、行列$A$の全ての値に対してそれぞれ平方を取り、その後$1$を引いた行列を得たい場合は、f.のように点を打つことで、行列の全要素に関数$f : \\mathbb{R} \\to \\mathbb{R}$を適用できる。\njulia\u0026gt; f.(A)\r3×4 Matrix{Int64}:\r24 35 8 8\r48 15 63 63\r-1 3 3 48 速度比較 多くの場合、ブロードキャスティングは性能面でも優れている。しかし、速度を性能の基準として性能を評価する部分には、かなり難しい点があるので、必ず以下の内容を確認してほしい。\n例として、以下は1から10万までの数に平方根を取るコードだ。\njulia\u0026gt; @time for x in 1:100000\rsqrt(x)\rend\r0.000001 seconds\rjulia\u0026gt; @time sqrt.(1:100000);\r0.000583 seconds (2 allocations: 781.297 KiB) 単純な速度だけを比較すると、ブロードキャスティングはforループよりも約500倍遅い。しかし、これは単純な計算から得られたベンチマークで、保存するプロセスまで含めた場合は話が変わる。\njulia\u0026gt; z = []\rAny[]\rjulia\u0026gt; @time for x in 1:100000\rpush!(z, sqrt(x))\rend\r0.005155 seconds (100.01 k allocations: 3.353 MiB)\rjulia\u0026gt; @time y = sqrt.(1:100000);\r0.000448 seconds (2 allocations: 781.297 KiB) 保存するプロセスを含めても、ブロードキャスティングを適用したコードには変わりはないが、空の配列に値を追加しなければならない反復文の場合、ベクトル化されたコードと比べて約10倍遅いことがわかる。これはsqrt()自体よりもpush!()が動的配列を扱う際に消費するコストが大きいと言えるが、とにかく結果としてブロードキャスティング側が速い。当然、反復文をより速くする方法もあるが（例えばAny[]がFloat64[]に変わるだけで改善されるだろう）、実際に遭遇するほとんどのコーディングで、ブロードキャスティングを使用する方が扱いやすく、速度面でも優れている。\nこれは単なる概念的な部分を超え、ジュリアがインタープリターよりもコンパイラ言語に近い1こととも関連している。次のループで何が起こるかわからないfor反復文よりも、タイプとサイズが具体的に決まっているベクトルに対してコンパイルする方が、コンパイラにとって楽ではないだろうか？\n99%程度の関数では、私たちが独自に反復文を使うよりも、ジュリアを作った人たちが考案した方法をそのまま使う方が速いと断言できる。コードを無理にベクトル化する必要はないが、ベクトル化できるコードなら、ほとんどの場合、ベクトル化した方が圧倒的に\u0026hellip;本当に圧倒的に速い。これはジュリアに限らず、マトラボ、Rのようにベクトル演算に特化した言語なら誰もが持っている特徴だが、関数型プログラミングのパラダイムを最もよく受け入れている新しい言語であり、速度面で自信を示している点が異なるだけだ。\n環境 OS: Windows julia: v1.7.0 https://docs.julialang.org/en/v1/manual/arrays/#Broadcasting\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2382,"permalink":"https://freshrimpsushi.github.io/jp/posts/2382/","tags":null,"title":"ジュリアのブロードキャスティング文法"},{"categories":"양자정보이론","contents":"定義1 以下のようなベクトル値ブール関数をフレドキンゲートFredkin gateと呼ぶ。\n$$ F : \\left\\{ 0, 1 \\right\\}^{3} \\to \\left\\{ 0, 1 \\right\\}^{3} $$\n$$ F (a, b, c) = \\Big(a, (\\lnot a \\land b) \\lor (a \\land c), (\\lnot a \\land c) \\lor (a \\land b) \\Big) $$\n$\\text{CSWAP}$ ゲートControlled SWAP(CSWAP) gateとも呼ばれる。 説明 エドワード・フレドキンEdward Fredkinによって紹介された。フレドキンゲートは、最初の入力を変えずに、最初の入力が$1$の場合には、残りの二つの値を交換swapして出力する。その具体的な計算は次のようである。\n$$ \\begin{align*} F (0,0,0) \u0026amp;= (0, (\\lnot 0 \\land 0) \\lor (0 \\land 0), (\\lnot 0 \\land 0) \\lor (0 \\land 0)) = (0, 0 \\lor 0, 0 \\lor 0) = (0, 0, 0) \\\\ F (0,0,1) \u0026amp;= (0, (\\lnot 0 \\land 0) \\lor (0 \\land 1), (\\lnot 0 \\land 1) \\lor (0 \\land 0)) = (0, 0 \\lor 0, 1 \\lor 0) = (0, 0, 1) \\\\ F (0,1,0) \u0026amp;= (0, (\\lnot 0 \\land 1) \\lor (0 \\land 0), (\\lnot 0 \\land 0) \\lor (0 \\land 1)) = (0, 1 \\lor 0, 0 \\lor 0) = (0, 1, 0) \\\\ F (0,1,1) \u0026amp;= (0, (\\lnot 0 \\land 1) \\lor (0 \\land 1), (\\lnot 0 \\land 1) \\lor (0 \\land 1)) = (0, 1 \\lor 0, 1 \\lor 0) = (0, 1, 1) \\\\ F (1,0,0) \u0026amp;= (1, (\\lnot 1 \\land 0) \\lor (1 \\land 0), (\\lnot 1 \\land 0) \\lor (1 \\land 0)) = (1, 0 \\lor 0, 0 \\lor 0) = (1, 0, 0) \\\\ F (1,0,1) \u0026amp;= (1, (\\lnot 1 \\land 0) \\lor (1 \\land 1), (\\lnot 1 \\land 1) \\lor (1 \\land 0)) = (1, 0 \\lor 1, 0 \\lor 0) = (1, 1, 0) \\\\ F (1,1,0) \u0026amp;= (1, (\\lnot 1 \\land 1) \\lor (1 \\land 0), (\\lnot 1 \\land 0) \\lor (1 \\land 1)) = (1, 0 \\lor 0, 0 \\lor 1) = (1, 0, 1) \\\\ F (1,1,1) \u0026amp;= (1, (\\lnot 1 \\land 1) \\lor (1 \\land 1), (\\lnot 1 \\land 1) \\lor (1 \\land 1)) = (1, 0 \\lor 1, 0 \\lor 1) = (1, 1, 1) \\\\ \\end{align*} $$\n上の表を見れば、$F$が可逆関数であることと、$F$を二回合成すると恒等関数になることが容易にわかる。\n$$ \\operatorname{Id} = F \\circ F $$\nまた、$\\left\\{ F \\right\\}$が機能的に完全であるため、$F$は汎用ゲートである。\nブール関数\rシンボル\r$F$\r真理値表\r入力\r出力\r$a$\r$b$\r$c$\r$a$\r$ (\\lnot a \\land b) \\lor (a \\land c)$\r$ (\\lnot a \\land c) \\lor (a \\land b)$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r$1$\r$1$\r$0$\r$1$\r$0$\r$1$\r$1$\r$1$\r$1$\r$1$\r$1$\r$1$\r整理 (射影と注入を制約なく使えると仮定すると)フレドキンゲート$F$は汎用ゲートである。つまり、$\\left\\{ F \\right\\}$は機能的に完全である。\n証明 定理\n$\\text{NOT}$ゲートと$\\text{AND}$ゲートの集合$\\left\\{ \\lnot, \\land \\right\\}$は機能的に完全である。\n上の定理に従って、射影、注入、$F$を適切に使用して$\\text{NOT}$、$\\text{AND}$を表現できることを示せば、証明は終了する。\n$\\text{NOT}$ゲート\n$$ \\lnot = p_{0} \\circ p_{1} \\circ F \\circ \\jmath_{2} \\circ \\imath_{1} $$\nが成り立つ。まず$\\jmath_{2} \\circ \\imath_{1} (a) = (a, 0, 1)$であるため、次を得る。\n$$ \\begin{equation} F \\circ \\jmath_{2} \\circ \\imath_{1}(a) = F(a, 0, 1) = (a, a, \\lnot a) \\end{equation} $$\nここで、最初の二つの値を消去するために$p_{0} \\circ p_{1}$を取ると、\n$$ p_{0} \\circ p_{1} \\circ F \\circ \\jmath_{2} \\circ \\imath_{1} (a) = p_{0} \\circ p_{1} (a, a, \\lnot a) = \\lnot a $$\n※ さらに$(1)$に$p_{2}$を適用すると、複製関数を得る。\n$\\text{AND}$ゲート\n$$ \\land = p_{0} \\circ p_{1} \\circ F \\circ \\imath_{2} $$\nが成り立つ。まず$F \\circ \\jmath_{2} (a, b)$は次のようである。\n$$ \\begin{align*} F \\circ \\jmath_{2} (a, b) = F(a, b, 0) \u0026amp;= (a, (\\lnot a \\land b) \\lor (a \\land 0), (\\lnot a \\land 0) \\lor (a \\land b)) \\\\ \u0026amp;= (a, (\\lnot a \\land b) \\lor 0, 0 \\lor (a \\land b)) \\\\ \u0026amp;= (a, \\lnot a \\land b, a \\land b) \\end{align*} $$\nしたがって、$p_{0} \\circ p_{1}$を取ると、\n$$ p_{0} \\circ p_{1} \\circ F \\circ \\imath_{2} (a, b) = p_{0} \\circ p_{1} (a, \\lnot a \\land b, a \\land b) = a \\land b $$\n■\n関連項目 $\\text{AND}$ゲート論理積 $\\text{OR}$ゲート論理和 $\\text{NOT}$ゲート論理否定 $\\text{XOR}$ゲート排他的論理和 $\\text{NAND}$ゲート否定論理積 $\\text{NOR}$ゲート否定論理和 $\\operatorname{CNOT}$ゲート トフォリゲート$\\text{CCNOT}$ゲート 金永勳·許在成, 量子情報理論 (2020), p90-93\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3412,"permalink":"https://freshrimpsushi.github.io/jp/posts/3412/","tags":null,"title":"フレドキン・CSWAPゲート"},{"categories":"양자정보이론","contents":"定義1 以下のようなベクトル値ブール関数をトフォリゲートToffoli gateと呼ぶ。\n$$ T : \\left\\{ 0, 1 \\right\\}^{3} \\to \\left\\{ 0, 1 \\right\\}^{3} $$\n$$ T (a, b, c) = (a, b, (a \\land b) \\oplus c) $$\n$\\text{CCNOT}$ ゲートControlled Controlled NOT(CCNOT) gateとも呼ばれる。 説明 トフォリゲートでは、最初の二つの入力が両方とも$1$であれば、三番目の入力が反転する。その他の場合は、入力と出力が同じである。具体的な計算は以下の通りである。\n$$ \\begin{align*} T (0,0,0) \u0026amp;= (0, 0, (0 \\land 0) \\oplus 0) = (0, 0, 0 \\oplus 0) = (0, 0, 0) \\\\ T (0,0,1) \u0026amp;= (0, 0, (0 \\land 0) \\oplus 1) = (0, 0, 0 \\oplus 1) = (0, 0, 1) \\\\ T (0,1,0) \u0026amp;= (0, 1, (0 \\land 1) \\oplus 0) = (0, 1, 0 \\oplus 0) = (0, 1, 0) \\\\ T (0,1,1) \u0026amp;= (0, 1, (0 \\land 1) \\oplus 1) = (0, 1, 0 \\oplus 1) = (0, 1, 1) \\\\ T (1,0,0) \u0026amp;= (1, 0, (1 \\land 0) \\oplus 0) = (1, 0, 0 \\oplus 0) = (1, 0, 0) \\\\ T (1,0,1) \u0026amp;= (1, 0, (1 \\land 0) \\oplus 1) = (1, 0, 0 \\oplus 1) = (1, 0, 1) \\\\ T (1,1,0) \u0026amp;= (1, 1, (1 \\land 1) \\oplus 0) = (1, 1, 1 \\oplus 0) = (1, 1, 1) \\\\ T (1,1,1) \u0026amp;= (1, 1, (1 \\land 1) \\oplus 1) = (1, 1, 1 \\oplus 1) = (1, 1, 0) \\\\ \\end{align*} $$\n上の表を見れば$T$が可逆関数であることと$T$を二回合成すると恒等関数になることが容易に分かる。\n$$ \\operatorname{Id} = T \\circ T $$\nまた、$\\left\\{ T \\right\\}$が機能的に完全であるため、$T$は汎用ゲートである。\n부울 함수\r기호\r진리표\r$T$\r입력\r출력\r$a$\r$b$\r$c$\r$a$\r$b$\r$(a \\land b) \\oplus c$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r$0$\r$1$\r$0$\r$0$\r$1$\r$0$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$1$\r$1$\r$1$\r$1$\r$1$\r$0$\r整理 (射影と注入を制約なく使えると仮定すると)トフォリゲート$T$は汎用ゲートである。つまり$\\left\\{ T \\right\\}$は機能的に完全である。\n証明 定理\n複製関数を許可すると、$\\left\\{ \\uparrow \\right\\}$は機能的に完全である。つまり$\\text{NAND}$ゲート$\\uparrow$は汎用ゲートである。\n上の定理に従い、射影、注入、$T$を適切に使用して複製関数$\\text{cl}$と$\\text{NAND}$ゲートを表現できることを示せば証明が完了する。\n複製関数\n$$ \\operatorname{cl} = p_{1} \\circ T \\circ \\imath_{2} \\circ \\jmath_{1} $$\nが成り立つ。まず$T \\circ \\imath_{2} \\circ \\jmath_{1} (a)$を計算すると以下のようになる。\n$$ T \\circ \\imath_{2} \\circ \\jmath_{1} (a) = T \\circ \\imath_{2} (a, 1) = T (a, 1, 0) $$\nここで$a = 1$であれば$T(1, 1, 0) = (1, 1, 1)$であり、$a = 0$であれば$T(0, 1, 0) = (0, 1, 0)$であるため、次が成り立つ。\n$$ T(a, 1, 0) = (a, 1, a) $$\nしたがって$p_{1}$を取れば、\n$$ p_{1} \\circ T \\circ \\imath_{2} \\circ \\jmath_{1} (a) = p_{1} (a, 1, a) = (a, a) = \\operatorname{cl}(a) $$\n$\\text{NAND}$ゲート\n$$ p_{0} \\circ p_{1} \\circ T \\circ \\jmath_{2} = \\uparrow \\\\ p_{0} \\circ p_{1} \\circ T \\circ \\jmath_{2} (a, b) = a \\uparrow b $$\nが成り立つ。順に計算すると以下のようになる。\n$$ \\begin{align*} p_{0} \\circ p_{1} \\circ T \\circ \\jmath_{2} (a, b) \u0026amp;= p_{0} \\circ p_{1} \\circ T (a, b, 1) \\\\ \u0026amp;= p_{0} \\circ p_{1} (a, b, (a \\land b) \\oplus 1) \\\\ \u0026amp;= p_{0} (a, (a \\land b) \\oplus 1) \\\\ \u0026amp;= (a \\land b) \\oplus 1 \\\\ \u0026amp;= \\lnot(a \\land b) = a \\uparrow b \\end{align*} $$\n最後の行は$\\text{XOR}$ゲートの性質によって成り立つ。\n■\n木村泰宏・許在成, 量子情報理論 (2020), p89-93\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3411,"permalink":"https://freshrimpsushi.github.io/jp/posts/3411/","tags":null,"title":"トッフォリ/CCNOTゲート"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 次のようなベクトル値ブール関数を**$\\operatorname{CNOT}$ゲート**Controlled NOT(CNOT) gateという。\n$$ \\operatorname{CNOT} : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\}^{2} $$\n$$ \\operatorname{CNOT} (a,b) = (a, a \\oplus b) $$\nファインマンゲートFeynman gateとも呼ばれる。2 説明 $\\operatorname{CNOT}$ゲートの入出力の具体的な計算は次のようになる。\n$$ \\begin{align*} \\operatorname{CNOT} (0,0) \u0026amp;= (0, 0 \\oplus 0) = (0, 0) \\\\ \\operatorname{CNOT} (0,1) \u0026amp;= (0, 0 \\oplus 1) = (0, 1) \\\\ \\operatorname{CNOT} (1,0) \u0026amp;= (1, 1 \\oplus 0) = (1, 1) \\\\ \\operatorname{CNOT} (1,1) \u0026amp;= (1, 1 \\oplus 1) = (1, 0) \\end{align*} $$\n上の表を見ると、$\\operatorname{CNOT}$が可逆関数であることと、$\\operatorname{CNOT}$を二回合成すると恒等関数になることが容易に分かる。\n$$ \\operatorname{Id} = \\operatorname{CNOT} \\circ \\operatorname{CNOT} $$\n出力の二番目の値だけを見ると、$\\text{XOR}$ゲートと同じであるため、可逆$\\text{XOR}$ゲートとも呼ばれる。\n부울 함수\r기호\r진리표\r$\\operatorname{CNOT}$\r입력\r출력\r$a$\r$b$\r$a$\r$a \\oplus b$\r$0$\r$0$\r$0$\r$0$\r$0$\r$1$\r$0$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$1$\r$1$\r$0$\rキム・ヨンフン・ホ・ジェソン, 量子情報理論 (2020), p88-89\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Controlled_NOT_gate\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3410,"permalink":"https://freshrimpsushi.github.io/jp/posts/3410/","tags":null,"title":"制御NOT(CNOT)ゲート"},{"categories":"위상데이터분석","contents":"定義 難しい定義 1 $$ \\Delta^{k} \\in K $$\n有限なシンプレックスの集合 $K$ が以下の二つの条件を満たす場合、シンプリシャル・コンプレックスSimplicial Complexと言う。\n(i): もし $\\sigma \\in K$ かつ $\\tau$ が $\\sigma$ のフェイスならば、$\\tau \\in K$ だ。 $$ \\sigma \\in K \\land \\tau \\le \\sigma \\implies \\tau \\in K $$ (ii): もし $\\sigma_{1}, \\sigma_{2} \\in K$ ならば、$\\sigma_{1} \\cap \\sigma_{2}$ は空集合か$\\sigma_{1}$ と $\\sigma_{2}$ のフェイスだ。 $$ \\sigma_{1} , \\sigma_{2} \\in K \\implies \\left( \\sigma_{1} \\cap \\sigma_{2} = \\empty \\right) \\lor \\left( \\sigma_{1} \\cap \\sigma_{2} \\le \\sigma_{1} \\land \\sigma_{1} \\cap \\sigma_{2} \\le \\sigma_{2} \\right) $$ $\\land$ は論理的に‘そして’を表す論理積記号だ。 $\\lor$ は論理的に‘または’を表す論理和記号だ。 シンプレックス $x$ のフェイスとは、$x$ から1点を取り除いて作られるシンプレックスを言う。 シンプレックス $\\tau$、$\\sigma$ に対して $\\tau \\le \\sigma$ というのは、$\\tau$ が $\\sigma$ のフェイスFaceであることを意味する。 簡単な定義 シンプレックスを連結した集合であり、すべての連結部分がシンプレックスであるコンプレックスをシンプリシャル・コンプレックスという。\n説明 シンプレックスはそれ自体で意味と役割があるが、シンプリシャル・コンプレックスを構成することで、ほぼすべての抽象的な対象の幾何学的特性の近似Approximationを得ることができる。2 例えば、以下はイルカの形状のトライアングレーションTriangulation、すなわち最大 $2$-シンプレックス(三角形)を集めて作ったシンプリシャル・コンプレックスだ。\n簡単な定義によると、集合を連結しているという表現はかなり曖昧であり、多くの文書や講演でこのようにざっくりとした定義を紹介してはそれをすんなりと超えてしまっている。これは、シンプリシャル・コンプレックスの実用的、応用的な面を説明するにあたって、厳密な定義を詳しく追求するよりも、図を1つ見せる方が理解しやすく説明もしやすいからだ。\nもちろん、一人で本を開いて勉強する時は、難しい定義を正確に理解する必要がある。シンプリシャル・コンプレックス $K$ はそもそも$k$ 個のアフィン独立な点の凸包であるシンプレックス $\\Delta^{k}$ の集合であり、集合の集合であるファミリーであるため、$\\sigma_{1} \\cap \\sigma_{2}$ といった共通集合を考えることができる。\nポリゴン 定義によれば、ポリゴンはシンプリシャル・コンプレックスのように見えるが、四角形などが含まれているため、シンプリシャル・コンプレックスではない。\n関連項目 集合 $K$ が与えられた条件をすべて満たした場合にシンプリシャル・コンプレックスであり、具体的にどのような外観であるべきかという指定はない。シンプレックスの定義方法によって、数え切れないほど多くのコンプレックスを想像することができ、同じ点（データ）を持っていても、それらの実践的なPractical特性によってシンプリシャル・コンプレックスは千差万別だ。\nEdelsbrunner, Harer. (2010). Computational Topology An Introduction: p63.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Triangulation_(topology)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2379,"permalink":"https://freshrimpsushi.github.io/jp/posts/2379/","tags":null,"title":"単体複合体の定義"},{"categories":"양자정보이론","contents":"定義1 ブール関数の集合$\\left\\{ f_{k} \\right\\} = \\left\\{ f_{k} : \\left\\{ 0, 1 \\right\\}^{n_{k}} \\to \\left\\{ 0, 1 \\right\\} \\right\\}_{k\\in \\Gamma}$が与えられたとしよう。$\\Gamma$は有限集合だ。任意のブール関数\n$$ \\left\\{ 0, 1 \\right\\}^{n} \\to \\left\\{ 0, 1 \\right\\}\\quad (n \\in \\mathbb{N}) $$\nが$\\left\\{ f_{k} \\right\\}$の合成で表現できるとき、集合$\\left\\{ f_{k} \\right\\}$を機能的に完全functionally completeだという。\n定理 $\\text{NOT}$ゲートと$\\text{AND}$ゲートの集合$\\left\\{ \\lnot, \\land \\right\\}$は機能的に完全だ。\n説明 つまり、全てのブール関数は$\\text{NOT}$と$\\text{AND}$の繰り返しで作ることができる。機能的に完全な集合は一意ではなく、無数にある。例えば$a \\land b = \\lnot(\\lnot a \\lor \\lnot b)$であるため、$\\left\\{ \\lnot, \\lor \\right\\}$も機能的に完全な集合だ。この定理から、任意の集合が$\\text{AND}$ゲートと$\\text{NOT}$ゲートを作ることができるかだけ示せば、機能的に完全な集合であることを証明できる。\n機能的に完全な集合の例:\n$\\text{NAND}$ゲート $\\text{NOR}$ゲート $T$(トフォリゲート) $F$(フレドキンゲート) $\\left\\{ \\lnot, \\lor \\right\\}$ $\\left\\{ \\oplus, \\land \\right\\}$ もちろん複製関数が必要な場合もある。これらの結果をコンピュータ回路の言葉で述べれば以下のようになる。全ての論理演算は:\n$\\text{NOT}$ゲートと$\\text{AND}$ゲートのみを使用して回路に実装できる。 $\\text{NAND}$ゲートと複製関数のみを使用して回路に実装できる。 $\\text{NOR}$ゲートと複製関数のみを使用して回路に実装できる。 トフォリゲートのみを使用して回路に実装できる。 フレドキンゲートのみを使用して回路に実装できる。 ユニバーサルゲート 特に、自分自身だけで構成された集合$\\left\\{ f \\right\\}$が機能的に完全なとき、そのような$f$をユニバーサルゲートという。ユニバーサルゲートは自分自身だけで全てのブール関数を表現できる。$\\text{NAND}$、$\\text{NOR}$、$T$、$F$はユニバーサルゲートだ。\n金英勳・許在成, 量子情報理論 (2020), p88\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3408,"permalink":"https://freshrimpsushi.github.io/jp/posts/3408/","tags":null,"title":"関数的に完全な集合とは何か？"},{"categories":"위상수학","contents":"定義 上述のような写像Mapによって、$1$スフィアの二乗である四角形$S^{1} \\times S^{1} = [0,1] \\times [0,1]$と、位相同型である商空間$T$をトーラスTorusという。図にあるように、一番右のドーナツ形がトーラスの一例である。\n説明 トーラスは、数学全般において非常に貴重に扱われる空間―具体的には図形である。一般に広く知られているトポロジーのイメージ(ドーナツはコーヒーカップと位相同型であるなど)に欠かせない存在である。\nプアンカレ予想 プアンカレ予想は、フランスの偉大な数学者ポアンカレPoincaréによって提案され、グリゴリー・ペレルマンГриго́рий Перельма́нによって証明された。\nプアンカレ予想: ある閉じた$3$次元の多様体上のすべての単純閉曲線がひとつの点に縮約することができるならば、その空間は球体に変形することができる。\nトーラスがこの予想に必要不可欠というわけではないが、専門家でなくてもすぐに理解できる簡単な例がトーラスである。たとえば、上の画像のようにトーラスの面に赤い糸で輪を作ったとする。中央に穴が開いているドーナツの穴のため、これを一点に縮約させることは不可能である。プアンカレ予想は、逆にこのような閉曲線を常に一点に縮小できる時、それがドーナツの穴のない球であることを保証できるかどうかを問うものである。\n代数トポロジー 実際に、トーラスを作るには、シンプレックスのコンプレックス、つまりシンプリシャルコンプレックスまでは不要で、四角形$S^{1} \\times S^{1}$だけで十分である。しかし、$\\Delta$-コンプレックス構造を持ち、それに関する有意義な代数的探究を行うには、後で説明する$6$の写像Mapが必要である。\nこれは上から見たトーラスの投影図である。$\\sigma_{a}$、$\\sigma_{b}$、$\\sigma_{v}$は、トーラスを作る際に、一種の「骨格」となる写像である。$\\sigma_{b}$は四角形を丸めて円筒を作り、$\\sigma_{a}$はその円筒の両端を結びつけてドーナツを作る。このとき四角形の頂点は正確に一点に集まる必要があり、$\\sigma_{v}$がその役割を果たす。\nこれはトーラスを横から見た投影図である。$\\sigma_{U}$、$\\sigma_{L}$は、骨格の間を埋める「面」をマッピングしている。繰り返しになるが、$\\sigma_{c}$はトーラスを考える上で必ずしも必要ではなく、四角形を二つの三角形の合併と見たときにその境界を担う写像である。\n周期境界条件 トーラスは、周期境界条件Periodic Boundary Conditionが与えられた単位正方形$[0,1] \\times [0,1]$と見なすこともできる。次の図で、ゴルファーの球は一見$S^{1} \\times S^{1}$の境界を越えて脱出するように見えるが、これがトーラス上でのショットならば、球は背後に落ちることになる。\n当然、これは左右の境界だけでなく、上下の周期境界$b$でも起こる。四角形をトーラスと見なすことは、このように「境界に周期性があり、一方の端に到達すると反対側の端から現れる」ということを簡潔に表現している。\n無限平面 周期境界条件と同じ言葉だが、この空間をどのように見るかによって、新しい応用が可能になることもある。例えば、特定の生物に関する生態系の研究に際して、それらが位置している地形を無視し、それらの相互作用が全体空間のどこでも均等に発生すると仮定してみよう。\n上の図のように、トーラスの展開図である四角形を境界に合わせて配置することを想像してみよ。単一のトーラスは、このように無限平面の一部を代表するものと考えることができる。シミュレーションを行う場合、単一のトーラス上でのシミュレーションは、一般性を失わずに無限平面でのシミュレーションと見なすことができる。\n特性 座標区画写像 中心からチューブまでの距離が$R$で、チューブの直径が$r$である3次元のトーラスの座標区画写像は、$(u_{1}, u_{2}) \\in [0, 2\\pi) \\times [0, 2\\pi)$に関して、次のようである。\n$$ \\mathbf{x}(u_{1}, u_{2}) = \\left( (R + r\\cos u_{2})\\cos u_{1}, (R + r\\cos u_{2})\\sin u_{1}, r\\sin u_{1} \\right) $$\n単純連結性 トーラス$T^{2}$は単純連結ではない。\n","id":2377,"permalink":"https://freshrimpsushi.github.io/jp/posts/2377/","tags":null,"title":"数学でのトーラスとは?"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 以下のようなブール関数を $\\text{NOR}$ゲートNOR gateまたは否定論理和と呼び、次のように表記する。\n$$ \\downarrow : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ 0\\downarrow 0 = 1,\\quad 0\\downarrow 1 = 0,\\quad 1\\downarrow 0 = 0,\\quad 1\\downarrow 1 = 0 $$\n説明 $\\text{NOT}$ゲートと$\\text{OR}$ゲートの合成であり、$\\text{N(OT)}$と$\\text{OR}$を取り入れて$\\text{NOR}$と名付けた。\n$$ \\begin{equation} \\downarrow = \\lnot \\circ \\lor \\end{equation} $$\n$$ a \\downarrow b = \\lnot (a \\lor b) $$\n$\\text{OR}$ゲートとは逆に動作し、すべての入力が偽のときのみ真を出力する。また、$\\left\\{ \\downarrow \\right\\}$は機能的に完全であり、$(1)$により当然と言える。\n부울 함수\r기호\r진리표\r$\\text{NOR}$\r$a$\r$b$\r$a \\downarrow b$\r$0$\r$0$\r$1$\r$0$\r$1$\r$0$\r$1$\r$0$\r$0$\r$1$\r$1$\r$0$\r結論 (複製関数を許容するなら) $\\left\\{ \\downarrow \\right\\}$は機能的に完全である。言い換えると、$\\downarrow$は汎用ゲートである。\n証明 定理\n$\\text{NOT}$と$\\text{OR}$ゲートの集合$\\left\\{ \\lnot, \\lor \\right\\}$は機能的に完全である。\n上の定理に従い、複製関数$\\text{cl}$と$\\downarrow$だけで$\\text{NOT}$ゲートと$\\text{OR}$ゲートを作ることができることを示せばよい。\n$\\text{NOT}$ゲート\n$$ \\lnot = \\downarrow \\circ \\operatorname{cl} \\\\ \\lnot a = a \\downarrow a $$\nが成立する。\n$$ \\begin{align*} \\downarrow \\circ \\operatorname{cl}(0) = 0 \\downarrow 0 = 1 = \\lnot 0 \\\\ \\downarrow \\circ \\operatorname{cl}(1) = 1 \\downarrow 1 = 0 = \\lnot 1 \\\\ \\end{align*} $$\n$\\text{OR}$ゲート\n$$ \\lor = \\downarrow \\circ \\operatorname{cl} \\circ \\downarrow \\\\ a \\lor b = (a \\downarrow b) \\downarrow (a \\downarrow b) $$\nが成立する。\n$$ \\begin{align*} (0 \\downarrow 0) \\downarrow (0 \\downarrow 0) = (1 \\downarrow 1) = 0 = 0 \\lor 0 \\\\ (0 \\downarrow 1) \\downarrow (0 \\downarrow 1) = (0 \\downarrow 0) = 1 = 0 \\lor 1 \\\\ (1 \\downarrow 0) \\downarrow (1 \\downarrow 0) = (0 \\downarrow 0) = 1 = 1 \\lor 0 \\\\ (1 \\downarrow 1) \\downarrow (1 \\downarrow 1) = (1 \\downarrow 1) = 1 = 1 \\lor 1 \\\\ \\end{align*} $$\n■\nキム・ヨンフン・ホ・ジェソン, 量子情報理論 (2020), p86-87\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3407,"permalink":"https://freshrimpsushi.github.io/jp/posts/3407/","tags":null,"title":"否定論理和、NORゲート"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 以下のブール関数を**$\\text{NAND}$ゲート**NAND gate、または否定論理積と呼び、次のように記す。\n$$ \\uparrow : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ 0\\uparrow 0 = 1,\\quad 0\\uparrow 1 = 1,\\quad 1\\uparrow 0 = 1,\\quad 1\\uparrow 1 = 0 $$\n説明 $\\text{NOT}$ゲートと$\\text{AND}$ゲートの合成であり、$\\text{N(OT)}$と$\\text{AND}$を引用して$\\text{NAND}$と命名されている。\n$$ \\begin{equation} \\uparrow = \\lnot \\circ \\land \\end{equation} $$\n$$ a \\uparrow b = \\lnot (a \\land b) $$\n$\\text{AND}$ゲートとは逆に動作し、すべての入力が真のときのみ偽を出力する。また、$\\left\\{ \\uparrow \\right\\}$は関数的に完全であるとされ、$(1)$により当然であると考えられる。\n부울 함수\r기호\r진리표\r$\\text{NAND}$\r$a$\r$b$\r$a \\uparrow b$\r$0$\r$0$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r定理 (複製関数を許容すると) $\\left\\{ \\uparrow \\right\\}$は関数的に完全である。つまり、$\\uparrow$はユニバーサルゲートである。\n証明 定理\n$\\text{NOT}$と$\\text{AND}$ゲートのセット$\\left\\{ \\lnot, \\land \\right\\}$は関数的に完全である。\n上記の定理に従い、複製関数$\\text{cl}$と$\\uparrow$のみで$\\text{NOT}$ゲートと$\\text{AND}$ゲートを作ることができることを示せばよい。\n$\\text{NOT}$ゲート\n$$ \\lnot = \\uparrow \\circ \\operatorname{cl} \\\\ \\lnot a = a \\uparrow a $$\nが成立する。\n$$ \\begin{align*} \\uparrow \\circ \\operatorname{cl}(0) = 0 \\uparrow 0 = 1 = \\lnot 0 \\\\ \\uparrow \\circ \\operatorname{cl}(1) = 1 \\uparrow 1 = 0 = \\lnot 1 \\\\ \\end{align*} $$\n$\\text{AND}$ゲート\n$$ \\land = \\uparrow \\circ \\operatorname{cl} \\circ \\uparrow \\\\ a \\land b = (a \\uparrow b) \\uparrow (a \\uparrow b) $$\nが成立する。\n$$ \\begin{align*} (0 \\uparrow 0) \\uparrow (0 \\uparrow 0) = (1 \\uparrow 1) = 0 = 0 \\land 0 \\\\ (0 \\uparrow 1) \\uparrow (0 \\uparrow 1) = (0 \\uparrow 0) = 0 = 0 \\land 1 \\\\ (1 \\uparrow 0) \\uparrow (1 \\uparrow 0) = (0 \\uparrow 0) = 0 = 1 \\land 0 \\\\ (1 \\uparrow 1) \\uparrow (1 \\uparrow 1) = (1 \\uparrow 1) = 1 = 1 \\land 1 \\\\ \\end{align*} $$\n■\nキム・ヨンフン, ホ・ジェソン, 量子情報理論 (2020), p86-87\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3406,"permalink":"https://freshrimpsushi.github.io/jp/posts/3406/","tags":null,"title":"負論理積、NANDゲート"},{"categories":"줄리아","contents":"コード 1 julia\u0026gt; Dict([\u0026#34;a\u0026#34;, \u0026#34;bc\u0026#34;] .=\u0026gt; [2,8])\rDict{String, Int64} with 2 entries:\r\u0026#34;a\u0026#34; =\u0026gt; 2\r\u0026#34;bc\u0026#34; =\u0026gt; 8 キーKeyとバリューValueとして使いたい二つの配列が与えられた時、Dict(Key .=\u0026gt; Value)を通じて辞書を作ることができる。本質的にはペアPairを作る演算子=\u0026gt;のブロードキャスティングBroadcastingに過ぎない。\n環境 OS: Windows julia: v1.7.0 https://discourse.julialang.org/t/create-a-dictionary-from-arrays-of-keys-and-values/13908/3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2375,"permalink":"https://freshrimpsushi.github.io/jp/posts/2375/","tags":null,"title":"ジュリアで配列から辞書を作成する方法"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 次のようなブール関数を $\\text{XOR}$ ゲートXOR gateまたは排他的論理和exclusive disjuction/orと呼び、以下のように表記する。\n$$ \\oplus : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ 0\\oplus 0 = 0,\\quad 0\\oplus 1 = 1,\\quad 1\\oplus 0 = 1,\\quad 1\\oplus 1 = 0 $$\n説明 $\\text{XOR}$ ゲートは、二つの真理値のうち一つだけが真のとき、つまり真が奇数のときに真を返す。つまり、二つの値が同じならば$0$、異なれば$1$を返すので、二つの値が同じかどうかを比較する機能を実装するのに役立つ。\n「パーセプトロンは$\\text{XOR}$問題を解くことができない」という指摘のため、AIの発展が停滞した1974年から1980年までをAIの冬AI winterと言う。\n부울 함수\r기호\r진리표\r$\\text{XOR}$\r$a$\r$b$\r$a \\oplus b$\r$0$\r$0$\r$0$\r$0$\r$1$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$0$\r特性 $\\text{NOT}$ ゲート、$\\text{AND}$ ゲート、$\\text{OR}$ ゲートで表現可能である。\n$$ \\begin{align*} a \\oplus b \u0026amp;= (a \\land \\lnot b) \\lor (\\lnot a \\land b) \\\\ \u0026amp;= (a \\lor b) \\land (\\lnot a \\lor \\lnot b) \\\\ \u0026amp;= (a \\lor b) \\land \\lnot (a \\land b) \\end{align*} $$\n$a \\oplus 1 = \\lnot a$が成立する。\n$a \\oplus 0 = a$が成立する。\nキム・ヨンフン・ホ・ジェソン, 量子情報理論 (2020), p85\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3405,"permalink":"https://freshrimpsushi.github.io/jp/posts/3405/","tags":null,"title":"排他的論理和、XORゲート"},{"categories":"위상데이터분석","contents":"概要 普通、数学ではコンプレックスComplexと言えば複素数を指すが、幾何学や位相数学ではコンプレックスと言えば、以下のような用語のことを言う。\n用語 トポロジカルにシンプルな$S$から成り立っていて、それらの交差点Intersectionは次元が低いだけで$S$と同じ種類のものを複雑なものComplexという。\n説明 この曖昧な表現から感じられるように、ちゃんとした「定義」ってわけではない。\nここでシンプルなものをシンプレックスSimplexと呼ぼうが、コンプレックスを複体Complexと呼ぼうが、小さなディテールにはこだわらなくてもいい。こちらでコンプレックスと言えば、ああ、そういうことねと思ってスルーしても構わない。コンプレックスはその方法によって種類も非常に多く、具体的な例をいくつか見ると自然と身に付くタイプの概念だ。\n","id":2374,"permalink":"https://freshrimpsushi.github.io/jp/posts/2374/","tags":null,"title":"位相数学における複体とは?"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 次のようなブール関数を $\\text{NOT}$ ゲートNOT gateまたは論理否定negationと言い、次のように表記する。\n$$ \\lnot : \\left\\{ 0, 1 \\right\\} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ \\lnot 0 = 1,\\quad \\lnot 1 = 0 $$\n説明 $\\text{NOT}$ ゲートは入力の反対を返す。\n$\\text{NOT}$ 게이트는 입력의 반대를 반환한다.\n부울 함수\r기호\r진리표\r$\\text{NOT}$\r$a$\r$\\lnot a$\r$0$\r$1$\r$1$\r$0$\rキム・ヨンフン、ホ・ジェソン, 量子情報理論 (2020), p84-85\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3404,"permalink":"https://freshrimpsushi.github.io/jp/posts/3404/","tags":null,"title":"論理否定、NOTゲート"},{"categories":"줄리아","contents":"概要 Juliaは、基本的にRと同じように複素数をサポートしている。\nコード 虚数単位 im julia\u0026gt; z = 3 + 4im\r3 + 4im imは純虚数 $i = \\sqrt{-1}$ を表す。常識的に使われている四則演算は全部使える。\njulia\u0026gt; typeof(z)\rComplex{Int64}\rjulia\u0026gt; typeof(3.0 + 4.0im)\rComplexF64 (alias for Complex{Float64}) タイプをチェックすると、同じ複素数でも、どんな複素数で構成されているかが違う。まるで抽象代数で整数の場合 $\\mathbb{Z} [i]$、あるいは実数の場合 $\\mathbb{R} [i]$と区別される感じが似ている。\n実部、虚部 real(), imag() julia\u0026gt; real(z)\r3\rjulia\u0026gt; imag(z)\r4 共役複素数、モジュラス conj(), abs() julia\u0026gt; conj(z)\r3 - 4im\rjulia\u0026gt; abs(z)\r5.0 一方で、ここでのモジュラス abs()は、特に複素数に対して新たに定義されたわけではなく、絶対値そのものとして使われている点に注意。Juliaは多態性を持っているので、このような設計が自然にうまく行われている。\n一般複素関数 julia\u0026gt; cos(z)\r-27.034945603074224 - 3.851153334811777im\rjulia\u0026gt; log(z)\r1.6094379124341003 + 0.9272952180016122im 当然だが、絶対値と同様に、三角関数や対数関数も複素数 $\\mathbb{C}$ でうまく定義されており、Juliaで特別な操作なしに直接使用できる。\n全コード z = 3 + 4im\rreal(z)\rimag(z)\rconj(z)\rabs(z)\rcos(z)\rlog(z) 環境 OS: Windows julia: v1.7.0 ","id":2373,"permalink":"https://freshrimpsushi.github.io/jp/posts/2373/","tags":null,"title":"ジュリアで複素数を使用する方法"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 以下のようなブール関数を**$\\text{OR}$ ゲート**OR gateまたは論理和disjunctionと呼び、以下のように表記する。\n$$ \\lor : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ 0\\lor 0 = 0,\\quad 0\\lor 1 = 1,\\quad 1\\lor 0 = 1,\\quad 1\\lor 1 = 1 $$\n説明 $\\text{OR}$ ゲートは2つの真理値を1つの真理値に変換し、2つの真理値のうち一方でも真であれば真を返す。\n부울 함수\r기호\r진리표\r$\\text{OR}$\r$a$\r$b$\r$a \\lor b$\r$0$\r$0$\r$0$\r$0$\r$1$\r$1$\r$1$\r$0$\r$1$\r$1$\r$1$\r$1$\r$\\text{NOT}$ ゲートと$\\text{AND}$ ゲートで表現可能である。\n$$ a \\lor b = \\lnot(\\lnot a \\land \\lnot b) $$\n김영훈·허재성, 양자 정보 이론 (2020), p84\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3403,"permalink":"https://freshrimpsushi.github.io/jp/posts/3403/","tags":null,"title":"論理和、ORゲート"},{"categories":"위상데이터분석","contents":"定義 1 ユークリッド空間 $\\left( \\mathbb{R}^{n} , \\left\\| \\cdot \\right\\| \\right)$では、次のような形を定義している。\n$D^{n} \\subset \\mathbb{R}^{n}$として定義されたものを、$n$-ユニットディスクと呼ぶ。 $$ D^{n} := \\left\\{ \\mathbf{x} \\in \\mathbb{R}^{n} : \\left\\| \\mathbf{x} \\right\\| \\le 1 \\right\\} $$ $S^{n} \\subset \\mathbb{R}^{n+1}$として定義されたものを、$n$-ユニットスフィアと呼ぶ。 $$ S^{n} := \\left\\{ \\mathbf{x} \\in \\mathbb{R}^{n+1} : \\left\\| \\mathbf{x} \\right\\| = 1 \\right\\} $$ $D^{n} \\setminus \\partial D^{n}$とホメオモーフィックな開集合 $e^{n}$も、$n$-セルとも言う。 性質 $n$-ディスクの境界は$n$-スフィアである。つまり、下記が成立する。 $$ \\partial D^{n} = S^{n-1} $$\n説明 ディスクとスフィアは、$n=2$の時点から見ると理解しやすい。表現上、$2$-ディスクは日常で接するディスク、中が全て埋まった円盤の形をしていて、$2$-スフィアはそれよりも上の$2+1$次元で、体積を持たず面積だけを持つ球自体を表す。\n定義されているように、$D^{3}$はディスクのように見えないが、ちゃんとディスクである。一方、セルは見ての通り、位相同型を通じて定義されるので、ディスクやスフィアのように集合として正確に定義される必要はない。\nこれは、セルが形やサイズ、位置などに自由であるという意味で理解してもいいし、このようにセルを考えることで、一般的に知られている位相数学の姿が明らかになる。\n$n=0$ の時 $n = 0$の場合は、$D^{0} = \\left\\{ 0 \\right\\}$であり$e^{n}$はそれにホメオモーフィックな唯一の点で構成されているが、$S^{0}$はすぐに$\\partial D^{1}$を意味するので、2つの点を持っている。\n関連項目 一般的な球の定義 一般的な球は、内積を通じてより数学的に定義することができ、実際、楕円体までも簡単に一般化できる。しかし、ディスクとスフィアが最もよく位相数学で言及されるのは、具体的な座標や幾何学的な性質があまり必要ではないからである。\nHatcher. (2002). Algebraic Topology: p xii.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2372,"permalink":"https://freshrimpsushi.github.io/jp/posts/2372/","tags":null,"title":"位相数学におけるディスクとスフィア"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 次のようなブール関数を $\\text{AND}$ ゲートAND gate、または論理積conjunctionと呼び、以下のように表記する。\n$$ \\land : \\left\\{ 0, 1 \\right\\}^{2} \\to \\left\\{ 0, 1 \\right\\} $$\n$$ 0\\land 0 = 0,\\quad 0\\land 1 = 0,\\quad 1\\land 0 = 0,\\quad 1\\land 1 = 1 $$\n説明 $\\text{AND}$ ゲートは二つの真理値を一つの真理値に変換し、二つの真理値が共に真の場合のみ真を返す。\n부울 함수\r기호\r진리표\r$\\text{AND}$\r$a$\r$b$\r$a \\land b$\r$0$\r$0$\r$0$\r$0$\r$1$\r$0$\r$1$\r$0$\r$0$\r$1$\r$1$\r$1$\r$\\text{NOT}$ ゲート と $\\text{OR}$ ゲート で表現可能である。\n$$ a \\land b = \\lnot(\\lnot a \\lor \\lnot b) $$\nキム・ヨンフン、ホ・ジェソン、 量子情報理論 (2020)、p84\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3402,"permalink":"https://freshrimpsushi.github.io/jp/posts/3402/","tags":null,"title":"論理積、ANDゲート"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 以下の関数をブール関数と呼ぶ。$n \\in \\mathbb{N}$に対して,\n$$ f : \\left\\{ 0, 1 \\right\\}^{n} \\to \\left\\{ 0, 1 \\right\\} $$\nここで、$1 =$は真(True)、$0 =$は偽(False)とする。\n一般化 $n, m \\in \\mathbb{N} (m \\gt 2)$に対して,\n$$ f : \\left\\{ 0, 1 \\right\\}^{n} \\to \\left\\{ 0, 1 \\right\\}^{m} $$\nこれをベクトル値ブール関数と呼ぶ。\n説明 イギリスの数学者ジョージ・ブールの名前にちなむ。ジョージ・ブールはブール代数の創設者で、論理学を代数的に扱い、記号論理学に大きな影響を与えた。\nブール関数は、ブール演算または論理演算/接続詞とも呼ばれる。\nコンピュータ理論では、$1$と$0$はそれぞれ電気信号が「存在する」「存在しない」を意味するため、電気信号が入って出るという意味でゲートと言われる。したがって、複数のゲートの合成を回路という。\n可逆関数 ランダウアーの原理によると、情報が失われるたびにエネルギーも使用されるため、コンピューティング効率の側面から一対一対応のブール関数に関心を持つ。一対一対応のブール関数を可逆、そうでないブール関数を不可逆と呼ぶ。可逆であるためには、自明のように$m=n$でなければならない。可逆関数としては、以下がある。\n$\\operatorname{CNOT}$ゲート トフォリゲート$\\text{CCNOT}$ゲート フレドキングート$\\text{CSWAP}$ゲート 種類 $\\text{AND}$ゲート論理積 $\\text{OR}$ゲート論理和 $\\text{NOT}$ゲート論理否定 $\\text{XOR}$ゲート排他的論理和 $\\text{NAND}$ゲートナンド $\\text{NOR}$ゲートノール\n${}$ クローニング関数 $\\text{cl}$ 射影 $p_{i}$ 注入$\\imath_{i}$, $\\jmath_{i}$ キム・ヨンフン・ホ・ジェソン, 量子情報理論 (2020), p85\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3401,"permalink":"https://freshrimpsushi.github.io/jp/posts/3401/","tags":null,"title":"ブール関数"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r정의1 이산확률변수 $X$에 대해서, $X=x$인 사건의 정보(량)information $I$를 다음과 같이 정의한다.\n$$ \\begin{equation} I(x) = -\\log_{2} p(x) \\end{equation} $$\n$p$는 $X$의 확률질량함수이다.\n설명 추상적 개념인 정보에 대한 정량적인 정의를 제시한 사람은 디지털 논리회로 이론과 정보이론을 창시한 클래드 섀넌Claude Shannon이다. 정보량를 '확률의 마이너스 로그'로 정의한 것을 처음 볼 때는 이해가 안되겠지만, 설명을 듣고 나면 이보다 자연스러울 수 없다는 생각이 들 것이다.\n정보의 가치는 일어나기 힘든 일일수록, 그러니까 일어날 확률이 희박할수록 크다. 가령 \u0026quot;내일 물리과 건물에 물리과 학과장님이 오신다\u0026quot;는 문장이 갖고 있는 정보량은 거의 없다고 볼 수 있다. 당연히 내일 학과장이 출근할 것이기 때문이다. 반면에 \u0026quot;내일 물리과 건물에 아이브가 온다\u0026quot;는 문장은 완전히 특급 정보이다. 아이브가 뜬금없이 물리과 건물에 등장할 확률은 거의 없다시피하므로, 이런 정보는 가치가 아주 높은 정보라고 할 수 있다. 다른 예로 \u0026quot;내일 삼성전자의 주식 상승폭이 $1 \\%$ 포인트 이내이다\u0026quot;는 거의 가치가 없는 정보이겠지만, \u0026quot;내일 삼성전자의 주식이 상한가를 친다\u0026quot;는 엄청난 정보이다. 따라서 일어날 확률이 적은 사건이 많은 정보를 갖고있다고 볼 수 있다.\n확률의 함숫값은 $0 \\le p \\le 1$이므로, $p$가 작을수록 정보의 함숫값이 커지도록 하려면 마이너스 로그를 취하면 된다. 따라서 자연스럽게 정보를 $(1)$과 같이 정의할 수 있다.\n$-\\log_{2}(x)$의 치역이 $[0, \\infty)$이므로 확률인 $1$인 사건, 그러니까 반드시 일어나는 일은 정보량이 $0$이다. 또한 일어날 확률이 낮아질수록 정보의 가치는 계속 커진다.\n확률변수 $X$ 자체에 대한 정보량은 엔트로피라 부른다.\n같이보기 확률정보이론에서 정의되는 정보 김영훈·허재성, 양자 정보 이론 (2020), p246\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3398,"permalink":"https://freshrimpsushi.github.io/jp/posts/3398/","tags":null,"title":"고전정보이론에서 정보량이란?"},{"categories":"행렬대수","contents":"定義 サイズが$m\\times n$で、全ての要素が$0$である行列を零行列zero matrixと言い、$O_{m\\times n}$または簡単に$O$と表記する。\n説明 他の表記方法には$Z_{m \\times n}$、$Z$、$\\mathbf{0}_{m\\times n}$、または$\\mathbf{0}$などがある。数字$0$と間違えないように太字で書く方が良い（実際には、ただ$O$を使うのが良い）。零行列は行列の加算における単位元である。つまり、任意の$m\\times n$行列$A$に対して、以下の式が成立する。\n$$ A + O_{m\\times n} = A = O_{m\\times n} + A $$\n","id":3394,"permalink":"https://freshrimpsushi.github.io/jp/posts/3394/","tags":null,"title":"ゼロ行列"},{"categories":"최적화이론","contents":"ビルドアップ $x_{1} , x_{2} \\ge 0$ について、次のような線形計画問題があるとしよう。 $$ \\begin{matrix} \\text{Maximize} \u0026amp; \u0026amp; 2x_{1} \u0026amp; + \u0026amp; 3x_{2} \\\\ \\text{subject to} \u0026amp; \u0026amp; 4x_{1} \u0026amp; + \u0026amp; 8x_{2} \u0026amp; \\le \u0026amp; 12 \\\\ \u0026amp; \u0026amp; 2x_{1} \u0026amp; + \u0026amp; x_{2} \u0026amp; \\le \u0026amp; 3 \\\\ \u0026amp; \u0026amp; 3x_{1} \u0026amp; + \u0026amp; 2x_{2} \u0026amp; \\le \u0026amp; 4 \\end{matrix} $$\n私たちの目標は、与えられた制約条件の下で目的関数 $\\zeta = 2x_{1} + 3x_{2}$ を最大にする最適解 $x_{1}^{\\ast}, x_{2}^{\\ast}$ を見つけることだ。しかし、式をよく見ると、問題を解かなくても最大値の上限があることが分かる。第一の制約条件から、 $$ 2x_{1} + 3x_{2} \\le 4x_{1} + 8x_{2} \\le 12 $$ 実際に、$4x_{1} + 8x_{2} \\le 12$ の両辺を $2$ で割ると、 $$ 2x_{1} + 3x_{2} \\le 2x_{1} + 4x_{2} \\le 6 $$ よって、目的関数の値は、どんなに大きくても $6$ を超えないことが分かる。それだけではない。第二の制約条件を考えると、 $$ 2x_{1} + 3x_{2} = {{ 1 } \\over { 3 }} \\left( 4x_{1} + 8x_{2} + 2x_{1} + x_{2} \\right) \\le {{ 1 } \\over { 3 }} (12 + 3) = 5 $$ のように上限をさらに減らせることもある。つまり、 $$ d_{1} x_{1} + d_{2} x_{2} \\le h $$ という不等式がある時に、この $h$ を縮めていくアプローチを考えることができるということだ。目的関数自体がバウンドされているから、この上限を続けて縮めていくうちに、これ以上縮められない、つまり上限の最小値を見つける時が来るが、よく考えると、この上限を縮めること自体が別の最適化問題である。直感的に、元の最大化問題を解くのも、上限の最小化問題を解くのも、同じ結論に到達できるように思える。\n特に、$d_{1} x_{1} + d_{2} x_{2} \\le h$ の形は「別の何らかの線形計画問題」の制約条件にも見える。そこで、次のようにその「別の何らかの線形計画問題」を定義する。\n定義 1 $$ \\begin{matrix} \\text{Maximize} \u0026amp; \\mathbf{c}^{T} \\mathbf{x} \\\\ \\text{subject to} \u0026amp; A \\mathbf{x} \\le \\mathbf{b} \\\\ \u0026amp; \\mathbf{x} \\ge \\mathbf{0} \\end{matrix} $$\n行列 $A \\in \\mathbb{R}^{m \\times n}$ と $\\mathbf{b} \\in \\mathbb{R}^{m \\times 1}$ と $\\mathbf{c} \\in \\mathbb{R}^{n}$ について、線形計画問題が上述のように与えられているとしよう。\n対応する上限の最小値を見つける次の線形計画問題をデュアル線形計画問題と呼ぶ。 $$ \\begin{matrix} \\text{Minimize} \u0026amp; \\mathbf{b}^{T} \\mathbf{y} \\\\ \\text{subject to} \u0026amp; A^{T} \\mathbf{y} \\ge \\mathbf{c} \\\\ \u0026amp; \\mathbf{y} \\ge \\mathbf{0} \\end{matrix} $$ デュアル線形計画問題に対比される表現として、与えられた元の最大化問題をプライマル線形計画問題と呼ぶ。 説明 2 デュアルとプライマルの解法を視覚的に表すと、上記の通りだ。プライマル問題は下から上に目的関数を最大化し、デュアル問題は上から下へ解の空間を縮小していく。それが正確に一致した時、私たちは疑いなくそれが最適解であると納得できる。\n双対性とは？ 双対性は数学全般で一般的に現れる概念で、関数解析の双対空間や幾何学的双対グラフなど、対称性と似ているがより高い抽象レベルの性質を総称する。\n関連項目 線形計画法における双対性 最適化を専攻しないが一科目として学ぶ場合、線形計画法ではシンプレックス法と双対性この二つを学べば全てを学んだと考えられる。もちろん、それで簡単だというわけではない。\nシンプレックス法が具体的な方法について論じるなら、双対性はより理論的―純粋数学的な議論で、プライマル問題に対してデュアル問題が存在し、その最適解が等しいことを述べる。これは次の双対性定理によって保証される。\n弱い双対性定理 $$ \\sum_{j=1}^{n} c_{j} x_{j}^{\\ast} \\le \\sum_{i=1}^{m} b_{i} y_{i}^{\\ast} $$\n強い双対性定理 $$ \\sum_{j=1}^{n} c_{j} x_{j}^{\\ast} = \\sum_{i=1}^{m} b_{i} y_{i}^{\\ast} $$\nMatousek. (2007). Understanding and Using Linear Programming: p82.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nVanderbei. (2020). Linear Programming(5th Edition): p63.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2348,"permalink":"https://freshrimpsushi.github.io/jp/posts/2348/","tags":null,"title":"線形計画法における双対性"},{"categories":"줄리아","contents":"##概要1\nframestyle 属性を使って図の軸や枠線のスタイルを変更できる。可能なオプションは次の通りだ。\n:box :semi :axes :origin :zerolines :grid :none コード デフォルト設定は :axes だ。\nusing Plots\rx = rand(10)\ry = rand(10)\rp = plot(scatter(y, title=\u0026#34;dafault\u0026#34;, label=\u0026#34;\u0026#34;), scatter(y, title=\u0026#34;:axes\u0026#34;, framestyle=:axse, label=\u0026#34;\u0026#34;), size=(600,300))\rsavefig(p, \u0026#34;default.png\u0026#34;) 各属性によるスタイルは次の通りだ。\np = scatter(fill(x, 6), fill(y, 6), framestyle=[:box :semi :origin :zerolines :grid :none], title=[\u0026#34;:box\u0026#34; \u0026#34;:semi\u0026#34; \u0026#34;:origin\u0026#34; \u0026#34;:zerolines\u0026#34; \u0026#34;:grid\u0026#34; \u0026#34;:none\u0026#34;], layout=6, label=\u0026#34;\u0026#34;, size=(800,450))\rsavefig(p, \u0026#34;framestyle.png\u0026#34;) https://docs.juliaplots.org/latest/generated/attributes_subplot/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3376,"permalink":"https://freshrimpsushi.github.io/jp/posts/3376/","tags":null,"title":"ジュリアプロットにおける軸のスタイルの変更方法 `framestyle`e`"},{"categories":"최적화이론","contents":"要旨 辞書において、$i = 1 , \\cdots , m$に対する以下の形式の方程式の組み合わせを辞書Dictionaryという。 $$ \\begin{align*} \\zeta \u0026amp;=\u0026amp; \u0026amp; \u0026amp; \\sum_{j=1}^{n} c_{j} x_{j} \\\\ x_{n+i} \u0026amp;=\u0026amp; b_{i} \u0026amp;-\u0026amp; \\sum_{j=1}^{n} a_{ij} x_{j} \\end{align*} $$ $\\zeta$を除いた左辺の変数を基底変数Basic Variable、右辺の変数を非基底変数Nonbasic Variableと呼ぶ。それらのインデックスは以下のように表される。 $$ \\begin{align*} \\mathcal{B} :=\u0026amp; \\left\\{ n+1 , n+2 , \\cdots , n+m \\right\\} \\\\ \\mathcal{N} :=\u0026amp; \\left\\{ 1 , 2 , \\cdots , n \\right\\} \\end{align*} $$\n線形計画法の シンプレックス法では、入り変数Entering Variableのインデックスを$\\mathcal{N}$から最小のものを選択し、出る変数Leaving Variableのインデックスを$\\mathcal{B}$から最小のものを選択することをブランドのルールBrand\u0026rsquo;s Ruleと呼ぶ。ブランドのルールによると、シンプレックス法はサイクルに陥らない。\n説明 線形計画問題を解く方法はシンプレックス法だけではなく、シンプレックス法で辞書を更新するピボットルールPivot Ruleも一つだけではないが、ブランドのルールがあることでシンプレックス法は必ず線形計画問題を解けることが保証される点が重要である。この定理を理解すれば、実質的に線形計画法で最も重要な二つのコンセプト、その中でもシンプレックス法をマスターしたと見なしても良い。\n証明 1 戦略: 難しい。ブランドのルールでピボットを行ってサイクルに陥ることが矛盾であることを示す。\nPart 1. $\\zeta = v + \\sum_{j = 1}^{n+m} c_{j}^{\\ast} x_{j}$\n一般性を失わずに、サイクリングが発生した時点から証明を始めよう。ある$k \\in \\mathbb{N}$に対して、辞書が以下のように循環する。 $$ D_{0} , D_{1} , \\cdots, D_{k-1} , D_{0} , D_{1} , \\cdots $$ この辞書の中で、あるものでは基底であり、あるものでは基底でない変数を移り気な変数Fickleと呼ぼう。中で$x_{t}$が最大のインデックスを持つ移り気な変数とし、$x_{t}$を出る変数として持つ辞書を$D$とするとき、一般性を失わずに$D = D_{0}$としたとき、$D$は$\\forall i \\in \\mathcal{B}$に対して以下のように書ける。 $$ \\begin{align*} \\zeta \u0026amp;=\u0026amp; v \u0026amp;+\u0026amp; \\sum_{j \\in \\mathcal{N}} c_{j} x_{j} \\\\ x_{n+i} \u0026amp;=\u0026amp; b_{i} \u0026amp;-\u0026amp; \\sum_{j \\in \\mathcal{N}} a_{ij} x_{j} \\end{align*} $$ ここで$x_{s}$が入る変数であれば$x_{t}$が出る変数であり、まだ$t \\in \\mathcal{B}$であり$s \\in \\mathcal{N}$である。今、$D^{\\ast}$を$x_{t}$が基底に入る辞書とし、$\\forall i \\in \\mathcal{B}^{\\ast}$に対して以下のように書けるとする。 $$ \\begin{align*} \\zeta \u0026amp;=\u0026amp; v^{\\ast} \u0026amp;+\u0026amp; \\sum_{j \\in \\mathcal{N}^{\\ast}} c_{j}^{\\ast} x_{j} \\\\ x_{n+i} \u0026amp;=\u0026amp; b_{i}^{\\ast} \u0026amp;-\u0026amp; \\sum_{j \\in \\mathcal{N}^{\\ast}} a_{ij}^{\\ast} x_{j} \\end{align*} $$ シンプレックス法がサイクルに陥ったことは、すべての$D_{1} , \\cdots, D_{k-1}$が退化しているDegeneratedことを意味し、$v^{\\ast} = v$であるため、目的関数 $\\zeta$は$c_{j}^{\\ast} = 0$として$j \\in \\mathcal{B}^{\\ast}$に対して以下のように書ける。 $$ \\zeta = v + \\sum_{j = 1}^{n+m} c_{j}^{\\ast} x_{j} $$\nPart 2. $\\zeta = v + c_{s} y$\n方程式の形で行ったように、入る変数$x_{s}$を増やし、他の$\\mathcal{N}$にある変数を$0$で固定して、どの変数も負にならない状況を考えよう。数式で表すと $$ \\begin{align*} x_{s} =\u0026amp; y \\\\ x_{j} =\u0026amp; 0 \u0026amp; , j \\in \\mathcal{N} \\setminus \\left\\{ s \\right\\} \\\\ x_{i} =\u0026amp; b_{i} - a_{is} y \u0026amp; , i \\in \\mathcal{B} \\end{align*} $$ であり、これは$y$を増やすことである。目的関数$\\zeta$に対してこれをもう一度書き直すと、以下を得る。 $$ \\zeta = v + c_{s} y $$\nPart 3.\nPart 1の$\\zeta = v + \\sum_{j = 1}^{n+m} c_{j}^{\\ast} x_{j}$を$y$に対して再度書き直すと $$ \\zeta = v + c_{s}^{\\ast} y + \\sum_{i \\in \\mathcal{B}} c_{i}^{\\ast} \\left( b_{i} - a_{is} y \\right) $$ であり、左辺をPart 2の$\\zeta = v + c_{s} y$で置き換えると $$ \\left( c_{s} - c_{s}^{\\ast} + \\sum_{i \\in \\mathcal{B}} c_{i}^{\\ast} a_{is} \\right) y = \\sum_{i \\in \\mathcal{B}} c_{i}^{\\ast} b_{i} $$ を得る。この方程式は、右辺が何であれ、すべての$y$に対して常に成立し、これにより括弧内が常に$0$であることが分かる。 $$ \\begin{equation} c_{s} - c_{s}^{\\ast} + \\sum_{i \\in \\mathcal{B}} c_{i}^{\\ast} a_{is} = 0 \\end{equation} $$ つまり、入る変数が$x_{s}$ならば $$ \\begin{equation} c_{s} \u0026gt; 0 \\end{equation} $$ であることが分かる。一方で$x_{t}$は最大インデックスを持つ移り気な変数であり、$x_{s}$も移り気な変数だったので、$s \u0026lt; t$が成り立つ。$D^{\\ast}$の入る変数は$x_{t}$だったので、$x_{s}$は入る変数ではなく、 $$ \\begin{equation} c_{s}^{\\ast} \\le 0 \\end{equation} $$\n$(1), (2), (3)$により $$ \\sum_{i \\in \\mathcal{B}} c_{i}^{\\ast} a_{is} \u0026lt; 0 $$ であり、以下を満たす$r \\in \\mathcal{B}$が存在しなければならない。 $$ c_{r}^{\\ast} a_{rs} \u0026lt; 0 $$ 結果として$c_{r}^{\\ast} \\ne 0$であり$r \\in \\mathcal{N}^{\\ast}$であるため、$x_{r}$は移り気な変数であり$r \\le t$である。\nPart 4. $r \u0026lt; t$\nシンプレックス法を考えているので、以下の二つを話すことができる。\n$x_{t}$が$D^{\\ast}$の入る変数であるため、$c_{t}^{\\ast} \u0026gt; 0$である。 $x_{t}$が$D$の出る変数であるため、$a_{ts} \u0026gt; 0$である。 したがって、$c_{t}^{\\ast} a_{ts} \u0026gt; 0$であるが、$c_{r}^{\\ast} a_{rs} \u0026lt; 0$であるため、$r \\ne t$、すなわち$r \u0026lt; t$まで言い切ることができる。\nPart 5.\n$r \u0026lt; t$ならば、$c_{r}^{\\ast} \\le 0$であるべきであるが、そうではなく$c_{r}^{\\ast} \u0026gt; 0$であった場合、Part 3により、$r$が$D^{\\ast}$に対する入る変数であるべきだったからである。これとPart 3の$c_{r}^{\\ast} a_{rs} \u0026lt; 0$により、 $$ \\begin{equation} a_{rs} \u0026gt; 0 \\end{equation} $$ である。一方、辞書がサイクルに陥ったということは、事実上すべて同じ解Solutionを表していることを意味し、それによりすべての移り気な変数はその辞書で$0$であることが分かり、特に$x_{r} = 0$である。しかし、$D$で$x_{r}$は基底変数Basic Variableであるため、 $$ \\begin{equation} b_{r} = 0 \\end{equation} $$ である。$(4)$、$(5)$により、$x_{r}$は$D$で出る変数の候補の一つであり、Part 4では$r \u0026lt; t$であるため、ブランドのルールに従ったのであれば、$x_{t}$の代わりに早く選択（出る）されていたはずである。この矛盾により、ブランドのルールを使用したにもかかわらず辞書がサイクルに陥っているという前提が偽であることが分かる。\n■\n参照 線形計画法の基本定理 この定理は線形計画法の基本定理の証明に使用される。\nVanderbei. (2020). Linear Programming(5th Edition): p34~36。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2344,"permalink":"https://freshrimpsushi.github.io/jp/posts/2344/","tags":null,"title":"シンプレックス法のブランドのルール"},{"categories":"줄리아","contents":"概要 ジュリアで、\u0026lt;condition\u0026gt; \u0026amp;\u0026amp; \u0026lt;statement\u0026gt;は\u0026lt;condition\u0026gt;が真のとき\u0026lt;statement\u0026gt;が実行される。関数としては真の場合\u0026lt;statement\u0026gt;の結果が返され、偽の場合\u0026lt;statement\u0026gt;は評価Evaluationさえされない。\n効率的かつ簡潔にコードを書くことができる一方で、可読性が落ちる可能性がある点は受け入れなければならない。また、自分が好んで使わないとしても、他人が書いたコードを読むためには理解しておく必要がある。何の文脈もなく突然出てきたとき、このような文法を知らないと全く理解できない。\n参照 ショートサーキット コード 基本使用例 julia\u0026gt; num = []\rAny[]\rjulia\u0026gt; iseven(2) \u0026amp;\u0026amp; push!(num, 2)\r1-element Vector{Any}:\r2 2は偶数なので、push!(num, 2)が評価され空の配列numに2が入った。\nリターン julia\u0026gt; check = iseven(4) \u0026amp;\u0026amp; push!(num, 4)\r2-element Vector{Any}:\r2\r4\rjulia\u0026gt; check\r2-element Vector{Any}:\r2\r4 \u0026amp;\u0026amp;も関数として何らかの値をリターンできる。この時、checkはcheck = push!(num, 4)をリターンされたと見ることができる。\njulia\u0026gt; check = iseven(5) \u0026amp;\u0026amp; push!(num, 5)\rfalse\rjulia\u0026gt; num\r2-element Vector{Any}:\r2\r4\rjulia\u0026gt; check\rfalse 一方で\u0026lt;statement\u0026gt;が偽の場合、\u0026lt;statement\u0026gt;は評価されず\u0026amp;\u0026amp;自体がfalseをリターンした。\n否定 julia\u0026gt; iseven(6) || push!(num, 6)\rtrue \u0026amp;\u0026amp;の代わりに||を使用する。\n全コード num = []\riseven(2) \u0026amp;\u0026amp; push!(num, 2)\rcheck = iseven(4) \u0026amp;\u0026amp; push!(num, 4)\rcheck\rcheck = iseven(5) \u0026amp;\u0026amp; push!(num, 5)\rnum\rcheck\riseven(6) || push!(num, 6) 環境 OS: Windows julia: v1.6.3 ","id":2341,"permalink":"https://freshrimpsushi.github.io/jp/posts/2341/","tags":null,"title":"ジュリアで条件文を簡潔に書く方法"},{"categories":"선형대수","contents":"定義1 $W_{1}, W_{2}$をベクトル空間$V$の部分空間としよう。$W_{1}$と$W_{2}$の和sumを$W_{1} + W_{2}$と表して、以下のように定義する。\n$$ W_{1} + W_{2} := \\left\\{ x + y : x\\in W_{1}, y \\in W_{2} \\right\\} $$\n一般化2 $W_{1}, W_{2}, \\dots, W_{k}$をベクトル空間$V$の部分空間としよう。これらの部分空間の和は$W_{1} + \\cdots + W_{k}$と表し、以下のように定義する。\n$$ W_{1} + \\cdots + W_{k} = \\sum\\limits_{i=1}^{k}W_{i} := \\left\\{ v_{1} + \\cdots + v_{k} : v_{i} \\in W_{i} \\text{ for } 1 \\le i \\le k \\right\\} $$\n説明 部分空間でなくても部分集合でも定義には問題ない。\n定義からわかるように、必ずしもベクトル空間である必要はなく、要素の加算がうまく定義されていれば良い。したがって、$W_{1}$、$W_{2}$が群の部分群であれば、定義するのに問題はない。反対に言えば、要素同士の加算がなければ、定義できない。\n参照 直和 Stephen H. Friedberg, Linear Algebra (第4版, 2002), p22\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStephen H. Friedberg, Linear Algebra (第4版, 2002), p275\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3371,"permalink":"https://freshrimpsushi.github.io/jp/posts/3371/","tags":null,"title":"ベクトル空間における部分空間の和"},{"categories":"최적화이론","contents":"ビルドアップ 1 $x_{1} , x_{2} \\ge 0$ に関して、次の線形計画問題が与えられたとしよう。 $$ \\begin{matrix} \\text{Maximize} \u0026amp; \u0026amp; x_{1} \u0026amp; + \u0026amp; x_{2} \\\\ \\text{subject to} \u0026amp;-\u0026amp; x_{1} \u0026amp; + \u0026amp; x_{2} \u0026amp; \\le \u0026amp; 1 \\\\ \u0026amp; \u0026amp; x_{1} \u0026amp; \u0026amp; \u0026amp; \\le \u0026amp; 3 \\\\ \u0026amp; \u0026amp; \u0026amp; \u0026amp; x_{2} \u0026amp; \\le \u0026amp; 2 \\end{matrix} $$ つまり、与えられたすべての制約条件を満たしながら $x_{1} + x_{2}$ を最大化したい。これを方程式形式に変えるためには、スラック変数 $x_{3}, x_{4}, x_{5} \\ge 0$ を導入して $$ \\begin{matrix} \\text{Maximize} \u0026amp; \u0026amp; x_{1} \u0026amp; + \u0026amp; x_{2} \\\\ \\text{subject to} \u0026amp;-\u0026amp; x_{1} \u0026amp; + \u0026amp; x_{2} \u0026amp; + \u0026amp; x_{3} \u0026amp; \u0026amp; \u0026amp; \u0026amp; \u0026amp; = \u0026amp; 1 \\\\ \u0026amp; \u0026amp; x_{1} \u0026amp; \u0026amp; \u0026amp; \u0026amp; \u0026amp; + \u0026amp; x_{4} \u0026amp; \u0026amp; \u0026amp; = \u0026amp; 3 \\\\ \u0026amp; \u0026amp; \u0026amp; \u0026amp; x_{2} \u0026amp; \u0026amp; \u0026amp; \u0026amp; \u0026amp; + \u0026amp; x_{5} \u0026amp; = \u0026amp; 2 \\end{matrix} $$ のように表せばいい。これをもう一度辞書またはテーブルに表すと、元の $x_{1}$、$x_{2}$ は非基底変数となって右側に残り、$x_{3}$、$x_{4}$、$x_{5}$ は基底変数になって左側に行く。 $$ \\begin{matrix} \\zeta \u0026amp; = \u0026amp; \u0026amp; 0 \u0026amp; + \u0026amp; x_{1} \u0026amp; + \u0026amp; x_{2} \\\\ x_{3} \u0026amp; = \u0026amp; \u0026amp; 1 \u0026amp; + \u0026amp; x_{1} \u0026amp; - \u0026amp; x_{2} \\\\ x_{4} \u0026amp; = \u0026amp; \u0026amp; 3 \u0026amp; - \u0026amp; x_{1} \u0026amp; \u0026amp; \\\\ x_{5} \u0026amp; = \u0026amp; \u0026amp; 2 \u0026amp; \u0026amp; \u0026amp; - \u0026amp; x_{2} \\end{matrix} $$ $$ \\mathcal{N} = \\left\\{ 1, 2 \\right\\} \\\\ \\mathcal{B} = \\left\\{ 3, 4, 5 \\right\\} $$ ここで、$\\zeta = \\bar{\\zeta} + x_{1} + x_{2}$ の $\\bar{\\zeta}$ は、最適解に対する目的関数の関数の値、つまり現在どのくらい最適化されているかを意味すると見なせる。以下のいくつかの方程式を通して変数置換を行うと、$\\zeta$ の右側にある非基底変数の符号を $-$ に統一できるかもしれない。すると、すべての非基底変数に $0$ を代入したとき $$ \\zeta = \\zeta_{?} - x_{?_{1}} - x_{?_{2}} $$ この形が最大の値になることだ。言い換えれば、これが実行可能基底を得ることだ。非基底変数の符号の中に $+$ があるということは、まだ $\\zeta_{?}$ を増やす可能性があることを意味し、すべての符号が $-$ であるということは、これ以上 $\\zeta_{?}$ を大きくすることはできないという意味になる。これがまさにシンプレックス・メソッドのアイデアだ。問題を代数的に解いているように見えるが、スラック変数を加えて方程式形式を作った時点で解空間がシンプレックスになるため、シンプレックスという表現を使う。\n実践 $$ \\begin{matrix} \\zeta \u0026amp; = \u0026amp; \u0026amp; 0 \u0026amp; + \u0026amp; x_{1} \u0026amp; + \u0026amp; x_{2} \\\\ x_{3} \u0026amp; = \u0026amp; \u0026amp; 1 \u0026amp; + \u0026amp; x_{1} \u0026amp; - \u0026amp; x_{2} \\\\ x_{4} \u0026amp; = \u0026amp; \u0026amp; 3 \u0026amp; - \u0026amp; x_{1} \u0026amp; \u0026amp; \\\\ x_{5} \u0026amp; = \u0026amp; \u0026amp; 2 \u0026amp; \u0026amp; \u0026amp; - \u0026amp; x_{2} \\end{matrix} $$ $$ \\mathcal{N} = \\left\\{ 1, 2 \\right\\} \\\\ \\mathcal{B} = \\left\\{ 3, 4, 5 \\right\\} \\\\ \\left( x_{1} , x_{2} , x_{3}, x_{4} , x_{5} \\right) = (0,0,1,3,2) \\\\ \\bar{\\zeta} = 0 $$\n実際に上の例をシンプレックス・メソッドで解いてみよう。\n$\\zeta$ の形を見ると $x_{1}$ と $x_{2}$ の両方を増やすことはできるが、制約条件から $x_{4} = 3 - x_{1}$ と $x_{5} = 2 - x_{2}$ を知っているので、今のところ大きな意味はなさそうだ。$x_{3}$ についての条件を参考にすると $$ x_{2} = 1 + x_{1} - x_{3} $$ 新しい辞書は次のようになる。\n$$ \\begin{matrix} \\zeta \u0026amp; = \u0026amp; \u0026amp; 1 \u0026amp; + \u0026amp; 2x_{1} \u0026amp; - \u0026amp; x_{3} \\\\ x_{2} \u0026amp; = \u0026amp; \u0026amp; 1 \u0026amp; + \u0026amp; x_{1} \u0026amp; - \u0026amp; x_{3} \\\\ x_{4} \u0026amp; = \u0026amp; \u0026amp; 3 \u0026amp; - \u0026amp; x_{1} \u0026amp; \u0026amp; \\\\ x_{5} \u0026amp; = \u0026amp; \u0026amp; 1 \u0026amp; - \u0026amp; x_{1} \u0026amp; + \u0026amp; x_{3} \\end{matrix} $$ $$ \\mathcal{N} = \\left\\{ 1, 3 \\right\\} \\\\ \\mathcal{B} = \\left\\{ 2, 4, 5 \\right\\} \\\\ \\left( x_{1} , x_{2} , x_{3}, x_{4} , x_{5} \\right) = (0,1,0,3,2) \\\\ \\bar{\\zeta} = 1 $$\n方程式で確認できるように、目的関数の値を元の比較で $1$ だけ増やした。この辞書を修正するプロセスを ピボットステップ2と呼び、非基底から基底に入った$x_{2}$ のような変数を入口変数、基底から非基底に出た $x_{3}$ のような変数を退出変数3と呼ぶ。ここで混乱してはいけないのは、目的関数自体を変えたわけではなく、これからも変えることはないということだ。ビルドアップからずっとそのように、既存の変数は単に他の変数を代表するだけであり、以前には見えなかった定数項が今見えるようになっただけだ。\n今の辞書で私たちがやるべきことはかなり明確だ。$x_{1}$ を増やせば、その倍の量だけ目的関数が大きくなりそうだ。同じ理由で $x_{5}$ を入口変数として使うと $$ x_{1} = 1 + x_{3} - x_{5} $$ となり、\n$$ \\begin{matrix} \\zeta \u0026amp; = \u0026amp; \u0026amp; 3 \u0026amp; + \u0026amp; x_{3} \u0026amp; - \u0026amp; 2x_{5} \\\\ x_{1} \u0026amp; = \u0026amp; \u0026amp; 1 \u0026amp; + \u0026amp; x_{3} \u0026amp; - \u0026amp; x_{5} \\\\ x_{2} \u0026amp; = \u0026amp; \u0026amp; 2 \u0026amp; \u0026amp; \u0026amp; - \u0026amp; x_{5} \\\\ x_{4} \u0026amp; = \u0026amp; \u0026amp; 2 \u0026amp; - \u0026amp; x_{3} \u0026amp; + \u0026amp; x_{5} \\end{matrix} $$ $$ \\mathcal{N} = \\left\\{ 3, 5 \\right\\} \\\\ \\mathcal{B} = \\left\\{ 1, 2, 4 \\right\\} \\\\ \\left( x_{1} , x_{2} , x_{3}, x_{4} , x_{5} \\right) = (1,2,0,2,0) \\\\ \\bar{\\zeta} = 3 $$\nとなる。$\\zeta$ の右側に $x_{3}$ がまだ増える余地があるので、$x_{3}$ を入口変数として使ってみると $$ x_{3} = 2 - x_{4} + x_{5} $$ となり、\n$$ \\begin{matrix} \\zeta \u0026amp; = \u0026amp; \u0026amp; 5 \u0026amp; - \u0026amp; x_{4} \u0026amp; - \u0026amp; x_{5} \\\\ x_{1} \u0026amp; = \u0026amp; \u0026amp; 3 \u0026amp; - \u0026amp; x_{4} \u0026amp; - \u0026amp; \\\\ x_{2} \u0026amp; = \u0026amp; \u0026amp; 2 \u0026amp; \u0026amp; \u0026amp; - \u0026amp; x_{5} \\\\ x_{3} \u0026amp; = \u0026amp; \u0026amp; 2 \u0026amp; - \u0026amp; x_{4} \u0026amp; + \u0026amp; x_{5} \\end{matrix} $$ $$ \\mathcal{N} = \\left\\{ 4, 5 \\right\\} \\\\ \\mathcal{B} = \\left\\{ 1, 2, 3 \\right\\} \\\\ \\left( x_{1} , x_{2} , x_{3}, x_{4} , x_{5} \\right) = (3,2,2,0,0) \\\\ \\bar{\\zeta} = 5 $$\nとなる。これで、$\\zeta$ の右側のすべての変数の符号が $-$ であり、どの変数を触っても $\\zeta$ が大きくなることはなく、最適化が完了したことがわかる。実際に、最初に与えられた線形計画問題 $$ \\begin{matrix} \\text{Maximize} \u0026amp; \u0026amp; x_{1} \u0026amp; + \u0026amp; x_{2} \\\\ \\text{subject to} \u0026amp;-\u0026amp; x_{1} \u0026amp; + \u0026amp; x_{2} \u0026amp; \\le \u0026amp; 1 \\\\ \u0026amp; \u0026amp; x_{1} \u0026amp; \u0026amp; \u0026amp; \\le \u0026amp; 3 \\\\ \u0026amp; \u0026amp; \u0026amp; \u0026amp; x_{2} \u0026amp; \\le \u0026amp; 2 \\end{matrix} $$ に $x_{1} = 3$ と $x_{2} = 2$ を代入して再計算してみると、すべての制約条件をよく満たし、目的関数の値も正確に $\\zeta = \\bar{\\zeta} - 0 = 5$ であることが確認できる。\nMatousek. (2007). 線形計画法を理解して使う：p57~60。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMatousek. (2007). 線形計画法を理解して使う：p59。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nVanderbei. (2020). 線形計画法(5版)：p0。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2336,"permalink":"https://freshrimpsushi.github.io/jp/posts/2336/","tags":null,"title":"線形計画法のシンプレックス法"},{"categories":"최적화이론","contents":"ノーテーション $$ \\begin{matrix} \\text{Maximize} \u0026amp; \\mathbf{c}^{T} \\mathbf{x} \\\\ \\text{subject to} \u0026amp; A \\mathbf{x} = \\mathbf{b} \\\\ \u0026amp; \\mathbf{x} \\ge \\mathbf{0} \\end{matrix} $$\nmatrix $A \\in \\mathbb{R}^{m \\times n}$、$\\mathbf{b} \\in \\mathbb{R}^{m \\times 1}$、$\\mathbf{c} \\in \\mathbb{R}^{n}$について、linear programming problemが上記のようなequation formで示されるとする。そして、その成分を以下のように記述しよう。 $$ \\begin{align*} A =\u0026amp; \\left( a_{ij} \\right) \\\\ \\mathbf{b} =\u0026amp; \\left( b_{1} , \\cdots , b_{m} \\right) \\\\ \\mathbf{c} =\u0026amp; \\left( c_{1} , \\cdots , c_{n} \\right) \\\\ \\mathbf{x} =\u0026amp; \\left( x_{1} , \\cdots , x_{n} \\right) \\end{align*} $$\nディクショナリ 1 $i = 1 , \\cdots , m$について、以下の形の連立方程式をディクショナリと呼ぶ。 $$ \\begin{align*} \\zeta \u0026amp;=\u0026amp; \u0026amp; \u0026amp; \\sum_{j=1}^{n} c_{j} x_{j} \\\\ x_{n+i} \u0026amp;=\u0026amp; b_{i} \u0026amp;-\u0026amp; \\sum_{j=1}^{n} a_{ij} x_{j} \\end{align*} $$\n$\\zeta$を除く左辺の変数を基底変数、右辺の変数を非基底変数と呼ぶ。まず、これらのインデックスを $$ \\begin{align*} \\mathcal{B} :=\u0026amp; \\left\\{ n+1 , n+2 , \\cdots , n+m \\right\\} \\\\ \\mathcal{N} :=\u0026amp; \\left\\{ 1 , 2 , \\cdots , n \\right\\} \\end{align*} $$ のように表す。ただし、これは最初に与えられたlinear programming problemをそのまま書き写したものであり、解が進むにつれて変化する係数にはバーをつけて、次のように表現する。 $$ \\begin{align*} \\zeta \u0026amp;=\u0026amp; \\bar{\\zeta} \u0026amp;+\u0026amp; \\sum_{j \\in \\mathcal{N}} \\bar{c}_{j} x_{j} \\\\ x_{n+i} \u0026amp;=\u0026amp; \\bar{b}_{i} \u0026amp;-\u0026amp; \\sum_{j \\in \\mathcal{N}} \\bar{a}_{ij} x_{j} \\end{align*} $$ ここで、$i \\in \\mathcal{B}$であり、表記からもわかるように、$\\mathcal{N}$と$\\mathcal{B}$は固定された集まりではなく、解が進むにつれて変わる。\nタブロー 2 feasible basis $B$によって決定されるシンプレックスタブロー$\\mathcal{T}(B)$は、与えられたlinear programming problem $A \\mathbf{x} = \\mathbf{b}, z = \\mathbf{c}^{T} \\mathbf{x}$と同じ解の集合を持つ、$m+1$次の連立線形方程式を意味する。$N := \\left\\{ 1 , \\cdots , n + m \\right\\} \\setminus B$に対して、$\\mathbf{x}_{B}$を基底変数のベクトル、$\\mathbf{x}_{N}$を非基底変数のベクトルとするとき、何らかのベクトル$\\mathbf{p} \\in \\mathbb{R}^{m}$、$\\mathbf{r} \\in \\mathbb{R}^{n-m}$と行列$Q \\in \\mathbb{R}^{m \\times (n-m)}$に対して、次のように表される。 $$ \\begin{align*} \\mathbf{x}_{B} \u0026amp;=\u0026amp; \\mathbf{p} \u0026amp;+\u0026amp; Q \\mathbf{x}_{N} \\\\ z \u0026amp;=\u0026amp; z_{0} \u0026amp;+\u0026amp; \\mathbf{r}^{T} \\mathbf{x}_{N} \\end{align*} $$\n説明 正直言って、ディクショナリとタブローは同じものであり、引用からもわかるように、著者によって表現の好みの違いに過ぎない。当然ながら、概念的には$B = \\mathcal{B}$と$N = \\mathcal{N}$は同じであり、使っている変数をそのまま使ったのか、行列の表記を使ったのかの違いにすぎない。\n投稿のために、多くのlinear programmingの教科書を調べてみたが、ディクショナリであれタブローであれ、これらについて詳しく説明したいと思っている著者はいなかった。\u0026lsquo;連立方程式\u0026rsquo;からしてそうだが、高度な抽象化に慣れている数学者は、\u0026lsquo;次のような形\u0026rsquo;や\u0026rsquo;アルゴリズムに従って変わる\u0026rsquo;といったあいまいな表現に対して極度の不安感、嫌悪感を抱く。そうした気持ちは理解できるが、著者たちもまた、ノーテーションに囚われるよりはlinear programmingそのものについての議論に早く進みたがっているように見えたし、真にlinear programmingに興味があるならば、読者も適当に納得して進むことをお勧めする。\nVanderbei. (2020). Linear Programming(5th Edition): p14.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMatousek. (2007). Understanding and Using Linear Programming: p65.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2334,"permalink":"https://freshrimpsushi.github.io/jp/posts/2334/","tags":null,"title":"線形計画法における辞書と表"},{"categories":"양자정보이론","contents":"\r양자정보이론\r[ 펼치기 · 접기 ]\r양자계산\r논리게이트\r비트\r· 부울함수(AND\r· OR\r· NOT\r· XOR\r· NAND\r· NOR\r· CNOT\r· CCNOT\r· CSWAP)\r· 범용 게이트\r· 복제 함수\r· 사영\r· 주입\r양자게이트\r큐비트\r· 양자 얽힘\r· 양자 회로\r· 양자 게이트(파울리 게이트\r· 위상 게이트\r· 아다마르 게이트\r· 양자 CNOT\r· 교환 게이트\r· 양자 CSWAP\r· 양자 CSWAP)\r· 솔베이-키타예프 정리\r· 복제 불가 정리\r정보이론\r고전정보이론\r정보량\r· 엔트로피(결합 엔트로피\r· 조건부 엔트로피\r· 상대적 엔트로피\r· 상호 정보\r)\r· 부호화\r· 복호화\r양자정보이론\rTBD\r· TBD\r· TBD\r관련 분야\r선형대수\r· 양자역학\r定義1 2 離散確率変数 $X$が $n$個の値 $x_{1}, x_{2}, \\dots, x_{n}$を取るとする。 $X$の確率質量関数を $p$とする。すると、$X$あるいは$p$のエントロピーShannon entropy$H$を次のように定義する。\n$$ \\begin{equation} H(X) = H(p) := E\\left[ I(x_{i}) \\right] = \\sum_{i=1}^{n} p(x_{i}) I(x_{i}) = -\\sum_{i=1}^{n} p(x_{i}) \\log_{2}p(x_{i}) \\end{equation} $$\nこの時、$I$は情報量、$E$は期待値である。\n$X$が連続確率変数の場合、\n$$ H(X) = H(p) = - \\int_{-\\infty}^{\\infty} p(x)\\log_{2}p(x) dx $$\n説明 簡単に言えば、エントロピーは情報の期待値(平均)です。エントロピーを通じて、符号化の効率や通信の限界について数学的に扱うことができます。\nエントロピーは一般に無秩序度と説明されますが、ここで言う秩序とは規則、傾向、パターンなどの意味で考えれば良いです。従って、エントロピーが高いとは無秩序度が高いことを意味し、確率変数$X$に対して規則やパターンを把握することが難しいという話です。\nここで、確率が操作されたコイン投げを考えてみましょう。表が出る確率を$p$とすれば、裏が出る確率は$1-p$で、エントロピーは次のようになります。\n$$ H = -p\\log_{2}p - (1-p)\\log_{2}(1-p) $$\n$p$に対する$H$をグラフにすると、次のようになります。\n表が出る確率が$\\dfrac{1}{2}$の時、エントロピーは$H = -\\dfrac{1}{2}\\log_{2}\\dfrac{1}{2}-\\dfrac{1}{2}\\log_{2}\\dfrac{1}{2} = 1$で最大値です。つまり、コイン投げのパターンや規則をよく知ることができないという意味です。実際にコイン投げの場合、私たちはコインのどの面が出るかを確信することはできません。ここで表が出る確率が少し変わると、エントロピーが下がります。例えば、表が出る確率が$\\dfrac{95}{100}$であれば、エントロピーは約$0.28$で無秩序度が低く、つまり何らかの規則やパターン(この例ではほぼ表が出るというパターン)があるという意味です。この内容を次のようにまとめることができます。\nエントロピーが高い = 無秩序度が高い = 規則性やパターンがない = 結果を予測するのが難しい エントロピーが低い = 無秩序度が低い = 規則性やパターンがある = 結果を予測するのが容易\r上の例から予想できるように、一般的に$n$個の場合があるとすると、エントロピーが最も高くなるのは全ての確率が$\\dfrac{1}{n}$で等しい時です。\n性質 確率変数$X$が$n$個の値 $x_{1}, x_{2}, \\dots, x_{n}$を取るとする。エントロピー$H$は次のような性質を持ちます。\n$H$は凹concave関数です。 ある$x_{i}$に対して$p(x_{i}) = 1$ならば、$H(X) = 0$です。 全ての確率が$p(x_{i}) = \\dfrac{1}{n}$で同じ時、エントロピーは最大で、その値は$\\log_{2}n$です。 平均が$\\mathbf{0}$で共分散行列が$K$のランダムベクトル$X \\in \\mathbb{R}^{n}$のエントロピーについて次が成立します。 $$ \\begin{equation} H(X) \\le \\dfrac{1}{2}\\ln \\left[ (2 \\pi e)^{p} \\left| K \\right| \\right] \\end{equation} $$ $\\left| K \\right|$は共分散行列の行列式です。$X$が正規分布なら等号が成立します。 平均$\\mu$と分散$\\sigma^{2}$が与えられた時、エントロピーが最大の分布は正規分布です。 確率変数$X$と推定量$\\hat{X}$に対して次が成立します。 $$ E\\left[ (X - \\hat{X})^{2} \\right] \\ge \\dfrac{1}{2\\pi e} e^{2H(X)} $$ 証明 4 便宜上$\\mathbf{x} = X$と表記しましょう。$g$を$\\displaystyle \\int g(\\mathbf{x})x_{i}x_{j} d \\mathbf{x} = K_{ij}$を満たす任意の確率密度関数とします。$\\phi$を\n正規分布$N(\\mathbf{0}, K)$の確率密度関数とします。 $$ \\phi (\\mathbf{x}) = \\dfrac{1}{\\sqrt{(2\\pi)^{p} \\left| K \\right|}} \\exp \\left( -\\dfrac{1}{2}\\mathbf{x}^{T} K^{-1} \\mathbf{x} \\right) $$ まず式$\\displaystyle \\int g(\\mathbf{x}) \\ln \\phi (\\mathbf{x}) d \\mathbf{x} = \\int \\phi (\\mathbf{x}) \\ln \\phi (\\mathbf{x}) d \\mathbf{x}$が成立することを示します。$\\ln \\phi (\\mathbf{x})$を先に計算すると、\n$$ \\begin{align*} \\ln \\phi (\\mathbf{x}) \u0026amp;= \\ln\\dfrac{1}{\\sqrt{(2\\pi)^{p} \\left| K \\right|}} - \\dfrac{1}{2}\\mathbf{x}^{T} K^{-1} \\mathbf{x} \\\\ \u0026amp;= C + \\sum a_{ij} x_{i} x_{j} \\end{align*} $$\n第一項はある定数$C$として表せ、第二項も$K^{-1}$に依存するある定数$a_{ji}$の二次形式として表せます。従って、\n$$ \\begin{align*} \\int g(\\mathbf{x}) \\ln \\phi (\\mathbf{x}) d \\mathbf{x} \u0026amp;= C \\int g(\\mathbf{x}) d \\mathbf{x} + \\int g(\\mathbf{x})\\sum a_{ij}x_{i}x_{j} d \\mathbf{x} \\\\ \u0026amp;= C + \\sum a_{ij} \\int g(\\mathbf{x}) x_{i}x_{j} d \\mathbf{x} \\\\ \u0026amp;= C + \\sum a_{ij}K_{ij} \\qquad \\text{by assumption for $g$} \\end{align*} $$\nまた、\n$$ \\begin{align*} \\int \\phi (\\mathbf{x}) \\ln \\phi (\\mathbf{x}) d \\mathbf{x} \u0026amp;= C \\int \\phi (\\mathbf{x}) d \\mathbf{x} + \\int \\phi (\\mathbf{x})\\sum a_{ij}x_{i}x_{j} d \\mathbf{x} \\\\ \u0026amp;= C + \\sum a_{ij} \\int \\phi (\\mathbf{x}) x_{i}x_{j} d \\mathbf{x} \\\\ \u0026amp;= C + \\sum a_{ij}K_{ij} \\qquad \\text{by definition of covariance} \\end{align*} $$\n相対エントロピーは常に$0$以上であるため、\n$$ \\begin{align*} 0 \u0026amp;\\le D(g \\| \\phi) \\\\ \u0026amp;= \\int g \\ln \\dfrac{g}{\\phi} \\\\ \u0026amp;= \\int g \\ln g - \\int g \\ln \\phi \\\\ \u0026amp;= - H(g) - \\int \\phi \\ln \\phi \\\\ \u0026amp;= - H(g) + H(\\phi) \\end{align*} $$\n正規分布のエントロピーは$\\dfrac{1}{2}\\ln \\left[ (2 \\pi e)^{n} \\left| K \\right| \\right]$なので、\n$$ H(X) = H(g) \\le H(\\phi) = \\dfrac{1}{2}\\ln \\left[ (2 \\pi e)^{n} \\left| K \\right| \\right] $$\nここで$X$を1次元確率変数としましょう。\n$$ \\begin{align*} E\\left[ (X - \\hat{X})^{2} \\right] \u0026amp;\\ge \\min_{X} E\\left[ (X - \\hat{X})^{2} \\right] \\\\ \u0026amp;= E\\left[ (X - E(X))^{2} \\right] \\\\ \u0026amp;= \\Var(X) \\end{align*} $$\n$(2)$が1次元の時、次の式を得ます。\n$$ \\begin{align*} \u0026amp;\u0026amp; H(X) \u0026amp;\\le \\dfrac{1}{2} \\ln(2\\pi e \\sigma^{2}) \\\\ \\implies \u0026amp;\u0026amp; 2H(X) \u0026amp;\\le \\ln(2\\pi e \\sigma^{2}) \\\\ \\implies \u0026amp;\u0026amp; e^{2H(X)} \u0026amp;\\le 2\\pi e \\sigma^{2} \\\\ \\implies \u0026amp;\u0026amp; \\dfrac{1}{2\\pi e}e^{2H(X)} \u0026amp;\\le \\sigma^{2} = \\Var(X) \\\\ \\end{align*} $$\nこの式に代入すると、\n$$ E\\left[ (X - \\hat{X})^{2} \\right] \\ge \\dfrac{1}{2\\pi e} e^{2H(X)} $$\n正規分布のエントロピー 正規分布$N(\\mu, \\sigma^{2})$のエントロピーは(自然対数を用いた場合)次のようになります。\n$$ H = \\dfrac{1}{2} \\ln (2\\pi e \\sigma^{2}) = \\ln \\sqrt{2\\pi e \\sigma^{2}} $$\n多変量正規分布$N_{n}(\\boldsymbol{\\mu}, K)$のエントロピーは次のようになります。\n$$ H = \\dfrac{1}{2}\\ln \\left[ (2 \\pi e)^{n} \\left| K \\right| \\right] = \\dfrac{1}{2}\\ln (\\det (2\\pi e K)) $$\n関連項目 確率情報理論で定義されるシャノンエントロピー 熱力学で定義されるエントロピー ギブスのエントロピー表現 キム・ヨンフン・ホ・ジェソン, 量子情報理論 (2020), p246\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStephen M. Barnett, Quantum Information (2009), p7-10\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3400,"permalink":"https://freshrimpsushi.github.io/jp/posts/3400/","tags":null,"title":"古典情報理論におけるシャノン・エントロピー"},{"categories":"줄리아","contents":"概要 特定の値に変更する方法は、列ごとに変更するので不便で、データフレーム全体でNaNを扱うときはもっといいトリックを使ってみる価値がある。\nコード julia\u0026gt; df = DataFrame(rand(1:9,3,3), :auto) ./ DataFrame(rand(0:1,3,3), :auto)\r3×3 DataFrame\rRow │ x1 x2 x3 │ Float64 Float64 Float64\r─────┼───────────────────────────\r1 │ 5.0 Inf 7.0\r2 │ Inf 8.0 Inf\r3 │ 4.0 1.0 4.0 例えば、上のデータフレームでInfを0に置換したい場合、次のようにたった一行で変更できる。\njulia\u0026gt; ifelse.(isinf.(df), 0, df)\r3×3 DataFrame\rRow │ x1 x2 x3 │ Float64 Float64 Float64\r─────┼───────────────────────────\r1 │ 5.0 0.0 7.0\r2 │ 0.0 8.0 0.0\r3 │ 4.0 1.0 4.0 もちろん、ifelse.(isinf.(df), 0, df)のisinfをisnanに変更すればNaNを扱い、0を任意の値に変更することができる。\n全コード using DataFrames, Random\rRandom.seed!(0)\rdf = DataFrame(rand(1:9,3,3), :auto) ./ DataFrame(rand(0:1,3,3), :auto)\rifelse.(isinf.(df), 0, df) 参考 データフレームの特定の値を変更する方法 環境 OS: Windows julia: v1.6.3 ","id":2330,"permalink":"https://freshrimpsushi.github.io/jp/posts/2330/","tags":null,"title":"JuliaのデータフレームでNaNを0に置き換える方法"},{"categories":"수리통계학","contents":"定義 1 パラメータ $\\theta$ の区間推定値 $\\left[ L \\left( \\mathbf{X} \\right), U \\left( \\mathbf{X} \\right) \\right]$ において、以下をカバレッジ確率Coverage Probabilityと呼ぶ。 $$ P_{\\theta} \\left( \\theta \\in \\left[ L \\left( \\mathbf{X} \\right), U \\left( \\mathbf{X} \\right) \\right] \\right) = P \\left( \\theta \\in \\left[ L \\left( \\mathbf{X} \\right), U \\left( \\mathbf{X} \\right) \\right] | \\theta \\right) $$ カバレッジ確率の下限を信頼係数Confidence Coefficientと呼ぶ。 $$ \\inf_{\\theta} P_{\\theta} \\left( \\theta \\in \\left[ L \\left( \\mathbf{X} \\right), U \\left( \\mathbf{X} \\right) \\right] \\right) $$ 説明 信頼区間 信頼係数は信頼水準Confidence Levelと同じ言葉で、区間推定値と信頼水準が共に現れた時、その区間を信頼区間Confidence Intervalと呼ぶ。\nまったく無駄な話をすべて省いて、数学的な表現のみで言えば、区間推定値の定義から、区間推定値は統計量によって作られたランダムインターバルであるとした。このシンプルな統計学的定義を見れば、信頼区間が何かをようやく感じることができるだろう。文化統計学の説明とベイジアンの信頼区間の違いを説明する時、もし$N$個の信頼区間を作ればなんとかこんな感じでパラメータの分布がなくてなんとかこんな感じで説明されたものが以下の表現に要約される。 $$ P \\left( \\theta \\in \\left[ L \\left( \\mathbf{X} \\right), U \\left( \\mathbf{X} \\right) \\right] | \\theta \\right) $$ 動くもの、変わるもの、ランダムなものはいつも信頼区間それ自体だった、$\\theta$ではなかった。式で見るように、$\\theta$は定数で、与えられて静止しているもので、それの上下が動くのだ。我々は$\\theta$の分布を知らないし、そもそも定数だから知る必要もない。\nどうしてこんがらがったのか このように考えられなかったのは、ほとんどの人が信頼区間を具体的に数字で書かれたものを見てきたからだ。例えば、平均が$3.14$で、信頼水準$95 \\%$で信頼区間が$[3.00, 3.28]$だとしよう。\n初めてこれを見て信頼区間が動くと思う人はいないだろう。普通の人の直感は「ああ、つまり$3.14$という値が$[3.00, 3.28]$の中にある確率が$95\\%$なんだね？でも、$5\\%$の確率で外に出ることもあるのか？だからこれを信じても完全には信じず、95％ぐらいで信じよう」と考えるのが自然だ。 $3.14$が$3.30$になって外れることを想像するよりも、信頼区間が$[2.00, 2.28]$として抽出されて$3.14$をカバーできないように外れるとは思えない。$3.00 = L \\left( \\mathbf{x} \\right)$で、$3.28 = U \\left( \\mathbf{x} \\right)$だ。 信頼集合への一般化 カバレッジ確率を通じて信頼区間を定義するにあたり、区間推定値の性質は全く必要なかった。例えば、トポロジー的な連結性などの仮定は必要なかったが、これにより区間を超えて集合それ自体に一般化したものを信頼集合Confidence Setと呼ぶ。信頼集合は当然ながらパラメータ空間$\\Theta$の部分集合であり、サンプル $\\mathbf{X}$に依存したランダム集合Random Set $C \\left( \\mathbf{X} \\right)$として表される。\n参照 信頼区間の簡単な定義 Casella. (2001). Statistical Inference(2nd Edition): p418.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2329,"permalink":"https://freshrimpsushi.github.io/jp/posts/2329/","tags":null,"title":"数理統計的な信頼集合の定義"},{"categories":"선형대수","contents":"定義1 $V$を$F$-ベクトル空間、$W \\le V$を部分空間としよう。$v \\in V$について、以下の集合\n$$ \\left\\{ v \\right\\} + W := \\left\\{ v + w : w \\in W \\right\\} $$\nを$v$を含む$W$の剰余類$W$ containing $v$の剰余類という。$+$は集合の和だ。\n説明 普通、$\\left\\{ v \\right\\} + W$を簡単に$v + W$と表記する。\n$W$の全ての剰余類の集合を$\\left\\{ v + W : v \\in V \\right\\}$と考える。加法と($F$による)スカラー倍を以下のように定義する。\n$$ (v_{1} + W) + (v_{2} + W) = (v_{1} + v_{2}) + W,\\quad \\forall v_{1}, v_{2} \\in V $$\n$$ a(v + W) = av + W\\quad \\forall v \\in V \\text{ and } a \\in F $$\nするとこの集合は再び$F$-ベクトル空間となる。このベクトル空間を$V/W$と表記し、$W$によって割った$V$の商空間$V$ modulo $W$の商空間という。\n定理 (a) $v + W$が$V$の部分空間であることは$v \\in W$と同値である。(代数での証明)\n(b) $v_{1}, v_{2} \\in V$に対して、$v_{1} + W = v_{2} + W$であることは$v_{1} - v_{2} \\in W$であることと同値である。(代数での証明)\n(c) $V/W$はベクトル空間であり、ゼロベクトルは$0_{V} + W = W$である。($0_{V}$は$V$のゼロベクトルだ。)\n証明 (a) $(\\Longrightarrow)$を仮定する\n$v + W$が$V$の部分空間だと仮定する。それならば、$0_{V}$を$V$のゼロベクトルとするとき、$0_{V} \\in v + W$が成り立つ。したがって、ある$w \\in W$に対して$0_{V} = v + w$及び$w = -v \\in W$が成り立ち、$W$は$V$の部分空間であるためスカラー倍に対して閉じており、$v = -(-v) \\in W$が成り立つ。\n$(\\Longleftarrow)$を仮定する\n$v \\in W$と仮定する。$v + W$が$V$の部分空間であることを示すには加法とスカラー倍に対して閉じていることを示せばよい。 $v + w_{1}, v + w_{2} \\in v + W$としよう。これを足すと以下のようになる。\n$$ (v + w_{1}) + (v_{1} + w_{2}) = v + (v + w_{1} + w_{2}) $$\n$W$は部分空間であるため加法に対して閉じており、仮定により$v$は$W$の元であるため、ある$w_{3} \\in W$に対して以下が成り立つ。\n$$ v + (v + w_{1} + w_{2}) = v + w_{3} \\in W $$\n今、$a \\in F$としよう。すると同様に、ある$w_{4} \\in W$に対して以下が成り立つ。\n$$ a(v + w) = v + \\left( (a-1)v + aw \\right) = v + w_{4} \\in W $$\n■\n(b) $(\\Longrightarrow)$を仮定する\n$v_{1} + W = v_{2} + W$と仮定する。それならば$V$のゼロベクトル$0_{V}$とある$w \\in W$に対して以下が成り立つ。\n$$ v_{1} + 0_{V} = v_{2} + w \\implies v_{1} - v_{2} = w \\in W $$\n$(\\Longleftarrow)$を仮定する\n$v_{1} - v_{2} \\in W$と仮定する。それならば\n$$ \\begin{align*} v_{2} + W \u0026amp;= \\left\\{ v_{2} + w : w \\in W \\right\\} \\\\ \u0026amp;= \\left\\{ v_{2} + \\left( (v_{1} - v_{2}) + w \\right) : w \\in W \\right\\} \\\\ \u0026amp;= \\left\\{ v_{1} + w : w \\in W \\right\\} \\\\ \u0026amp;= v_{1} + W \\end{align*} $$\n■\n参照 抽象代数学での剰余類 剰余類の性質 Stephen H. Friedberg, 線形代数(第4版, 2002), p23\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3359,"permalink":"https://freshrimpsushi.github.io/jp/posts/3359/","tags":null,"title":"線形代数学における剰余類と商空間"},{"categories":"줄리아","contents":"概要 ジュリアでのA ? B : Cは、いわゆる三項演算子Ternary Operatorで、Aが真ならB、偽ならCを返す関数だ。数学的に二項演算が関数として定義されるように、三項演算もまた関数だ。条件文と似ているけれども、このような本質的な違いがあるため、慣れれば非常に便利に使える。ただし、読みやすいコードからはちょっと離れることがあるので、無理に乱用する必要はないし、逆に気に入らなくても他の人が使うかもしれないから、ある程度は慣れておく必要がある。\nコード julia\u0026gt; x = iseven(2) ? \u0026#34;even\u0026#34; : \u0026#34;odd\u0026#34;; x\r\u0026#34;even\u0026#34;\rjulia\u0026gt; y = iseven(3) ? \u0026#34;even\u0026#34; : \u0026#34;odd\u0026#34;; y\r\u0026#34;odd\u0026#34; 上の命令は、与えられた数が偶数か奇数かに応じて、変数x、yに\u0026quot;even\u0026quot;または\u0026quot;odd\u0026quot;という文字列を代入している。\njulia\u0026gt; x * y\r\u0026#34;evenodd\u0026#34; 条件文じゃなく関数だから、こんなに便利なコードが書ける。同じ機能を条件文だけでやろうとすると、スコープScopeなどの問題で無駄に長くなりがちだ。\n全体のコード x = iseven(2) ? \u0026#34;even\u0026#34; : \u0026#34;odd\u0026#34;; x\ry = iseven(3) ? \u0026#34;even\u0026#34; : \u0026#34;odd\u0026#34;; y\rx * y 環境 OS: Windows julia: v1.6.3 ","id":2328,"permalink":"https://freshrimpsushi.github.io/jp/posts/2328/","tags":null,"title":"ジュリアの三項演算子 ? :"},{"categories":"줄리아","contents":"概要 replace!() メソッドを使えばいい1。最初の引数には変更するデータフレームのカラムが入り、二番目の引数にはペア [ペア](../2201) A =\u0026gt; B` が入る。ここで、データフレームのカラムが入ることが重要だ。\nコード julia\u0026gt; WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다영 99 161 쪼꼬미\r2 │ 다원 97 167 메보즈\r3 │ 루다 97 157 쪼꼬미\r4 │ 소정 95 166 보스즈\r5 │ 수빈 96 159 쪼꼬미\r6 │ 연정 99 165 메보즈\r7 │ 주연 98 172 보스즈\r8 │ 지연 95 163 보스즈\r9 │ 진숙 99 162 쪼꼬미\r10 │ 현정 94 165 보스즈 例として使用する WJSN データフレームは上記の通りだ。\njulia\u0026gt; replace!(WJSN.member, \u0026#34;진숙\u0026#34; =\u0026gt; \u0026#34;여름\u0026#34;); WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다영 99 161 쪼꼬미\r2 │ 다원 97 167 메보즈\r3 │ 루다 97 157 쪼꼬미\r4 │ 소정 95 166 보스즈\r5 │ 수빈 96 159 쪼꼬미\r6 │ 연정 99 165 메보즈\r7 │ 주연 98 172 보스즈\r8 │ 지연 95 163 보스즈\r9 │ 여름 99 162 쪼꼬미\r10 │ 현정 94 165 보스즈 :member 列の \u0026quot;진숙\u0026quot; を \u0026quot;여름\u0026quot; に変えた。ここで replace() ではなく replace!() を使用した点、データフレームそのものではなくその特定の列を指定した点に注意しよう。\njulia\u0026gt; replace!(WJSN.unit, \u0026#34;보스즈\u0026#34; =\u0026gt; \u0026#34;더블랙\u0026#34;); WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다영 99 161 쪼꼬미\r2 │ 다원 97 167 메보즈\r3 │ 루다 97 157 쪼꼬미\r4 │ 소정 95 166 더블랙\r5 │ 수빈 96 159 쪼꼬미\r6 │ 연정 99 165 메보즈\r7 │ 주연 98 172 더블랙\r8 │ 지연 95 163 더블랙\r9 │ 여름 99 162 쪼꼬미\r10 │ 현정 94 165 더블랙 :unit 列の \u0026quot;보스즈\u0026quot; を \u0026quot;더블랙\u0026quot; に一括変更した。\n全体のコード using DataFrames\rWJSN = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;다원\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;소정\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;연정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;진숙\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [99,97,97,95,96,99,98,95,99,94],\rheight = [161,167,157,166,159,165,172,163,162,165],\runit = [\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;보스즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;보스즈\u0026#34;,\u0026#34;보스즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;보스즈\u0026#34;]\r)\rWJSN\rreplace!(WJSN.member, \u0026#34;진숙\u0026#34; =\u0026gt; \u0026#34;여름\u0026#34;); WJSN\rreplace!(WJSN.unit, \u0026#34;보스즈\u0026#34; =\u0026gt; \u0026#34;더블랙\u0026#34;); WJSN 併せて見る データフレーム全体を一度にNaNを0に変える方法 環境 OS: Windows julia: v1.6.3 https://dataframes.juliadata.org/stable/man/working_with_dataframes/#Replacing-Data\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2326,"permalink":"https://freshrimpsushi.github.io/jp/posts/2326/","tags":null,"title":"ジュリアでのデータフレーム特定値の変更方法"},{"categories":"줄리아","contents":"概要 1 FreqTables.jlパッケージのfreqtable()関数を使えばいい。Rのfreq()関数と似た機能を持っている。\nコード 配列 julia\u0026gt; compartment = rand([\u0026#39;S\u0026#39;,\u0026#39;I\u0026#39;,\u0026#39;R\u0026#39;], 1000);\rjulia\u0026gt; freqtable(compartment)\r3-element Named Vector{Int64}\rDim1 │\r──────┼────\r\u0026#39;I\u0026#39; │ 316\r\u0026#39;R\u0026#39; │ 342\r\u0026#39;S\u0026#39; │ 342 上記のように配列を入れると、各階級ごとにカウントしてくれる。\nデータフレーム freqtable()は特にデータフレームに便利だ。Rでの質的変数を含む回帰分析の例と同様に、組み込みデータToothGrowthを読み込んでみよう。\njulia\u0026gt; ToothGrowth = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;ToothGrowth\u0026#34;)\r60×3 DataFrame\rRow │ Len Supp Dose │ Float64 Cat… Float64\r─────┼────────────────────────\r1 │ 4.2 VC 0.5\r2 │ 11.5 VC 0.5\r3 │ 7.3 VC 0.5\r4 │ 5.8 VC 0.5\r⋮ │ ⋮ ⋮ ⋮\r58 │ 27.3 OJ 2.0\r59 │ 29.4 OJ 2.0\r60 │ 23.0 OJ 2.0\r53 rows omitted\rjulia\u0026gt; freqtable(ToothGrowth, :Len)\r43-element Named Vector{Int64}\rLen │\r─────┼──\r4.2 │ 1\r5.2 │ 1\r5.8 │ 1\r6.4 │ 1\r⋮ ⋮\r29.5 │ 1\r30.9 │ 1\r32.5 │ 1\r33.9 │ 1\rjulia\u0026gt; freqtable(ToothGrowth, :Supp)\r2-element Named Vector{Int64}\rSupp │\r──────┼───\r\u0026#34;OJ\u0026#34; │ 30\r\u0026#34;VC\u0026#34; │ 30\rjulia\u0026gt; freqtable(ToothGrowth, :Dose)\r3-element Named Vector{Int64}\rDose │\r──────┼───\r0.5 │ 20\r1.0 │ 20\r2.0 │ 20 ToothGrowthは、ビタミンCまたはオレンジジュース:Suppを異なる量:Doseで餌として与えられたモルモットの歯の長さ:Lenを記録したデータだ。各カラムごとに頻度を計算すると、上記のようにきれいに整理される。ここで、データが必ずしもカテゴリカルデータである必要はないことが確認できる。\njulia\u0026gt; freqtable(ToothGrowth, :Supp, :Dose)\r2×3 Named Matrix{Int64}\rSupp ╲ Dose │ 0.5 1.0 2.0\r────────────┼──────────────\r\u0026#34;OJ\u0026#34; │ 10 10 10\r\u0026#34;VC\u0026#34; │ 10 10 10 もちろん、このようなテーブルはカテゴリカルデータを扱う時に最も効果的だ。:Supp, :Doseに対する頻度を計算すると、自動的に2次元にカテゴリを分けて頻度を計算してくれた。\njulia\u0026gt; freqtable(ToothGrowth, :Len, :Dose, :Supp)\r43×3×2 Named Array{Int64, 3}\r[:, :, Supp=\u0026#34;OJ\u0026#34;] =\rLen ╲ Dose │ 0.5 1.0 2.0\r───────────┼──────────────\r4.2 │ 0 0 0\r⋮ ⋮ ⋮ ⋮\r33.9 │ 0 0 0\r[:, :, Supp=\u0026#34;VC\u0026#34;] =\rLen ╲ Dose │ 0.5 1.0 2.0\r───────────┼──────────────\r4.2 │ 1 0 0\r⋮ ⋮ ⋮ ⋮\r33.9 │ 0 0 1 3つ以上のカラムに対する計算は、ただの2次元テーブルを階級の数だけリターンする。この辺りになると、データを探索したり要約する意味はほとんどなくなる。\n性能比較 julia\u0026gt; @time for t in 1:10^4\rfreqtable(compartment)\rend\r@time for t in 1:10^4\rcount(compartment .== \u0026#39;S\u0026#39;)\rcount(compartment .== \u0026#39;I\u0026#39;)\rcount(compartment .== \u0026#39;R\u0026#39;)\rend\r0.068229 seconds (340.00 k allocations: 27.466 MiB)\r0.059198 seconds (180.00 k allocations: 134.125 MiB, 36.71% gc time) テーブルを離れて、頻度数の計算自体が有用だと思われる。直接カウントすることとfreqtable()を通じて一度に頻度数を計算する速度を色々と計測してみたけど、どちらかが必ずしも速かったわけではなかった。データの量や階級の数によって、その都度前後したが、全体的にfreqtable()が遅い傾向にあった。それでも大きく遅れを取るわけではないので、速度を考慮に入れずに、使うときはよく考えて使おう。\n全コード using FreqTables\rcompartment = rand([\u0026#39;S\u0026#39;,\u0026#39;I\u0026#39;,\u0026#39;R\u0026#39;], 1000);\rfreqtable(compartment)\rusing RDatasets\rToothGrowth = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;ToothGrowth\u0026#34;)\rfreqtable(ToothGrowth, :Len)\rfreqtable(ToothGrowth, :Supp)\rfreqtable(ToothGrowth, :Dose)\rfreqtable(ToothGrowth, :Supp, :Dose)\rfreqtable(ToothGrowth, :Len, :Dose, :Supp)\r@time for t in 1:10^4\rfreqtable(compartment)\rend\r@time for t in 1:10^4\rcount(compartment .== \u0026#39;S\u0026#39;)\rcount(compartment .== \u0026#39;I\u0026#39;)\rcount(compartment .== \u0026#39;R\u0026#39;)\rend 環境 OS: Windows julia: v1.6.3 FreqTables v0.4.5 https://github.com/nalimilan/FreqTables.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2324,"permalink":"https://freshrimpsushi.github.io/jp/posts/2324/","tags":null,"title":"ジュリアで周波数を計算する方法"},{"categories":"선형대수","contents":"概要 $\\beta = v_{1}, \\dots, v_{k}$を線形変換 $T : V \\to V$の固有ベクトルの集合としよう。ならば、$T$が$\\span{\\beta}$を$\\span{\\beta}$にマッピングすることがわかる。このように自分自身を自分自身にマッピングする部分空間を不変部分空間と定義する。\n定義1 $V$をベクトル空間、$T : V \\to V$を線形変換としよう。部分空間 $W$が次の条件を満たすならば、$W$を**$T$-不変部分空間**$T$-invariant subspaceという。\n$$ T(W) \\subset W $$\nつまり、\n$$ T(v) \\in W\\quad \\forall v \\in W $$\nの$W$を$T$-不変部分空間という。\n説明 線形変換$T : V \\to V$において、次のものが$T$-不変の部分空間である。\n$\\left\\{ 0 \\right\\}$ $V$ 値域 $R(T)$ 零空間 $N(T)$ 固有空間 $E_{\\lambda}$ 1と2は自明である。全ての部分集合$A \\subset V$に対して、$T(A) \\subset R(T)$なので$R(T)$は$T$-不変である。$0 \\in N(T)$なので、$T(N(T)) \\subset N(T)$である。$T(\\lambda x) = \\lambda (\\lambda x)$なので、$T(E_{\\lambda}) \\subset E_{\\lambda}$である。\n$W$が$T : V \\to V$の不変部分空間ならば、制限写像 $T|_{W} : W \\to W$を自然に定義できる。この場合、$T|_{W}$は$T$の性質を受け継ぎ、次の定理は$T$と$T|_{W}$の間の一つの関係性を示している。簡単に言えば、$T|_{W}$の特性多項式は$T$の特性多項式の因数である。この結論自体は別の定理の系としても得られる。\n定理 $V$を$n$次元のベクトル空間、$T : V \\to V$を線形変換、$W$を$T$-不変としよう。すると、$T|_{W}$の特性多項式は$T$の特性多項式を割る。\n証明 $W$の順序基底 $\\gamma = \\left\\{ v_{1} ,\\dots, v_{k} \\right\\}$を一つ選ぶ。そして、これを$V$の順序基底 $\\beta = \\left\\{ v_{1}, \\dots, v_{k}, v_{k+1}, \\dots, v_{n} \\right\\}$に拡張しよう。$A = \\begin{bmatrix} T \\end{bmatrix}_{\\beta}$、$B_{1} = \\begin{bmatrix} T|_{W} \\end{bmatrix}_{\\gamma}$とする。すると、行列 $A$を次のようなブロック行列で表せる。\n$$ A = \\begin{bmatrix} B_{1} \u0026amp; B_{2} \\\\ O \u0026amp; B_{3} \\end{bmatrix} $$\n$f(t)$を$T$の特性多項式、$g(t)$を$T|_{W}$の特性多項式とする。すると、ブロック行列の行列式の公式によって次が得られる。（$I$は行列計算が可能な適切な次元の単位行列である。）\n$$ f(t) = \\det(A-tI) = \\det \\begin{bmatrix} B_{1}-tI \u0026amp; B_{2} \\\\ O \u0026amp; B_{3}-tI \\end{bmatrix} = g(t) \\det(B_{3}-tI) $$\n従って、$g(t)$は$f(t)$を割る。\n■\n参照 循環部分空間 Stephen H. Friedberg, Linear Algebra (4th Edition, 2002), p313-315\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3353,"permalink":"https://freshrimpsushi.github.io/jp/posts/3353/","tags":null,"title":"ベクトル空間の不変部分空間"},{"categories":"줄리아","contents":"ガイド 上のようなexample.csvファイルがあるとしよう。このデータフレームに読み込むとき、データ全体ではなく、列名だけを保持し、中身が空のデータフレームを作りたい場合がある。空のデータフレームが必要な場合があるから、このような場合も間違いなくある。\nusing CSV # 行なしでデータフレームを読み込む df_empty = CSV.read(\u0026#34;example.csv\u0026#34;, DataFrame; limit = 0) 上の実行結果で作成されたデータフレームは3つの列をそのまま持っているが、その内容物は全く持っていなかった。\nCSV.read(limit = 1)\nlimit = 1オプションを通じて、データフレームを1行だけ読んだ。 CSV.read()[[false],:]\n長さ$1$のビット配列Bit Array[false]で、どの行も参照していなかった。これによって空のデータフレームが残った。 # 列名が正常に保持されていることを確認 column_names = names(df_empty) names()関数で列名を確認すると、列名が正常に保持されていることが分かる。\n# 空のデータフレームに新しいデータを挿入 push!(df_empty, [1, \u0026#34;new_data\u0026#34;, 3.14]) push!()で新しいデータを挿入すると、元々あった配列のようにきちんと動作することが確認できる。\n単にlimit = 0をすると？ # limit 0でデータフレームを読み込み、空のデータフレームを得ようとする試み df_empty_error = CSV.read(\u0026#34;example.csv\u0026#34;, DataFrame; limit = 0) StackOverflowErrorエラーが発生する。\n環境 OS: Windows julia: v1.6.3 ","id":2322,"permalink":"https://freshrimpsushi.github.io/jp/posts/2322/","tags":null,"title":"JuliaでCSVファイルから列だけを読み込む方法"},{"categories":"줄리아","contents":"ガイド 1 using RDatasets\riris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\rdescribe(iris) describe() 関数を使えばいい。iris データを要約してみよう。\njulia\u0026gt; describe(iris)\r5×7 DataFrame\rRow │ variable mean min median max nmissing eltype\r│ Symbol Union… Any Union… Any Int64 DataType\r─────┼────────────────────────────────────────────────────────────────────────────────────────────\r1 │ SepalLength 5.84333 4.3 5.8 7.9 0 Float64\r2 │ SepalWidth 3.05733 2.0 3.0 4.4 0 Float64\r3 │ PetalLength 3.758 1.0 4.35 6.9 0 Float64\r4 │ PetalWidth 1.19933 0.1 1.3 2.5 0 Float64\r5 │ Species setosa virginica 0 CategoricalValue{String, UInt8} 環境 OS: Windows julia: v1.6.3 https://dataframes.juliadata.org/stable/man/working_with_dataframes/#Summarizing-Data\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2320,"permalink":"https://freshrimpsushi.github.io/jp/posts/2320/","tags":null,"title":"ジュリアでデータフレームの要約を見る方法"},{"categories":"선형대수","contents":"定義1 $V$と$n$を次元ベクトル空間、$T : V \\to V$を線形変換としよう。$\\lambda$を$T$の固有値とする。以下のように定義された集合$E_{\\lambda}$を固有値$\\lambda$に対応する$T$の固有空間eigenspaceという。\n$$ E_{\\lambda} = V_{\\lambda} := \\left\\{ x \\in V : Tx = \\lambda x \\right\\} = N(T - \\lambda I) $$\nこの場合、$N$は零空間だ。\n同様に、正方行列$A$の固有空間とは、$L_{A}$の固有空間として定義される。\n説明 固有ベクトルになる条件にはゼロベクトルでないという条件が必要だが、$E_{\\lambda}$の定義には$x$が固有ベクトルである必要は特にない。したがって、$E_{\\lambda}$は固有値$\\lambda$に対応する固有ベクトルとゼロベクトルの集合である。$E_{\\lambda}$が部分空間になるためには、ゼロベクトルが存在する必要があることに注意しよう。実際、$T$が線形変換であるため、加法とスカラー乗法について閉じている(部分空間の判定法)ことは自明である。$x, y \\in E_{\\lambda}$とすると、\n$$ T(ax + y) = aT(x) + T(y) = a\\lambda x + \\lambda y = \\lambda (ax + y) $$\nよって、$ax + y \\in E_{\\lambda}$が成り立つ。\n幾何的重複度 固有値$\\lambda$に対応する固有空間$E_{\\lambda}$の次元を$\\lambda$の幾何的重複度geometric multiplicityという。\nつまり、$\\lambda$に対応する線形独立な固有ベクトルの数であり、したがって、少なくとも$1$以上の整数となる。最大値は代数的重複度に関連している。\n定理 $T$の固有値$\\lambda$の代数的重複度を$m$とする。代数的重複度は幾何的重複度以上である。\n$$ 1 \\le \\dim E_{\\lambda} \\le m $$\n証明 $E_{\\lambda}$の順序基底を$\\gamma = \\left\\{ v_{1}, \\dots, v_{p} \\right\\}$とする。拡張された$V$の順序基底を$\\beta = \\left\\{ v_{1}, \\dots, v_{p}, v_{p+1}, \\dots, v_{n} \\right\\}$とする。$T$の行列表現を$A = \\begin{bmatrix} T \\end{bmatrix}_{\\beta}$とする。すると、$A$は以下のブロック行列である。\n$$ A = \\begin{bmatrix} \\begin{bmatrix} T|_{E_{\\lambda}} \\end{bmatrix}_{\\gamma} \u0026amp; B \\\\ O_{n-p} \u0026amp; C \\end{bmatrix} $$\n$O_{n-p}$は$n-p \\times n-p$零行列だ。この場合、$1 \\le i \\le p$について、$Tv_{i} = \\lambda v_{i}$なので、$Tv_{i}$の座標ベクトルは以下の通りである。\n$$ \\begin{bmatrix} Tv_{i} \\end{bmatrix}_{\\gamma} = \\begin{bmatrix} 0 \\\\ \\vdots \\\\ \\lambda \\\\ \\vdots \\\\ 0 \\end{bmatrix}i\\text{-th row} $$\nしたがって、$\\begin{bmatrix} T|_{E_{\\lambda}} \\end{bmatrix}_{\\gamma} = \\begin{bmatrix} \\begin{bmatrix} Tv_{1} \\end{bmatrix}_{\\gamma} \u0026amp; \\cdots \u0026amp; \\begin{bmatrix} Tv_{p} \\end{bmatrix}_{\\gamma} \\end{bmatrix}$なので、\n$$ A = \\begin{bmatrix} \\lambda I_{p} \u0026amp; B \\\\ O \u0026amp; C \\end{bmatrix} $$\n$I_{p}$は$p \\times p$単位行列だ。\nブロック行列の行列式\n$A = \\begin{bmatrix} A_{1} \u0026amp; A_{2} \\\\ O \u0026amp; A_{3} \\end{bmatrix}$をブロック行列とする。すると、以下が成り立つ。\n$$ \\det A = \\det A_{1} \\det A_{3} $$\nしたがって、$T$の特性多項式は以下の通りである。\n$$ \\begin{align*} f(t) = \\det (A - t I_{n}) \u0026amp;= \\det \\begin{bmatrix} \\lambda I_{p} - \\lambda I_{p} \u0026amp; B \\\\ O \u0026amp; C - tI_{n-p} \\end{bmatrix} \\\\ \u0026amp; = \\det \\left( (\\lambda - t)I_{p} \\right) \\det \\left( C - tI_{n-p} \\right) \\\\ \u0026amp; = (\\lambda - t)^{p} \\det \\left( C - tI_{n-p} \\right) \\\\ \u0026amp; = (-1)^{p}(t - \\lambda)^{p} \\det \\left( C - tI_{n-p} \\right) \\\\ \\end{align*} $$\nこれを見ると、$(t - \\lambda)^{p}$が特性多項式$f(t)$の因数であることがわかる。したがって、$f(t)$は少なくとも$\\lambda$を$p$重根として持ち、代数的重複度の定義により、代数的重複度は幾何的重複度以上となる。\n■\nStephen H. Friedberg, Linear Algebra (第4版, 2002), p264\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3349,"permalink":"https://freshrimpsushi.github.io/jp/posts/3349/","tags":null,"title":"線形変換の固有空間と幾何的重複度"},{"categories":"줄리아","contents":"概要 JuliaのCategoricalArrays.jlパッケージは、Rのfactorと似た機能を果たす。\nコード julia\u0026gt; A = [\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;red\u0026#34;, \u0026#34;green\u0026#34;]\r4-element Vector{String}:\r\u0026#34;red\u0026#34;\r\u0026#34;blue\u0026#34;\r\u0026#34;red\u0026#34;\r\u0026#34;green\u0026#34;\rjulia\u0026gt; B = categorical(A)\r4-element CategoricalArray{String,1,UInt32}:\r\u0026#34;red\u0026#34;\r\u0026#34;blue\u0026#34;\r\u0026#34;red\u0026#34;\r\u0026#34;green\u0026#34;\rjulia\u0026gt; levels(B)\r3-element Vector{String}:\r\u0026#34;blue\u0026#34;\r\u0026#34;green\u0026#34;\r\u0026#34;red\u0026#34; categorical() categorical()関数で通常の配列をカテゴリカル配列にキャストCastできる。\nlevels() levels()関数では、カテゴリーを確認できる。当然、カテゴリーに重複はなく、配列にそのカテゴリーに対応する要素がなくてもカテゴリー自体は維持される。\njulia\u0026gt; B[2] = \u0026#34;red\u0026#34;; B\r4-element CategoricalArray{String,1,UInt32}:\r\u0026#34;red\u0026#34;\r\u0026#34;red\u0026#34;\r\u0026#34;red\u0026#34;\r\u0026#34;green\u0026#34;\rjulia\u0026gt; levels(B)\r3-element Vector{String}:\r\u0026#34;blue\u0026#34;\r\u0026#34;green\u0026#34;\r\u0026#34;red\u0026#34; このように配列の状態に関係なくカテゴリーが維持される特徴は、特定のコーディングで非常に便利な特性となる。特にデータ分析と関連した作業では、データセットのサブセットSubsetを多く扱うが、その時カテゴリカル配列を知っていれば大きな助けとなる。\n最適化 わざわざlevels()を使わなくても、ただの通常の配列でunique()を使えば似たような実装はできる。\njulia\u0026gt; @time for t in 1:10^6\runique(A)\rend\r0.543157 seconds (6.00 M allocations: 579.834 MiB, 17.33% gc time)\rjulia\u0026gt; @time for t in 1:10^6\rlevels(B)\rend\r0.013324 seconds しかし、その速さは約40倍の差がある。元々配列が変化するたびにカテゴリーが更新されるため、別途計算する必要なく直接参照できる。\n全コード using CategoricalArrays\rA = [\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;red\u0026#34;, \u0026#34;green\u0026#34;]\rB = categorical(A)\rlevels(B)\rB[2] = \u0026#34;red\u0026#34;; B\rlevels(B)\r@time for t in 1:10^6\runique(A)\rend\r@time for t in 1:10^6\rlevels(B)\rend 環境 OS: Windows julia: v1.6.3 CategoricalArrays v0.10.2 ","id":2318,"permalink":"https://freshrimpsushi.github.io/jp/posts/2318/","tags":null,"title":"ジュリアのカテゴリカル配列"},{"categories":"기하학","contents":"定義 1 アフィン独立(../2315)な $v_{0}, v_{1} , \\cdots , v_{n} \\in \\mathbb{R}^{n+1}$ の 凸包を $n$-シンプレックス$n$-simplex $\\Delta^{n}$ と言い、ベクトル $v_{k}$ を 頂点Vertexと言う。数式的には次のようになる。 $$ \\Delta^{n} := \\left\\{ \\sum_{k} t_{k} v_{k} : v_{k} \\in \\mathbb{R}^{n+1} , t_{k} \\ge 0 , \\sum_{k} t_{k} = 1 \\right\\} $$ $\\Delta^{n}$ から一つの頂点を取り除いてできる $n-1$-シンプレックス $\\Delta^{n-1}$ を$\\Delta^{n}$ の 面Faceと言い、$\\Delta^{n}$ の全ての面の合併を $\\Delta^{n}$ の 境界Boundaryと言って、$\\partial \\Delta^{n}$で表す。 シンプレックスの内部 $\\left( \\Delta^{n} \\right)^{\\circ} := \\Delta^{n} \\setminus \\partial \\Delta^{n}$ を オープンシンプレックスOpen Simplexと言う。 アフィン独立(../2315)とは、$v_{1} - v_{0} , v_{2} - v_{0} , \\cdots , v_{n} - v_{0}$ が線形独立であることを指す。 説明 シンプレックスは線形計画法や代数トポロジーなどで出会う概念で、その名が示す通り、単純さが特徴である。韓国語では 단체と呼ばれている。\n$n$-シンプレックス 定義から、凸包と$n$-シンプレックスの違いは与えられたベクトルがアフィン独立(../2315)であることだけである。集合 $X$ の凸包とは異なり、正確に$v_{0}, v_{1} , \\cdots , v_{n}$だけで表現され、そのようにのみ表現される図形であること。\n$$ \\left\\{ \\left( t_{0} , t_{1} , \\cdots , t_{n} \\right) \\in \\mathbb{R}^{n+1} : t_{k} \\ge 0 , \\sum_{k} t_{k} = 1 \\right\\} $$\nこの集合を 標準 $n$-シンプレックスStandard $n$-simplexと呼ぶ。ベクトル $v_{0}, v_{1} , \\cdots , v_{n}$ の長さなどすべて無視してその組み合わせだけを表すので、標準化と呼ぶに相応しい。\n例として、$\\Delta^{n} , n = 3,2,1,0$ を見てみよう。\n見る通り、$3$-シンプレックスは四面体、$2$-シンプレックスは三角形、$1$-シンプレックスは線分、$0$-シンプレックスはただの一つの点として表される。$2$-シンプレックスの三点が一直線上にあるなどの場合は、アフィン独立(../2315)の仮定から除外される。$n \\ge 4$ の場合は、幾何学的に表すことはできないが、一般化には何の問題もない。\n境界とオープンシンプレックス 本質的に、境界とオープンシンプレックスは、距離空間で語られた境界と内部と変わらないし、表記も同一だ。\n例で、$3$-シンプレックスの四面体の面は$2$-シンプレックスの三角形として現れ、面表面という表現がぴったり合うことがわかるだろう。さらに、$1$-シンプレックスの線分の面も両端の$0$-シンプレックスの点だ。この面を集めたものを境界境界と呼ぶのも非常に合理的だ。\n$\\Delta^{3}$ の境界 $\\partial \\Delta^{3}$ とオープンシンプレックス $\\left( \\Delta^{3} \\right)^{\\circ}$ は、図のように直感的に理解しても問題ない。\nHatcher. (2002). Algebraic Topology: p103.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2317,"permalink":"https://freshrimpsushi.github.io/jp/posts/2317/","tags":null,"title":"シンプレックスの定義"},{"categories":"줄리아","contents":"ガイド RDatasets.jl パッケージを使えば大丈夫。以下は最も簡単な iris データセットを読み込む例です。基本組み込みデータセットの他にも様々なデータセットが含まれているから、GitHubをチェックするといい1。\njulia\u0026gt; using RDatasets\rjulia\u0026gt; iris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\r150×5 DataFrame\rRow │ SepalLength SepalWidth PetalLength PetalWidth Species\r│ Float64 Float64 Float64 Float64 Cat…\r─────┼─────────────────────────────────────────────────────────────\r1 │ 5.1 3.5 1.4 0.2 setosa\r2 │ 4.9 3.0 1.4 0.2 setosa\r3 │ 4.7 3.2 1.3 0.2 setosa\r⋮ │ ⋮ ⋮ ⋮ ⋮ ⋮\r149 │ 6.2 3.4 5.4 2.3 virginica\r150 │ 5.9 3.0 5.1 1.8 virginica\r145 rows omitted 参考 Rで組み込みデータセットを読み込む方法 環境 OS: Windows julia: v1.6.3 https://github.com/JuliaStats/RDatasets.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2316,"permalink":"https://freshrimpsushi.github.io/jp/posts/2316/","tags":null,"title":"ジュリアでRで使用されていた組み込みデータセットを読み込む方法"},{"categories":"선형대수","contents":"説明 アフィンAffineって言葉がついてると、だいたい平行移動に関連する何かだ。定義でベクトル $v_{0}$ を引くことが、その平行移動に当たるんだ。\n数学全般でよくある表現ではないけど、シンプレックスを議論するときは必ず言及されるんだ。\n","id":2315,"permalink":"https://freshrimpsushi.github.io/jp/posts/2315/","tags":null,"title":"アフィン独立の定義"},{"categories":"줄리아","contents":"## ガイド 例として`Plots.jl`パッケージのバージョンを確認してみよう。REPLで`]`キーを押すとパッケージモードに入る。ここで`status foo`と入力すれば、次のように`foo`パッケージのバージョンを確認できる。 ![20211204_193048.png](20211204_193048.png#center) ## 環境 - OS: Windows - julia: v1.6.3 ","id":2313,"permalink":"https://freshrimpsushi.github.io/jp/posts/2313/","tags":null,"title":"ジュリアでパッケージバージョンを確認する方法"},{"categories":"줄리아","contents":"概要 isempty() 関数を使用すればいい。\nコード julia\u0026gt; isempty([])\rtrue\rjulia\u0026gt; isempty(Set())\rtrue\rjulia\u0026gt; isempty(\u0026#34;\u0026#34;)\rtrue タイトルでは配列とされているが、実際には集合や文字列でも良い。\n最適化 もちろん配列が空かどうかは、length()が $0$ かどうかで確認しても構わない。\njulia\u0026gt; @time for t in 1:10^6\risempty([])\rend\r0.039721 seconds (1000.00 k allocations: 76.294 MiB, 27.85% gc time)\rjulia\u0026gt; @time for t in 1:10^6\rlength([]) == 0\rend\r0.041762 seconds (1000.00 k allocations: 76.294 MiB, 19.18% gc time) 見てのとおり、空の配列の場合、二つの方法の性能差はない。\njulia\u0026gt; x = 1:10^6;\rjulia\u0026gt; @time for t in 1:10^6\risempty(x)\rend\r0.017158 seconds\rjulia\u0026gt; @time for t in 1:10^6\rlength(x) == 0\rend\r0.043243 seconds (1000.00 k allocations: 15.259 MiB) 一方で、配列が空でない場合、上記のように2倍以上の速度差が見られる。length()は具体的な長さを返さなければならないのに対し、isempty()は最初の要素が存在するかだけを確認すれば良いので、これは当然のことである。可読性の側面や条件文を使用する状況まで考えると、isempty()を使用する方がより望ましい。\n全コード isempty([])\risempty(Set())\risempty(\u0026#34;\u0026#34;)\r@time for t in 1:10^6\risempty([])\rend\r@time for t in 1:10^6\rlength([]) == 0\rend\rx = 1:10^6\r@time for t in 1:10^6\risempty(x)\rend\r@time for t in 1:10^6\rlength(x) == 0\rend 環境 OS: Windows julia: v1.6.3 ","id":2311,"permalink":"https://freshrimpsushi.github.io/jp/posts/2311/","tags":null,"title":"ジュリアで配列が空かどうかを確認する方法"},{"categories":"위상데이터분석","contents":"定義 1 2 $n \\in \\mathbb{N}_{0}$ とする。アーベル群 $C_{n}$ とホモモルフィズム $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ のチェーン $$ \\cdots \\longrightarrow C_{n+1} \\overset{\\partial_{n+1}}{\\longrightarrow} C_{n} \\overset{\\partial_{n}}{\\longrightarrow} C_{n-1} \\longrightarrow \\cdots \\longrightarrow C_{1} \\overset{\\partial_{1}}{\\longrightarrow} C_{0} \\overset{\\partial_{0}}{\\longrightarrow} 0 $$ が全ての $n$ に対して $$ \\partial_{n} \\circ \\partial_{n+1} = 0 $$ を満たす場合、チェインコンプレックスChain Complexと呼ぶ。 剰余群 $H_{n} := \\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$ を $\\mathcal{C}$ の**$n$番目のホモロジーグループ**$n$-th Homology Groupと呼ぶ。 ホモモルフィズム $\\partial_{n} : C_{n} \\longrightarrow C_{n-1}$ を境界Boundaryまたは微分Differentialオペレーターと呼ぶ。 $Z_{n} := \\ker \\partial_{n}$ の要素を**$n$-サイクル**Cycles、$B_{n} := \\text{Im} \\partial_{n+1}$ の要素を**$n$-バウンダリー**Boundaryと呼ぶ。 群 $0$ は $\\left\\{ 0 \\right\\}$ で定義されたマグマだ。つまり、空の代数構造だ。 ホモモルフィズム $\\partial^{2} = 0$ はゼロモルフィズムだ。 $\\text{Im}$ は像だ。 $\\ker$ は核だ。 説明 難しいと感じるのは当然だ。紹介された定義は非常に厳格な代数的ステートメントのみを含んでいるため、直感的な理解を得るためにはすぐに単体複体ホモロジーへ移行することをお勧めする。（それも容易ではないが）境界や微分という用語を幾何学的に見なければ、代数的な表現だけで受け入れるのは難しい。\n一般化の可能性 実際、チェインコンプレックスにおけるインデックス集合は$\\mathbb{N}_{0} = \\left\\{ 0, 1, 2, \\cdots \\right\\}$以外にも負の数への拡張はもとより、実数に対しても一般化できるとされているが、$0$を基準に負へと行くと、トポロジーや幾何学的な意味は大きく薄れる。\nホモロジーグループの存在性 定理\n$U, V, W$がベクトル空間、$T_{1} : U \\to V$、$T_{2} : V \\to W$が線形変換とする。すると次が成り立つ。\n$$ T_{2}T_{1} = 0 \\iff \\operatorname{Im} (T_{1}) \\subset \\ker (T_{2}) $$\nチェインコンプレックスの条件$\\partial_{n} \\circ \\partial_{n+1} = 0$は、通常$\\partial^{2} = 0$と略されることがよくある。$\\text{Im} \\partial_{n+1}$が何であれ$\\partial_{n}$を取れば$0$に行くということは、つまり$\\ker \\partial_{n}$が$\\text{Im} \\partial_{n+1}$を完全に包含できるほど十分に大きいという意味、言い換えれば$\\text{Im} \\partial_{n+1} \\subset \\ker \\partial_{n}$である。\n$\\partial^{2} = 0$から$\\ker \\partial_{n} / \\text{Im} \\partial_{n+1}$が出てくるのは突然かもしれないが、歴史的に見れば、$\\ker f / \\text{Im} g$のように核を像に分割する代数構造について多くの研究があり、$\\partial^{2} = 0$はその直感的な意味よりも、洗練された表現のために定義に含まれたと考えるのが適切だ。\n境界と微分 $n$-サイクル $Z_{n}$ の「Zyklus」はドイツ語からきている。\n$\\partial_{n}$は単体の境界として見ると、その命名が自然であり、微分と呼ばれることも $$ \\lim_{h \\to 0} {{ f(x + h) - f(x) } \\over { h }} $$ において $$ \\partial \\left[ v_{0} , v_{1} \\right] = \\left[ v_{1} \\right] - \\left[ v_{0} \\right] $$ と定義されるように、差分Differenceから数式的な形で直感的に理解できる。しかし、ホモロジーグループの単純な定義だけでは理解はできない。これらの説明は、$\\partial_{n}$の具体的な定義が与えられ、その普遍的な使用法が理解された後にしか妥当しない。今はその名前自体に固執せずに進もう。\n悪評 ホモロジーは意外にも一般大衆にかなり知られている概念だ。彼らがホモロジーという単語を覚えているわけではないが、Twitterで話題となり、ソウル大学生でさえも簡単に説明できない難しい何かとして広く知られている。\nHatcher. (2002). Algebraic Topology: p106.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMunkres. (1984). Elements of Algebraic Topology: p41.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2310,"permalink":"https://freshrimpsushi.github.io/jp/posts/2310/","tags":null,"title":"ホモロジー群の定義"},{"categories":"선형대수","contents":"定義 1 $V$を有限次元のベクトル空間とする。$T : V \\to V$を線形変換とする。$T$の行列表示$\\begin{bmatrix} T \\end{bmatrix}_{\\beta}$が対角行列になるような順序基底$\\beta$が存在する場合、$T$は対角化可能diagonalizeableであると言われる。\n正方行列$A$について、$L_{A}$が対角化可能であれば、行列$A$は対角化可能であると言われる。\n説明 線形変換$T : V \\to V$が対角化可能だとしよう。$\\beta = \\left\\{ v_{1}, \\dots, v_{n} \\right\\}$を$V$の順序基底とする。そして$D = \\begin{bmatrix} T \\end{bmatrix}_{\\beta}$が対角行列であるとする。その場合、各$v_{j} \\in \\beta$に対して、次のことを得る。\n$$ T(v_{j}) = \\sum_{i} D_{ij} v_{i} = D_{jj}v_{j} $$\nこの時$\\lambda_{j} = D_{jj}$と言われる場合、\n$$ T(v_{j}) = \\lambda_{j} v_{j} $$\n$\\begin{bmatrix} T \\end{bmatrix}_{\\beta}$を対角行列にする順序基底の要素は、このような特別な方程式の形を満たすようになる。したがって、このような順序基底$\\beta$によって表されるベクトルは、単に各成分にスカラー$\\lambda_{j}$を掛けるだけで、線形変換$T$を適用したのと同じになる。この特別なベクトル$v_{j}$とスカラー$\\lambda_{j}$を、我々は固有ベクトルと固有値と呼ぶ。したがって、対角化可能な条件を固有値と関連づけると、\n$n$個の線形独立な固有ベクトルの存在 = 固有ベクトルで構成される基底の存在 $\\iff$ 対角化可能\nStephen H. Friedberg, Linear Algebra (4th版, 2002), p245-246\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3335,"permalink":"https://freshrimpsushi.github.io/jp/posts/3335/","tags":null,"title":"対角化可能な線型変換"},{"categories":"줄리아","contents":"概要 地の果てまで一人で居る辛さを知ってる人は、ああ、分かるんだ\nコーディング中にわからないエラーに苦労した人は、プログラミングにおいてエラーがとても大事だということを分かっている\u0026hellip;\nJuliaでは、error() 関数や @error マクロを使ってエラーを出すことができる。現在のJulia v1.63を基準に、25種類の組み込み例外が定義されている1。\nコード julia\u0026gt; log(1 + 2im)\r0.8047189562170501 + 1.1071487177940904im 例えば、プログラムで 対数関数 $\\log$ を使う時、入力は実数だけ許されるべきだと考えられる。しかし、Juliaでは基本的に複素数に拡張された $\\log_{\\mathbb{C}}$ を提供している。プログラムがエラーなしで動いていることは良いことではない。意図しない計算は予期しない問題を引き起こすから、望まない計算が行われたら、最初からエラーを出してはいけない。\n元のlogの定義域を実数 $\\mathbb{R}$ に限定するコードを作ってみよう。\nerror() 関数 julia\u0026gt; function Rlog(x)\rif typeof(1 + 2im) \u0026lt;: Real\rreturn log(x)\relse\rerror(DomainError, \u0026#34;: Rlog allow real number only\u0026#34;)\rend\rend\rRlog(1 + 2im)\rERROR: LoadError: DomainError: Rlog allow real number only\rStacktrace:\r[1] error(::Type, ::String)\r@ Base .\\error.jl:42\r[2] Rlog(x::Complex{Int64})\r@ Main c:\\admin\\REPL.jl:7\r[3] top-level scope\r@ c:\\admin\\REPL.jl:11\rin expression starting at c:\\admin\\REPL.jl:11 上の Rlogでは、入力が実数でなければDomainErrorをレイズするように限定した。\n@error マクロ julia\u0026gt; function Rlog2(x)\rif typeof(1 + 2im) \u0026lt;: Real\rreturn log(x)\relse\r@error \u0026#34;Rlog2 also allow Real number only\u0026#34;\rend\rend\rRlog2(1 + 2im)\r┌ Error: Rlog2 also allow Real number only\r└ @ Main c:\\admin\\REPL.jl:17 上の Rlog2では、入力が実数でなければ、とりあえずエラーをスローするように限定した。\nレイズとスローは、どちらもエラーを起こすという意味で、大きな文脈での違いはない。レイズはPythonなどで使われる表現で、スローはJavaなどで使われる表現だ。\n全体のコード log(1 + 2im)\rfunction Rlog(x)\rif typeof(1 + 2im) \u0026lt;: Real\rreturn log(x)\relse\rerror(DomainError, \u0026#34;: Rlog allow real number only\u0026#34;)\rend\rend\rRlog(1 + 2im)\rfunction Rlog2(x)\rif typeof(1 + 2im) \u0026lt;: Real\rreturn log(x)\relse\r@error \u0026#34;Rlog2 also allow Real number only\u0026#34;\rend\rend\rRlog2(1 + 2im) 環境 OS: Windows julia: v1.6.3 https://docs.julialang.org/en/v1/manual/control-flow/#Built-in-Exceptions\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2309,"permalink":"https://freshrimpsushi.github.io/jp/posts/2309/","tags":null,"title":"ジュリアで例外処理する方法"},{"categories":"선형대수","contents":"概要 線形変換の特性多項式を定義する。以下の定理から、式$\\det(A - \\lambda I) = 0$を解くことが固有値を見つけることと同じであることが分かる。したがって、$\\det(A - \\lambda I)$に名前をつけることは非常に自然であり、これを特性多項式という。\n定理1 $F$を任意の体、$A \\in M_{n\\times n}(F)$とする。$\\lambda \\in F$が$A$の固有値であることは、$\\det (A-\\lambda I) = 0$であることと同値である。\n証明 $\\lambda$を$A$の固有値と仮定する。すると、\n$$ \\begin{align*} \\lambda \\text{ is eigenvalue of } A \u0026amp;\\iff \\exist \\text{non-zero } v \\text{ such that } Av = \\lambda v \\\\ \u0026amp;\\iff \\exist \\text{non-zero } v \\text{ such that } (A - \\lambda I)v = 0 \\end{align*} $$\n可逆行列である同値条件\n$A$を大きさが$n\\times n$の正方行列とする。すると、以下の命題は全て同値である。\n$A$は可逆行列である。 同次線形システム$A\\mathbf{x}=\\mathbf{0}$はただ一つの自明な解を持つ。 $\\det{A} \\ne 0$ 可逆行列である同値条件によれば、$A - \\lambda I$は可逆行列ではなく、$\\det (A - \\lambda I) = 0$である。\n定義 $A \\in M_{n \\times n}(F)$とする。多項式$f(t) = \\det(A - tI)$を$A$の特性多項式characteristic polynomialとする。$f(t) = 0$を特性方程式characteristic equationとする。\n$V$を$n$次元ベクトル空間とする。$T : V \\to V$を線形変換とする。$\\beta$を$V$の順序基底とする。$T$の特性多項式$f(t)$を$T$の行列表現の特性多項式として定義する。つまり、$f(t)$は以下の通りである。\n$$ f(t) = \\det\\left( \\begin{bmatrix} T \\end{bmatrix}_{\\beta} - t I \\right) $$\n説明 定義によれば、$T : V \\to V$の特性多項式の根は固有値そのものであり、特性多項式が分解されると、$T$は$n = \\dim(V)$個の固有値を持つ(異なるとは言っていない)。\n定義で、$T$の特性多項式は順序基底$\\beta$の選び方に依存するように思えるかもしれないが、実際にはそうではない。この理由から、線形変換$T$の特性多項式を以下のように記述することもある。\n$$ \\det (T - \\lambda I) $$\n確認しよう。$\\beta$、$\\beta^{\\prime}$を$V$の順序基底、$Q$を$\\beta$-座標を$\\beta^{\\prime}$-座標に変換する座標変換行列とすると、\n$$ \\begin{align*} \\det( \\begin{bmatrix} T \\end{bmatrix}_{\\beta} - tI) \u0026amp;= \\det( \\begin{bmatrix} T \\end{bmatrix}_{\\beta} - tI ) \\det Q^{-1} \\det Q \\\\ \u0026amp;= \\det Q^{-1} \\det( \\begin{bmatrix} T \\end{bmatrix}_{\\beta} - tI ) \\det Q \\\\ \u0026amp;= \\det \\left( Q^{-1} (\\begin{bmatrix} T \\end{bmatrix}_{\\beta} - tI) Q \\right) \\\\ \u0026amp;= \\det \\left( Q^{-1}\\begin{bmatrix} T \\end{bmatrix}_{\\beta}Q - tI \\right) \\\\ \u0026amp;= \\det \\left( \\begin{bmatrix} T \\end{bmatrix}_{\\beta^{\\prime}} - tI \\right) \\end{align*} $$\nStephen H. Friedberg, Linear Algebra (4th Edition, 2002), p248\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3339,"permalink":"https://freshrimpsushi.github.io/jp/posts/3339/","tags":null,"title":"線形変換の特性多項式"},{"categories":"줄리아","contents":"概要 nrow(), ncol(), size() を使用できる。Rと違って、length()はエラーになる。\nコード julia\u0026gt; df = DataFrame(rand(100000,5), :auto)\r100000×5 DataFrame\rRow │ x1 x2 x3 x4 x5 │ Float64 Float64 Float64 Float64 Float64\r────────┼─────────────────────────────────────────────────────\r1 │ 0.474921 0.942137 0.0523668 0.588696 0.0176242\r2 │ 0.842828 0.910385 0.216194 0.794668 0.664883\r3 │ 0.0350312 0.96542 0.837923 0.920311 0.748409\r4 │ 0.613249 0.731643 0.941826 0.688649 0.161736\r⋮ │ ⋮ ⋮ ⋮ ⋮ ⋮\r99998 │ 0.767794 0.242687 0.965885 0.557483 0.723849\r99999 │ 0.743936 0.67815 0.529923 0.247698 0.861302\r100000 │ 0.628269 0.252583 0.985485 0.24541 0.942741\r99993 rows omitted dfは、行が10万で、列が5つのデータフレームだ。\njulia\u0026gt; nrow(df)\r100000\rjulia\u0026gt; ncol(df)\r5\rjulia\u0026gt; size(df)\r(100000, 5) nrow()とncol()はそれぞれ行と列の数を返し、size()は行と列のサイズをタプルで返す。これを行、列の順序で参照すると、行と列のサイズを別々に知ることができる。一見すると、size()の方がずっと便利に見えるが、その性能を比較してみよう。\n最適化 julia\u0026gt; @time for i in 1:10^6\rnrow(df)\rend\r0.051730 seconds (1000.00 k allocations: 15.259 MiB)\rjulia\u0026gt; @time for i in 1:10^6\rsize(df)[1]\rend\r0.536297 seconds (3.00 M allocations: 61.035 MiB, 5.44% gc time) 上はnrow()とsize()の速度比較だ。当たり前だが、一つの機能しかしていないnrow()の方が速い。データフレームが大きくなるケース―ビッグデータを扱う場合や、size()を使ってもあまり差がないと思って時間の無駄を省ける。\nまた、コードの可読性の面では大きな差がある。nrow()とncol()は他の言語でも一般的に使用される関数名で、疑問の余地なく行、列の数だが、size()は後ろに付くインデックスのために、コードの可読性を大きく下げる。可能であれば、nrow()とncol()を使用することにしよう。\n全コード using DataFrames\rdf = DataFrame(rand(100000,5), :auto)\rnrow(df)\rncol(df)\rsize(df)\r@time for i in 1:10^6\rnrow(df)\rend\r@time for i in 1:10^6\rsize(df)[1]\rend 環境 OS: Windows julia: v1.6.3 ","id":2307,"permalink":"https://freshrimpsushi.github.io/jp/posts/2307/","tags":null,"title":"ジュリアでデータフレームのサイズを確認する方法"},{"categories":"선형대수","contents":"定義1 $V$を有限次元$F$-ベクトル空間としよう。$T : V \\to V$を線形変換とする。$\\lambda \\in F$について、 $$ Tx = \\lambda x $$ これを満たすゼロベクトルではない$x \\in V$を$T$の固有ベクトルeigenvectorと呼ぶ。\nこの時のスカラー$\\lambda \\in F$を固有ベクトル$x$に対応する固有値eigenvalueと呼ぶ。\n説明 固有ベクトルをcharacteristic vectorやproper vectorで、固有値をcharacteristic valueやproper valueで置き換えれることもあるが、筆者は見たことがない。\n固有値と固有ベクトルは、線形変換の対角化と関連がある。\n定理 $n$次元ベクトル空間$V$上の線形変換$T : V \\to V$が対角化可能であるのは、$T$の固有ベクトルからなる$V$の順序基底$\\beta$が存在する場合、かつその場合に限る。つまり$T$が対角化可能であるのは、$T$の固有ベクトルが$n$個の線形独立する場合と同じである。\nさらに$T$が対角化可能であり、$\\beta = \\left\\{ v_{1}, \\dots, v_{n} \\right\\}$が$T$の固有ベクトルの順序基底であり、$D = \\begin{bmatrix} T \\end{bmatrix}_{\\beta}$である場合、$D$は対角行列で、$D_{jj}$は$v_{j}$に対応する固有値である。\n参照 行列の固有値、固有ベクトル Stephen H. Friedberg, Linear Algebra (第4版, 2002), p245~264\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3337,"permalink":"https://freshrimpsushi.github.io/jp/posts/3337/","tags":null,"title":"有限次元の線形変換の固有値と固有ベクトル"},{"categories":"위상데이터분석","contents":"定義 1 インデックス集合 $I \\ne \\emptyset$ に対して、集合 $A := \\left\\{ a_{i} : i \\in I \\right\\}$ を アルファベットAlphabetと呼び、その要素 $a_{i} \\in A$ を レターLetterとしよう。 整数 $n \\in \\mathbb{Z}$ に対して、$a_{i}^{n}$ のような形を 音節Syllableと言い、これらの有限な並べ替えJuxtapostionである文字列 $w$ を 単語Wordとする。 音節 $a_{i}^{n} a_{i}^{m}$ は、$a_{i}^{n+m}$ のように表現でき、これを 初等縮約Elementary Contractionと言う。これ以上初等縮約できない単語を 縮約語Reduced Wordとし、特に $1 := a_{i}^{0}$ を 空単語Empty Wordとする。 アルファベット $A$ のレターで作れる全ての縮約語の集合を $F [A]$ としよう。二つの単語 $w_{1} , w_{2} \\in F[A]$ に対して、$w_{1} \\cdot w_{2}$ が縮約語形で表される 二項演算 $\\cdot : F[A]^{2} \\to F[A]$ を定義する。群 $\\left( F[A], \\cdot \\right)$ を $A$ によって生成されたフリーグループFree Group Generated by $A$と呼ぶ。 $G$ が集合 $A := \\left\\{ a_{i} : i \\in I \\right\\}$ の要素を 生成元として持つ群であり、$\\phi \\left( a_{i} \\right) = a_{i}$ の 同型 $\\phi : G \\to F [A]$ が存在するなら、$G$ を $A$ 上でフリーFree on $A$であると言い、$a_{i}$ を$G$ の フリージェネレーターFree Generatorと呼ぶ。 $A \\ne \\emptyset$ 上でフリーな群を フリーグループFree Groupと定義し、$A$ の 濃度 $|A|$ をフリーグループの ランクRankと呼ぶ。 説明 定義が長くて読むのが嫌になるかもしれないけど、実際に例を考えてみると全然難しくない。\u0026lsquo;アルファベット\u0026rsquo;や\u0026rsquo;単語\u0026rsquo;のような用語が出てくる点に戸惑わないでほしい。代数学代數学という言葉自体が「数を代わりに文字を使うことについての勉強」という意味だからだ。ここまで来て考えてみれば、集合に演算を加えて考えるというアプローチがあまりにも抽象的だったかもしれない。実際、フリーグループについて定義が終わった後は、上で出てきた単語はほとんど使わない。心配せずに例を見てみよう。\nアルファベットとレター $$ A = \\left\\{ a, b \\right\\} $$\n上のような アルファベットを考えると、レターはただの $a$ と $b$ だけだ。\n音節と単語 アルファベット $A$ に対して $$ a^{2} , b^{3}, b^{-1} $$ は全て 音節だ。これらを有限に、重複を許して並べるという意味で、並べ替えJuxtapostionという表現が使われ、定義としては、ただ 単語と言った。 $$ a^{2} b \\\\ bbab \\\\ b^{-2} a a a^{-2} b a^{-24} $$\n縮約語と空単語 例として、最後の単語 $b^{-2} a a a^{-2} b a^{-24}$ が縮約される過程を見てみよう。 $$ \\begin{align*} \u0026amp; b^{-2} a a a^{-2} b a^{-24} \\\\ =\u0026amp; b^{-2} a^{2} a^{-2} b a^{-24} \\\\ =\u0026amp; b^{-2} a^{0} b a^{-24} \\\\ =\u0026amp; b^{-2} 1 b a^{-24} \\\\ =\u0026amp; b^{-2} b a^{-24} \\\\ =\u0026amp; b^{-1} a^{-24} \\end{align*} $$ ここで $a^{0} = 1$ はまるで 単位元のように機能しており、実際にも音節がないという意味でEmpty Wordと呼ばれる。群になった後は特に $1$ をアルファベットで書く必要はない。\n$A$ によって生成されたフリーグループ ここまでのビルドアップから、$\\left( F[A], \\cdot \\right)$ は自然に 群になる。単位元は空単語 $1$ であり、全ての単語 $w$ に対して、次を満たす 逆元 $w^{-1}$ が存在する。 $$ w \\cdot w^{-1} = w^{-1}\\cdot w = 1 $$ 最初から $F[A]$ は具体的な単位元と逆元を与えられていたので、当然群だ。このようにフリーグループとは他でもない「群になるために作られた群」だ。\n$A$ 上でフリーな群 $$ F[A] = \\left\\{ \\cdots , a^{-2} b^{-1} , a^{-1} b^{-1}, a^{-1}, b^{-1} , 1 , a , b , ab , a b a \\cdots \\right\\} $$ $F[A]$ の要素を具体的に並べてみると上のようだ。ここまでの定義はもちろん直感的で理解しやすいけど、実際には $G$ そのものに興味がある。例えば、整数群 $\\left( \\mathbb{Z} , + \\right)$ を考えると、$\\left\\{ 1 \\right\\}$ の要素を 生成元として持つ巡回群であり、$F \\left[ \\left\\{ a \\right\\} \\right]$ と同型なので、$\\left\\{ a \\right\\}$ 上でフリーであると言える。\nフリーグループとランク ここまでの例から、$\\mathbb{Z}$ は単元素集合 $\\left\\{ a \\right\\}$ 上でフリーなのでランク $1$ であり、$A = \\left\\{ a,b \\right\\}$ によって生成されたフリーグループ $F[A]$ はランク $2$ だ。\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p341~342.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2306,"permalink":"https://freshrimpsushi.github.io/jp/posts/2306/","tags":null,"title":"抽象代数学における自由群"},{"categories":"줄리아","contents":"概要 ネームド・タプルが使える。ネームド・タプルを作る方法は、左の括弧のすぐ後ろにセミコロン;をつけることだ。例えば、DataFrame(; x, y)とすると、カラム名が:x、:yで、内容もそれぞれx、yのデータフレームが作られる。\nコード julia\u0026gt; MyCol7 = rand(5); B = 1:5;\rjulia\u0026gt; DataFrame(; MyCol7, B)\r5×2 DataFrame\rRow │ MyCol7 B │ Float64 Int64\r─────┼─────────────────\r1 │ 0.911763 1\r2 │ 0.93374 2\r3 │ 0.116779 3\r4 │ 0.467364 4\r5 │ 0.473437 5 環境 OS: Windows julia: v1.6.3 ","id":2305,"permalink":"https://freshrimpsushi.github.io/jp/posts/2305/","tags":null,"title":"ジュリアで変数名をカラム名として持つデータフレームを作成する方法"},{"categories":"줄리아","contents":"概要 名前付きタプルは、一般的なタプルとは異なり、辞書や構造体のように使用できるタプルだ。シンボルの配列をキーとして持ち、キーを使ってバリューにアクセスしつつ、タプルのようにも使用できる。\nコード x = rand(Bool, 5); y = rand(Bool, 5);\rz = (; x, y)\rtypeof(z)\rz.x 上のコードを実行して、名前付きタプルの使用方法を確認してみよう。\njulia\u0026gt; z = (; x, y)\r(x = Bool[0, 0, 1, 1, 0], y = Bool[1, 1, 0, 0, 0])\rjulia\u0026gt; typeof(z)\rNamedTuple{(:x, :y), Tuple{Vector{Bool}, Vector{Bool}}} 名前付きタプルを簡単に作る方法は、タプルを作りながら開き括弧の直後にセミコロン ; を付けることだ。例えば、(; x) は (; x=x) と同じだ。\njulia\u0026gt; z.x\r5-element Vector{Bool}:\r0\r0\r1\r1\r0\rjulia\u0026gt; z[2]\r5-element Vector{Bool}:\r1\r1\r0\r0\r0 名前付きタプルは、上のように名前のシンボルでアクセスすることも、インデックスでアクセスすることもできる。\n環境 OS: Windows julia: v1.6.3 ","id":2303,"permalink":"https://freshrimpsushi.github.io/jp/posts/2303/","tags":null,"title":"ジュリアのネームドタプル"},{"categories":"추상대수","contents":"参照してください 線形代数学でのベクトル空間 抽象代数学でのベクトル空間 下記の文書で言及されている$F$-ベクトル空間は、実際、上記の文書のベクトル空間と一ミリも違いがない。しかし、視点が少し違っていて、線形代数学でのベクトル空間は直感的なユークリッド空間の抽象化で、抽象代数学でのベクトル空間はそれを真の意味での「代数」に持ち込むものと見ることができる。\n逆に$R$-モジュールは$F$-ベクトル空間のスカラー・フィールド $F$ を スカラー・リング $R$ に一般化することに意味があり、したがって$F$-ベクトル空間の歴史や意義に無関心なネーミングでそのアイデンティティを示している。グループ $G$ の視点から見れば、環 $R$ に新しい操作 $\\mu$ が追加されたものは同様に加群加群である。\n抽象代数学でのR-モジュール 抽象代数学での$F$-ベクトル空間 ","id":2300,"permalink":"https://freshrimpsushi.github.io/jp/posts/2300/","tags":null,"title":"抽象代数におけるF-ベクトル空間"},{"categories":"추상대수","contents":"定義 1 アーベル群$(G,+)$(../309)と乗法の単位元$1 \\ne 0$を持つリング$(R,+,\\cdot)$(../587)が二項演算$$ \\mu : R \\times G \\to G $$に対して次の三つの条件を満たせば、$\\left( G, +, R, \\cdot ; \\mu \\right)$は$R$-モジュールR-Moduleと言う:\n(M1) 二元加法性: $\\forall \\alpha, \\beta \\in R$と$\\forall x,y \\in G$に対して $$ \\begin{align*} \\mu \\left( \\alpha + \\beta , x \\right) =\u0026amp; \\mu \\left( \\alpha , x \\right) + \\mu \\left( \\beta , x \\right) \\\\ \\mu \\left( \\alpha , x + y \\right) =\u0026amp; \\mu \\left( \\alpha , x \\right) + \\mu \\left( \\alpha , y \\right) \\end{align*} $$ (M2): $\\forall \\alpha, \\beta \\in R$と$\\forall x \\in G$に対して $$ \\mu \\left( \\alpha , \\mu \\left( \\beta , x \\right) \\right) = \\mu \\left( \\alpha \\beta , x \\right) $$ (M3): $\\forall x \\in G$に対して $$ \\mu (1, x) = x $$ ここで、$R$は基礎リングGround Ring、またはベースリングBase Ringとも呼ばれる。\n説明 ここで$\\mu$はスカラー乗法Scalar Multiplicationと呼ばれ、その演算結果である要素、スカラー積Scalar Productは$\\mu (\\alpha, x) := \\alpha x$のように示される。この表現に従って、上記の三つの条件は次のように表せる:\n(M1) 分配法則: $$ \\begin{align*} \\left( \\alpha + \\beta \\right) x =\u0026amp; \\alpha x + \\beta x \\\\ \\alpha \\left( x + y \\right) =\u0026amp; \\alpha x + \\alpha y \\end{align*} $$ (M2) 結合法則: $$ \\alpha \\left( \\beta x \\right) = \\left( \\alpha \\beta \\right) x $$ (M3) 単位元: $$ 1 x = x $$ これらはスカラー乗法という言葉から始まり、どこか見覚えがあるが、それは線形代数でのベクトル空間の定義を飽きるほど見てきたからだ。$R$-モジュールはこの点で、$F$-ベクタースペースの一般化だ。\n参照 線形代数学でのベクトル空間 抽象代数学でのベクトル空間 以下の文書で言及されている$F$-ベクタースペースは、上述のベクトル空間と何も変わりはない。ただし、見方が少し違う。線形代数学でのベクトル空間は直感的なユークリッド空間の抽象化であり、抽象代数学でのベクトル空間はそれを真の意味での「代数」に取り入れることと見ることができる。\n反対に、$R$-モジュールは$F$-ベクタースペースのスカラー体$F$をスカラーリング$R$に一般化し、その意義を示している。そして、$F$-ベクタースペースの歴史や意味に無関心なネーミングでその身元を示している。グループ$G$の立場から見れば、リング$R$と新しい操作$\\mu$が加えられることにより、それは加群加群とも言える。\n抽象代数学でのR-モジュール 抽象代数学での$F$-ベクタースペース Sze-Tsen Hu. (1968). Introduction to Homological Algebra: p1.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2298,"permalink":"https://freshrimpsushi.github.io/jp/posts/2298/","tags":null,"title":"R-加群における抽象代数"},{"categories":"수리통계학","contents":"定義 1 仮説検定: $$ \\begin{align*} H_{0} :\u0026amp; \\theta \\in \\Theta_{0} \\\\ H_{1} :\u0026amp; \\theta \\in \\Theta_{0}^{c} \\end{align*} $$\n検定力関数 $\\beta (\\theta)$が全ての$\\theta_{0} \\in \\Theta_{0}$と$\\theta_{1} \\in \\Theta_{0}^{c}$に対して次を満たす場合、偏りがないUnbiased検定力関数という。 $$ \\beta \\left( \\theta_{0} \\right) \\le \\beta \\left( \\theta_{1} \\right) $$ $\\mathcal{C}$が上記の仮説検定を集めた集合だとする。$\\mathcal{C}$内の検定力関数$\\beta (\\theta)$を持つ仮説検定$A$が、全ての$\\theta \\in \\Theta_{0}^{c}$と$\\mathcal{C}$の全ての仮説検定の検定力関数$\\beta ' (\\theta)$に対して $$ \\beta ' (\\theta) \\le \\beta (\\theta) $$ を満たす場合、仮説検定$A \\in \\mathcal{C}$を最も強力な検定(一様に) 最も強力な検定, UMPという。 説明 偏りがない検定力関数 $$ \\beta (\\theta) := P_{\\theta} \\left( \\mathbf{X} \\in \\mathbb{R} \\right) $$ 検定力関数は、確率$P$、正確には$X$の確率分布と棄却域$R$に応じて変わるため、定義だけからは$\\beta$の形を完全に思い浮かべるのは難しい。しかし、常識的に良い検定力関数が備えるべき性質は、帰無仮説よりも対立仮説の下で検定力帰無仮説を棄却する力が高くなるべきだ。$\\theta_{0}$と$\\theta_{1}$の選び方に関わらず、これを満たす性質を検定力関数の偏りのなさといい、このような検定力関数の関数値を比較するコンセプトは、次に紹介する最も強力な検定へと続く。\n最も強力な検定 最も強い\u0026hellip; 単なるワクワクする少年の漫画ではなく、文字通り最強の仮説検定だ。\n定義のステイトメントから、検定が最も強いということは、帰無仮説が正当に棄却されるべき全ての$\\theta \\in \\Theta_{0}^{c}$に対して、どんな検定力関数$\\beta '$を考えても、最も強力な検定の検定力関数$\\beta$が最も強力な検定力を持つということだ。\nCasella. (2001). Statistical Inference(2nd Edition): p387~388.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2293,"permalink":"https://freshrimpsushi.github.io/jp/posts/2293/","tags":null,"title":"不便検定力関数と最強力検定"},{"categories":"행렬대수","contents":"定義 $A$を行列$m \\times n$とする。\n$$ A = \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\\\ \\end{bmatrix} $$\nこの時、行列を切る任意の垂直線、水平線を考えよう。\n$$ A = \\left[ \\begin{array}{cc|ccc|c|} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \u0026amp; a_{14} \u0026amp; a_{15} \u0026amp; \\cdots \u0026amp; a_{1n-1} \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \u0026amp; a_{24} \u0026amp; a_{25} \u0026amp; \\cdots \u0026amp; a_{2n-1} \u0026amp; a_{2n} \\\\ \\hline a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \u0026amp; a_{34} \u0026amp; a_{35} \u0026amp; \\cdots \u0026amp; a_{3n-1} \u0026amp; a_{3n} \\\\ a_{41} \u0026amp; a_{42} \u0026amp; a_{43} \u0026amp; a_{44} \u0026amp; a_{45} \u0026amp; \\cdots \u0026amp; a_{4n-1} \u0026amp; a_{4n} \\\\ \\hline \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\\\ \\hline a_{m1} \u0026amp; a_{m2} \u0026amp; a_{m3} \u0026amp; a_{m4} \u0026amp; a_{m5} \u0026amp; \\cdots \u0026amp; a_{m-1n} \u0026amp; a_{mn} \\end{array} \\right] $$\nそれぞれの線で切られた部分を$A$のブロックという。\n$$ A_{11} = \\begin{bmatrix} a_{11} \u0026amp; a_{12} \\\\ a_{21} \u0026amp; a_{22} \\end{bmatrix},\\quad A_{12} = \\begin{bmatrix} a_{13} \u0026amp; a_{14} \u0026amp; a_{15}\\\\ a_{23} \u0026amp; a_{24} \u0026amp; a_{25} \\end{bmatrix},\\quad \\cdots,\\quad A_{kl} = \\begin{bmatrix} a_{m-1n} \u0026amp; a_{mn}\\end{bmatrix} $$\n行列$A$を次のようにブロックで表したものをブロック行列という。\n$$ A = \\begin{bmatrix} A_{11} \u0026amp; A_{12} \u0026amp; \\cdots \u0026amp; A_{1l} \\\\ A_{21} \u0026amp; A_{22} \u0026amp; \\cdots \u0026amp; A_{2l} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ A_{k1} \u0026amp; A_{k2} \u0026amp; \\cdots \u0026amp; A_{kl} \\\\ \\end{bmatrix} $$\n説明 行列をブロック行列として扱うことは、行列の計算を容易にする。実際、ブロック行列の計算も行列の計算と同じように行えばいい。\n$$ A = \\begin{bmatrix} A_{11} \u0026amp; A_{12} \\\\ A_{21} \u0026amp; A_{22} \\end{bmatrix}\\quad \\text{and} \\quad B = \\begin{bmatrix} B_{11} \u0026amp; B_{12} \\\\ B_{21} \u0026amp; B_{22} \\end{bmatrix} \\\\[1em] \\implies AB = \\begin{bmatrix} A_{11}B_{11} + A_{12}B_{21} \u0026amp; A_{11}B_{12} + A_{12}B_{22} \\\\ A_{21}B_{11} + A_{22}B_{21} \u0026amp; A_{21}B_{12} + A_{22}B_{22} \\end{bmatrix} $$\n従って、行列をゼロ行列や単位行列が含まれるブロック行列形に変えれば、行列の乗算を簡単に計算できる。\n$$ A = \\begin{bmatrix} A_{11} \u0026amp; I \\\\ O \u0026amp; A_{22} \\end{bmatrix}\\quad \\text{and} \\quad B = \\begin{bmatrix} B_{11} \u0026amp; B_{12} \\\\ B_{21} \u0026amp; B_{22} \\end{bmatrix} \\\\[1em] \\implies AB = \\begin{bmatrix} A_{11}B_{11} + B_{21} \u0026amp; A_{11}B_{12} + B_{22} \\\\ A_{22}B_{21} \u0026amp; A_{22}B_{22} \\end{bmatrix} $$\n行列を行ベクトルと列ベクトルに分けたものもブロック行列である。\n$$ \\begin{align*} A =\u0026amp; \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{r}_{1} \\\\ \\mathbf{r}_{2} \\\\ \\vdots \\\\ \\mathbf{r}_{m} \\end{bmatrix} \\\\ =\u0026amp; \\begin{bmatrix} \\mathbf{c}_{1} \u0026amp; \\mathbf{c}_{2} \u0026amp; \\cdots \u0026amp; \\mathbf{c}_{n} \\end{bmatrix} \\end{align*} $$\n従って、$A \\mathbf{x}$を次のように表せる。\n$$ \\begin{align*} A \\mathbf{x} \u0026amp;= \\begin{bmatrix} \\mathbf{c}_{1} \u0026amp; \\mathbf{c}_{2} \u0026amp; \\cdots \u0026amp; \\mathbf{c}_{n} \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n} \\end{bmatrix} \\\\ \u0026amp;= \\sum_{i}^{n} x_{i}\\mathbf{c}_{i} \\end{align*} $$\nまた、$A$を$m \\times p$行列とし、$\\mathbf{a}_{i}$を$A$の行ベクトルとし、$B$を$p \\times n$行列とし、$\\mathbf{b}_{i}$を$B$の列ベクトルとする。そうすると、二つの行列の積は次のようになる。\n$$ AB = \\begin{bmatrix} \\mathbf{a}_{1} \\\\ \\mathbf{a}_{2} \\\\ \\vdots \\\\ \\mathbf{a}_{m} \\end{bmatrix} \\begin{bmatrix} \\mathbf{b}_{1} \u0026amp; \\mathbf{b}_{2} \u0026amp; \\cdots \u0026amp; \\mathbf{b}_{n} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{a}_{1} \\mathbf{b}_{1} \u0026amp; \\mathbf{a}_{1} \\mathbf{b}_{2} \u0026amp; \\cdots \u0026amp; \\mathbf{a}_{1} \\mathbf{b}_{n} \\\\ \\mathbf{a}_{2} \\mathbf{b}_{1} \u0026amp; \\mathbf{a}_{2} \\mathbf{b}_{2} \u0026amp; \\cdots \u0026amp; \\mathbf{a}_{2} \\mathbf{b}_{n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\mathbf{a}_{m} \\mathbf{b}_{1} \u0026amp; \\mathbf{a}_{m} \\mathbf{b}_{2} \u0026amp; \\cdots \u0026amp; \\mathbf{a}_{m} \\mathbf{b}_{n} \\\\ \\end{bmatrix} $$\n要約 $A = \\begin{bmatrix} A_{1} \u0026amp; A_{2} \\\\ O \u0026amp; A_{3} \\end{bmatrix}$をブロック行列とする。その行列式に対して次が成り立つ。\n$$ \\det A = \\det A_{1} \\det A_{3} $$\n結論 ブロック行列$A = \\begin{bmatrix} A_{1} \u0026amp; A_{2} \\\\ O \u0026amp; I \\end{bmatrix}$の行列式は$\\det A_{1}$と同じである。 行と列に順列をとって次のような形に表せる行列$A$を簡約可能行列と呼び、その行列式$\\det A$は$\\det B \\det D$か$-\\det B \\det D$のどちらかである。 $$ \\widetilde{A} = \\begin{bmatrix} B \u0026amp; O \\\\ C \u0026amp; D \\end{bmatrix} $$ 証明 ブロック行列$A$を次のように3つのブロック行列の積に分解できる。\n$$ \\begin{align*} A \u0026amp;= \\begin{bmatrix} A_{1} \u0026amp; A_{2} \\\\ O \u0026amp; A_{3} \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} IA_{1} + OO \u0026amp; IA_{2} + OI \\\\ OA_{1} + A_{3}O \u0026amp; OA_{2} + A_{3}I \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} I \u0026amp; O \\\\ O \u0026amp; A_{3} \\end{bmatrix} \\begin{bmatrix} A_{1} \u0026amp; A_{2} \\\\ O \u0026amp; I \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} I \u0026amp; O \\\\ O \u0026amp; A_{3} \\end{bmatrix} \\begin{bmatrix} IA_{1} + A_{2}O \u0026amp; IO + A_{2}I \\\\ OA_{1} + IO \u0026amp; OO + II \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} I \u0026amp; O \\\\ O \u0026amp; A_{3} \\end{bmatrix} \\begin{bmatrix} I \u0026amp; A_{2} \\\\ O \u0026amp; I \\end{bmatrix} \\begin{bmatrix} A_{1} \u0026amp; O \\\\ O \u0026amp; I \\end{bmatrix} \\end{align*} $$\n積の行列式は行列式の積と同じであるから\n$$ \\begin{align*} \\det A \u0026amp;= \\det \\left( \\begin{bmatrix} I \u0026amp; O \\\\ O \u0026amp; A_{3} \\end{bmatrix} \\begin{bmatrix} I \u0026amp; A_{2} \\\\ O \u0026amp; I \\end{bmatrix} \\begin{bmatrix} A_{1} \u0026amp; O \\\\ O \u0026amp; I \\end{bmatrix} \\right) \\\\ \u0026amp;= \\det \\left( \\begin{bmatrix} I \u0026amp; O \\\\ O \u0026amp; A_{3} \\end{bmatrix} \\right) \\det \\left( \\begin{bmatrix} I \u0026amp; A_{2} \\\\ O \u0026amp; I \\end{bmatrix} \\right) \\det \\left( \\begin{bmatrix} A_{1} \u0026amp; O \\\\ O \u0026amp; I \\end{bmatrix} \\right) \\end{align*} $$\n行列式のラプラス展開を考えると、\n$$ \\det \\left( \\begin{bmatrix} I \u0026amp; O \\\\ O \u0026amp; A_{3} \\end{bmatrix} \\right) = \\det A_{3},\\quad \\det \\left( \\begin{bmatrix} A_{1} \u0026amp; O \\\\ O \u0026amp; I \\end{bmatrix} \\right) = \\det A_{1}, $$\n$$ \\text{and} \\quad \\det \\left( \\begin{bmatrix} I \u0026amp; A_{2} \\\\ O \u0026amp; I \\end{bmatrix} \\right)=1 $$\nこれにより、\n$$ \\det A = \\det A_{1} \\det A_{3} $$\n■\n","id":3323,"permalink":"https://freshrimpsushi.github.io/jp/posts/3323/","tags":null,"title":"ブロック行列"},{"categories":"수리통계학","contents":"定義 1 仮説検定: $$ \\begin{align*} H_{0} :\u0026amp; \\theta \\in \\Theta_{0} \\\\ H_{1} :\u0026amp; \\theta \\in \\Theta_{0}^{c} \\end{align*} $$\nこのような仮説検定が与えられていて、$\\alpha \\in [0,1]$ とする。\nパラメータ $\\theta$ に対して、棄却域が $R$ の関数 $\\beta (\\theta) := P_{\\theta} \\left( \\mathbf{X} \\in \\mathbb{R} \\right)$ を 検定力関数Power Functionという。 $\\sup_{\\theta \\in \\Theta_{0}} \\beta (\\theta) = \\alpha$ の場合、与えられた仮説検定を サイズSize $\\alpha$ の仮説検定という。 $\\sup_{\\theta \\in \\Theta_{0}} \\beta (\\theta) \\le \\alpha$ の場合、与えられた仮説検定を レベルLevel $\\alpha$ の仮説検定という。 説明 パワー？ 数学でパワーと言えば、冪乗 powや冪を覆う、べき字を使って冪関数 $f(x) = x^{-\\alpha}$ と言っていることが多いけど、統計学の文脈では、ただ仮説検定の検定する力Powerと考えればいい。\n検定力？ $\\beta$ は確率 $P_{\\theta}$ を通じて定義されるので、当然その値域は $[0,1]$ の部分集合だ。$\\beta (\\theta)$ の値が大きいということ―検定力、検定する力が強いということは、帰無仮説を棄却する力のことだ。対立仮説を棄却しても検定は検定だろうか？という疑問が生じるかもしれないが、本来どんな仮説検定でも、帰無仮説を基準に話すことが多いし、$R$ が帰無仮説の棄却域なので、検定とは帰無仮説が棄却されるか否かだけを気にすればいいのだ。\nこの検定力関数を通じて仮説検定の良し悪しを評価する。もっといい検定方法があれば、それを より強力なMore Powerfulと言うが、ここでの表現から、一般の数学のPowerと違って、本当に力を意味していることがわかる。数理統計学の観点から、どんな仮説検定が合理的か、効率的かを考えるのはとても自然な動機だ。\nただ、単に検定力そのものを良し悪しの指標とするわけにはいかない。例えば、どんなサンプルが入ってきても帰無仮説を棄却してしまう $\\beta (\\theta) = 1 = 100 \\%$ を考えてみれば、検定力自体はとても強いが、強力すぎて第一種の過誤（帰無仮説が真の場合に棄却してしまう過誤）を全く捕らえることができない。\nサイズとレベル 通常はサイズとレベルという言葉を区別せずに使うことが多いが、定義された場合、自然にレベル $\\alpha$ テストの集合がサイズ $\\alpha$ テストの集合を含む。この違いを細かく考察して研究する際は、用語を厳密に区別して使うべきだ。\n$\\alpha$ の意味は？サイズであれレベルであれ、$\\alpha$ が高いとは、帰無仮説が真のときに棄却される確率が大きいパラメータがあるということだ。$\\alpha$ が大きければ大きいほど、寛容に帰無仮説を棄却し、もし$\\alpha$ がとても小さければ、非常に保守的な検定となる。このような違いは棄却域によって生じる。一方で レベルLevelとこの説明で 有意水準Critical Levelが頭に浮かぶのは自然だけど、結局は別の話で、無理に結びつける必要もなく、事実結びつけてはいけない。概念的に受け入れよう。\n例: 正規分布 $$ \\begin{align*} H_{0} :\u0026amp; \\theta \\le \\theta_{0} \\\\ H_{1} :\u0026amp; \\theta \u0026gt; \\theta_{0} \\end{align*} $$ 分散が既知の正規分布 $N \\left( \\theta , \\sigma^{2} \\right)$ のランダムサンプル $X_{1} , \\cdots , X_{n}$ に対する上記のような仮説検定を考えると、zスコアがある定数 $c$ よりも大きければ、帰無仮説を棄却できるだろう。検定力関数 $\\beta$ は、$\\displaystyle {{ \\bar{X} - \\theta_{0} } \\over { \\sigma / \\sqrt{n} }}$ がある確率 $P$ を$\\theta$ に関する式に変換することで求めることができる。 $$ \\begin{align*} \\beta \\left( \\theta \\right) =\u0026amp; P_{\\theta} \\left( {{ \\bar{X} - \\theta_{0} } \\over { \\sigma / \\sqrt{n} }} \u0026gt; c \\right) \\\\ =\u0026amp; P_{\\theta} \\left( {{ \\bar{X} - \\theta } \\over { \\sigma / \\sqrt{n} }} \u0026gt; c + {{ \\theta_{0} - \\theta } \\over { \\sigma / \\sqrt{n} }} \\right) \\\\ =\u0026amp; P_{\\theta} \\left( Z \u0026gt; c + {{ \\theta_{0} - \\theta } \\over { \\sigma / \\sqrt{n} }} \\right) \\end{align*} $$ ここで、$\\displaystyle Z := {{ \\bar{X} - \\theta_{0} } \\over { \\sigma / \\sqrt{n} }}$ は標準正規分布に従う確率変数だ。\nCasella. (2001). Statistical Inference(2nd Edition): p383, 385.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2291,"permalink":"https://freshrimpsushi.github.io/jp/posts/2291/","tags":null,"title":"仮説検定の検定力関数"},{"categories":"선형대수","contents":"要約1 $S$を有限次元ベクター空間$V$の有限部分集合とする。\n(a) $S$が$V$を生成しつつ$V$の基底ではない場合、$S$の要素を適切に削除して$V$の基底に縮小することができる。\n(b) $S$が線形独立だが$V$の基底ではない場合、$S$に適切に要素を追加して$V$の基底に拡張することができる。\n系 $W \\le V$を$V$の$n$次元ベクター空間の部分空間とする。$\\gamma = \\left\\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{k} \\right\\}$を$W$の基底とする。すると、$\\gamma$に適切な要素を追加して$V$の基底$\\beta = \\left\\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{k}, \\mathbf{v}_{k+1}, \\dots, \\mathbf{v}_{n} \\right\\}$に拡張できる。\n証明 (a) $\\span(S) = V$だが、$S$が$V$の基底でない場合、それは$S$が線形従属であることを意味する。したがって、$S$のあるベクトル$\\mathbf{v}_{1}$は他のベクトルの線形結合で表すことができる。加法/減法の定理により、$S \\setminus \\left\\{ \\mathbf{v}_{1} \\right\\}$も同様に$V$を生成する。$S \\setminus {\\mathbf{v}_{1}}$が線形独立なら証明は完了である。線形独立ではない場合、同じ論理で$V$を生成する$S \\setminus \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2} \\right\\}$を考えることができる。この過程を繰り返すと、$S$から適切な要素を削除して$V$の基底となる集合を得る。\n(b) $\\dim(V) = n$と仮定する。$S$が線形独立だが$V$の基底ではない場合、それは$S$が$V$を生成しないことを意味する。そして、加法/減法の定理により、あるベクトル$\\mathbf{v}_{1} \\notin \\span(S)$を$S$に追加しても、$S \\cup \\left\\{ \\mathbf{v}_{1} \\right\\}$は引き続き線形独立である。この方法を繰り返して、要素の数が$n$である線形独立集合を$S$に適切なベクトルを追加して得ることができる。$n$次元ベクター空間で要素が$n$個の線形独立集合は基底であるため、証明は完了である。\n■\nHoward Anton, Elementary Linear Algebra: Aplications Version (12th Edition, 2019), p251-254\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3321,"permalink":"https://freshrimpsushi.github.io/jp/posts/3321/","tags":null,"title":"基底の拡張と縮小"},{"categories":"줄리아","contents":"概要 JuliaとNumPy、PyTorch（以降、便宜上Pythonと呼ぶ）の高次元配列を扱う際、各次元が意味するものが異なるため注意が必要だ。この違いはJuliaの配列が列優先であり、Pythonの配列が行優先であるために生じる。ちなみに同じ列優先のMatlabはJuliaとの違いがないため、Matlabに慣れているユーザーは特に注意する必要はないが、Pythonに慣れている人はインデックスの間違いに注意しよう。\n配列の次元とベクトルの次元を混同して使っているので、しっかり理解しよう。 説明 1次元配列 Juliaでは、サイズが$n$の配列は$n$次元の列ベクトルを意味する。\njulia\u0026gt; ones(3)\r3-element Vector{Float64}:\r1.0\r1.0\r1.0 Pythonでは、サイズが$n$の配列は$n$次元の行ベクトルを意味する。\n\u0026gt;\u0026gt;\u0026gt; import numpy as np\r\u0026gt;\u0026gt;\u0026gt; np.ones(3)\rarray([1., 1., 1.]) 列か行かの違いはあるが、1次元配列であるため、インデックスに特に注意する点はない。\n2次元配列 表面上は2次元までは違いがないように見える。しかし、その意味は異なるので注意が必要だ。まず、Juliaでは配列の次元が後方に伸びる。つまり$(m,n)$配列とは、サイズが$m$の1次元配列（列ベクトル）が$n$個あることを意味する。具体的には$(3,2)$配列は、3次元の列ベクトルが2個あるということだ。\njulia\u0026gt; ones(3,2)\r3×2 Matrix{Float64}:\r1.0 1.0\r1.0 1.0\r1.0 1.0 また、Juliaは「列優先」なので、成分のインデックスは上から下へ先に、そして左から右に大きくなる。\njulia\u0026gt; A = reshape(range(1,6), (3,2))\r3×2 reshape(::UnitRange{Int64}, 3, 2) with eltype Int64:\r1 4\r2 5\r3 6\rjulia\u0026gt; for i ∈ 1:6\rprintln(A[i])\rend\r1\r2\r3\r4\r5\r6 一方、Pythonの配列では新しい次元が前方へ伸びる。つまり$(m,n)$配列とは、サイズが$n$の1次元配列（行ベクトル）が$m$個あるということだ。以下の結果からは、配列が行単位で区分されているのがわかる。\n\u0026gt;\u0026gt;\u0026gt; np.ones([3,2])\rarray([[1., 1.],\r[1., 1.],\r[1., 1.]]) つまり、見た目だけでは、JuliaとPythonの$(m,n)$配列はどちらも$m \\times n$の行列だが、列優先/行優先の違いのためにインデックスの順序が異なる。インデックスの方向はJuliaでは上下左右、Pythonでは左右上下だ。\n# julia에서 2차원 배열의 인덱싱은 위에서 아래로, 그 다음 좌에서 우로\rjulia\u0026gt; A = reshape(range(1,6), (3,2))\r3×2 reshape(::UnitRange{Int64}, 3, 2) with eltype Int64:\r1 4\r2 5\r3 6\r# python에서 2차원 배열의 인덱싱은 좌에서 우로, 그 다음 위에서 아래로 \u0026gt;\u0026gt;\u0026gt; np.arange(6).reshape(3,2)\rarray([[0, 1],\r[2, 3],\r[4, 5]]) 3次元配列 Juliaでは、配列の新しい次元が後ろに追加されると言った。従って、$(m,n,k)$配列は、$(m,n)$配列が$k$個あることを意味する。\njulia\u0026gt; ones(3,2,4)\r3×2×4 Array{Float64, 3}:\r[:, :, 1] =\r1.0 1.0\r1.0 1.0\r1.0 1.0\r[:, :, 2] =\r1.0 1.0\r1.0 1.0\r1.0 1.0\r[:, :, 3] =\r1.0 1.0\r1.0 1.0\r1.0 1.0\r[:, :, 4] =\r1.0 1.0\r1.0 1.0\r1.0 1.0 一方、Pythonでは$(m,n,k)$配列は、$(n,k)$配列が$m$個あるということだ。\n\u0026gt;\u0026gt;\u0026gt; np.ones([3,2,4])\rarray([[[1., 1., 1., 1.],\r[1., 1., 1., 1.]],\r[[1., 1., 1., 1.],\r[1., 1., 1., 1.]],\r[[1., 1., 1., 1.],\r[1., 1., 1., 1.]]]) 機械学習で 画像データで、$H=\\text{hieht}$が高さ、$W=\\text{width}$が幅、$C=\\text{channel}$がチャンネル数、$B=\\text{batch size}$がバッチサイズとした場合、PyTorchでは$(B,C,H,W)$配列であり、Juliaでは$(H,W,C,B)$である。\n環境 OS: Windows11 バージョン: Julia 1.7.1, Python 3.9.2, numpy 1.19.5 ","id":3315,"permalink":"https://freshrimpsushi.github.io/jp/posts/3315/","tags":null,"title":"ジュリア、Python（NumPy、PyTorch）の配列の次元の違い"},{"categories":"확률론","contents":"定義 状態空間が可算集合の確率過程 $\\left\\{ X_{t} \\right\\}$ が与えられているとする。\n二つの時点 $t_{1} \u0026lt; t_{2}$ に対して、遷移確率 $p_{ij} \\left( t_{1} , t_{2} \\right)$ を次のように定義する。 $$ p_{ij} \\left( t_{1} , t_{2} \\right) := P \\left( X_{t_{2}} = j \\mid X_{t_{1}} = i \\right) $$\nこの時、（現在の）状態を意味する $i$ をソースステート、目的状態を意味する $j$ をターゲットステートという。特に離散的確率過程 $\\left\\{ X_{t} \\right\\}_{t \\in \\mathbb{N}}$ において $t_{1} = n \\in \\mathbb{N}$ かつ $t_{2} = n + k \\in \\mathbb{N}$ の場合、その遷移確率は次のように簡単に示されることもある。 $$ \\begin{align*} p_{ij}^{(k)} :=\u0026amp; P \\left(n + k = j \\mid X_{n} = i \\right) \\\\ p_{ij} :=\u0026amp; p_{ij}^{(1)} \\end{align*} $$ 時点に関わらず、遷移確率が区間 $\\Delta t = t_{2} - t_{1}$ にのみ依存している場合、つまり、以下の条件を満たす場合は定常あるいは同質の遷移確率という。 $$ p_{ij} (\\Delta t) := \\left( X_{t_{2} - t_{1}} = j \\mid X_{0} = i \\right) $$ 定常遷移確率に対して、次のように定義された行列関数 $P(t)$ と $P^{(k)}$ を遷移確率行列という。 $$ \\begin{align*} \\left( P(t) \\right)_{ij} :=\u0026amp; \\left( p_{ij} (t) \\right) \\\\ \\left( P^{(k)} \\right)_{ij} :=\u0026amp; \\left( p_{ij}^{(k)} \\right) \\end{align*} $$ 連続的確率過程の遷移確率行列 $P(t)$ が微分可能な行列関数であるとする。次のように定義された行列 $$ Q := P\u0026rsquo; (0) $$\nを微分素行列といい、その成分 $\\left( Q \\right)_{ij}$ を遷移率という。 参照 チャップマン-コルモゴロフ方程式 ","id":2284,"permalink":"https://freshrimpsushi.github.io/jp/posts/2284/","tags":null,"title":"確率過程の遷移確率"},{"categories":"수리통계학","contents":"定義 1 パラメーターに関する命題を仮説Hypothesisという。 与えられたサンプルに基づき仮説$H_{0}$を真と受け入れるか、仮説$H_{0}$を棄却して$H_{1}$を採用する問題を仮説検定Hypothesis Testという。仮説検定では補足的な仮説$H_{0}$、$H_{1}$をそれぞれ帰無仮説Null Hypothesis、対立仮説Alternative Hypothesisと呼ぶ。 帰無仮説$H_{0}$を棄却するような、サンプル空間 $\\Omega$の部分集合$R \\subset \\Omega$を棄却域Rejection Regionという。 説明 統計学科でなくても、新入生レベルの統計学に触れることで仮説検定についての説明を受ける。それだけで十分と感じる人も多いが、上の定義はできるだけ数学的に、あいまいさなく、精密に仮説検定を説明している。\n以下の説明は、読者がすでに仮説検定というコンセプトにある程度慣れているという前提で書かれている。数理統計的に概念を掴もう。\n仮設 定義によれば、仮説はただの\u0026rsquo;言葉\u0026rsquo;ではなく、命題である。パラメーターに関する命題であるという言及が重要である。例えば「正規性検定」のようにその命名自体はパラメーターではなく分布そのものに対する検定のように見える場合でも、詳しく見ると必ずパラメーターが隠されている。例として、ハーク-ベラ・テストは正規性検定であり、実は歪度と尖度を通じて仮説検定を行う。\n仮説検定 「採用する」と下線が引かれているが、注意を促す意味である。ご存知の通り、ほとんどの教科書ではRejectとAcceptという表現を両方使用するが、ほとんどの教授は「採用」という表現に注意を促す。対立仮説を採用するとは、実際に対立仮説を真と受け入れるというよりは、帰無仮説を棄却したということであり、帰無仮説を真と受け入れるということも、帰無仮説を棄却できないということであって、積極的に「採用」するという表現は避ける方が良い。\n帰無仮説と対立仮説を定義する際に、補足的という表現を使ったが、これも$H_{0}$と$H_{1}$が必ずしも論理的否定ではないことを強調する表現である。仮説検定で帰無仮説が真ではないということは、必ずしも対立仮説が真であるという結論にはつながらない。もっと実践的に説明すると、帰無仮説と共存できなければ対立仮説が十分である。例えば、 $$ H_{0} : \\theta = 0 \\\\ H_{1} : \\theta \u0026lt; 0 $$ のような仮説検定は全く問題ないが、 $$ H_{0} : \\theta \\in [-1,0] \\\\ H_{1} : \\theta \\in [0,+1] $$ のような場合は$\\theta = 0$の時、帰無仮説と対立仮説が両方真である可能性があるため、問題がある。\n棄却域 定義によれば、棄却域は事象である。仮説検定を一回の試行と見なすならば、$H_{0}$が棄却される確率は、棄却という事象が発生する確率と同じである。この確率がかなり低く、例えば$\\alpha = 0.05$よりも低いにもかかわらず、発生した場合、それは普通の出来事ではなく、注目すべき事象と見なすことができる。こうしたストーリーテリングから、有意水準（p-value）のような概念が自然に思い浮かぶかもしれない。\n参照 仮説検定の簡単な定義: 厳密さよりも、適度に受け入れやすい定義を紹介する。 帰無仮説と対立仮説の決定方法: その定義にどんな問題があるか説明する。 仮説検定の厳しい定義: 比較的厳密な数理統計的な仮説検定の定義を紹介する。 棄却域の簡単な定義 Casella. (2001)『Statistical Inference』(第2版): p373~374.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2283,"permalink":"https://freshrimpsushi.github.io/jp/posts/2283/","tags":null,"title":"数理統計的な仮説検定の定義"},{"categories":"기하학","contents":"定義1 $M$をリーマン多様体、$\\frak{X}(M)$を$M$の上のすべてのベクトル場の集合としよう。\n$$ \\frak{X}(M) = \\text{the set of all vector fileds of calss } C^{\\infty} \\text{ on } M $$\n$M$の曲率$R$は$X, Y \\in \\frak{X}(M)$を$R(X, Y) : \\frak{X}(M) \\to \\frak{X}(M)$に対応させる関数だ。この時、$R(X, Y)$は以下のように与えられる。\n$$ \\begin{equation} R(X, Y) Z = \\nabla_{Y} \\nabla_{X} Z - \\nabla_{X} \\nabla_{Y} Z + \\nabla_{[X,Y]}Z, \\quad Z \\in \\frak{X}(M) \\end{equation} $$\nこのような$R$をリーマン曲率またはリーマン曲率テンソルという。\n$\\nabla$は$M$上のレヴィチビタ接続、$[ \\cdot, \\cdot]$はリー括弧だ。\n説明 つまり、$R$は二つのベクトル場$X, Y$を$R(X, Y)$という関数に対応させ、さらに$R(X, Y)$はベクトル場$Z$を$(1)$のように対応させる関数だ。従って、実際には次のように表記しても問題はない。\n$$ R : \\frak{X}(M) \\times \\frak{X}(M) \\times \\frak{X}(M) \\to \\frak{X}(M) \\\\[1em] R(X,Y,Z) = \\nabla_{Y} \\nabla_{X} Z - \\nabla_{X} \\nabla_{Y} Z + \\nabla_{[X,Y]}Z, \\quad Z \\in \\frak{X}(M) $$\nしかし、$R(X,Y,Z)$の値からわかるように、これは$Z$にきれいにまとめられる。さらに$X, Y$は微分する変数として使われ、$Z$は微分される変数として使われるため、このような役割を区別するために、慣例的に$R(X, Y) Z$のように表記される。\nまた、定義$(1)$では教科書によって符号の違いがあるかもしれないが、本質的には同じである。\n重複して使用される表記法 リーマン曲率テンソル$R$について、$R: \\frak{X}(M) \\times \\frak{X}(M) \\times \\frak{X}(M) \\times \\frak{X}(M) \\to \\mathcal{D}(M)$を次のように定義する。\n$$ R(X, Y, Z, W) := g(R(X, Y)Z, W) = \\left\\langle R(X, Y)Z, W \\right\\rangle $$\nこれをリーマン曲率テンソルという。定義で紹介したのもリーマン曲率テンソル$R$であり、これもリーマン曲率テンソル$R$だ。このように重複して使う理由は、これらがメトリックを掛けた違いしかなく、事実上同じものであるためだ。\n座標系表現 $T_{p}M$の基底を$\\left\\{ X_{i} \\right\\}$としよう。$R(X_{i},X_{j})X_{k}$は次のように表記される。\n$$ R(X_{i},X_{j})X_{k} = \\sum_{s}R_{ijk}^{s}X_{s} $$\n$R(X_{i}, X_{j}, X_{k}, X_{l})$は次のように表記される。\n$$ R_{ijkl} = R(X_{i}, X_{j}, X_{k}, X_{l}) = g\\left( R(X_{i}, X_{j})X_{k}, X_{l} \\right) = \\sum_{s}R_{ijk}^{s}g_{sl} $$\n例：ユークリッド空間 $M = \\mathbb{R}^{n}$としよう。ユークリッド空間は曲がっていない平らな空間だ。従って、$R(X,Y)Z = 0$が出ることを期待する。逆にこの結果が出なければ、定義$(1)$は価値ある意味を持たないと言える。$X, Z$を次のように設定しよう。\n$$ X = (X^{1}, \\dots, X^{n}) = \\sum X^{i}\\dfrac{\\partial }{\\partial x_{i}} \\text{ and } Z = (Z^{1}, \\dots, Z^{n}) = \\sum Z^{k}\\dfrac{\\partial }{\\partial x_{k}} $$\n$\\nabla_{X}Z$は次のようである。\n$$ \\nabla_{X}Z = \\sum_{i,k} \\left( X^{i}\\dfrac{\\partial Z^{k}}{\\partial x_{i}} + \\sum_{j}X^{i}Z^{j}\\Gamma_{ij}^{k} \\right) \\dfrac{\\partial }{\\partial x_{k}} $$\nこの時、ユークリッド空間では$\\Gamma_{ij}^{k} = 0$なので、次を得る。\n$$ \\begin{align*} \\nabla_{X} Z \u0026amp;= \\sum_{i,k} X^{i} \\left( \\dfrac{\\partial Z^{k}}{\\partial x_{i}} \\right) \\dfrac{\\partial }{\\partial x_{k}} \\\\ \u0026amp;= \\sum_{k} \\left( \\sum_{i} X^{i} \\dfrac{\\partial Z^{k}}{\\partial x_{i}} \\right) \\dfrac{\\partial }{\\partial x_{k}} \\\\ \u0026amp;= \\sum_{k} X Z^{k} \\dfrac{\\partial }{\\partial x_{k}} \\\\ \u0026amp;= \\left( XZ^{1}, \\dots, XZ^{n} \\right) \\end{align*} $$\n同様に次を得る。\n$$ \\nabla_{Y} \\nabla_{X} Z = \\left( YXZ^{1}, \\dots, YXZ^{n} \\right) $$\n従って、\n$$ \\begin{align*} R(X, Y) Z \u0026amp;= \\nabla_{Y} \\nabla_{X} Z - \\nabla_{X} \\nabla_{Y} Z + \\nabla_{[X,Y]}Z \\\\ \u0026amp;= \\left( YXZ^{1}, \\dots, YXZ^{n} \\right) - \\left( XYZ^{1}, \\dots, XYZ^{n} \\right) \\\\ \u0026amp;\\quad + \\left( (XY-YX)Z^{1}, \\dots, (XY-YX)Z^{n} \\right) \\\\ \u0026amp;= 0 \\end{align*} $$\n■\n性質 (a) $R$は双線形である。すなわち、\n$$ \\begin{align*} R(f X_{1} + gX_{2}, Y_{1}) \u0026amp;= fR(X_{1}, Y_{1}) + gR(X_{2}, Y_{1}) \\\\[1em] R(X_{1}, fY_{1} + gY_{2}) \u0026amp;= fR(X_{1}, Y_{1}) + gR(X_{1}, Y_{2}) \\end{align*} $$\nここで$f, g \\in \\mathcal{D}(M)$、$\\quad X_{1}, X_{2}, Y_{1}, Y_{2} \\in \\frak{X}(M)$である。\n(b) 任意の$X, Y \\in \\frak{X}(M)$に対して、$R(X, Y)$は線形である。すなわち、\n$$ \\begin{align*} R(X, Y) (Z + W) \u0026amp;= R(X, Y) Z + R(X, Y) W \\\\[1em] R(X, Y) fZ \u0026amp;= f R(X, Y) Z \\end{align*} $$\nここで$f \\in \\mathcal{D}(M)$、$\\quad Z, W \\in \\frak{X}(M)$である。\n証明 (b) 最初の性質は接続の定義により自明である。従って、二行目のみを証明する。\n$$ R(X, Y) fZ = \\nabla_{Y} \\nabla_{X} fZ - \\nabla_{X} \\nabla_{Y} fZ + \\nabla_{[X,Y]}fZ $$\nまず最初の項を接続の定義により計算すると、\n$$ \\begin{align*} \\nabla_{Y} \\nabla_{X} (fZ) \u0026amp;= \\nabla_{Y}(f\\nabla_{X} Z + (Xf)Z) \\\\ \u0026amp;= \\nabla_{Y}(f\\nabla_{X}Z) + \\nabla_{Y}((Xf)Z) \\\\ \u0026amp;= f\\nabla_{Y}\\nabla_{X}Z + (Yf)\\nabla_{X}Z + (Xf)\\nabla_{Y}(Z) + (Y(Xf))Z \\end{align*} $$\n同様に計算すると、\n$$ \\nabla_{X}\\nabla_{Y}(fZ) = f\\nabla_{X}\\nabla_{Y}Z + (Xf)\\nabla_{Y}Z + (Yf)\\nabla_{X}(Z) + (X(Yf))Z $$\n従って、次を得る。\n$$ \\begin{align*} \\nabla_{Y} \\nabla_{X} (fZ) - \\nabla_{X}\\nabla_{Y}(fZ) \u0026amp;= {\\color{blue}f\\nabla_{Y}\\nabla_{X}Z} + {\\color{red}\\cancel{\\color{black}(Yf)\\nabla_{X}Z}} + {\\color{green}\\cancel{\\color{black}(Xf)\\nabla_{Y}Z}} + YXfZ \\\\ \u0026amp;\\quad - \\left( {\\color{blue}f\\nabla_{X}\\nabla_{Y}Z} + {\\color{green}\\cancel{\\color{black}(Xf)\\nabla_{Y}Z}} + {\\color{red}\\cancel{\\color{black}(Yf)\\nabla_{X}Z}} + XYfZ \\right) \\\\ \u0026amp;= {\\color{blue}f(\\nabla_{Y}\\nabla_{X}Z - \\nabla_{X}\\nabla_{Y}Z)} + YXfZ - XYfZ \\\\ \u0026amp;= f(\\nabla_{Y}\\nabla_{X}Z - \\nabla_{X}\\nabla_{Y}Z) - ([X,Y]f)Z \\end{align*} $$\nさらに、$\\nabla_{[X,Y]} fZ = f\\nabla_{[X,Y]}Z + ([X,Y]f)Z$であるため、最後に計算すると次のようになる。\n$$ \\begin{align*} R(X, Y) fZ \u0026amp;= \\nabla_{Y} \\nabla_{X} fZ - \\nabla_{X} \\nabla_{Y} fZ + \\nabla_{[X,Y]} fZ \\\\ \u0026amp;= f(\\nabla_{Y}\\nabla_{X}Z - \\nabla_{X}\\nabla_{Y}Z) - ([X,Y]f)Z + f\\nabla_{[X,Y]}Z + ([X,Y]f)Z \\\\ \u0026amp;= f(\\nabla_{Y}\\nabla_{X}Z - \\nabla_{X}\\nabla_{Y}Z) + f\\nabla_{[X,Y]}Z \\\\ \u0026amp;= f(\\nabla_{Y}\\nabla_{X}Z - \\nabla_{X}\\nabla_{Y}Z + \\nabla_{[X,Y]}Z) \\\\ \u0026amp;= fR(X, Y) Z \\end{align*} $$\n■\nManfredo P. Do Carmo, Riemannian Geometry (Eng Edition, 1992), p89-90\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3314,"permalink":"https://freshrimpsushi.github.io/jp/posts/3314/","tags":null,"title":"微分多様体の曲率"},{"categories":"머신러닝","contents":"概要 レファレンスと数式の番号や表記法は、論文をそのまま踏襲する。 Physics-informed neural networks (PINN[ピン]と読む)は、数値的に解くために設計された微分方程式の人工ニューラルネットワークであり、2018年Journal of Computational Physicsに発表された論文Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equationsで紹介されました。この論文の著者は、応用数学、機械工学のM. Raissi, P. Perdikaris, G.E. Karniadakisです。\nこの論文で述べられている物理情報physics informationは、壮大に見えるかもしれませんが、実際には与えられた微分方程式自体を意味すると考えても良いでしょう。つまり、\u0026lsquo;微分方程式を人工ニューラルネットワークで解く際、与えられた微分方程式を利用します\u0026rsquo;と言っているのと同じです。機械学習の論文を読むときは、このように見栄えの良い名前に惑わされないよう注意が必要です。\n微分方程式の数値的解法においてPINNが注目される理由は、損失関数に関するアイデアがシンプルで理解しやすく、実装も簡単だからでしょう。実際に論文の例では非常にシンプルなDNNが紹介されています。\n一般的に言われるPINNはSection 3.1で紹介されるモデルを指します。\n0. 抄録 著者はPINNを\u0026rsquo;与えられた非線形偏微分方程式を満たしながら、教師あり学習問題を解くために訓練された人工ニューラルネットワーク\u0026rsquo;と紹介しています。この論文で主に扱う2つの問題は、\u0026lsquo;data-driven solution and data-driven discovery of partial differential equations\u0026rsquo;です。性能評価のために、流体力学、量子力学、拡散方程式などの問題を解いてみました。\n1. 序論 最近の機械学習とデータ分析の進歩は、画像認識image recognition、認知科学cognitive science、ゲノム学genomicsなどの科学分野で革新的な結果をもたらしていますが、複雑な物理的、生物学的、工学的システムに対しては（データ収集コストが高いため）少ない情報で望ましい結果を導き出す必要がある困難があります。このような*小さなデータ領域small data regime*では、DNN、CNN、RNNなどの先進技術の収束性が保証されていません。\n[4-6]で、データ効率が良く（=少ないデータで）、物理情報を学習できる（=微分方程式を解くことができる）方法についての研\n究が進んでいます。非線形問題への拡張は、この論文の著者であるRaissiの後続研究[8,9]で提案されました。\n2. 問題設定 人工ニューラルネットワークで表される関数は、入力値（偏微分方程式でのソリューション$u$の座標$x, t$を指す）とパラメータによって関数値が決定されるが、これら2種類の変数に対して微分を行うために自動微分automatic differentiationを活用する。\nこのようなニューラルネットワークは、観測されたデータを支配する物理法則に起因する任意の対称性、不変性、または保存原理を尊重するように制約されている。これは、一般的な時間依存かつ非線形の偏微分方程式によってモデル化される。\nこの論文でこの文章が難しいと感じられるかもしれないが、私の考えでは簡単に言えば提案された人工ニューラルネットワークであるPINNが、与えられた微分方程式を満たす必要があるということだ。後述するが、微分方程式を満たす必要があるという条件を損失関数として使用するためである。\nこの論文の目的は、数理物理学におけるディープラーニングを進化させる新しいパラダイムのモデリングと計算パラダイムを提示することである。そのために、前述したように、この論文では主に2つの問題を扱う。一つは偏微分方程式のデータ駆動ソリューションdata-driven solutionであり、もう一つは偏微分方程式のデータ駆動発見data-driven discoveryである。使用された全てのコードとデータセットはhttps://github.com/maziarraissi/PINNsで確認できる。この論文では、$L1$、$L2$、ドロップアウトなどの正則化なしに、ハイパーボリックタンジェントを活性化関数として用いたシンプルなMLPが使用されている。各例では、ニューラルネットワークの構造、オプティマイザー、学習率などが具体的に紹介される。\nこの論文では、以下のようなパラメータ化された非線形偏微分方程式の一般的な形parameterized and nonlinear partial differential equations of the general formを扱う。\n$$ \\begin{equation} u_{t} + \\mathcal{N}[u; \\lambda] = 0,\\quad x \\in \\Omega,\\quad t \\in [0,T] \\end{equation} $$\nここで、$u=u(t,x)$は(1)を満たす隠れた(=与えられていない=知られていない)関数、つまり(1)のソリューションであり、$\\mathcal{N}[\\cdot; \\lambda]$は$\\lambda$でパラメータ化された非線形演算子（NonlinearのNに由来する）であり、$\\Omega \\subset \\mathbb{R}^{D}$である。多くの数理物理学の問題problems in mathematical physicsは上記のような形で表される。例えば、1次\n元粘性バーガース方程式を見てみよう。\n$$ u_{t} + uu_{x} = \\nu u_{xx} $$\nこれは(1)で$\\mathcal{N}[u; \\lambda] = \\lambda_{1} uu_{x} - \\lambda_{2}u_{xx}$、$\\lambda = (\\lambda_{1}, \\lambda_{2})$の場合である。与えられた方程式(1)に対して、扱うべき2つの問題はそれぞれ以下の通りである。\n偏微分方程式のデータ駆動ソリューション: 固定された$\\lambda$に対して、システムのソリューション$u(t,x)$は何か？ 偏微分方程式のデータ駆動発見: 観測されたデータを最もよく表現するパラメータ$\\lambda$は何か？ 3. 偏微分方程式のデータ駆動ソリューション セクション3では、以下の形式の偏微分方程式からデータに基づいたソリューションを見つける問題について扱う。\n$$ \\begin{equation} u_{t} + \\mathcal{N}[u] = 0,\\quad x \\in \\Omega,\\quad t \\in [0,T] \\end{equation} $$\nつまり、$(1)$でパラメータ $\\lambda$ が固定されている状況である。セクション3.1とセクション3.2ではそれぞれ連続時間モデルと離散時間モデルを扱う。方程式を見つける問題はセクション4で扱う。ここで言う\u0026rsquo;データ\u0026rsquo;の意味は以下で詳しく説明する。\n3.1. 連続時間モデル $(t,x) \\in \\mathbb{R} \\times \\mathbb{R}$ とすると、$u : \\mathbb{R}^{2} \\to \\mathbb{R}$ である。これを人工ニューラルネットワークで近似するが、次のように実装されるシンプルなMLPを使用する。Juliaでは、\nusing Flux\ru = Chain(\rDense(2, 10, relu),\rDense(10, 10, relu),\rDense(10, 1)\r) PyTorchでは、\nimport torch\rimport torch.nn as nn\rimport torch.nn.functional as F\rlayers = [2, 10, 10, 1]\rclass network(nn.Module):\rdef __init__(self):\rsuper(network, self).__init__()\rlayer_list = [nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)]\rself.linears = nn.ModuleList(layer_list)\rdef forward(self, tx):\ru = tx\rfor i in range(len(layers)-2):\ru = self.linears[i](u)\ru = F.relu(u)\ru = self.linears[-1](u)\rreturn u\ru = network() これで $u$ は入力ノードが $2$ つ、出力ノードが $1$ つの人工ニューラルネットワークとなる。$(2)$ の左辺を次のような関数 $f = f(t,x; u)$ として定義しよう。\n$$ \\begin{equation} f := u_{t} + \\mathcal{N}[u] \\end{equation} $$\nここで $u$ は人工ニューラルネットワークであるため、$f$ も隠れ層のパラメータを持つ一種の人工ニューラルネットワークである。上記のような $f$ を物理情報に基づいたニューラルネットワークphysics-informed neural network, PINNと呼ぶ。言い換えれば、与えられた偏微分方程式そのものである。$f$ に含まれる微分は自動微分で実装され、$u$ と同じパラメータを共有する。人工ニューラルネットワーク $u$ が $(2)$ のソリューションを適切に近似していれば、$f$ の関数値はどこでも $0$ であるべきだ。ここから、$ f \\to 0$ になるように人工ニューラルネットワークを学習させることが推測できる。\n$(t_{u}^{i}, x_{u}^{i})$ を初期値、境界値が定義された領域の点とする。\n$$ (t_{u}^{i}, x_{u}^{i}) \\in( \\Omega \\times \\left\\{ 0 \\right\\}) \\cup (\\partial \\Omega \\times [0, T]) $$\n$u_{\\ast}$ を実際のソリューションとすると、初期条件と境界条件が与えられたということは、次のような値が与えられたということと同じである。\n$$ \\left\\{ t_{u}^{i}, x_{u}^{i}, u^{i} \\right\\}_{i=1}^{N_{u}},\\quad u^{i} = u_{\\ast}(t_{u}^{i}, x_{u}^{i}) $$\n理論上はこれらの値を無限に持つことになるが、数値的な問題では有限の点のみを扱えるので、$N_{u}$ 個を持っているとする。人工ニューラルネットワーク $u$ は $(t_{u}^{i}, x_{u}^{i})$ を入力として受け取り、$u^{i}$ を出力する必要があるので、これらがそれぞれ入力と対応するラベルとなる。\n$$ \\text{input} = (t_{u}^{i}, x_{u}^{i}),\\qquad \\text{label} = u^{i} $$\nこれがPINNで学習する\u0026lsquo;データ\u0026rsquo;である。それでは、損失関数を次のように設定できる。\n$$ MSE_{u} = \\dfrac{1}{N_{u}} \\sum\\limits_{i=1}^{N_{u}} \\left| u(t_{u}^{i},x_{u}^{i}) - u^{i} \\right|^{2} $$\nまた、$f$は適切な点集合（理論的には解 $u_{\\ast}$ が定義されるすべての点で満たされるべきだが、数値的には有限の点しか扱えない）$\\left\\{ t_{f}^{i}, x_{f}^{i} \\right\\}_{i=1}^{N_{f}}$で$(2)$を満たさなければならない。これらの適切な点を論文ではコロケーションポイントcollocation pointsと呼ぶ。コロケーションポイントに対して以下の損失関数を設定する。\n$$ MSE_{f} = \\dfrac{1}{N_{f}}\\sum\\limits_{i=1}^{N_{f}} \\left| f(t_{f}^{i}, x_{f}^{i}) \\right|^{2} $$\nつまり、$MSE_{f}$が$0$に近づくことは、物理的情報（偏微分方程式）を満たすことを意味する。したがって、人工ニューラルネットワーク $u$ を訓練するための最終的な損失関数は以下の通りである。\n$$ MSE = MSE_{u} + MSE_{f} $$\n論文では、$MSE_{f}$を使用することで物理的情報を制約として設けることは、[15, 16]で初めて研究されたが、PINN論文ではこれを現代的な計算ツールで検討し、より困難なダイナミックシステムに適用したと説明されている。\n物理情報に基づく機械学習physics-informed machine learningという用語自体は、Wangの乱流モデリングturbulence modelingに関する研究[17]で初めて使用されたとされる。しかし、PINN以前の研究では、サポートベクターマシン、ランダムフォレスト、FNNなどの機械学習アルゴリズムが単に使用されていたと説明されている。PINNがこれらと区別される点は、一般的に機械学習に使用されるパラメータに対する微分だけでなく、解の座標 $x, t$ に関する微分も考慮している点である。つまり、パラメータ $w$ を持つ人工ニューラルネットワークで近似された解を $u(t,x; w)$ とするとき、以前に提案された方法は偏微分 $u_{w}$ のみを利用したが、PINNは $u_{t}$ や $u_{x}$ などを利用して解を求める。このようなアプローチにより、少量のデータでも解をうまく見つけることができると説明されている。\nこの手続きがグローバル最小値に収束するという理論的な保証はないにもかかわらず、与えられた偏微分方程式が適切に定義されており、その解が一意であり、十分に表現力のあるニューラルネットワークアーキテクチャと十分な数のコロケーションポイント $N_{f}$ が与えられている場合、我々の方法は良好な\n予測精度good prediction accuracyを達成することが経験的に確認されていると論文には述べられている。\n3.1.1. 例（シュレーディンガー方程式） この例では、周期的な境界条件と複素数値を取る解に対して、提案された方法がうまく機能するかを重点的に確認する。例として、以下の初期条件と境界条件が与えられるシュレーディンガー方程式を扱う。\n$$ \\begin{align*} ih_{t} + 0.5h_{xx} + \\left| h \\right|^{2}h \u0026amp;= 0,\\quad x\\in [-5, 5], t\\in[0, \\pi/2], \\\\ h(0,x) \u0026amp;= 2\\operatorname{sech} (x), \\\\ h(t,-5) \u0026amp;= h(t,5), \\\\ h_{x}(t,-5) \u0026amp;= h_{x}(t,5) \\end{align*} $$\n問題の解 $h_{\\ast}(t,x)$ は $h_{\\ast} : [0, \\pi/2] \\times [-5, 5] \\to \\mathbb{C}$ として複素関数値を持つ。しかし、関数の出力が複素数になるように人工ニューラルネットワークを定義するのではなく、実部を担当する $u(t,x)$ と虚部を担当する $v(t,x)$ の2次元ベクトルが出力されるように定義する。簡単に言えば、入力と出力のノードがそれぞれ2つのMLPとして定義することである。\n$$ h(t,x) = \\begin{bmatrix} u(t,x) \\\\[0.5em] v(t,x) \\end{bmatrix} $$\nこの問題におけるPINN $f$ は以下の通りである。\n$$ f := ih_{t} + 0.5h_{xx} + \\left| h \\right|^{2} h $$\n$h(t,x)$ と $f(t,x)$ のパラメータは、初期値に対する損失 $MSE_{0}$、境界値に対する損失 $MSE_{b}$、物理情報に対する損失 $MSE_{f}$ を最小化するように学習される。\n$$ MSE = MSE_{0} + MSE_{b} + MSE_{f} $$\n$$ \\begin{align*} \\text{where } MSE_{0} \u0026amp;= \\dfrac{1}{N_{0}}\\sum_{i=1}^{N_{0}} \\left| h(0, x_{0}^{i}) - h_{0}^{i} \\right|^{2} \\quad (h_{0}^{i} = 2\\operatorname{sech} (x_{0}^{i})) \\\\ MSE_{b} \u0026amp;= \\dfrac{1}{N_{b}}\\sum_{i=1}^{N_{b}} \\left( \\left| h(t_{b}^{i}, -5) - h(t_{b}^{i}, 5) \\right|^{2} + \\left| h_{x}(t_{b}^{i},-5) - h_{x}(t_{b}^{i},5) \\right|^{2} \\right) \\\\ MSE_{f} \u0026amp;= \\dfrac{1}{N_{f}} \\sum\\limits_{i=1}^{N_{f}} \\left| f(t_{f}^{i}, x_{f}^{i}) \\right|^{2} \\end{align*} $$\n論文には $MSE_{b}$ の式にタイプミスがあるので注意すること。 ここで、$\\left\\{ x_{0}^{i}, h_{0}^{i} \\right\\}_{i=1}^{N_{0}}$ は初期値データ、$\\left\\{ t_{b}^{i} \\right\\}_{i=1}^{N_{b}}$ は境界でのコロケーションポイント、$\\left\\{ t_{f}^{i}, x_{f}^{i} \\right\\}_{i=1}^{N_{f}}$ は $f$ に対するコロケーションポイントである。\nデータセットを生成するために、従来のスペクトルメソッドspectral methodsを使用した。$h(0,x)$ での初期値データの数は $N_{0} = 50$、境界値データの数は $N_{b} = 50$ とし、ランダムに選んだ。また、$f$ のコロケーションポイントの数は $N_{f} = 20,000$ である。人工ニューラルネットワークは、100個のノードを持つ線形層を5層、層間の活性化関数としてハイパーボリックタンジェント $\\tanh$ を使用して構築した。\nFigure 1.\nFigure 1では、上の図は予測された解 $\\left| h(t, x) \\right|$ のヒートマップを示している。下の図は、時間がそれぞれ $t = 0.59, 0.79, 0.98$ のときの予測された解と実際の解がどれだけ一致しているかを示している。相対的 $L_{2}$ ノルムrelative $L_{2}$-norm は $0.00197 = 1.97 \\cdot 10^{-3}$ で、予測された解が正確な解と比較して約 $0.02%$ の差異があることを意味している。したがって、PINNは少ない初期値データでシュレーディンガー方程式の非線形挙動を正確に捉えることができる。\n現在扱っている連続時間モデルは、初期値が少なくてもうまく機能するが、コロケーションポイントの数 $N_{f}$ が十分に多くなければならないという潜在的な制約がある。これは空間の次元が2以下の場合はあまり問題にならないが、高次元の場合、必要なコロケーションポイントの数が指数関数的に増加する可能性があるため、問題になる可能性がある。そのため、次のセクションでは、多くのコロケーションポイントを必要としないようにするために、古典的なルンゲ・クッタ時間ステップスキームを活用した、より構造化されたニューラルネットワークを提案する。\n3.2. 離散時間モデル セクション3.1では、解を連続時間に対して近似した。この場合、人工ニューラルネットワークは全体の領域に対して同時に学習され、任意の点 $(x,t)$ に対して出力がある。このセクションでは、セクション3.1とは異なり、離散時間について扱う。つまり $t_{n}$ の値を知っているとき、$t_{n+1}$ の値を人工ニューラルネットワークで近似する方法について説明する。$(2)$ に $q$ ステージのルンゲ・クッタ法を適用すると、以下のようになる。\n$$ u(t_{n+1}, x) = u(t_{n}, x) - \\Delta t \\sum_{j=1}^{q} b_{j}\\mathcal{N}\\left[ u(t_{n}+c_{j} \\Delta t, x) \\right] $$\nここで $u^{n}(x) = u(t_{n}, x)$, $u^{n+c_{j}} = u(t_{n} + c_{j}\\Delta t, x)$ と表記すると、\n$$ \\begin{equation} \\begin{aligned} u^{n+1} \u0026amp;= u^{n} - \\Delta t \\sum_{j=1}^{q} b_{j}\\mathcal{N}\\left[ u^{n+c_{j}}\\right] \\\\ \\text{where } u^{n+c_{j}} \u0026amp;= u^{n} - \\Delta t \\sum_{i=1}^{q} a_{j,i}\\mathcal{N}\\left[ u^{n+c_{i}}\\right] \\quad j=1,\\dots,q \\end{aligned}\\tag{7} \\end{equation} $$\n上記の $q+1$ 個の式で、右辺の $\\sum$ 項を全て左辺に移行する。そして、左辺を $u_{i}^{n}$ のように表記する。\n$$ \\begin{equation} \\begin{aligned} u_{q+1}^{n} \u0026amp;:= u^{n+1} + \\Delta t \\sum_{j=1}^{q} b_{j}\\mathcal{N}\\left[ u^{n+c_{j}}\\right] = u^{n} \\\\ \\\\ u_{1}^{n} \u0026amp;:= u^{n+c_{1}} + \\Delta t \\sum_{i=1}^{q} a_{1,i}\\mathcal{N}\\left[ u^{n+c_{i}}\\right] = u^{n} \\\\ u_{2}^{n} \u0026amp;:= u^{n+c_{2}} + \\Delta t \\sum_{i=1}^{q} a_{2,i}\\mathcal{N}\\left[ u^{n+c_{i}}\\right] = u^{n} \\\\ \u0026amp;\\vdots \\\\ u_{q}^{n} \u0026amp;:= u^{n+c_{q}} + \\Delta t \\sum_{i=1}^{q} a_{q,i}\\mathcal{N}\\left[ u^{n+c_{i}}\\right] = u^{n} \\end{aligned}\\tag{9} \\end{equation} $$\nすると、これらの全ての値が $u^{n}$ に等しくなることがわかる。\n$$ u^{n} = u_{1}^{n} = u_{2}^{n} = \\cdots = u_{q+1}^{n} \\tag{8} $$\nしたがって、セクション3.2で言及されている物理情報とは、与えられた初期条件・境界条件と $(8)$ を指す。今度は $u(t_{n+1}, x)$ を求めるために二つの人工ニューラルネットワークを定義する。セクション3.1で使用した人工ニューラルネットワークは、正確な解 $u_{\\ast}$ に収束することを期待する $u$ と、$u$ が満たすべき微分方程式 $f$ だったが、ここでは少し異なる。まず、人工ニューラルネットワーク $U$ を次の関数として定義する。\n$$ U : \\mathbb{R} \\to \\mathbb{R}^{q+1} $$\nつまり、入力層のノードが $1$ つ、出力層のノードが $q+1$ つのニューラルネットワークである。このニューラルネットワークの出力を以下の値とする。\n$$ U(x) = \\begin{bmatrix} u^{n+c_{1}}(x) \\\\[0.5em] u^{n+c_{2}}(x) \\\\ \\vdots \\\\[0.5em] u^{n+c_{q}}(x) \\\\[0.5em] u^{n+1}(x) \\end{bmatrix} \\tag{10} $$\nこのニューラルネットワークは、添付されたコード内の PhysicsInformedNN クラスで定義されている neural_net に該当する。\nつまり、以下の学習プロセスで $U$ の出力の最後の成分が $u(t_{n+1}, x)$ に収束することを期待している。二番目のニューラルネットワークは、$U$ の出力と $(7)$ の定義を利用して、次のように定義される関数である。\n3.2.1. 例（アレン・カーン方程式） 離散時間モデルにおける例として、以下の初期条件と周期的な境界条件が与えられるアレン・カーン方程式を取り上げる。\n$$ \\begin{equation} \\begin{aligned} \u0026amp;u_{t} - 0.0001u_{xx} + 5 u^{3} - 5u = 0,\\qquad x\\in [-1, 1], t\\in[0, 1], \\\\ \u0026amp;u(0,x) = x^{2} \\cos (\\pi x), \\\\ \u0026amp;u(t,-1) = u(t,1), \\\\ \u0026amp;u_{x}(t,-1) = u_{x}(t,1) \\end{aligned}\\tag{12} \\end{equation} $$\nこの例における $(9)$ に含まれる非線形演算子は以下の通りである。\n$$ \\mathcal{N}[u^{n+c_{j}}] = -0.0001u_{xx}^{n+c_{j}} + 5(u^{n+c_{j}})^{3} - 5u^{n+c_{j}} $$\nタイムステップ $t^{n}$ での $u$ の値を $u^{n,i}$ と表記する。\n$$ u^{n,i} = u^{n}(x^{n,i}) = u(t^{n}, x^{n,i}),\\qquad i=1,\\dots,N_{n} $$\n問題は $u^{n}$ が与えられたときに $u^{n+1}$ を計算することであり、$\\left\\{ x^{n,i}, u^{n,i} \\right\\}_{i=1}^{N_{n}}$ は与えられたデータセットである。$(8)$ により、このデータセットに対して以下が成り立つ。\n$$ u^{n,i} = u_{1}^{n}(x^{n,i}) = \\cdots = u_{q+1}^{n}(x^{n,i}) $$\nしたがって、以下のような二乗誤差の合計sum of squared error (SSE)を損失関数とする。\nここではなぜ $MSE$ ではなく $SSE$ を使用するのかは明確ではない。連続時間モデルでは $MSE$ を使用していたが、離散時間モデルでは $SSE$ を使用しており、何らかの実験的な理由があると思われる。 $$ SSE_{n} = \\sum\\limits_{j=1}^{q+1} \\sum\\limits_{i=1}^{N_{n}} \\left| u_{j}^{n} (x^{n,i}) - u^{n,i} \\right|^{2} $$\n各 $u_{j}^{n}$ は $(9)$ によって計算され、この計算にはニューラルネットワーク $U$ の出力が使用される。このロスは、添付されたコード内の PhysicsInformedNN クラスで定義されている net_U0 に対応する。そして $U$ の出力は $(12)$ の境界条件を満たさなければならないため、以下のような損失関数を設定する。\n$$ \\begin{align*} SSE_{b} \u0026amp;= \\sum\\limits_{i=1}^{q} \\left| u^{n+c_{i}}(-1) - u^{n+c_{i}}(1) \\right|^{2} + \\left| u^{n+1}(-1) - u^{n+1}(1) \\right|^{2} \\\\ \u0026amp;\\quad+ \\sum\\limits_{i=1}^{q} \\left| u_{x}^{n+c_{i}}(-1) - u_{x}^{n+c_{i}}(1) \\right|^{2} + \\left| u_{x}^{n+1}(-1) - u_{x}^{n+1}(1) \\right|^{2} \\ \\end{align*} $$\nこれらの合計が最終的なロスである。\n$$ SSE = SSE_{n} + SSE_{b} $$\nFigure 2.\nFig. 2では、上の図が正確な解のヒートマップを示している。下の図では、$t=0.1$ での $u$ を知っているときに $t=0.9$ での値を予測した結果を示している。下の左側の図では、青い線が正確な解であり、$\\color{red}\\mathsf{X}$ がデータとして使用された点を示している。下の右側の図では、青い線が正確な解であり、赤い線が予測された解である。\n暗黙のルンゲ・クッタ法 (IRK) では $u^{n+c_{j}}$ を計算するためにすべての $j$ に対する連立方程式を解く必要があるため、$q$ が大きくなると計算コストが大幅に増加するが、この論文で提案されている方法では $q$ が大きくなってもそれに伴う追加コストは非常に少ないと説明されている。また、$q$ が小さい場合、IRK ではタイムステップ $\\Delta t$ が大きいと正確な予測ができないが、PINN の場合は $\\Delta t$ が大きくても正確に予測できると説明されている。\n4. 偏微分方程式のデータ駆動発見 この章では、観測データがある場合に、偏微分方程式 $(1)$ のパラメータ $\\lambda$ を見つける問題について扱う。詳細は以下の例を通じて説明する。\n4.1. 連続時間モデル $f$ を以下のように $(1)$ の左辺として定義しよう。\n$$ f = u_{t} + \\mathcal{N}[u; \\lambda] $$\nセクション3の $(3)$ と異なる点は、$\\lambda$ が固定された定数ではなく、学習が必要な未知のパラメータになったことである。\n4.1.1. 例（ナヴィエ–ストークス方程式） セクション4.1.1では、非圧縮性流体の実際のデータに関する例として、ナヴィエ–ストークス方程式によって表されるケースを紹介する。以下の2次元ナヴィエ–ストークス方程式を考える。\n$$ \\begin{equation} \\begin{aligned} u_{t} + \\lambda_{1}(uu_{x} + vu_{y}) \u0026amp;= -p_{x} + \\lambda_{2}(u_{xx} + u_{yy}) \\\\ v_{t} + \\lambda_{1}(uv_{x} + vv_{y}) \u0026amp;= -p_{y} + \\lambda_{2}(v_{xx} + v_{yy}) \\end{aligned} \\tag{15} \\end{equation} $$\nここで、$u(t,x,y)$ は流体の速度ベクトルの $x$ 成分、$v(t,x,y)$ は $y$ 成分である。また、$p(t,x,y)$ は圧力、$\\lambda = (\\lambda_{1}, \\lambda_{2})$ は未知のパラメータである。ナヴィエ–ストークス方程式の解は発散が $0$ となる条件を満たすため、以下が成立する。\n$$ \\begin{equation} u_{x} + v_{y} = 0 \\tag{17} \\end{equation} $$\nある潜在関数 $\\psi (t, x, y)$ に対して、以下のように仮定する。\n$$ u = \\psi_{y},\\quad v = -\\psi_{x} $$\nつまり、流体の速度ベクトルを $\\begin{bmatrix} \\psi_{y} \u0026amp; -\\psi_{x}\\end{bmatrix}$ とすると、$u_{x} + v_{y} = \\psi_{yx} - \\psi_{xy} = 0$ であるため、自然に $(17)$ を満たす。$u$ と $v$ を個別に求めるのではなく、$\\psi$ を人工ニューラルネットワークで近似し、その偏微分によって $u, v$ を得る。実際の速度ベクトル場に対して、以下のように測定された情報があるとする。\n$$ \\left\\{ t^{i}, x^{i}, y^{i}, u^{i}, v^{i} \\right\\}_{i=1}^{N} $$\nこれに基づいて損失関数を以下のようにする。ここで、$u = \\phi_{y}$, $v = -\\psi_{x}$ であることを思い出す。\n$$ \\dfrac{1}{N} \\sum\\limits_{i=1}^{N} \\left( \\left| u(t^{i}, x^{i}, y^{i}) - u^{i} \\right|^{2} + \\left| v(t^{i}, x^{i}, y^{i}) - v^{i} \\right|^{2} \\right) $$\nそして、$(15)$ の右辺を左辺に定理し、それぞれを $f$ と $g$ として定義する。\n$$ \\begin{equation} \\begin{aligned} f \u0026amp;:= u_{t} + \\lambda_{1}(uu_{x} + vu_{y}) + p_{x} - \\lambda_{2}(u_{xx} + u_{yy}) \\\\ g \u0026amp;:= v_{t} + \\lambda_{1}(uv_{x} + vv_{y}) + p_{y} - \\lambda_{2}(v_{xx} + v_{yy}) \\end{aligned}\\tag{18} \\end{equation} $$\nすると $f, g$ の値は $\\psi$ によって以下のように表される。（$p$ もニューラルネットワークで近似する）\n$$ \\begin{align*} f \u0026amp;= \\phi_{yt} + \\lambda_{1}(\\psi_{y} \\psi_{yx} - \\psi_{x}\\psi_{yy}) + p_{x} -\\lambda_{2}(\\psi_{yxx} + \\psi_{yyy}) \\\\ g \u0026amp;= -\\phi_{xt} + \\lambda_{1}(-\\psi_{y} \\psi_{xx} + \\psi_{x}\\psi_{xy}) + p_{y} + \\lambda_{2}(\\psi_{xxx} + \\psi_{xyy}) \\\\ \\end{align*} $$\n損失関数に $f(t^{i}, x^{i}, y^{i}) = 0 = g(t^{i}, x^{i}, y^{i})$ という情報を加え、最終的に以下のようにする。\n$$ \\begin{aligned} MSE \u0026amp;:= \\dfrac{1}{N} \\sum\\limits_{i=1}^{N} \\left( \\left| u(t^{i}, x^{i}, y^{i}) - u^{i} \\right|^{2} + \\left| v(t^{i}, x^{i}, y^{i}) - v^{i} \\right|^{2} \\right) \\\\ \u0026amp;\\qquad + \\dfrac{1}{N} \\sum\\limits_{i=1}^{N} \\left( \\left| f(t^{i}, x^{i}, y^{i}) \\right|^{2} + \\left| g(t^{i}, x^{i}, y^{i}) \\right|^{2} \\right) \\end{aligned} \\tag{19} $$\n次に、入力ノードが $3$ つ、出力ノードが $2$ つの人工ニューラルネットワークを定義する。この出力を $\\begin{bmatrix} \\psi (t, x, y) \u0026amp; p(t, x, y) \\end{bmatrix}$ とする。すると、上記の損失関数を計算することができる。\nノイズがある場合とない場合のデータについて実験を行い、どちらの場合も高い精度で $\\lambda_{1}, \\lambda_{2}$ を予測できたことが示されている。また、圧力 $p$ に関するデータが与えられていない場合でも、人工ニューラルネットワークがパラメータと共に $p$ もかなり正確に近似できることが示された。具体的な実験設定、結果、参照解の求め方については、論文に詳しく記載されている。\n5. 結論 この論文では、与えられたデータが満たす物理法則をエンコードする能力があり、偏微分方程式で説明できる新しい種類のニューラルネットワーク構造である物理情報に基づいたニューラルネットワークを紹介した。この結果から、物理モデルに対してディープラーニングが学習できることがわかった。これは多くの物理的シミュレーションに応用可能である。\nしかし、著者は提案された方法が偏微分方程式を解くための既存の方法、例えば有限要素法finite element method、スペクトル方法spectral methodsなどを置き換えるものだと考えるべきではないと述べている。実際にセクション3.2.ではルンゲ・クッタ法をPINNに適用している。\nPINNを実装するために、どれくらいの深さのニューラルネットワークが必要か、どれくらいのデータが必要かなどのハイパーパラメータに関する問題についても、著者は解決策を提案しようとした。しかし、ある方程式で効果的な設定が他の方程式ではそうではないことが観察されたと述べている。\n","id":3313,"permalink":"https://freshrimpsushi.github.io/jp/posts/3313/","tags":null,"title":"論文レビュー: 物理情報基盤ニューラルネットワーク(PINN)"},{"categories":"줄리아","contents":"日本語訳 コード println(ARGS[1] * \u0026#34; + \u0026#34; * ARGS[2] * \u0026#34; = \u0026#34; * string(parse(Float64, ARGS[1]) + parse(Float64, ARGS[2]))) 上記の通り、example.jlというファイルが1行で構成されているとしよう。Juliaでは、ARGSを通じてコマンドラインからの引数を配列で受け取ることができ、Pythonのsys.argvがコマンドライン引数を配列で受け取るのと似ている。書かれたコードは、二つの数字を受け取ってその和を出力するプログラムだ。実行結果は以下の通り。\n環境 OS: Windows julia: v1.6.3 ","id":2280,"permalink":"https://freshrimpsushi.github.io/jp/posts/2280/","tags":null,"title":"ジュリアでコマンドライン引数を挿入する方法"},{"categories":"줄리아","contents":"## 概要 Juliaでの記号演算は、`SymEngine.jl`[^1]パッケージを通じて使うことができる。 [^1]: https://symengine.org/SymEngine.jl/ ## コード ### シンボルの定義 シンボルは、以下の方法で定義することができる。 julia\u0026gt; using SymEngine\njulia\u0026gt; x = symbols(:x) x\njulia\u0026gt; x, y = symbols(\u0026ldquo;x y\u0026rdquo;) (x, y)\njulia\u0026gt; @vars x, y (x, y)\njulia\u0026gt; x = symbols(:x) x\njulia\u0026gt; f = 2x + x^2 + cos(x) 2*x + x^2 + cos(x)\n### ベクトルと行列 julia\u0026gt; v = [symbols(\u0026ldquo;v_▷eq1◁i$j\u0026rdquo;) for i in 1:2, j in 1:3] 2×3 Matrix{Basic}: a_11 a_12 a_13 a_21 a_22 a_23\njulia\u0026gt; Av 2-element Vector{Basic}: v_1a_11 + v_2a_12 + v_3a_13 v_1a_21 + v_2a_22 + v_3*a_23\njulia\u0026gt; @vars a, b, c, d, x, y (a, b, c, d, x, y)\njulia\u0026gt; [a b; c d] * [a x; b y] 2×2 Matrix{Basic}: a^2 + b^2 ax + by ac + bd cx + dy\n### 微分 記号微分は、[`Calculus.jl`](../3135)パッケージでも使用することができる。 julia\u0026gt; f = 2x + x^2 + cos(x) 2*x + x^2 + cos(x)\njulia\u0026gt; diff(f, x) 2 + 2*x - sin(x)\n## 環境 - OS: Windows10 - バージョン: Julia v1.7.1, SymEngine v0.8.7 ","id":3311,"permalink":"https://freshrimpsushi.github.io/jp/posts/3311/","tags":null,"title":"ジュリアでのシンボリック演算の方法"},{"categories":"줄리아","contents":"コード ジュリアでは、run()関数を使ってバックティックBacktickで囲まれた文字列を実行します。Pythonで言うところの[osモジュールのos.system()`](../2147)を使用したことに似ています。\njulia\u0026gt; txt = \u0026#34;helloworld\u0026#34; \u0026#34;helloworld\u0026#34; julia\u0026gt; typeof(`echo $txt`)\rCmd 위와 같이 백틱으로 감싸진 문자열은 Cmd라는 타입을 가지고, run() 함수로써 실행할 수 있다.\njulia\u0026gt; run(`cmd /C echo $txt`) helloworld Process(`cmd /C echo helloworld`, ProcessExited(0)) この例に限って言えば、Windowsではcmd内のechoを実行しなければならず、少し複雑になりますが、Linuxでは単にecho $txtを使用することができます。Windowsでこのようなコマンドを頻繁に使用する場合は、環境変数を修正することを検討してください1。\n環境 OS: Windows julia: v1.6.3 https://docs.julialang.org/en/v1/manual/running-external-programs/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2278,"permalink":"https://freshrimpsushi.github.io/jp/posts/2278/","tags":null,"title":"Juliaで外部プログラムを実行する方法"},{"categories":"줄리아","contents":"コード parse(type, str)を使えばいいんだ。文字列strをtypeタイプの数字に変更してくれる。\njulia\u0026gt; parse(Int, \u0026#34;21\u0026#34;)\r21\rjulia\u0026gt; parse(Float64, \u0026#34;3.14\u0026#34;)\r3.14 なんでPythonみたいにInt64(\u0026quot;21\u0026quot;)みたいなのができないんだろう\u0026hellip;それは、\u0026quot;21\u0026quot;を21に変えることがタイプを変えることではなく、文字列\u0026quot;21\u0026quot;を読んで数字として解釈することだから、parseを使うのが妥当だって言われてるんだ1。\n環境 OS: Windows julia: v1.6.3 https://discourse.julialang.org/t/why-are-there-all-these-strange-stumbling-blocks-in-julia/92644/2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2276,"permalink":"https://freshrimpsushi.github.io/jp/posts/2276/","tags":null,"title":"ジュリアで文字列を数値に変換する方法"},{"categories":"행렬대수","contents":"定義1 ある行列 $n \\times n$ $A$について、$A^{k} = O$を満たす正の数 $k$が存在する場合、$A$を冪零nilpotentという。このとき$O$は$n \\times n$零行列である。\n説明 nilは「ゼロ」または「なし」を意味する。potentの意味は「強力な」であり、potentialの語根である。したがって、nilpotentという言葉は「$0$になる可能性/潜在力があるもの」と理解すればよい。「冪」は数学で冪乗を意味し、「零」は数字$0$を意味する。従って、冪零という言葉は文字通り「冪乗してゼロになる」という意味である。\n要約 [2]: 正方行列 $A \\in \\mathbb{R}^{n \\times n}$の全ての固有値が$0$であることと、$A$が冪零行列であることは同値である。 つまり、冪零行列は逆行列が存在しない。 証明 1 数学的帰納法で示される。\n[2] 2 $(\\implies)$\n$A$は正方行列であるため、あるユニタリ行列$Q$と上三角行列$T$に対して$A = Q T Q^{\\ast}$のように表される。$A$の全ての固有値が$0$であるため、$T$は対角成分が全て$0$の上三角行列であり、正方上三角行列は冪零行列であるため、$T$はある$k \\in \\mathbb{N}$に対して$T^{k} = O$の冪零行列である。それゆえ少なくとも$k$に対して$A^{k} = Q T^{k} Q^{*} = O$となり、$A$も冪零行列である。\n$(\\impliedby)$\n$A$を冪零行列とする。 $$ A^{k} = O $$ 行列の積の行列式は行列式の積と等しい、 $$ (\\det(A))^{k} = \\det(A^{k}) = \\det(O) = 0 \\implies \\det(A) = 0 $$\n■\n3 証明は省略される3。\n参考 冪零線形変換 Stephen H. Friedberg, Linear Algebra (4th Edition, 2002), p229\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://math.stackexchange.com/a/256012/459895\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://math.stackexchange.com/a/2078773/459895\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3307,"permalink":"https://freshrimpsushi.github.io/jp/posts/3307/","tags":null,"title":"べき乗行列"},{"categories":"행렬대수","contents":"定義1 主対角線より上の要素がすべて$0$である行列$A = [a_{ij}]$を下三角行列lower triangular matrixという。\n$$ A \\text{ is lower triangluar matrix if } a_{ij} = 0 \\text{ whenever } i \\lt j $$\n主対角線より下の要素がすべて$0$である行列$A = [a_{ij}]$を上三角行列upper triangular matrixという。\n$$ A \\text{ is upper triangluar matrix if } a_{ij} = 0 \\text{ whenever } i \\gt j $$\n特に主対角要素がすべて$0$である三角行列を厳密（上/下）三角行列strictly (upper/lower) triangular matrixという。\n説明 例えば、$A$が$4 \\times 5$とする。$A$が下三角行列なら、\n$$ A= \\begin{bmatrix} a_{11} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ a_{21} \u0026amp; a_{22} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \u0026amp; 0 \u0026amp; 0 \\\\ a_{41} \u0026amp; a_{42} \u0026amp; a_{43} \u0026amp; a_{44} \u0026amp; 0 \\\\ \\end{bmatrix} $$\n上三角行列なら、以下のようになる。\n$$ A= \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \u0026amp; a_{14} \u0026amp; a_{15} \\\\ 0 \u0026amp; a_{22} \u0026amp; a_{23} \u0026amp; a_{24} \u0026amp; a_{25} \\\\ 0 \u0026amp; 0 \u0026amp; a_{33} \u0026amp; a_{34} \u0026amp; a_{35} \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; a_{44} \u0026amp; a_{44} \\\\ \\end{bmatrix} $$\n定義により、対角行列は下三角行列でありつつ、上三角行列でもある。\n性質 下三角行列の転置は上三角行列であり、上三角行列の転置は下三角行列である。\n下三角行列の積は下三角行列であり、上三角行列の積は上三角行列である。\n三角行列が可逆であるための必要十分条件は、すべての主対角要素が$0$ではないことである。\n可逆な下三角行列の逆行列は下三角行列であり、可逆な上三角行列の逆行列は上三角行列である。\n正方形の厳密三角行列は冪零である。(逆は成り立たない)\nStephen H. Friedberg, Linear Algebra (4th Edition, 2002), p21\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3305,"permalink":"https://freshrimpsushi.github.io/jp/posts/3305/","tags":null,"title":"三角行列"},{"categories":"수리통계학","contents":"定義 1 パラメータ $\\theta$ が与えられているとする。偏りのない推定量 $W^{\\ast}$ が、他の全ての偏りのない推定量 $W$ に対して以下を満たす場合、それを 最良偏りのない推定量 や 一様最小分散偏りのない推定量UMVUE, Uniform Minimum Variance Unbiased Estimator と呼ぶ。 $$ \\text{Var}_{\\theta} W^{\\ast} \\le \\text{Var}_{\\theta} W \\qquad , \\forall \\theta $$\n説明 UMVUEは、最前のUniformを取り除いて単にMVUEとも呼ばれることがある。UMVUEは少々長すぎる上に、「最良」は非常に合っているが、「最小分散」も直感的であり、Bestが学問的な言葉ではないためか、最良偏りのない推定量という表現は韓国語でも英語でもあまり使われない。\n効率的推定量との違い 一見すると、効率的推定量 と似ているが、効率的推定量はその分散が正確に ラオ-クレーマーの下限 まで下がり、理論的にこれ以上良くなることができない 偏りのない推定量 であり、最良偏りのない推定量は理論的な限界には達していなくても、他の全ての偏りのない推定量を超えればよい。最善を尽くしても、効率性が $1$ になる保証はなく、分散を理論的な下限まで最小化できなくても、最良偏りのない推定量であることに何の問題もない。\n効率的推定量であれば最良偏りのない推定量であるが、その逆は成り立たない。\nCasella. (2001). Statistical Inference(2nd Edition): p334.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2273,"permalink":"https://freshrimpsushi.github.io/jp/posts/2273/","tags":null,"title":"最良不偏推定量、最小分散不偏推定量 UMVUE"},{"categories":"줄리아","contents":"概要 1 可変引数関数とは、プログラミングで一般にVarargs Functionと呼ばれるもので、複数の引数を制限なく受け入れることができる関数のことだ。Juliaでは、変数の後ろに...を付けることで簡単に可変引数を設定できる。例のコードを見て理解しよう。\nちなみに、この...は スプラットオペレータSplat Operatorと呼ばれている。2\nコード アイザック・ニュートンは、単純に階乗の逆数を足すと$e$に収束する次の定理を発見した。 $$ e = {{ 1 } \\over { 0! }} + {{ 1 } \\over { 1! }} + {{ 1 } \\over { 2! }} + \\cdots = \\sum_{k=0}^{\\infty} {{ 1 } \\over { k! }} $$ この例では、オイラー定数$e = 2.71828182 \\cdots$に収束する数列を見ることにする。\nfunction f(x...)\rzeta = 0\rfor x_ in x\rzeta += 1/prod(1:x_)\rend\rreturn zeta\rend 上記のように、xの後ろにドットを付けてx...と書くと、与えられた引数が自動的に配列と捉えられる。関数の内容は上の数式からわかるように、階乗の逆数を順番に取り、足してリターンするだけだ。\njulia\u0026gt; f(0)\r1.0\rjulia\u0026gt; f(0,1)\r2.0\rjulia\u0026gt; f(0,1,2)\r2.5\rjulia\u0026gt; f(0,1,2,3,4,5,6,7,8,9,10)\r2.7182818011463845 実行結果は、自然数を長く与えるほど、オイラー定数に近づくことが確認できる。ここで注目すべき点は、可変的に入った引数が自動的にxという配列にまとめられて使用された点だ。例えば、次のように概念的に配列を入れるとエラーになる可能性がある。\njulia\u0026gt; f(0:10)\rERROR: MethodError: no method matching (::Colon)(::Int64, ::UnitRange{Int64}) 環境 OS: Windows julia: v1.6.3 https://docs.julialang.org/en/v1/manual/functions/#Varargs-Functions\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/base/#\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2266,"permalink":"https://freshrimpsushi.github.io/jp/posts/2266/","tags":null,"title":"ジュリアで可変引数関数を定義する方法"},{"categories":"줄리아","contents":"概要 eltype() 関数を使うだけだ。多分 element typeからきた名前だろう。\nコード julia\u0026gt; set_primes = Set([2,3,5,7,11,13])\rSet{Int64} with 6 elements:\r5\r13\r7\r2\r11\r3\rjulia\u0026gt; arr_primes = Array([2,3,5,7,11,13])\r6-element Vector{Int64}:\r2\r3\r5\r7\r11\r13 次のように$13$までの素数を要素とする二つのコンテナを考えてみよう。正直、同じデータを含んでいるが、上はセットで、下は配列という違いがあるだけだ。\njulia\u0026gt; typeof(set_primes)\rSet{Int64}\rjulia\u0026gt; eltype(set_primes)\rInt64\rjulia\u0026gt; typeof(arr_primes)\rVector{Int64} (alias for Array{Int64, 1})\rjulia\u0026gt; eltype(arr_primes)\rInt64 これらにtypeof()を適用すれば、セットか配列かの区別がつくが、eltype()はコンテナが何であれ、内部の要素がどのようなタイプかを返す。\njulia\u0026gt; typeof(1:10)\rUnitRange{Int64}\rjulia\u0026gt; eltype(1:10)\rInt64\rjulia\u0026gt; typeof(1:2:10)\rStepRange{Int64, Int64}\rjulia\u0026gt; eltype(1:2:10)\rInt64 上に示された1:10と1:2:10の違いは、タイプに過度にこだわるジュリアプログラミングの世界で、eltype()がどのように役立つかを示している。\n環境 OS: Windows julia: v1.6.3 ","id":2264,"permalink":"https://freshrimpsushi.github.io/jp/posts/2264/","tags":null,"title":"ジュリアのコンテナ内部の要素タイプをチェックする方法"},{"categories":"줄리아","contents":"コード default() 関数を使用すればいい。\nusing Plots\rdefault(size = (400,400), color = :red)\rdefault(:size, (400,400))\rfor key in [:size, :color], value in [(400,400), :red]\rdefault(key, value)\rend 普通の plot() 関数のように設定する方法と、キーとバリューを与えて一つずつ変更する方法がある。普通は前者の方が便利だが、設定が非常に複雑な場合は、ループを利用して下の方法を使用することもできる。\n初期化 全てのデフォルト設定を初期化したい場合は、default()を使用すればいい。\n環境 OS: Windows julia: v1.6.3 ","id":2262,"permalink":"https://freshrimpsushi.github.io/jp/posts/2262/","tags":null,"title":"ジュリアプロットの基本設定を変更する方法"},{"categories":"기하학","contents":"定理1 $(M,g)$をリーマン多様体とする。すると、次を満たす$M$上のアフィン接続$\\nabla$が唯一に存在する。\n$\\nabla$が対称的である。 $\\nabla$が$g$と両立可能である。 このような$\\nabla$は具体的に以下の式を満たす。\n$$ \\begin{align*} g(Z, \\nabla_{Y}X) =\u0026amp;\\ \\dfrac{1}{2}\\Big( X g(Y, Z) + Y g(Z, X) - Z g(X, Y) \\\\ \u0026amp;\\ - g([X, Z], Y) - g([Y, Z], X) - g([X, Y], Z) \\Big) \\tag{1} \\end{align*} $$\n説明 このような接続$\\nabla$をレヴィ-チヴィタ(またはリーマン)接続と言う。\n接ベクトル空間の基底を$\\left\\{ \\dfrac{\\partial }{\\partial x_{i}} \\right\\} \\overset{\\text{denote}}{=} \\left\\{ X_{i} \\right\\}$として表示しよう。接続の定義により$\\nabla_{X_{i}}X_{j}$自体もベクトル場である。したがって$X_{k}$の線形結合で表せる。アインシュタインの表記法により、\n$$ \\nabla_{X_{i}}X_{j} = \\sum_{k}\\Gamma_{ij}^{k}X_{k} = \\Gamma_{ij}^{k}X_{k} $$\nこのベクトル場は$X_{i}, X_{j}$によって決まるので、係数を$\\Gamma_{ij}^{k}$と表示しよう。これを接続の係数またはクリストッフェル記号と言う。微分幾何学では、クリストッフェル記号を座標切片写像$\\mathbf{x}$の2次導関数$\\mathbf{x}_{ij}$の係数と定義しているが、これらが同じであることが示される。$(1)$の左辺に$X_{i}, X_{j}, X_{k}$を代入してみると、\n$$ \\begin{align*} g(\\nabla_{X_{j}}X_{i}, X_{k}) = g\\left( \\Gamma_{ji}^{l}X_{l}, X_{k} \\right) = \\Gamma_{ji}^{l}g_{lk} \\end{align*} $$\n右辺を計算すると、$[X_{i}, X_{j}] = 0$となるので、\n$$ \\begin{align*} \u0026amp; \\dfrac{1}{2}\\left( X_{i}g(X_{j}, X_{k}) + X_{j}g(X_{i}, X_{k}) - X_{k}g(X_{i}, X_{j}) \\right) \\\\ =\u0026amp; \\dfrac{1}{2}\\left( X_{i}g_{jk} + X_{j}g_{ik} - X_{k}g_{ij} \\right) \\\\ \\end{align*} $$\n従って、\n$$ \\begin{align*} \u0026amp;\u0026amp; \\Gamma_{ji}^{l}g_{lk} \u0026amp;= \\dfrac{1}{2}\\left( X_{i}g_{jk} + X_{j}g_{ik} - X_{k}g_{ij} \\right) \\\\ \\implies \u0026amp;\u0026amp; \\sum_{k}\\Gamma_{ji}^{l}g_{lk}g^{ks} \u0026amp;= \\sum_{k}\\dfrac{1}{2}g^{ks}\\left( X_{i}g_{jk} + X_{j}g_{ik} - X_{k}g_{ij} \\right) \\\\ \\implies \u0026amp;\u0026amp; \\Gamma_{ji}^{l}\\delta_{l}^{s} \u0026amp;= \\sum_{k}\\dfrac{1}{2}g^{ks}\\left( X_{i}g_{jk} + X_{j}g_{ik} - X_{k}g_{ij} \\right) \\\\ \\implies \u0026amp;\u0026amp; \\Gamma_{ji}^{s} \u0026amp;= \\sum_{k}\\dfrac{1}{2}g^{ks}\\left( X_{i}g_{jk} + X_{j}g_{ik} - X_{k}g_{ij} \\right) \\\\ \\end{align*} $$\n従って、まとめると次を得る。\n$$ \\Gamma_{ij}^{k} = \\dfrac{1}{2}g^{mk}\\left( \\dfrac{\\partial }{\\partial x_{i}}g_{jm} + \\dfrac{\\partial }{\\partial x_{j}}g_{im} - \\dfrac{\\partial }{\\partial x_{m}}g_{ij} \\right) $$\nこれは、微分幾何学で$\\mathbb{R}^{3}$上の曲面に対して得られた式と同じである。特に、ユークリッド空間$\\mathbb{R}^{n}$では、メトリックが$g_{ij} = \\delta_{ij}$として定数であるため、$\\Gamma_{ij}^{k} = 0$である。\n最初にアフィン接続を定義するとき、$\\nabla_{X}Y$は明示的に与えられず、特定の性質を満たす抽象的な概念としてのみ定義された。しかし、このような接続$\\nabla$にリーマンメトリック$g$が与えられると、メトリックの係数$g_{ij}$によって$\\nabla_{X}Y$が明確に決まることがわかる。$X = u^{i}X_{i}, Y= v^{j}X_{j}$と表示すると、\n$$ \\begin{align*} \\nabla_{X}Y = \\nabla_{u^{i}X_{i}}v^{j}X_{j} \u0026amp;= u^{i}X_{i}(v^{j})X_{j} + u^{i}v^{j}\\nabla_{X_{i}}X_{j} \\\\ \u0026amp;= u^{i}X_{i}(v^{j})X_{j} + u^{i}v^{j}\\Gamma_{ij}^{k}X_{k} \\\\ \u0026amp;= u^{i}X_{i}(v^{k})X_{k} + u^{i}v^{j}\\Gamma_{ij}^{k}X_{k} \\\\ \u0026amp;= \\left( u^{i}X_{i}(v^{k}) + u^{i}v^{j}\\Gamma_{ij}^{k}\\right)X_{k} \\\\ \u0026amp;= \\left( u^{i}X_{i}(v^{k}) + u^{i}v^{j}\\Gamma_{ij}^{k}\\right)X_{k} \\\\ \\end{align*} $$\n$X = X^{i}\\dfrac{\\partial }{\\partial x_{i}}, Y = Y^{i}\\dfrac{\\partial }{\\partial x_{j}}$と表示すると、\n$$ \\begin{align*} \\nabla_{X}Y \u0026amp;= \\left( X^{i}\\dfrac{\\partial Y^{k}}{\\partial x_{i}} + X^{i}Y^{j}\\Gamma_{ij}^{k}\\right)\\dfrac{\\partial }{\\partial x_{k}} \\\\ \u0026amp;= \\sum_{i,k}\\left( X^{i}\\dfrac{\\partial Y^{k}}{\\partial x_{i}} + \\sum_{j}X^{i}Y^{j}\\Gamma_{ij}^{k}\\right)\\dfrac{\\partial }{\\partial x_{k}} \\end{align*} $$\nまた、ベクトル場$V = v^{j}X_{j}$の共変微分は以下の通りである。\n$$ \\dfrac{DV}{dt} = \\sum_{k} \\left( \\dfrac{d v^{k}}{dt} + \\sum_{i,j} v^{j}\\frac{dc_{i}}{dt} \\Gamma_{ij}^{k} \\right) X_{k} $$\n証明 Part 1. 唯一性\n定理の条件を満たす接続$\\nabla$が存在すると仮定しよう。すると$\\nabla$が両立可能であるため、ベクトル場$X,Y,Z \\in$$\\mathfrak{X}(M)$に対して次が成り立つ。\n$$ \\begin{align*} X g(Y, Z) =\u0026amp;\\ g(\\nabla_{X}Y, Z) + g(Y, \\nabla_{X}Z) \\\\ Y g(Z, X) =\u0026amp;\\ g(\\nabla_{Y}Z, X) + g(Z, \\nabla_{Y}X) \\\\ Z g(X, Y) =\u0026amp;\\ g(\\nabla_{Z}X, Y) + g(X, \\nabla_{Z}Y) \\\\ \\end{align*} $$\n最初の式と2番目の式を加えて、3番目の式を引くと、$\\nabla$が対称的であるため、次を得る。\n$$ \\begin{align*} \u0026amp;\\ X g(Y, Z) + Y g(Z, X) - Z g(X, Y) \\\\ =\u0026amp;\\ g(\\nabla_{X}Y, Z) + {\\color{red}g(Y, \\nabla_{X}Z)} + {\\color{blue}g(\\nabla_{Y}Z, X)} + g(Z, \\nabla_{Y}X) - {\\color{red}g(\\nabla_{Z}X, Y)} - {\\color{blue}g(X, \\nabla_{Z}Y)} \\\\ =\u0026amp;\\ {\\color{red}g(\\nabla_{X}Z-\\nabla_{Z}X, Y)} + {\\color{blue}g(\\nabla_{Y}Z - \\nabla_{Z}Y, X)} + g(\\nabla_{X}Y, Z) + g(Z, \\nabla_{Y}X) \\\\ =\u0026amp;\\ g([X, Z], Y) - g([Y, Z], X) - g(\\nabla_{X}Y, Z) + g(Z, \\nabla_{Y}X) \\end{align*} $$\nこれに$0=g(\\nabla_{Y}X, Z)-g(\\nabla_{Y}X, Z)$を加えて整理すると\n$$ X g(Y, Z) + Y g(Z, X) - Z g(X, Y) \\\\ = g([X, Z], Y) + g([Y, Z], X) + g([X, Y], Z) + 2g(Z, \\nabla_{Y}X) $$\n右辺の最後の項を基準に整理すると次のようになる。\n$$ \\begin{align*} g(Z, \\nabla_{Y}X) =\u0026amp;\\ \\dfrac{1}{2}\\Big( X g(Y, Z) + Y g(Z, X) - Z g(X, Y) \\\\ \u0026amp;\\ - g([X, Z], Y) - g([Y, Z], X) - g([X, Y], Z) \\Big) \\tag{1} \\end{align*} $$\n現にこのような別の接続$\\nabla^{\\prime}$が存在するとしよう。\n$$ \\begin{align*} g(Z, \\nabla^{\\prime}_{Y}X) =\u0026amp;\\ \\dfrac{1}{2}\\Big( X g(Y, Z) + Y g(Z, X) - Z g(X, Y) \\\\ \u0026amp;\\ - g([X, Z], Y) - g([Y, Z], X) - g([X, Y], Z) \\Big) \\end{align*} $$\nこれら2つの式を引くと、\n$$ g(Z, \\nabla_{Y}X)-g(Z, \\nabla^{\\prime}_{Y}X) = g(Z, \\nabla_{Y}X - \\nabla^{\\prime}_{Y}X) = 0 $$\n内積の性質によって、上の式がすべての$Z$に対して成り立つためには、$\\nabla_{Y}X - \\nabla^{\\prime}_{Y}X=0$でなければならない。したがって、このような接続$\\nabla$は唯一である。\n$$ \\nabla_{Y}X = \\nabla^{\\prime}_{Y}X $$\nPart 2. 存在性\n$\\nabla$を$(1)$のように定義すると、よく定義されており、定理の条件をよく満たしていることがわかる。\n■\nManfredo P. Do Carmo, Riemannian Geometry (Eng Edition, 1992), p55-56\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3292,"permalink":"https://freshrimpsushi.github.io/jp/posts/3292/","tags":null,"title":"レヴィチビタ接続、リーマン接続、接続の係数、クリストッフェル記号"},{"categories":"줄리아","contents":"概要 インデックスを取るときは、Not() 関数を使用すればいいんだ1。カラム名そのままのシンボル、またはシンボルの配列を入れると、それらのカラムのみが除外されてインデックス化されるんだ。\nコード using DataFrames\rWJSN = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;다원\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;소정\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;연정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;진숙\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [99,97,97,95,96,99,98,95,99,94],\rheight = [161,167,157,166,159,165,172,163,162,165],\runit = [\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;]\r) 上の例のコードを実行して、その結果を確認しよう。\njulia\u0026gt; WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 소정 95 166 더블랙\r3 │ 지연 95 163 더블랙\r4 │ 수빈 96 159 쪼꼬미\r5 │ 다원 97 167 메보즈\r6 │ 루다 97 157 쪼꼬미\r7 │ 주연 98 172 더블랙\r8 │ 다영 99 161 쪼꼬미\r9 │ 연정 99 165 메보즈\r10 │ 진숙 99 162 쪼꼬미 WJSN データフレームは、上のようになっているんだ。\njulia\u0026gt; WJSN[:,Not(:height)]\r10×3 DataFrame\rRow │ member birth unit │ String Int64 String ─────┼───────────────────────\r1 │ 다영 99 쪼꼬미\r2 │ 다원 97 메보즈\r3 │ 루다 97 쪼꼬미\r4 │ 소정 95 더블랙\r5 │ 수빈 96 쪼꼬미\r6 │ 연정 99 메보즈\r7 │ 주연 98 더블랙\r8 │ 지연 95 더블랙\r9 │ 진숙 99 쪼꼬미\r10 │ 현정 94 더블랙 :height だけが入って、:height 列が削除されたよ。\njulia\u0026gt; WJSN[:,Not([:birth, :unit])]\r10×2 DataFrame\rRow │ member height │ String Int64 ─────┼────────────────\r1 │ 다영 161\r2 │ 다원 167\r3 │ 루다 157\r4 │ 소정 166\r5 │ 수빈 159\r6 │ 연정 165\r7 │ 주연 172\r8 │ 지연 163\r9 │ 진숙 162\r10 │ 현정 165 シンボルの配列 [:birth, :unit] が入って、:birth 列と :unit 列が削除されたんだ。\n環境 OS: Windows julia: v1.6.3 https://dataframes.juliadata.org/stable/man/working_with_dataframes/#Taking-a-Subset\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2260,"permalink":"https://freshrimpsushi.github.io/jp/posts/2260/","tags":null,"title":"ジュリアでデータフレームの特定の行を削除する方法"},{"categories":"줄리아","contents":"概要 縦線と横線を引くには、vline!() と hline!() 関数を使用すればいい。\nコード @time using Plots\rplot(rand(100))\rhline!([0.5], linewidth = 2)\rvline!([25, 75], linewidth = 2)\rpng(\u0026#34;result\u0026#34;) 線が引かれる位置は配列で渡す。配列の要素が複数あれば、一度に複数の線を引くことができる。\n環境 OS: Windows julia: v1.6.3 ","id":2258,"permalink":"https://freshrimpsushi.github.io/jp/posts/2258/","tags":null,"title":"ジュリアで図に垂直線と水平線を挿入する方法"},{"categories":"줄리아","contents":"概要 RecipesBase.jlは、ユーザーが新しい図のスタイルを自分で作れるパッケージだ。Rプログラミング言語でのggplotがそうであるように、元のジュリアとはまた別の独自の文法1がある。例を通して覚えよう。\nコード using Plots\rusing DataFrames\rdf = DataFrame(x = 1:10, y = rand(10))\rplot(df)\r@userplot TimeEvolution\r@recipe function f(te::TimeEvolution)\rdf = te.args[1]\rlinealpha --\u0026gt; 0.5\rcolumn_names = names(df)\rfor (column_index, column_name) ∈ enumerate(column_names)\r@series begin\rlabel --\u0026gt; column_name\rdf[:,column_index]\rend\rend\rend\rtimeevolution(df); png(\u0026#34;1\u0026#34;)\rtimeevolution(df, legend = :left); png(\u0026#34;2\u0026#34;) 最初に、上記のコードを実行すると、次のようなエラーが発生する。\njulia\u0026gt; df = DataFrame(x = 1:10, y = rand(10))\r10×2 DataFrame\rRow │ x y\r│ Int64 Float64 ─────┼──────────────────\r1 │ 1 0.636532\r2 │ 2 0.463764\r3 │ 3 0.604559\r4 │ 4 0.654089\r5 │ 5 0.883409\r6 │ 6 0.91667\r7 │ 7 0.0609783\r8 │ 8 0.602259\r9 │ 9 0.460372\r10 │ 10 0.479944\rjulia\u0026gt; plot(df)\rERROR: Cannot convert DataFrame to series data for plotting これは、基本的にplot()がデータフレームを受け取って図を描く方法が定義されていないためだ。\n@userplotと@recipe @userplot TimeEvolution\r@recipe function f(te::TimeEvolution)\r...\rend 例では、時系列データをそのまま折れ線チャートで描いてみる。\n@userplot：plot()関数の性質を継承する関数の名前を指定する。ここでは大文字と小文字の区別があるが、完成する関数は小文字のみ使うことができることに注意しろ。 @recipe：具体的に図のスタイルを指定する。これの後にくる関数の名前は、通常慣習的にfを使用する。 f(te::TimeEvolution) df = te.args[1]\rlinealpha --\u0026gt; 0.5\rcolumn_names = names(df)\rfor (column_index, column_name) ∈ enumerate(column_names)\r@series begin\rlabel --\u0026gt; column_name\rdf[:,column_index]\rend\rend では、上記のコードを一行一行理解していこう。\ndf = te.args[1] 受け取った引数の中で最初のものをdfとみなす。この例では、与える引数がデータフレームであると仮定されているため、その略称であるdfを使った。このように、fは私たちが使用する関数でもなく、その引数teも直接使用されないことに注意せよ。\nlinealpha --\u0026gt; 0.5 この図で描かれる線の透明度を0.5に設定する。オプションを与えるようにlinealpha = 0.5ではなく、--\u0026gt;で値を与えることに注意せよ。これは、通常のジュリアの文法と完全に異なる。\ncolumn_names = names(df)\rfor (column_index, column_name) ∈ enumerate(column_names)\r...\rend plot()が2次元配列を直接図に描くとしても、ラベルはy1、y2などと自動的に与えられる。これを防ぐために、上記のようにデータフレームの列名を取得して別々にラベルを与える。enumerate()関数を使って、そのインデックスと列名を同時に巡回する。詳しくはenumerate()に関する説明を参照せよ。\n@series for ...\r@series begin\rlabel --\u0026gt; column_name\rdf[:,column_index]\rend\rend @seriesマクロを通じて、繰り返し描かれるデータとそのスタイルを指定する。label --\u0026gt; column_nameを通じて列名をラベルとして与え、一番下の行にdf[:,column_index]と書くことで、その列名のデータが描かれることになる。この時、描かれるデータは一番下の行になければならないことに注意しろ。\n結果 この結果によって、timeevolution()またはtimeevolution!()で、私たちが指定したスタイルで図を描くことができるようになった。\ntimeevolution(df) データフレームを入れてもエラーなく図がうまく描かれていることを確認できる。折れ線の透明度は0.5で、ラベルもデータフレームにあった列名をそのまま引き継いでいる。\ntimeevolution(df, legend = :left) 任意に凡例の位置を調整してみた。fを定義する時にlegendについて何の言及もなかったにも関わらず、うまく適用されていることを確認できる。これは、timeevolution()がplot()の残りの要素を継承しているためだ。\n環境 OS: Windows julia: v1.6.3 https://docs.juliahub.com/RecipesBase/8e2Mm/1.0.1/syntax/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2256,"permalink":"https://freshrimpsushi.github.io/jp/posts/2256/","tags":null,"title":"ジュリアでアートスタイルを作る方法"},{"categories":"선형대수","contents":"定義1 二つのベクトル空間$V, W$に対して、可逆な線形変換$T : V \\to W$が存在する場合、$V$が$W$と同型$V$ is isomorphic to $W$であると言い、以下のように示される。\n$$ V \\cong W $$\nまた、$T$を同型写像isomorphismと言う。\n説明 可逆である同値条件によって、$T$が同型写像であるということは、$T$が全単射関数であるということと同じである。従って、全単射関数$T : V \\to W$が存在すれば、$V, W$は同型である。\n$V, W$が同型であるということは、$V$も$W$も事実上変わりがないということである。\n定理 $V, W$が有限次元ベクトル空間であるとする。すると、$V$と$W$が同型である必要十分条件は、$\\dim (V) = \\dim (W)$が成立することである。\n系 $V$をベクトル空間とする。すると、$V$が$\\mathbb{R}^{n}$と同型である必要十分条件は、$\\dim (V) = n$であることである。\n証明 $(\\Longrightarrow)$\n$T : V \\to W$が同型写像であると仮定する。すると、$T$は可逆であり、可逆線形変換の性質により\n$$ \\dim (V) = \\dim (W) $$\n$(\\Longleftarrow)$\n$\\dim (V) = \\dim (W)$と仮定する。$\\beta = \\left\\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{n} \\right\\}, \\gamma = \\left\\{ \\mathbf{w}_{1}, \\dots, \\mathbf{w}_{n} \\right\\}$をそれぞれ$V, W$の基底とする。すると、有限次元ベクトル空間の間には次のような線形変換が存在する。\n$$ T : V \\to W \\quad \\text{ by } \\quad T(\\mathbf{v}_{i}) = \\mathbf{w}_{i} $$\nまた、それならば$T(\\beta) = \\gamma$が真であり、$T(\\beta)$が$R(T)$を生成するので、\n$$ R(T) = \\span (T(\\beta)) = \\span (\\gamma) = W $$\n従って、$T$は全射である。すると、$\\dim (V) = \\dim (W)$と仮定したからには$T$も単射である。 従って、全単射関数$T : V \\to W$が存在し、$V$と$W$は同型である。\n■\nStephen H. Friedberg, Linear Algebra (4th Edition, 2002), p102-103\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3287,"permalink":"https://freshrimpsushi.github.io/jp/posts/3287/","tags":null,"title":"同型写像"},{"categories":"줄리아","contents":"概要 groupby()を使ってグループ別に分け、combine()を使って計算すればいいんだ1。\ngroupby(df, :colname)\n:colnameを基準にしてGroupedDataFrameを返す。 combine(gdf, :colname =\u0026gt; fun)\ngdfはグループ別に分かれたGroupedDataFrameだ。 :colname =\u0026gt; funは、計算したい値が入った列の名前のシンボル:colnameと、計算する関数funのペアだ。 コード using DataFrames\rusing StatsBase\rWJSN = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;다원\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;소정\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;연정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;진숙\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [99,97,97,95,96,99,98,95,99,94],\rheight = [161,167,157,166,159,165,172,163,162,165],\runit = [\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;]\r)\rsort!(WJSN, :birth)\runique(WJSN, :unit)\runits = groupby(WJSN, :unit)\runits[1]\runits[2]\runits[3]\rcombine(units, :height =\u0026gt; mean) 上の例のコードを実行して、その結果を確認してみよう。\njulia\u0026gt; WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 소정 95 166 더블랙\r3 │ 지연 95 163 더블랙\r4 │ 수빈 96 159 쪼꼬미\r5 │ 다원 97 167 메보즈\r6 │ 루다 97 157 쪼꼬미\r7 │ 주연 98 172 더블랙\r8 │ 다영 99 161 쪼꼬미\r9 │ 연정 99 165 메보즈\r10 │ 진숙 99 162 쪼꼬미 WJSNのデータフレームは上のようだ。\nグループ別に分ける groupby() julia\u0026gt; units = groupby(WJSN, :unit)\rGroupedDataFrame with 3 groups based on key: unit\rFirst Group (4 rows): unit = \u0026#34;더블랙\u0026#34;\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 소정 95 166 더블랙\r3 │ 지연 95 163 더블랙\r4 │ 주연 98 172 더블랙\r⋮\rLast Group (2 rows): unit = \u0026#34;메보즈\u0026#34;\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다원 97 167 메보즈\r2 │ 연정 99 165 메보즈 :unit列を基準にデータフレームが三つのグループに分かれた。\njulia\u0026gt; units[1]\r4×4 SubDataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 소정 95 166 더블랙\r3 │ 지연 95 163 더블랙\r4 │ 주연 98 172 더블랙\rjulia\u0026gt; units[2]\r4×4 SubDataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 수빈 96 159 쪼꼬미\r2 │ 루다 97 157 쪼꼬미\r3 │ 다영 99 161 쪼꼬미\r4 │ 진숙 99 162 쪼꼬미\rjulia\u0026gt; units[3]\r2×4 SubDataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다원 97 167 메보즈\r2 │ 연정 99 165 메보즈 上のようにGroupedDataFrameにインデクシングをすることで、分かれたデータフレームにアクセスできる。\nグループ別に計算する combine() julia\u0026gt; combine(units, :height =\u0026gt; mean)\r3×2 DataFrame\rRow │ unit height_mean │ String Float64 ─────┼─────────────────────\r1 │ 더블랙 166.5\r2 │ 쪼꼬미 159.75\r3 │ 메보즈 166.0 上のコードは、:unitを基準にグループ化されたデータフレームunitsの中で、WJSNのデータフレームの:heightの平均meanを計算したものだ。概要で言及された通り、このStatBase.mean()は平均を計算する関数だ。これをsum()に変えれば合計、min()に変えればグループ別の最小値を計算する。この例では、:unit別に:heightの平均を計算し、쪼꼬미グループが159.75で一番低いことがわかる。\nhttps://stackoverflow.com/questions/64226866/groupby-with-sum-on-julia-dataframe\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2254,"permalink":"https://freshrimpsushi.github.io/jp/posts/2254/","tags":null,"title":"ジュリアでデータフレームをグループ分けして計算する方法"},{"categories":"선형대수","contents":"定義1 $V, W$をベクトル空間、$T : V \\to W$を線形変換とする。線形変換$U : W \\to V$が次を満たすなら、$U$を$T$の逆inverse of $T$または逆変換という。\n$$ TU = I_{W} \\quad \\text{and} \\quad UT = I_{V} $$\n$TU$は$U$と$T$の合成、$I_{X} : X \\to X$は恒等変換だ。$T$に逆変換がある場合、$T$を可逆invertible変換という。$T$が可逆なら、逆変換$U$は唯一で、$T^{-1} = U$と表記される。\n説明 定義により、変換が可逆であることは全単射関数であることと同等だ。\n性質 (a) $(TU)^{-1} = U^{-1}T^{-1}$\n(b) $(T^{-1})^{-1} = T$\n(c) $T : V \\to W$が線形変換で、$V, W$が同じ次元の有限次元ベクトル空間である場合、\n$$ \\rank (T) = \\dim (V) $$\n$\\rank (T)$は$T$の階数だ。\n(d) 線形変換$T : V \\to W$の逆$T^{-1} : W \\to V$も線形変換だ。\n(e) 可逆線形変換$T : V \\to W$に関して、$V$が有限次元であることの必要十分条件は$W$が有限次元であることだ。この場合$\\dim(V) = \\dim(W)$が成り立つ。\n(f) $T$が可逆であることは$[T]_{\\beta}^{\\gamma}$が可逆であることと同等だ。さらに$[T^{-1}]_{\\beta}^{\\gamma} = ([T]_{\\beta}^{\\gamma})^{-1}$。この時、$[T]_{\\beta}^{\\gamma}$は$T$の行列表現だ。\n証明 (d) $\\mathbf{x}_{1}, \\mathbf{x}_{2} \\in V$とし、$k$を任意の定数とする。すると、$T$が線形であるため、次が成り立つ。\n$$ \\begin{align*} T^{-1} \\left( T(\\mathbf{x}_{1}) + k T(\\mathbf{x}_{2}) \\right) \u0026amp;= T^{-1} \\left( T(\\mathbf{x}_{1} + k \\mathbf{x}_{2}) \\right) \\\\ \u0026amp;= \\mathbf{x}_{1} + k \\mathbf{x}_{2} \\\\ \u0026amp;= T^{-1}\\left( T(\\mathbf{x}_{1}) \\right) + kT^{-1}\\left( T(\\mathbf{x}_{2}) \\right) \\end{align*} $$\n■\n(e)2 $V$が有限次元で、$\\beta = \\left\\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{n} \\right\\}$が$V$の基底であるとする。すると、$T(\\beta)$は値域$R(T)=W$を生成する。\n$$ \\span (T(\\beta)) = R(T) = W $$\nしたがって、$W$は有限次元だ。逆も同様に成り立つ。\n今、$V, W$が有限次元であると仮定する。$T$が単射で全射であるため、\n$$ \\nullity (T) = 0 \\quad \\text{and} \\quad \\rank(T) = \\dim(R(T)) = \\dim(W) $$\n次元定理により、\n$$ \\dim(V) = \\rank(T) + \\nullity(T) = \\dim(W) $$\n■\n(f)2 $(\\Longrightarrow)$\n$T : V \\to W$が可逆であると仮定する。すると(e)により$\\dim(V) = n = \\dim(W)$だ。すると、$[T]_{\\beta}^{\\gamma}$は$n \\times n$行列だ。$T$の逆$T^{-1}$について$T^{-1}T = I_{V}, TT^{-1} = I_{W}$なので、\n$I_{n} = [I_{V}]_{\\beta} = \\href{../3074}{[T^{-1}T]_{\\beta} = {[T^{-1}]_{\\gamma}^{\\beta}[T]_{\\beta}^{\\gamma}}}$\n同様に、$I_{n} = [I_{W}]_{\\beta} = [TT^{-1}]_{\\beta} = [T]_{\\beta}^{\\gamma}[T^{-1}]_{\\gamma}^{\\beta}$が成り立つ。したがって、$[T]_{\\beta}^{\\gamma}$は可逆行列であり、逆行列は$([T]_{\\beta}^{\\gamma})^{-1} = [T^{-1}]_{\\gamma}^{\\beta}$だ。\n$(\\Longleftarrow)$\n$A = [T]_{\\beta}^{\\gamma}$が可逆行列であると仮定する。すると、$AB = BA = I$を満たす$n \\times n$行列$B$が存在する。すると、次のように定義される線形変換$U : W \\to V$が唯一に存在する。\n$$ U(\\mathbf{w}_{j}) = \\sum_{i=1}^{n}B_{ij}\\mathbf{v}_{i} \\quad \\text{ for } j = 1,\\dots,n $$\nこの時、$\\gamma = \\left\\{ \\mathbf{w}_{1}, \\dots, \\mathbf{w}_{n} \\right\\}, \\beta = \\left\\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{n} \\right\\}$だ。すると、$U$の行列表現は$[U]_{\\gamma}^{\\beta} = B$だ。すると、次が成り立つ。\n$$ [UT]_{\\beta} = [U]_{\\gamma}^{\\beta} [T]_{\\gamma}^{\\beta} = BA = I_{n} = [I_{V}]_{\\beta} $$\nしたがって、$UT = I_{V}$であり、同様に、$TU = I_{W}$が成り立つ。したがって、$T$は可逆変換だ。\n■\nStephen H. Friedberg, Linear Algebra (4th Edition, 2002), p99-100\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStephen H. Friedberg, Linear Algebra (4th Edition, 2002), p101\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3285,"permalink":"https://freshrimpsushi.github.io/jp/posts/3285/","tags":null,"title":"線形変換の逆"},{"categories":"줄리아","contents":"概要 これを実現するためには、unique()を使えばいい。正確には、重複した行を削除するというよりも、一つだけ残すことだ。\nコード using DataFrames\rWJSN = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;다원\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;소정\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;연정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;진숙\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [99,97,97,95,96,99,98,95,99,94],\rheight = [161,167,157,166,159,165,172,163,162,165],\runit = [\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;]\r)\rsort!(WJSN, :birth)\runique(WJSN, :unit) 上の例のコードを実行して、その結果を確認してみよう。\njulia\u0026gt; WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 소정 95 166 더블랙\r3 │ 지연 95 163 더블랙\r4 │ 수빈 96 159 쪼꼬미\r5 │ 다원 97 167 메보즈\r6 │ 루다 97 157 쪼꼬미\r7 │ 주연 98 172 더블랙\r8 │ 다영 99 161 쪼꼬미\r9 │ 연정 99 165 메보즈\r10 │ 진숙 99 162 쪼꼬미 WJSNデータフレームは上のようになっている。\n単一の列で重複した行を削除する unique() julia\u0026gt; unique(WJSN, :unit)\r3×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 현정 94 165 더블랙\r2 │ 수빈 96 159 쪼꼬미\r3 │ 다원 97 167 메보즈 :unitシンボルごとに、一つの行だけが残っているのを確認できる。\n環境 OS: Windows julia: v1.6.3 ","id":2252,"permalink":"https://freshrimpsushi.github.io/jp/posts/2252/","tags":null,"title":"JuliaでDataFrameの重複した行を削除する方法"},{"categories":"수리통계학","contents":"定義 累積分布関数 $F$ について $F_{\\theta}$ が全ての $x$ に対し $F_{\\theta} (x) = F \\left( x - \\theta \\right)$ を満たすとする。\n$\\left\\{ F_{\\theta} : \\theta \\in \\mathbb{R} \\right\\}$ をロケーションファミリーと呼ぶ。\n例 1 パラメーター $\\theta$ のランダムサンプル $X_{1} , \\cdots , X_{n}$ が累積分布関数 $F_{0} (x) = F (x - 0) = F(x)$ を持つと考えると、サンプル $Z_{1} , \\cdots , Z_{n}$ は $$ X_{i} = Z_{i} + \\theta $$ と表される。このサンプルの統計量としての範囲Rangeの長さ $R = X_{n} - X_{(1)}$ は、実際には $\\theta$ がどうであれ一定であるべきだ。$\\theta$ は単に値の大きさを増加させたり減少させたりするだけで、その分散には影響を与えないからである。実際に $R$ のジョイント累積分布関数は $$ \\begin{align*} F_{R} \\left( r ; \\theta \\right) =\u0026amp; P_{\\theta} \\left( R \\le r \\right) \\\\ =\u0026amp; P_{\\theta} \\left( X_{(n)} - X_{(1)} \\le r \\right) \\\\ =\u0026amp; P_{\\theta} \\left( \\max_{k} X_{k} - \\min_{k} X_{k} \\le r \\right) \\\\ =\u0026amp; P_{\\theta} \\left( \\max_{k} \\left( Z_{k} + \\theta \\right) - \\min_{k} \\left( Z_{k} + \\theta \\right) \\le r \\right) \\\\ =\u0026amp; P_{\\theta} \\left( \\max_{k} \\left( Z_{k} \\right) + \\theta - \\min_{k} \\left( Z_{k} \\right) - \\theta \\le r \\right) \\\\ =\u0026amp; P_{\\theta} \\left( Z_{(n)} - Z_{(1)} \\le r \\right) \\end{align*} $$ である。言い換えると、$R$ は$\\theta$ の補助統計量である。\n参照 指数ファミリー スケールファミリー Casella. (2001). Statistical Inference(2nd Edition): p283.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2251,"permalink":"https://freshrimpsushi.github.io/jp/posts/2251/","tags":null,"title":"ロケーションファミリー"},{"categories":"기하학","contents":"ビルドアップ 微分多様体上のベクトル場$\\mathbf{V}$が与えられたとしよう。多様体上で定義された関数は、ベクトル場を通して微分できる。すると、自然とベクトル場自体を微分したくなる。しかし、$\\mathbb{R}^{3}$のベクトル場の微分を微分幾何の観点からは不可能であると以下のように理解できる。\n一番目の場合\n$S \\subset \\mathbb{R}^{3}$を面、$c : I \\to S$を$S$上で定義された曲線としよう。そして、$\\mathbf{V}$は$c$に従うベクトル場とする。すると、$\\mathbf{V}(t)$は$c(t)$上の接ベクトルになる。\n$$ \\mathbf{V}(t) \\in T_{c(t)}S $$\nそれにより、以下のように座標ベクトルで表すことができる。\n$$ \\mathbf{V}(t) = \\left( V_{1}(t), V_{2}(t), V_{3}(3) \\right) $$\nだから、次のようにベクトルを微分したくなるだろう。\n$$ \\dfrac{d \\mathbf{V}}{d t}(t) = \\left( V_{1}^{\\prime}(t), V_{2}^{\\prime}(t), V_{3}^{\\prime}(3) \\right) $$\nしかし、$\\mathbf{V}$の導関数を上記のように定義すると、これは一般に接ベクトルにならない。\n$$ \\dfrac{d \\mathbf{V}}{d t}(t) \\notin T_{c(t)}S $$\n微分幾何では、内在的な性質を持つ対象に関心があるが、上記の定義ではベクトル場の導関数が内在的でなくなり、そのため、ベクトル場を再度接束$TS$に投影してそれを導関数として扱う。$\\Pi : \\mathbb{R}^{3} \\to TS$を直交射影としよう。すると、ベクトル場の導関数を以下のように定義する。\n$$ \\dfrac{D \\mathbf{V}}{d t}(t) := \\Pi \\circ \\dfrac{d \\mathbf{V}}{d t}(t) $$\nこれを共変導関数と呼び、内在的である。\n二番目の場合\n以下のように極限で定義した関数の微分を考えよう。\n$$ \\dfrac{d \\mathbf{v}}{d t}(t) = \\lim\\limits_{h \\to 0} \\dfrac{\\mathbf{V}(t+h) - \\mathbf{V}(t)}{h} $$\nしかし、$\\mathbf{V}(t+h) \\in T_{c(t+h)}S$であり$\\mathbf{V}(t) \\in T_{c(t)}S$であるため、分子の二項は異なる空間の要素である。だから、加算演算が不可能である。\nこれらの理由から、ベクトル場の微分は、微分が持つべき形式的な条件を満たす抽象的な概念として定義される。\n定義 $\\mathfrak{X}(M)$1を、微分多様体$M$上の$C^{\\infty}$ベクトル場の集合としよう。\n$$ \\mathfrak{X}(M) := \\left\\{ \\text{all vector fields of class } C^{\\infty} \\text{ on } M \\right\\} $$\n$\\mathcal{D}(M)$を$M$上で定義された$C^{\\infty}$関数の集合としよう。\n$$ \\mathcal{D}(M) := \\left\\{ \\text{all real-valued functions of class } C^{\\infty} \\text{ defined on } M \\right\\} $$\nそれでは、微分多様体$M$上のアファイン接続$\\nabla$は\n$$ \\begin{align*} \\nabla : \\mathfrak{X}(M) \\times \\mathfrak{X}(M)\u0026amp; \\to \\mathfrak{X}(M) \\\\ (X, Y) \u0026amp;\\mapsto \\nabla_{X}Y \\end{align*} $$\nこのような写像であり、以下の性質を満たすものと定義される。\n$\\nabla_{fX + gY} Z = f \\nabla _{X}Z + g\\nabla_{Y}Z$ $\\nabla_{X}(Y + Z) = \\nabla_{X}Y + \\nabla_{X}Z$ $\\nabla_{X}(fX) = f\\nabla_{X}Y + X(f) Y$ 説明 $\\nabla_{X}Y$で、$X$は微分される変数であり、$Y$は微分される関数を意味する。したがって、1. ~ 3.はそれぞれ微分の以下のような性質を表している。\n1.　$\\left( a\\dfrac{\\partial }{\\partial x} + b\\dfrac{\\partial }{\\partial y} \\right)f = a\\dfrac{\\partial f}{\\partial x} + b\\dfrac{\\partial f}{\\partial y}$\n2.　$\\dfrac{\\partial }{\\partial x}(f+ g) = \\dfrac{\\partial f}{\\partial x} + \\dfrac{\\partial g}{\\partial x}$\n3.　$\\dfrac{\\partial }{\\partial x}(fg) = \\dfrac{\\partial f}{\\partial x}g + f\\dfrac{\\partial g}{\\partial x}$\nしたがって、$\\nabla_{X}$は$\\dfrac{\\partial}{\\partial x}$と解釈され、$Y$は$f$のように解釈される。\n定理 $(\\nabla_{X}Y)(p)$は、$X(p)$および$Y(\\gamma (t))$にのみ依存する。この時、$\\gamma$は\n$$ \\gamma : (-\\epsilon, \\epsilon) \\to M \\\\ \\gamma (0) = p \\\\ \\gamma^{\\prime}(0) = X(p) $$\nこの条件を満たす曲線である。\n証明 座標$\\mathbf{x} : U \\to M$を1つ選ぼう。そして、$X, Y$をベクトル場とする。\n$$ X = \\sum_{i} X_{i} \\dfrac{\\partial }{\\partial x_{i}},\\quad Y = \\sum_{j} Y_{j}\\dfrac{\\partial }{\\partial x_{j}} $$\nすると、$\\nabla$の性質により、\n$$ \\begin{align*} \\nabla_{X}Y =\u0026amp; \\nabla_{\\sum_{i} X_{i}\\frac{\\partial}{\\partial x_{i}}}\\sum_{j}Y_{j}\\dfrac{\\partial }{\\partial x_{j}} \\\\ =\u0026amp; \\sum_{i,j} \\nabla_{X_{i}\\frac{\\partial}{\\partial x_{i}}}Y_{j}\\dfrac{\\partial }{\\partial x_{j}} \u0026amp;\\text{by 1. and 2.}\\\\ =\u0026amp; \\sum_{i,j} X_{i}\\nabla_{\\frac{\\partial}{\\partial x_{i}}}Y_{j}\\dfrac{\\partial }{\\partial x_{j}} \u0026amp;\\text{by 1.} \\\\ =\u0026amp; \\sum_{i,j} X_{i} \\left( \\dfrac{\\partial Y_{j}}{\\partial x_{i}}\\dfrac{\\partial }{\\partial x_{j}} + Y_{j}\\nabla_{\\frac{\\partial}{\\partial x_{i}}}\\dfrac{\\partial }{\\partial x_{j}} \\right) \u0026amp;\\text{by 3.} \\end{align*} $$\nこの時、$\\nabla_{\\frac{\\partial}{\\partial x_{j}}}\\dfrac{\\partial }{\\partial x_{j}}$はベクトル場と無関係であり、純粋に座標の選択にのみ依存する値であることがわかる。これもアファイン接続の定義によるベクトル場であるため、係数を$\\Gamma_{ij}^{k}$とすると、以下のように書ける。\n$$ \\nabla_{\\frac{\\partial }{\\partial x_{i}}}\\dfrac{\\partial }{\\partial x_{j}} = \\sum_{k} \\Gamma_{ij}^{k} \\dfrac{\\partial }{\\partial x_{k}} $$\nこれを代入すると、\n$$ \\begin{align*} \\nabla_{X}Y =\u0026amp; \\sum_{i,j} X_{i} \\left( \\dfrac{\\partial Y_{j}}{\\partial x_{i}}\\dfrac{\\partial }{\\partial x_{j}} + Y_{j}\\nabla_{\\frac{\\partial}{\\partial x_{i}}}\\dfrac{\\partial }{\\partial x_{j}} \\right) \\\\ =\u0026amp; \\sum_{i,j} X_{i} \\left( \\dfrac{\\partial Y_{j}}{\\partial x_{i}}\\dfrac{\\partial }{\\partial x_{j}} + Y_{j}\\sum_{k} \\Gamma_{ij}^{k} \\dfrac{\\partial }{\\partial x_{k}} \\right) \\\\ =\u0026amp; \\sum_{i,j} X_{i} \\dfrac{\\partial Y_{j}}{\\partial x_{i}}\\dfrac{\\partial }{\\partial x_{j}} + \\sum_{i,j,k} X_{i}Y_{j}\\Gamma_{ij}^{k} \\dfrac{\\partial }{\\partial x_{k}} \\end{align*} $$\nここで、$i,j,k$はダミーインデックスなので、前項の$j$を$k$に変えよう。そうすると、\n$$ \\begin{align*} \\nabla_{X}Y =\u0026amp; \\sum_{i,k} X_{i} \\dfrac{\\partial Y_{k}}{\\partial x_{i}}\\dfrac{\\partial }{\\partial x_{k}} + \\sum_{i,j,k} X_{i}Y_{j}\\Gamma_{ij}^{k} \\dfrac{\\partial }{\\partial x_{k}} \\\\ =\u0026amp; \\sum_{i,k} X_{i} \\left( \\dfrac{\\partial Y_{k}}{\\partial x_{i}} + \\sum_{j} Y_{j}\\Gamma_{ij}^{k}\\right) \\dfrac{\\partial }{\\partial x_{k}} \\\\ \\end{align*} $$\nここで、$\\Gamma_{ij}^{k}, \\dfrac{\\partial }{\\partial x_{k}}$は与えられた座標によって決定される。$\\dfrac{\\partial Y_{k}}{\\partial x_{i}}$も$Y_{k}$が決まれば同様に座標があれば決定される。したがって、上記の式は純粋に$X(p), Y(\\gamma (t))$の値にのみ依存することがわかる。\n■\nリー環Xである。\\mathfrak{X}\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3282,"permalink":"https://freshrimpsushi.github.io/jp/posts/3282/","tags":null,"title":"アフィン接続"},{"categories":"줄리아","contents":"概要 Juliaでは、サブプロットに関連するオプションはlayoutオプションを通して制御できる。\n整数を入力すると、その数だけのグリッドをうまく作ってくれる。 整数の2-タプルを入力すると、指定どおりのグリッドを作ってくれる。 @layoutマクロを使ってPlots.GridLayoutタイプの複雑なレイアウトを構成する。 コード using Plots\rleft = plot(randn(100), color = :red)\rright = plot(randn(100), color = :blue)\rplot(left, right)\rpng(\u0026#34;easyone\u0026#34;)\rdata = rand(10, 6)\rplot(data, layout = 6)\rpng(\u0026#34;easytwo\u0026#34;)\rplot(data, layout = (3,2))\rpng(\u0026#34;easygrid\u0026#34;)\rl = @layout [p1 ; p2 p2]\rp = plot(\rplot(rand(10)),\rplot(rand(100)),\rplot(rand(1000)),\rlayout = l)\rpng(\u0026#34;hardgrid\u0026#34;) 簡単な列挙 left = plot(randn(100), color = :red)\rright = plot(randn(100), color = :blue)\rplot(left, right) ただ、プロットを複数同時にまとめてプロットし直しても、とりあえずサブプロットにはなる。\n簡単なレイアウトlayout plot(data, layout = 6) plot(data, layout = (3,2)) 簡単に整数でやるのもいいし、具体的にタプルを渡して望んだ通りのグリッドを作ることができる。もちろん、整数を渡さなくても同じように動くから、タプルを渡す場合だけ覚えておけばいい。\n複雑なレイアウト@layout l = @layout [p1 ; p2 p2] 単純なグリッドではなく、1行目に1つの絵、2行目に2列の絵が入るように指示するPlots.GridLayoutタイプのlを定義した。これをlayoutの入力として与えると、次のようにずっと複雑なレイアウトが表現される。\np = plot(\rplot(rand(10)),\rplot(rand(100)),\rplot(rand(1000)),\rlayout = l) 空白の作成_ 上記の例で、1行目の絵を下の行と同じサイズで、中央に配置したい場合は、両側に空間を作ればいい。_で空白を示すことができる。{0.5w}で幅を全体の半分に合わせれば、\nl = @layout([_ p{0.5w} _; p p])\rplot(\rplot(rand(10)),\rplot(rand(100)),\rplot(rand(1000)),\rlayout = l) 環境 OS: Windows julia: v1.6.3 ","id":2250,"permalink":"https://freshrimpsushi.github.io/jp/posts/2250/","tags":null,"title":"ジュリアでレイアウトを使ってサブプロットを描く方法"},{"categories":"선형대수","contents":"定義1 $V$をベクトル空間としよう。$V$から$\\mathbb{C}$（または$\\mathbb{R}$）への写像$f$を汎関数functionalという。\n$$ f : V \\to \\mathbb{C} $$\n$f$が線形ならば、線形汎関数という。\nもっと詳しい定義2 $V$をフィールド$F$上のベクトル空間としよう。この時、フィールド$F$自体が$F$上の$1$次元ベクトル空間になる。線形変換 $f : V \\to F$を線形汎関数linear functionalという。\n言い換えれば、線形汎関数とはベクトル空間とその体の間の線形変換である。\n説明 よく見る定義は最初の定義だ。通常、二番目のように抽象的に定義することはない。\n汎関数を韓国語に訳すと「범함수」となり、あまり特徴がないが、英語ではfunctionalが形容詞ではなく名詞であることに注意する必要がある。また、汎関数（汎函数）という翻訳はgeneralized function一般化関数からの影響を受けている。\n線形作用素との区別は、値域が$\\mathbb{R}$または$\\mathbb{C}$と定義されるという点のみで、まさにこの違いが双対空間のような空間を考える価値があるということである。すぐにノルム$\\| \\cdot \\| = \\| \\cdot \\|_{V}$はそれ自体が汎関数になり、測度論との関連で考えると、有用であることが避けられない。\n例 トレース $V = M_{n\\times n}(\\mathbb{R})$としよう。関数$f$を次のように定義しよう。\n$$ f : M_{n\\times n}(\\mathbb{R}) \\to \\mathbb{R} \\quad \\text{ by } \\quad f(A) = \\tr(A) $$\nこの時、$\\tr$は行列のトレースである。したがって、$f$は線形汎関数である。\nフーリエ係数 $V$を$f : [0, 2\\pi] \\to \\mathbb{R}$を満たす連続関数のベクトル空間としよう。固定された$g \\in V$に対して、$h : V \\to \\mathbb{R}$を次のように定義しよう。\n$$ h(f) = \\dfrac{1}{2\\pi} \\int_{0}^{2\\pi} f(x)g(x)dx $$\nすると、$h$は線形汎関数である。$g$が$\\cos nx$または$\\sin nx$の時、$h(f)$は$f$のフーリエ係数になる。\n座標関数 $V$を有限次元ベクトル空間としよう。$\\beta = \\left\\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{n} \\right\\}$を$V$の順序基底としよう。$\\mathbf{x} \\in V$の座標ベクトルが次のようであるとしよう。\n$$ [\\mathbf{x}]_{\\beta} = \\begin{bmatrix} a_{1} \\\\ a_{2} \\\\ \\vdots \\\\ a_{n} \\end{bmatrix} $$\n今、$1 \\le i \\le n$に対して次のような関数を考えよう。\n$$ f_{i}(\\mathbf{x}) = a_{i} $$\nすると、$f_{i}$は$V$上で定義された線形汎関数であり、これを**$i$番目の座標関数**$i$th coordinate function with respect to the ordered basis $\\beta$という。すると、$f_{i}(\\mathbf{v}_{i}) = \\delta_{ij}$が成立する。$\\delta_{ij}$はクロネッカーのデルタである。座標関数は双対空間を語る上で重要な役割を果たす。\nKreyszig. (1989). Introductory Functional Analysis with Applications: p103~104.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStephen H. Friedberg, Linear Algebra (4th Edition, 2002), p119\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3281,"permalink":"https://freshrimpsushi.github.io/jp/posts/3281/","tags":null,"title":"線形汎関数"},{"categories":"수리통계학","contents":"定義 1 $S$がサンプルの$\\mathbf{X}$の統計量だとしよう。$S \\left( \\mathbf{X} \\right)$の分布が母数$\\theta$に依存しない場合、補助統計量Ancillary Statisticと言う。\n説明 実際に話している時、誰も補助統計量とは言わず、[アンシラリースタティスティック]と発音する。\n十分統計量が$\\theta$に関して全ての情報を持つ感じだったら、補助統計量は$\\theta$に関する情報が全くない統計量と考えられる。\n例えば、正規分布$N \\left( \\mu , \\sigma^{2} \\right)$から得られたランダムサンプル$X_{1} , \\cdots , X_{n}$を考えてみよう。統計量の一つである標本分散 $$ S^{2} = {{ 1 } \\over { n -1 }} \\sum_{k=1}^{n} X_{k}^{2} $$ は母分散$\\sigma^{2}$に対する十分統計量だが、スチューデントの定理によると、 $$ {{ n-1 } \\over { \\sigma^{2} }} S^{2} \\sim \\chi_{n-1}^{2} $$ となる。つまり、標本分散が従うカイ二乗分布には$\\mu$が現れず、$\\mu$に対しては補助統計量である。\nCasella. (2001). Statistical Inference(2nd Edition): p282.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2249,"permalink":"https://freshrimpsushi.github.io/jp/posts/2249/","tags":null,"title":"補助統計量"},{"categories":"줄리아","contents":"概要 1 plot() 関数の legend オプションで、凡例の位置を自由に調整できる。$0$ から $1$ までの値で構成された2-タプルを与えると、正確にその位置に表示されるし、それ以外はシンボルで制御できる。\nシンボルの場合は、top/bottom と left/right を順に結びつけて組み合わせる。最初に outer を付けると、図の外側に凡例が表示される。組み合わせで作られるシンボルの例は以下の通り:\n:bottom :left :bottomleft :outertopright したがって、:leftbottom や :toprightouter のようなシンボルは許されない。\nコード data = randn(100, 2)\rplot(data)\rplot(data, legend = (0.5, 0.7)); png(\u0026#34;tuple\u0026#34;)\rSymbols = [:none, :bottom, :left, :bottomleft, :outertopright, :inline]\rfor symbol ∈ Symbols\rplot(data, legend = symbol)\rpng(string(symbol))\rend 正確な位置の指定 legend = (0.5, 0.7) タプル (0.5, 0.7) は、横軸の50%、縦の高さの70%ぐらいに凡例が表示される。\n凡例の除去 :none 上下左右 :bottom, :left 組み合わせ :bottomleft 外側 :outertopright 図の外側に凡例を出力した。このため、図が歪むことに注意が必要だ。\n線の終わり :inline 多くの線があり、色で区別するのが難しい、または最後の値が特に重要な場合に便利だ。\n環境 OS: Windows julia: v1.6.3 https://docs.juliaplots.org/latest/generated/attributes_subplot/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2248,"permalink":"https://freshrimpsushi.github.io/jp/posts/2248/","tags":null,"title":"ジュリアで図の凡例の位置を調整する方法"},{"categories":"선형대수","contents":"定義1 $V$を有限次元ベクトル空間としよう。$V$の基底に特定の順序が与えられた場合、これを順序基底という。\n$\\beta = \\left\\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{n} \\right\\}$を$V$の順序基底とする。すると、基底表示の一意性により、$\\mathbf{v} \\in V$に対して、以下のように成立するスカラー$a_{i}$が唯一に存在する。\n$$ \\mathbf{v} = a_{1}\\mathbf{v}_{1} + \\dots a_{n}\\mathbf{v}_{n} $$\n$a_{1},\\dots,a_{n}$を基底$\\beta$に関する$\\mathbf{v}$の座標と言い、$i$番目の座標を$i$番目の成分として持つ行列を基底$\\beta$に関する$\\mathbf{v}$の座標ベクトルまたは座標行列と言い、$[\\mathbf{v}]_{\\beta}$のように表記する。\n$$ [\\mathbf{v}]_{\\beta} = \\begin{bmatrix} a_{1} \\\\ a_{2} \\\\ \\vdots \\\\ a_{n} \\end{bmatrix} $$\nまた、順序基底$\\beta$を座標系と言う。\n説明 基底は集合として定義されるが、集合を表現する際の要素の列挙順序は関係ない、つまり$\\alpha = \\left\\{ \\mathbf{e}_{1}, \\mathbf{e}_{2}, \\mathbf{e}_{3} \\right\\} = \\left\\{ \\mathbf{e}_{2}, \\mathbf{e}_{3}, \\mathbf{e}_{1} \\right\\} = \\beta$である。したがって、\u0026lsquo;座標\u0026rsquo;という概念を抽象化するためには、基底の要素に順序を与える必要がある。今、$\\alpha, \\beta$を順序基底とすると、\n$$ \\alpha = \\left\\{ \\mathbf{e}_{1}, \\mathbf{e}_{2}, \\mathbf{e}_{3} \\right\\} \\ne \\left\\{ \\mathbf{e}_{2}, \\mathbf{e}_{3}, \\mathbf{e}_{1} \\right\\} = \\beta $$\n$[\\mathbf{v}_{i}]_{\\beta} = \\mathbf{e}_{i}$が成立する。$\\mathbf{e}_{i}$は標準基底である。\n関数$T : \\mathbf{v} \\mapsto [\\mathbf{v}]_{\\beta}$は$V$から$\\mathbb{R}^{n}$への線形変換となる。\nベクトル空間$\\mathbb{R}^{n}$に対して、$\\left\\{ \\mathbf{e}_{1}, \\dots, \\mathbf{e}_{n} \\right\\}$を標準順序基底と言う。\nStephen H. Friedberg, Linear Algebra (第4版, 2002), p79-80\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3279,"permalink":"https://freshrimpsushi.github.io/jp/posts/3279/","tags":null,"title":"順序基底と座標ベクトル"},{"categories":"줄리아","contents":"概要 1 グラフの幅と高さを調整するには、オプションにratioを入れるといい。他の推奨される別名には、aspect_ratios, axis_ratioがある。\nratio = :none: デフォルト値で、グラフのサイズに合わせて比率が調整される。 ratio = :equal: グラフのサイズにかかわらず、横軸と縦軸が1対1の比率に調整される。 ratio = Number: Numberに従って比率が調整される。Numberは${{세로} \\over {가로}} = {{\\Delta y} \\over {\\Delta x}}$の比率として与えられる。 コード using Plots\rx = rand(100)\ry = randn(100)\rplot(x,y,seriestype = :scatter, ratio = :none)\rpng(\u0026#34;none\u0026#34;)\rplot(x,y,seriestype = :scatter, ratio = :equal)\rpng(\u0026#34;equal\u0026#34;)\rplot(x,y,seriestype = :scatter, ratio = 0.5)\rpng(\u0026#34;0.5\u0026#34;)\rplot(x,y,seriestype = :scatter, ratio = 1)\rpng(\u0026#34;1\u0026#34;)\rplot(x,y,seriestype = :scatter, ratio = 2)\rpng(\u0026#34;2\u0026#34;) デフォルト :none plot(x,y,seriestype = :scatter, ratio = :none) 1対1 :equal plot(x,y,seriestype = :scatter, ratio = :equal) 特定の比率 Number Numberは${{세로} \\over {가로}}$の比率として与えられる。\nplot(x,y,seriestype = :scatter, ratio = 0.5) plot(x,y,seriestype = :scatter, ratio = 1) plot(x,y,seriestype = :scatter, ratio = 2) 環境 OS: Windows julia: v1.6.3 https://docs.juliaplots.org/latest/generated/attributes_subplot/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2246,"permalink":"https://freshrimpsushi.github.io/jp/posts/2246/","tags":null,"title":"ジュリア集合の絵のアスペクト比を調整する方法"},{"categories":"기하학","contents":"定義1 $n$次元の微分可能多様体 $M$に対するリーマン計量Riemannian metric, リーマン計量 $g$とは、各点$p \\in M$を$g_{p}$に対応させる関数のことだ。ここで、$g_{p}$は$p$上の接空間 $T_{p}M$で定義される内積である。\n$$ \\begin{align*} g : M \u0026amp;\\to \\left\\{ \\text{all inner products on tangent space } T_{p}M \\right\\} \\\\ p \u0026amp;\\mapsto g_{p}=\\left\\langle \\cdot, \\cdot \\right\\rangle_{p} \\end{align*} $$\n$$ \\begin{align*} g_{p} : T_{p}M \\times T_{p}M \u0026amp;\\to \\mathbb{R} \\\\ (\\mathbf{X}_{p}, \\mathbf{Y}_{p}) \u0026amp;\\mapsto g_{p}(\\mathbf{X}_{p}, \\mathbf{Y}_{p})=\\left\\langle \\mathbf{X}_{p}, \\mathbf{Y}_{p} \\right\\rangle_{p} \\end{align*} $$\nこの場合、$g_{p}$は次の意味で微分可能でなければならない。\n$\\mathbf{x} : U \\subset \\mathbb{R}^{n} \\to M$を$p$の周りの座標系とし、$\\mathbf{x}(x_{1}, \\dots, x_{n}) = p$とする。すると、次の$g_{ij} : \\mathbb{R}^{n} \\to \\mathbb{R}$が微分可能でなければならない。\n$$ g_{ij} (x_{1}, \\dots, x_{n}) = \\left\\langle \\left. \\dfrac{\\partial }{\\partial x_{i}} \\right|_{p}, \\left. \\dfrac{\\partial }{\\partial x_{j}} \\right|_{p}\\right\\rangle_{p} $$\n$g_{ij}$をリーマン計量の局所表現と言う。リーマン計量が与えられた微分多様体$(M, g)$をリーマン多様体と呼ぶ。\n説明 リーマン多様体$(M, g)$を研究することをリーマン幾何学と呼ぶ。$g$をRiemannian structureとも呼ぶ。$g_{ij}$は微分幾何学で第1基本形式の係数と学ぶ。\n$\\mathbf{X}_{p}, \\mathbf{Y}_{p} \\in T_{p}M$としよう。すると、$T_{p}M$は基底が$\\left\\{ \\left. \\dfrac{\\partial }{\\partial x_{i}} \\right|_{p} \\right\\}$の$n$次元ベクトル空間であるため、\n$$ \\mathbf{X}_{p} = X^{i}(p)\\left. \\dfrac{\\partial }{\\partial x_{i}} \\right|_{p} \\text{ and } \\mathbf{Y}_{p} = Y^{j}(p)\\left. \\dfrac{\\partial }{\\partial x_{j}} \\right|_{p} $$\nだから、\n$$ g_{p}(\\mathbf{X}_{p}, \\mathbf{Y}_{p}) = \\left\\langle \\mathbf{X}_{p}, \\mathbf{Y}_{p} \\right\\rangle = X^{i}(p)Y^{j}(p) \\left\\langle \\left. \\dfrac{\\partial }{\\partial x_{i}} \\right|_{p}, \\left. \\dfrac{\\partial }{\\partial x_{j}} \\right|_{p} \\right\\rangle = X^{i}(p)Y^{j}(p)g_{ij}(p) $$\n$p$を一般化すると、\n$$ g(\\mathbf{X}, \\mathbf{Y}) = X^{i}Y^{j} \\left\\langle \\dfrac{\\partial }{\\partial x_{i}}, \\dfrac{\\partial }{\\partial x_{j}} \\right\\rangle = X^{i}Y^{j}g_{ij} $$\n微分可能性の条件の別の表現は次の通り。\n多様体上で定義される関数$g(\\mathbf{X}, \\mathbf{Y}) : M \\to \\mathbb{R}$が微分可能である。\n$$ g(\\mathbf{X}, \\mathbf{Y}) (p) = g_{p} (\\mathbf{X}_{p}, \\mathbf{Y}_{p}), \\quad \\mathbf{X}_{p}, \\mathbf{Y}_{p} \\in T_{p}M $$\nこの場合、$g(\\mathbf{X}, \\mathbf{Y})_{p} = g(\\mathbf{X}, \\mathbf{Y}) (p)$と表記されることもある。\n「すべての微分可能多様体はリーマン計量を持つ」という事実が知られている。したがって、研究の方向性は、「多様体$M$がリーマン計量を持つ条件」ではなく、「多様体$M$にどのような良いリーマン計量を与えることができるか」となる。\n誘導された計量 微分多様体$M, N$間のイマージョン $f : M \\to N$が与えられたとする。$(N, h)$がリーマン多様体であるとする。次のように定義される$M$上のリーマン計量$g$を$f$から誘導された計量と呼ぶ。\n$$ g_{p}(v, w) := h_{f(p)}(df_{p}(v), df_{p}(w)) $$\nこの場合、$df_{p}$は点$p$での$f$の微分である。\nユークリッド空間 微分可能多様体としてのユークリッド空間$M = \\mathbb{R}^{n}$を考えよう。すると、$T_{p}\\mathbb{R}^{n} \\approx \\mathbb{R}^{n}$であり、基底$\\left\\{ \\dfrac{\\partial }{\\partial x}_{i} \\right\\}$はユークリッド空間の標準基底$\\left\\{ e_{i} = (0, \\dots, 1, \\dots, 0) \\right\\}$と同じである。したがって、計量の係数は次の通りである。\n$$ g_{ij} = \\left\\langle e_{i}, e_{j} \\right\\rangle = \\delta_{ij} $$\nしたがって、ユークリッド空間のリーマン計量は、ユークリッド空間自体で標準的に定義される内積そのものである。$(\\mathbb{R}^{n}, g)$を研究することをユークリッド幾何学と呼ぶ。\nManfredo P. Do Carmo, Riemannian Geometry (Eng Edition, 1992), p38-39\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3276,"permalink":"https://freshrimpsushi.github.io/jp/posts/3276/","tags":null,"title":"リーマン計量とリーマン多様体"},{"categories":"줄리아","contents":"エラー using DataFrames, CSV\rexample = DataFrame(x = 1:10, 가 = \u0026#34;나다\u0026#34;)\rCSV.write(\u0026#34;example.csv\u0026#34;, example) JuliaでCSVファイルに出力するとき、上のように韓国語が文字化けする現象が見られる。\n原因 実際には韓国語が文字化けするわけではなく、Unicodeエンコーディングの問題で、特にUTF-8エンコーディングのBOMバイトオーダーマークが原因で起こる。PythonなどでエンコーディングをUTF-8-sigとすることで解決できる。\n解決方法 1 CSV.write(\u0026#34;example.csv\u0026#34;, example, bom = true) CSV.jlでは、単にbom = trueというオプションを指定すると、以下のように文字化けせずに出力される。\n環境 OS: Windows julia: v1.6.3 https://csv.juliadata.org/stable/writing.html#CSV.write\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2244,"permalink":"https://freshrimpsushi.github.io/jp/posts/2244/","tags":null,"title":"ジュリアでCSV出力時の文字化け解決方法"},{"categories":"줄리아","contents":"概要 ジュリアでテキスト出力を飾るパッケージとしてCrayons.jlが知られている1。\n組み込み関数だけで飾りたい場合は、printstyled()を使えばいい。\nコード using Crayons\rprint(Crayon(background = :red), \u0026#34;빨강\u0026#34;)\rprint(Crayon(foreground = :blue), \u0026#34;파랑\u0026#34;)\rprint(Crayon(bold = true), \u0026#34;볼드\u0026#34;)\rprint(Crayon(italics = true), \u0026#34;이탤릭\u0026#34;)\rprint(Crayon(bold = true, italics = true), \u0026#34;볼드 이탤릭\u0026#34;) 上記のコンソールを実行すると、次のように飾られた結果が得られる。\nCrayon(...)\nforeground: テキスト自体の色を変更する。シンボルや整数のトリプル(r,g,b)、0から255の間の整数で値を与えることができる。 background: テキストの背景色を変更する。引数を指定する方法はforegroundと同じだ。 ブール値で指定するオプションは以下のものがある。上記の例では、それぞれ個別に実行した結果と同時に実行した結果が示されている。\nbold: 太字。 italics: 斜体。 underline: 下線。 環境 OS: Windows julia: v1.6.3 https://github.com/KristofferC/Crayons.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2242,"permalink":"https://freshrimpsushi.github.io/jp/posts/2242/","tags":null,"title":"- ジュリアでのテキスト出力装飾パッケージ"},{"categories":"줄리아","contents":"コード 宇宙少女のデータフレームが以下のように与えられたとしよう。\nWJSN = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;다원\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;소정\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;연정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;진숙\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [99,97,97,95,96,99,98,95,99,94],\rheight = [161,167,157,166,159,165,172,163,162,165],\r) julia\u0026gt; WJSN\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161\r2 │ 다원 97 167\r3 │ 루다 97 157\r4 │ 소정 95 166\r5 │ 수빈 96 159\r6 │ 연정 99 165\r7 │ 주연 98 172\r8 │ 지연 95 163\r9 │ 진숙 99 162\r10 │ 현정 94 165 データフレームに新しい列を追加するコードは以下の通りである。\ndataframe[!, :\u0026quot;column_name\u0026quot;] = values ユニットに関する列を追加してみると、\nWJSN[!, :\u0026#34;unit\u0026#34;] = [\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;메보즈\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;더블랙\u0026#34;,\u0026#34;쪼꼬미\u0026#34;,\u0026#34;더블랙\u0026#34;]\rjulia\u0026gt; WJSN\r10×4 DataFrame\rRow │ member birth height unit │ String Int64 Int64 String ─────┼───────────────────────────────\r1 │ 다영 99 161 쪼꼬미\r2 │ 다원 97 167 메보즈\r3 │ 루다 97 157 쪼꼬미\r4 │ 소정 95 166 더블랙\r5 │ 수빈 96 159 쪼꼬미\r6 │ 연정 99 165 메보즈\r7 │ 주연 98 172 더블랙\r8 │ 지연 95 163 더블랙\r9 │ 진숙 99 162 쪼꼬미\r10 │ 현정 94 165 더블랙 環境 OS: Windows10 Version: Julia 1.7.1, DataFrames 1.3.2 ","id":3273,"permalink":"https://freshrimpsushi.github.io/jp/posts/3273/","tags":null,"title":"ジュリアでデータフレームに新しい列を追加する方法"},{"categories":"기하학","contents":"定義1 微分多様体 $M$ における二つの微分可能なベクトル場 $X, Y$に対して、$[X, Y]$を以下のように定義し、(リー-)ブラケット(Lie-)bracket、リー演算またはリー代数Lie-algebraと呼ぶ。\n$$ \\begin{equation} [X, Y] := XY - YX \\end{equation} $$\n説明 ベクトル場 $X, Y$は、$\\mathcal{D}(M)$に作用するオペレーターと見ることができ、$XY$はベクトル場にならないが、$[X, Y] = XY - YX$はベクトル場になる。\n$(1)$のような式を満たす$[\\cdot, \\cdot]$は一般に交換子commutatorと呼ばれる。\n以下の定理では(a)、(b)、(c)は、リー・ブラケットでなくても、交換子であれば一般的に満たされる性質である。特に(c)はヤコビ恒等式Jacobi identityと呼ばれる。\n定理 $X, Y, Z$を$M$上の微分可能なベクトル場とする。$a, b$を実数、$f, g$を$M$上の微分可能な関数とする。すると、以下が成立する。\n(a) $[X, Y] = -[Y, X]$\n(b) $[aX + bY, Z] = a[X, Y] + b[Y, Z]$\n(c) $[ [X, Y], Z] + [ [Y, Z], X] + [ [Z, X], Y] = 0$\n(d) $[fX, gY] = fg[X, Y] + fX(g)Y - gY(f)X$\n証明 (d) 積の微分$X(gY) = X(g)Y + gXY$が成立するため、\n$$ \\begin{align*} [fX, gY] \u0026amp;= fX(gY) - gY(fX) \\\\ \u0026amp;= \\left( fX(g)Y + fgXY \\right) - \\left( gY(f)X - gfYX \\right) \\\\ \u0026amp;= fgXY - fgYX + fX(g)Y + gY(f)X\\\\ \u0026amp;= fg(XY - YX) + fX(g)Y + gY(f)X\\\\ \u0026amp;= fg[X, Y] + fX(g)Y + gY(f)X \\end{align*} $$\n■\nManfredo P. Do Carmo, Riemannian Geometry (Eng Edition, 1992), p27-28\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3272,"permalink":"https://freshrimpsushi.github.io/jp/posts/3272/","tags":null,"title":"ベクトル場のリー括弧"},{"categories":"줄리아","contents":"コード using Plots\rscatter(rand(100), randn(100))\rplot!([0,1],[0,1])\rpng(\u0026#34;example1\u0026#34;)\rplot!([.00,.25,.50],[-2,0,-2])\rpng(\u0026#34;example2\u0026#34;)\rθ = 0:0.01:2π\rplot!(.5 .+ cos.(θ)/3, 1.5sin.(θ))\rpng(\u0026#34;example3\u0026#34;) このコードを実行して、図に線分を入れる方法を見てみよう。\n線分 plot!([0,1],[0,1]) 単に一つの線分を引くか何か別のものを描くか、方法は同じだ。線分には二つの点が必要だから、x座標の配列とy座標の配列を与えればいい。\n複数の線分 plot!([.00,.25,.50],[-2,0,-2]) y3は一度に二つの線分を描いたものだ。線分間の始点と終点が繋がっている。\n楕円 plot!(.5 .+ cos.(θ)/3, 1.5sin.(θ)) 複数の線分を引く方法を応用して、楕円を描くことができる。文法的にも計算的にも、他の言語に比べて描きやすい方だ。\n環境 OS: Windows julia: v1.6.3 ","id":2240,"permalink":"https://freshrimpsushi.github.io/jp/posts/2240/","tags":null,"title":"ジュリア集合の画像に線を挿入する方法"},{"categories":"수리통계학","contents":"定義 1 サンプル $\\mathbf{X} := \\left( X_{1} , \\cdots , X_{n} \\right)$ のジョイント確率密度関数または確率質量関数を$f(\\mathbf{x}|\\theta)$ としよう。その実現が$\\mathbf{x}$ 与えられた時、$f(\\mathbf{x}|\\theta)$を$\\theta$ の関数として考える $$ L \\left( \\theta | \\mathbf{x} \\right) := f \\left( \\mathbf{x} | \\theta \\right) $$ を尤度関数Likelihood Functionという。\n説明 最尤推定量を議論する文脈では、サンプルはiidである必要があるが、尤度原理Likelihood Principleを議論する時は、確率変数を特別に考える必要なく、確率ベクターそのものを見ても大丈夫である。\nパラメータ$\\theta$について、二つのパラメータ$\\theta_{1}$と$\\theta_{2}$が $$ L \\left( \\theta_{1} | \\mathbf{x} \\right) \\ge L \\left( \\theta_{2} | \\mathbf{x} \\right) $$ の場合、$\\theta$に対して$\\theta_{1}$が$\\theta_{2}$よりもっともっともらしいPlausibleと言われる。\nCasella. (2001). Statistical Inference(2nd Edition): p290.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2239,"permalink":"https://freshrimpsushi.github.io/jp/posts/2239/","tags":null,"title":"尤度関数の定義"},{"categories":"기하학","contents":"ビルドアップ1 ベクトル場の簡単な定義を考えてみよう。3次元空間でのベクトル場ベクトル関数、ベクトル場とは、3次元ベクトルを3次元ベクトルにマッピングする関数 $X : \\mathbb{R}^{3} \\to \\mathbb{R}^{3}$である。これを多様体と考えると、$X$は微分多様体 $\\mathbb{R}^{3}$の点 $p$を$\\mathbb{R}^{3}$のベクトル $\\mathbf{v}$にマッピングするが、このベクトル $\\mathbf{v}$をオペレーターとして扱い、方向微分(=接ベクトル)と考えることができる。したがって、ベクトル場とは、多様体 $\\mathbb{R}^{3}$の点 $p$を$p$の接ベクトル $\\mathbf{v}_{p} \\in T_{p}\\mathbb{R}^{3}$にマッピングする関数である。\nすると、ベクトル場の値域は全ての点の接ベクトルの集合である。したがって、ベクトル場 $X$は以下のように定義される関数である。\n$$ X : \\mathbb{R}^{3} \\to \\bigcup \\limits_{p\\in \\mathbb{R}^{3}} T_{p}\\mathbb{R}^{3} $$\nこの概念を多様体に一般化するために、微分多様体 $M$の接束tangent bundle $TM$を次のように定義しよう。\n$$ TM := \\bigsqcup \\limits_{p\\in M} T_{p}M $$\nこのとき $\\bigsqcup$は直和である。\n定義 微分多様体 $M$上のベクトル場vector field $X$とは、各点 $p \\in M$を$p$の接ベクトル $X_{p} \\in T_{p}M$にマッピングする関数である。\n$$ \\begin{align*} X : M \u0026amp;\\to TM \\\\ p \u0026amp;\\mapsto X_{p} \\end{align*} $$\n説明 ベクトル場の関数値 接束の定義を考えると、$TM$の要素は$(p, X_{p})$であるが、定義で$X_{p}$をマッピングすると書かれているので、疑問に思うことがあるかもしれない。\n$$ \\begin{equation} TM := \\bigsqcup \\limits_{p \\in M } T_{p}M = \\bigcup_{p \\in M} \\left\\{ p \\right\\} \\times T_{p}M = \\left\\{ (p, X_{p}) : p \\in M, X_{p} \\in T_{p}M \\right\\} \\end{equation} $$\nつまり、正確に言うと、直和の定義によると$TM$の要素は順序対 $(p, X_{p})$が正しいが、実際には$X_{p}$と同じものとして扱われる。\n接束の定義を再考する。接束の定義で本当にしたいのは、順序対 $(p, X_{p})$を集めることではない。各点 $p$上の接ベクトルをすべて集めたいのである。しかし、各$T_{p}M$は$\\mathbb{R}^{n}$と同型であるため、和集合をとるときに曖昧さが生じる可能性がある。\n$$ T_{p}M \\approxeq \\mathbb{R}^{n} \\approxeq T_{q}M $$\n例えば、$M$が3次元多様体であるとすると、$T_{p}M \\approxeq \\mathbb{R}^{3}$で表されるベクトル $X_{p}$と$T_{q}M \\approxeq \\mathbb{R}^{3}$で表されるベクトル $X_{q}$を同じものとして扱う曖昧さがある。したがって、$TM$を順序対の集合として定義する理由は、$X_{p}$と$X_{q}$が同じ対象ではなく、明確に異なるものとして区別するためである。ここで自然に$\\iota_{p} : (p, X_{p}) \\mapsto X_{p}$のような全単射関数を考え、$(p, X_{p}) \\approx X_{p}$として扱うことができる。\nある教科書では、このような説明を特にしたくない場合や、読者が十分に理解していると仮定する場合に、接束 $TM$を次のように定義することもある。\n$$ TM := \\bigcup\\limits_{p\\in M} T_{p}M = \\left\\{ X_{p} \\in T_{p}M : \\forall p \\in M \\right\\} $$\nもちろん、再度言うが、上の定義も$(1)$も本質的には同じである。また、上の定義によると$X$の関数値は関数 $X_{p}$であることに注意しよう。\n$$ X_{p} : \\mathcal{D} \\to \\mathbb{R} $$\nオペレーターとしてのベクトル場 $M$を$n$次元微分多様体としよう。$M$の微分可能な関数の集合を$\\mathcal{D} = \\mathcal{D}(M)$としよう。\n$$ \\mathcal{D} = \\mathcal{D}(M) := \\left\\{ \\text{all real-valued functions of class } C^{\\infty} \\text{ defined on } M \\right\\} $$\n参照 微分多様体上の微分可能な関数の集合 $\\mathcal{D}(M)$ 微分多様体上の微分可能なベクトル場の集合 $\\frak{X}(M)$ Manfredo P. Do Carmo, Riemannian Geometry (英語版, 1992), p25-27\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3270,"permalink":"https://freshrimpsushi.github.io/jp/posts/3270/","tags":null,"title":"微分可能多様体上のベクトル場"},{"categories":"줄리아","contents":"コード using DataFrames\rUnit1 = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;진숙\u0026#34;],\rbirth = [99,97,96,99],\rheight = [161,157,159,162]\r)\rUnit2 = DataFrame(\rmember = [\u0026#34;소정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [95,98,95,94],\rheight = [166,172,163,165]\r)\rWJSN = vcat(Unit1, Unit2)\rpush!(WJSN, [\u0026#34;다원\u0026#34;,97,167])\rpush!(WJSN, [\u0026#34;연정\u0026#34;,99,165])\rsort(WJSN, 3)\rsort(WJSN, :birth)\rsort(WJSN, [:birth, :height])\rsort(WJSN, :birth, rev = true) 上の例のコードを実行して、その結果を確認してみましょう。\njulia\u0026gt; WJSN\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161\r2 │ 루다 97 157\r3 │ 수빈 96 159\r4 │ 진숙 99 162\r5 │ 소정 95 166\r6 │ 주연 98 172\r7 │ 지연 95 163\r8 │ 현정 94 165\r9 │ 다원 97 167\r10 │ 연정 99 165 WJSNのデータフレームは上記の通りです。\n列番号でソート sort(df, cols::integer) cols番目の列を基準にソートします。\njulia\u0026gt; sort(WJSN, 3)\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 루다 97 157\r2 │ 수빈 96 159\r3 │ 다영 99 161\r4 │ 진숙 99 162\r5 │ 지연 95 163\r6 │ 현정 94 165\r7 │ 연정 99 165\r8 │ 소정 95 166\r9 │ 다원 97 167\r10 │ 주연 98 172 3番目の列であるheightを基準にソートされたことが確認できます。\n列名でソート sort(df, cols::Symbol) 名前のシンボルcolsの列を基準にソートします。\njulia\u0026gt; sort(WJSN, :birth)\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 현정 94 165\r2 │ 소정 95 166\r3 │ 지연 95 163\r4 │ 수빈 96 159\r5 │ 루다 97 157\r6 │ 다원 97 167\r7 │ 주연 98 172\r8 │ 다영 99 161\r9 │ 진숙 99 162\r10 │ 연정 99 165 :birthが入ってbirthを基準にソートされたことが確認できます。\nソートの優先順位 sort(df, cols::Array) colsの順番に従って優先順位をつけてソートします。\njulia\u0026gt; sort(WJSN, [:birth, :height])\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 현정 94 165\r2 │ 지연 95 163\r3 │ 소정 95 166\r4 │ 수빈 96 159\r5 │ 루다 97 157\r6 │ 다원 97 167\r7 │ 주연 98 172\r8 │ 다영 99 161\r9 │ 진숙 99 162\r10 │ 연정 99 165 birthでソートしつつ、heightもソートされました。ただbirthを基準にソートしただけと比較すると、2行目と3行目が逆転しています。\n逆順でソート sort(df, rev::Bool=false) rev = trueを指定すればOKです。デフォルトはfalseです。\njulia\u0026gt; sort(WJSN, :birth, rev = true)\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161\r2 │ 진숙 99 162\r3 │ 연정 99 165\r4 │ 주연 98 172\r5 │ 루다 97 157\r6 │ 다원 97 167\r7 │ 수빈 96 159\r8 │ 소정 95 166\r9 │ 지연 95 163\r10 │ 현정 94 165 環境 OS: Windows julia: v1.6.3 ","id":2238,"permalink":"https://freshrimpsushi.github.io/jp/posts/2238/","tags":null,"title":"Juliaでデータフレームを並べ替える方法"},{"categories":"기하학","contents":"定義1 $M$を$n$次元の微分多様体とする。点$p \\in M$での接空間を$T_{p}M$とする。$M$の接束tangent bundle, 接バンドル $TM$を次のように定義する。\n$$ \\begin{align*} TM \u0026amp;:= \\bigsqcup \\limits_{p \\in M } T_{p}M \\\\ \u0026amp;= \\bigcup_{p \\in M} \\left\\{ p \\right\\} \\times T_{p}M \\\\ \u0026amp;= \\left\\{ (p, v) : p \\in M, v \\in T_{p}M \\right\\} \\end{align*} $$\nこのとき、$\\bigsqcup$は分離合併である。\n説明 定義によれば、接束は微分多様体$M$上のすべての点と、その点でのすべての接ベクトルの順序対の集合である。分離合併のドキュメントを見れば分かるが、$(p,v)$と$v$の間の自然なマッピングを考えることができ、実質的に同じものとして扱うため、$\\bigsqcup$の代わりに$\\bigcup$と表示することもある。\n$$ TM := \\bigcup_{p \\in M} T_{p}M $$\n$M$が$n$次元の微分多様体であれば、$TM$自体も再び$2n$次元の微分多様体となる。\nManfredo P. Do Carmo, Riemannian Geometry (Eng Edition, 1992), p15-16\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3268,"permalink":"https://freshrimpsushi.github.io/jp/posts/3268/","tags":null,"title":"微分可能多様体上の接空間バンドル"},{"categories":"줄리아","contents":"コード using DataFrames\rUnit1 = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;진숙\u0026#34;],\rbirth = [99,97,96,99],\rheight = [161,157,159,162]\r)\rUnit2 = DataFrame(\rmember = [\u0026#34;소정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [95,98,95,94],\rheight = [166,172,163,165]\r)\rWJSN = vcat(Unit1, Unit2)\rpush!(WJSN, [\u0026#34;다원\u0026#34;,97,167])\rpush!(WJSN, [\u0026#34;연정\u0026#34;,99,165]) 上の例のコードを実行して、その結果を確認しよう。\n二つのデータフレームの行を結合する vcat() julia\u0026gt; Unit1 = DataFrame(\rmember = [\u0026#34;다영\u0026#34;,\u0026#34;루다\u0026#34;,\u0026#34;수빈\u0026#34;,\u0026#34;진숙\u0026#34;],\rbirth = [99,97,96,99],\rheight = [161,157,159,162]\r)\r4×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161 2 │ 루다 97 157 3 │ 수빈 96 159 4 │ 진숙 99 162 julia\u0026gt; Unit2 = DataFrame(\rmember = [\u0026#34;소정\u0026#34;,\u0026#34;주연\u0026#34;,\u0026#34;지연\u0026#34;,\u0026#34;현정\u0026#34;],\rbirth = [95,98,95,94],\rheight = [166,172,163,165]\r)\r4×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 소정 95 166\r2 │ 주연 98 172\r3 │ 지연 95 163\r4 │ 현정 94 165\rjulia\u0026gt; WJSN = vcat(Unit1, Unit2)\r8×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161\r2 │ 루다 97 157\r3 │ 수빈 96 159\r4 │ 진숙 99 162\r5 │ 소정 95 166\r6 │ 주연 98 172\r7 │ 지연 95 163\r8 │ 현정 94 165 当然だけど、二つのデータフレームの列は同じでなければならない。\n一行を挿入する push!() julia\u0026gt; push!(WJSN, [\u0026#34;다원\u0026#34;,97,167])\r9×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161\r2 │ 루다 97 157\r3 │ 수빈 96 159\r4 │ 진숙 99 162\r5 │ 소정 95 166\r6 │ 주연 98 172\r7 │ 지연 95 163\r8 │ 현정 94 165\r9 │ 다원 97 167\rjulia\u0026gt; push!(WJSN, [\u0026#34;연정\u0026#34;,99,165])\r10×3 DataFrame\rRow │ member birth height │ String Int64 Int64 ─────┼───────────────────────\r1 │ 다영 99 161\r2 │ 루다 97 157\r3 │ 수빈 96 159\r4 │ 진숙 99 162\r5 │ 소정 95 166\r6 │ 주연 98 172\r7 │ 지연 95 163\r8 │ 현정 94 165\r9 │ 다원 97 167\r10 │ 연정 99 165 push!()でデータを追加する時は、列の数が一致する配列を入れてやる必要がある。\n環境 OS: Windows julia: v1.6.2 ","id":2236,"permalink":"https://freshrimpsushi.github.io/jp/posts/2236/","tags":null,"title":"ジュリアでデータフレームに新しい行を挿入する方法"},{"categories":"줄리아","contents":"概要 Infinities.jlは、Juliaで無限大記号を使用できるように支援するパッケージだ1。科学計算のコーディングにおいて、無限大は意外と便利である。\nコード julia\u0026gt; 8 \u0026lt; Inf\rtrue 序論で無限大を使用するのではなく、無限大記号を使用できるように支援すると述べた理由は、実際にパッケージなしでそれを使用できるからである。\njulia\u0026gt; using Infinities\rjulia\u0026gt; 8 \u0026lt; ∞\rtrue\rjulia\u0026gt; -∞ \u0026lt; 8\rtrue\rjulia\u0026gt; max(∞, 10, 11)\r∞\rjulia\u0026gt; sin(∞)\rERROR: MethodError: no method matching AbstractFloat(::Infinities.Infinity) 見ての通り、無限大の一般的な性質を全て備えている。\njulia\u0026gt; ℵ₀ \u0026lt; ℵ₁\rtrue\rjulia\u0026gt; ℵ₀ \u0026gt; ℵ₁\rfalse\rjulia\u0026gt; ∞ == ℵ₁\rfalse\rjulia\u0026gt; ∞ == ℵ₀\rtrue\rjulia\u0026gt; ∞ === ℵ₀\rfalse さらに、無限集合の基数である$\\aleph_{0}$と$\\aleph_{1}$を使用できるが、これにより計算上では同じ無限大でありながら、並び替えや比較などに使用できるように順序を付けることができる。\n環境 OS: Windows julia: v1.6.2 https://github.com/JuliaMath/Infinities.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2234,"permalink":"https://freshrimpsushi.github.io/jp/posts/2234/","tags":null,"title":"ジュリアで無限大を使う方法"},{"categories":"행렬대수","contents":"定義1 $m \\times n$行列$A$について、以下の線形システムを考えよう。\n$$ A \\mathbf{x} = \\mathbf{b} $$\nもし$m \\gt n$ならば、未知数より制約条件が多い場合であり、このような線形システムを過剰決定系overdetermined systemと呼ぶ。\nもし$m \\lt n$ならば、未知数より制約条件が少ない場合であり、このような線形システムを過小決定系underdetermined systemと呼ぶ。\n定理 1 ランクが$r$である$m \\times n$行列$A$についての線形システムを考えよう。\n$$ A \\mathbf{x} = \\mathbf{b} $$\n上記線形システムの一般解は$n-r$個のパラメーターを持つ。\n証明 次元定理\n$$ \\rank(A) + \\nullity(A) = \\dim(A) $$\nまず次元定理により、$\\nullity(A) = n-r$である。それから次の定理より、パラメーターの数とヌル次数の数が同じであることがわかる。\n$\\mathbf{x}_{0}$が$A\\mathbf{x} = \\mathbf{b}$のある解だとしよう。$S= \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{k} \\right\\}$を$\\mathcal{N}(A)$の基底としよう。その時、$A\\mathbf{x} = \\mathbf{b}$の全ての解は次のように表現できる。\n$$ \\mathbf{x} = \\mathbf{x}_{0} + c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + \\cdots + c_{k}\\mathbf{v}_{k} $$\nつまり、パラメーターの数と$\\nullity(A)$が同じである。\n■\n定理 2 $m \\times n$行列$A$について、以下の線形システムを考えよう。\n$$ A \\mathbf{x} = \\mathbf{b} $$\n過剰決定\n線形システムが過剰決定系の場合、すなわち$m \\gt n$の場合、線形システムは$\\mathbb{R}^{m}$にある少なくとも1つのベクトル$\\mathbf{b}$に対して解を持たない。つまり、一般解を持たない。\n過小決定\n線形システムが過小決定系の場合、すなわち$m \\lt n$の場合、線形システムは$\\mathbb{R}^{m}$にある全てのベクトル$\\mathbf{b}$に対して解を持たないか、または無限に多くの解を持つ。\n証明 過剰決定\n$m \\gt n$とすると、$A$の列ベクトルは$n$個であるため、$\\mathbb{R}^{m}$を生成することはできない（ベクトルの数が次元の数よりも少ないため）。したがって、$A$の列空間に属さない少なくとも1つのベクトル$\\mathbf{b}$が存在する。\n補助定理\n線形システム$A \\mathbf{x} = \\mathbf{b}$が解を持つ必要十分条件は、$\\mathbf{b}$が$A$の列空間の要素であることである。\nこの補助定理によって、$A \\mathbf{x} = \\mathbf{b}$は解を持たない。\n過小決定\n$m \\lt n$と仮定しよう。\n解を持たない場合\nこの場合は証明が完了である。\n解を持つ場合\n解を持つならば、定理 1により、一般解は$n-r$個のパラメーターを持つ。$r = \\rank (A)$は$m$と$n$のうちの小さい値以下であるので、次が成り立つ。\n$$ n-r \\le n-m \u0026gt; 0 $$\nこれは、線形システムの一般解が1つ以上のパラメーターを持つことを意味し、解が存在するならば、それは無数に多くのものである。\n■\nHoward Anton, Elementary Linear Algebra: Aplications Version (12th Edition, 2019), p259\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3265,"permalink":"https://freshrimpsushi.github.io/jp/posts/3265/","tags":null,"title":"過飽和系と未飽和系"},{"categories":"줄리아","contents":"ガイド 1 (@v1.6) pkg\u0026gt; status JuMP Status `C:\\Users\\rmsms\\.julia\\environments\\v1.6\\Project.toml` [4076af6c] JuMP v0.20.0 REPLで]キーを押すとパッケージモードに入る。例えば、v0.20.0のバージョンのパッケージをv0.21にバージョンアップしたい場合は、以下のようにパッケージの後に@x.yyを付けてインストールすればいい。\n(@v1.6) pkg\u0026gt; add JuMP@0.21 Resolving package versions... ... (@v1.6) pkg\u0026gt; status JuMP Status `C:\\Users\\rmsms\\.julia\\environments\\v1.6\\Project.toml` [4076af6c] JuMP v0.21.4 もう一度バージョンを確認すると、パッケージのバージョンが正常に変更されたことを確認できる。\n環境 OS: Windows julia: v1.6.2 https://pkgdocs.julialang.org/dev/api/#General-API-Reference\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2232,"permalink":"https://freshrimpsushi.github.io/jp/posts/2232/","tags":null,"title":"ジュリアで特定バージョンのパッケージをインストールする方法"},{"categories":"기하학","contents":"概要 微分多様体上のプルバックを定義する。微分多様体が難しい場合は、$M = \\mathbb{R}^{m}$、$N = \\mathbb{R}^{n}$と考えてもよい。\n定義1 二つの微分多様体 $M, N$と微分可能な関数 $f : M \\to N$が与えられたとする。そこで、$N$の$k$-形式を$M$の$k$-形式に送る関数$f^{\\ast}$を考えることができる。$\\omega$を多様体$N$の$k$-形式とするとき、多様体$M$の$k$-形式$f^{\\ast}\\omega$を$\\omega$のプルバックpull back, 引き戻しと呼び、以下のように定義する。\n$$ \\begin{equation} (f^{\\ast}\\omega)(p) (v_{1}, \\dots, v_{k}) := \\omega (f(p))\\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right),\\quad v_{i} \\in T_{p}M \\end{equation} $$\n説明 プルバックという名前には、($f$が$M$から$N$へのマッピングであるのに対して)$f^{\\ast}$は$N$から$M$へのマッピングであるという意味がある。定義と表記法がかなり難しいが、少しずつ理解していこう。\n$f^{\\ast}$ $f^{\\ast}$は$N$の$k$-形式を$M$の$k$-形式に送るマップである。したがって、$\\omega$を$N$の$k$-形式とすると、$f^{\\ast}\\omega = f^{\\ast}(\\omega)$は$M$の$k$-形式である。\n$f^{\\ast}\\omega (p)$ 多様体$M$上の$k$-形式は、$p \\in M$を$\\Lambda^{k}(T_{p}^{\\ast}M)$の元にマッピングする。\n$$ f^{\\ast}\\omega : M \\to \\Lambda^{k}(T_{p}^{\\ast}M) $$\n$$ \\Lambda^{k} (T_{p}^{\\ast}M) := \\left\\{ \\varphi : \\underbrace{T_{p}M \\times \\cdots \\times T_{p}M}_{k \\text{ times}} \\to \\mathbb{R}\\ | \\ \\varphi \\text{ is k-linear alternating map} \\right\\} $$\nつまり、$f^{\\ast}\\omega (p) \\in \\Lambda^{k} (T_{p}^{\\ast}M)$もまた、一つの関数である。$\\Lambda^{k} (T_{p}^{\\ast}M)$の定義により、$f^{\\ast}\\omega (p)$は「$p$上の接ベクトル」$k$個を変数とする。これで、$(1)$はこの関数の関数値を具体的に定義した式であることがわかる。$f^{\\ast}(p)$自体が一つの関数であることをより強調するため、以下のような表記を使うことにしよう。\n$$ (f^{\\ast}\\omega)_{p} = f^{\\ast}\\omega (p) $$\n$\\omega (f(p))$ $\\omega$は$N$の$k$-形式であるため、$N$の点$f(p)$を$\\Lambda^{k}(T_{f(p)}^{\\ast}N)$の元にマッピングする。\n$$ \\Lambda^{k} (T_{f(p)}^{\\ast}N) := \\left\\{ \\varphi : \\underbrace{T_{f(p)}N \\times \\cdots \\times T_{f(p)}N}_{k \\text{ times}} \\to \\mathbb{R}\\ | \\ \\varphi \\text{ is k-linear alternating map} \\right\\} $$\n$\\Lambda^{k} (T_{f(p)}^{\\ast}N)$の定義により、$\\omega (f(p))$もまた、一つの関数である。$\\omega (f(p))$は「$f(p)$上の接ベクトル」$k$個を変数とする。ここでも同様に、$\\omega (f(p))$自体が一つの関数であることを強調するために、以下のような表記を使おう。\n$$ \\omega_{f(p)} = \\omega (f(p)) $$\n$df_{p}v_{i}$ $$ df_{p} : T_{p}M \\to T_{f(p)}N $$\n$f : M \\to N$に対して、$f$の微分 $df_{p}$は上記のように定義される。したがって、$v_{i} \\in T_{p\n}M$であれば、$df_{p}v_{i} = df_{p}(v_{i})$は$T_{f(p)}N$の元である。\nこれを総合すると、$(1)$を得る。\n$$ (f^{\\ast}\\omega)_{p} (v_{1}, \\dots, v_{k}) := \\omega_{f(p)}\\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right),\\quad v_{i} \\in T_{p}M $$\n上記の二つの関数の定義域を見ると、以下のような違いがある。\n$$ \\begin{align*} (f^{\\ast}\\omega)_{p} : \u0026amp;\u0026amp; \\underbrace{T_{p}M \\times \\cdots \\times T_{p}M}_{k \\text{ times}} \u0026amp;\\to \\mathbb{R} \\\\ \\omega_{f(p)} : \u0026amp;\u0026amp; \\underbrace{T_{f(p)}N \\times \\cdots \\times T_{f(p)}N}_{k \\text{ times}} \u0026amp;\\to \\mathbb{R} \\end{align*} $$\nこの違いを微分$df_{p} : T_{p}M \\to T_{f(p)}N$が繋いでいると考えればよい。そのため、$df_{p}$をプッシュフォワードpush forward, 押し出しとも呼ぶ。$1$-形式$\\varphi$に対して、以下が成立する。\n$$ \\begin{equation} \\varphi( dfv) = f^{\\ast}\\varphi(v) \\end{equation} $$\n$0$-形式のプルバック $f : M \\to N$を二つの微分多様体間で定義された関数とする。$g : N \\to \\mathbb{R}$を関数($N$での$0$-形式)とする。$g$のプルバック$f^{\\ast}g : M \\to \\mathbb{R}$は、以下のように定義される関数($M$での$0$-形式)である。\n$$ f^{\\ast}g := g \\circ f $$\n座標変換 関数$f : \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$が与えられたとする。$\\mathbf{x} = (x_{1}, \\dots ,x_{n}) \\in \\mathbb{R}^{n}$であり、$\\mathbf{y} = (y_{1}, \\dots ,y_{m}) \\in \\mathbb{R}^{m}$である。\n$$ f(x_{1}, \\dots, x_{n}) = (f_{1}(\\mathbf{x}), \\dots, f_{m}(\\mathbf{x}) )= (y_{1}, \\dots ,y_{m}) $$\nそして、$\\omega = \\sum\\limits_{I} a_{I} dy_{I}$を$\\mathbb{R}^{m}$上の$k$-形式とする。そのとき、$\\omega$のプルバック$f^{\\ast}\\omega$は以下の特性により、次のようになる。\n$$ \\begin{align*} f^{\\ast} \\omega \u0026amp;= f^{\\ast} \\left( \\sum a_{I}dy_{I} \\right) \\\\ \u0026amp;= \\sum f^{\\ast} \\left( a_{I}dy_{I} \\right) \\\\ \u0026amp;= \\sum f^{\\ast}a_{I} f^{\\ast}dy_{I} \\\\ \u0026amp;= \\sum f^{\\ast}a_{I} f^{\\ast}(dy_{i1} \\wedge \\cdots \\wedge dy_{ik}) \\\\ \u0026amp;= \\sum f^{\\ast}a_{I} (f^{\\ast}dy_{i1} \\wedge \\cdots \\wedge f^{\\ast}dy_{ik}) \\end{align*} $$\nこの時、$(2)$により$f^{\\ast}dy_{i1}(v) = dy_{i1}(df(v)) = d(y_{i1}\\circ f)(v) = df_{i1}(v)$であり、$f^{\\ast}a_{I} = a_{I} \\circ f$であるため、\n$$ \\begin{equation} f^{\\ast} \\omega = \\sum a_{I}(f_{1}, \\dots f_{m}) df_{i1} \\wedge \\cdots \\wedge df_{ik} \\end{equation} $$\n上記の式は座標変換を意味し、具体的にどのようになるかは以下の例で見てみよう。\n例 $\\mathbb{R}^{2} \\setminus \\left{ 0, 0 \\right}$上の$1$-形式$\\omega$が以下のようであるとする。\n$$ \\omega = - \\dfrac{y}{x^{2} + y^{2}}dx + \\dfrac{x}{x^{2} + y^{2}}dy = a_{1}dx + a_{2}dy $$\nこの直交座標上の$1$-形式を極座標に変換してみよう。$U = \\left{ (r,\\theta) : 0 \u0026lt; r, 0 \\le \\theta \u0026lt; 2\\pi \\right}$とする。そして、$f : U \\to \\mathbb{R}^{2}$を以下のようにする。\n$$ f(r,\\theta) = (r\\cos\\theta, r\\sin\\theta) = (f_{1}, f_{2}) $$\nここで、$df_{1}, df_{2}$を計算してみよう。$f_{1} = r\\cos\\theta, f_{2}=r\\sin\\theta$であるため、\n$$ \\begin{align*} df_{1} \u0026amp;= \\dfrac{\\partial f_{1}}{\\partial r}dr + \\dfrac{\\partial f_{1}}{\\partial \\theta}d\\theta = \\cos\\theta dr - r \\sin \\theta d\\theta \\\\ df_{2} \u0026amp;= \\dfrac{\\partial f_{2}}{\\partial r}dr + \\dfrac{\\partial f_{2}}{\\partial \\theta}d\\theta = \\sin\\theta dr + r \\cos \\theta d\\theta \\\\ \\end{align*} $$\nそれにより、$(3)$に従い、\n$$ \\begin{align*} f^{\\ast} \\omega \u0026amp;= a_{1}(f_{1}, f_{2})df_{1} + a_{2}(f_{1}, f_{2})df_{2} \\\\ \u0026amp;= - \\dfrac{f_{2}}{f_{1}^{2} + f_{2}^{2}}(\\cos\\theta dr - r \\sin \\theta d\\theta) + \\dfrac{f_{1}}{f_{1}^{2} + f_{2}^{2}}df_{2}(\\sin\\theta dr + r \\cos \\theta d\\theta) \\\\ \u0026amp;= - \\dfrac{r\\sin\\theta}{r^{2}\\cos^{2}\\theta + r^{2}\\sin^{2}\\theta}(\\cos\\theta dr - r \\sin \\theta d\\theta) \\\\ \u0026amp;\\quad + \\dfrac{r\\cos\\theta}{r^{2}\\cos^{2}\\theta + r^{2}\\sin^{2}\\theta}(\\sin\\theta dr + r \\cos \\theta d\\theta) \\\\ \u0026amp;= -\\dfrac{\\sin\\theta \\cos\\theta}{r}dr + \\sin^{2}\\theta d\\theta + \\dfrac{\\cos\\theta \\sin\\theta}{r}dr + \\cos^{2}\\theta d\\theta \\\\ \u0026amp;= d\\theta \\end{align*} $$\nしたがって、\n$$ \\int - \\dfrac{y}{x^{2} + y^{2}}dx + \\dfrac{x}{x^{2} + y^{2}}dy = \\int d\\theta $$\n■\n特性 $M, N$をそれぞれ$m, n$次元微分多様体、$f : M \\to N$とする。$\\omega, \\varphi$を$N$上の$k$-形式とする。$g$を$N$上の$0$-形式とする。$\\varphi_{i}$たちを$N$上の$1$-形式とする。すると、以下が成立する。\n$$ \\begin{align} f^{\\ast} (\\omega + \\varphi) =\u0026amp;\\ f^{\\ast}\\omega + f^{\\ast}\\varphi \\tag{a} \\\\ f^{\\ast} (g \\omega) =\u0026amp;\\ (f^{\\ast}g) (f^{\\ast}\\omega) \\tag{b} \\\\ f^{\\ast} (\\varphi_{1} \\wedge \\cdots \\wedge \\varphi_{k}) =\u0026amp;\\ f^{\\ast}(\\varphi_{1}) \\wedge \\cdots \\wedge f^{\\ast}(\\varphi_{k}) \\tag{c} \\end{align} $$\nこのとき、$+$と$\\wedge$はそれぞれ$k$-形式の合計とくさび積である。\n$\\omega, \\varphi$を$N$上の任意の二つの形式とする。$L$を$l$次元微分多様体、$g : L \\to N$とする。\n$$ \\begin{align*} f^{\\ast}(\\omega \\wedge \\varphi) \u0026amp;= (f^{\\ast}\\omega) \\wedge (f^{\\ast}\\varphi) \\tag{d} \\\\ (f \\circ g)^{\\ast} \\omega \u0026amp;= g^{\\ast}(f^{\\ast}\\omega) \\tag{e} \\end{align*} $$\n証明 証明 $(a)$ $$ \\begin{align*} (f^{\\ast}(\\omega + \\varphi))_{p} (v_{1}, \\dots, v_{k}) =\u0026amp;\\ (\\omega + \\varphi)_{f(p)} \\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right) \\\\ =\u0026amp;\\ \\omega_{f(p)} \\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right) + \\varphi_{f(p)} \\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right) \\\\ =\u0026amp;\\ (f^{\\ast} \\omega)_{p}(v_{1}, \\dots, v_{k}) + (f^{\\ast} \\varphi)_{p}(v_{1}, \\dots, v_{k}) \\\\ =\u0026amp;\\ \\left( f^{\\ast}\\omega + f^{\\ast}\\varphi \\right)_{p}(v_{1}, \\dots, v_{k}) \\end{align*} $$\n■\n証明 $(b)$ $0$-形式$g$と$k$-形式$\\omega$の積を以下のように定義する。\n$$ (g\\omega)(p) = g(p) \\omega (p) $$\nここで、$g(p) = g_{p}$はスカラー、$\\omega (p) = \\omega_{p}$は関数であることに注意。それにより、\n$$ \\begin{align*} (f^{\\ast} (g\\omega))_{p} (v_{1}, \\dots, v_{k}) =\u0026amp;\\ g\\omega_{f(p)} \\left( df_{p}v_{1}, \\dots, df_{p}v_{k} \\right) \\\\ =\u0026amp;\\ g_{f(p)} \\omega_{f(p)} (df_{p}v_{1}, \\dots, df_{p}v_{k}) \\\\ =\u0026amp;\\ g\\circ f(p) \\omega_{f(p)} (df_{p}v_{1}, \\dots, df_{p}v_{k}) \\\\ =\u0026amp;\\ (f^{\\ast}g)_{p} (f^{\\ast}\\omega)_{p} (v_{1}, \\dots, v_{k}) \\end{align*} $$\n■\n証明 $(c)$ $$ \\begin{align*} (f^{\\ast}\\left( \\varphi_{1} \\wedge \\cdots \\wedge \\varphi_{k} \\right))_{p} (v_{1}, \\dots, v_{k}) =\u0026amp;\\ (\\varphi_{1} \\wedge \\dots \\wedge \\varphi_{k})_{f(p)} \\left( df_{1}, \\dots, df_{k} \\right) \\\\ =\u0026amp;\\ \\det [\\varphi_{i}df(v_{j})] \\\\ =\u0026amp;\\ \\det [ f^{\\ast} \\varphi_{i}(v_{j})] \\\\ \\end{align*} $$\n■\nManfredo P. Do Carmo, Differential Forms and Applications, p6-8\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3262,"permalink":"https://freshrimpsushi.github.io/jp/posts/3262/","tags":null,"title":"微分幾何学におけるプルバック"},{"categories":"줄리아","contents":"概要 多くの言語でデータフレームがサポートされているにも関わらず、毎回新しくてイライラすることが空の配列の作成です。\nコード タイプ指定 julia\u0026gt; using DataFrames\rjulia\u0026gt; df1 = DataFrame(x = Int64[], y = String[])\r0×2 DataFrame 実際に空の配列をデータとして入れればいいです。この時、タイプが指定され、データが全くない場合は、カラム名とタイプも表示されません。\njulia\u0026gt; push!(df1, [3, \u0026#34;three\u0026#34;])\r1×2 DataFrame\r│ Row │ x │ y │\r│ │ Int64 │ String │\r├─────┼───────┼────────┤\r│ 1 │ 3 │ three │\rjulia\u0026gt; push!(df1, [3.14, \u0026#34;pi\u0026#34;])\r┌ Error: Error adding value to column :x.\r└ @ DataFrames C:\\Users\\rmsms\\.julia\\packages\\DataFrames\\GtZ1l\\src\\dataframe\\dataframe.jl:1606\rERROR: InexactError: Int64(3.14) データを入れると、正常にカラム名とタイプが出力されます。タイプが合わない場合は、データが追加されないので注意が必要です。\nタイプ未指定 julia\u0026gt; df2 = DataFrame(x = [], y = String[])\r0×2 DataFrame\rjulia\u0026gt; push!(df2, [3, \u0026#34;three\u0026#34;])\r1×2 DataFrame\r│ Row │ x │ y │\r│ │ Any │ String │\r├─────┼─────┼────────┤\r│ 1 │ 3 │ three │\rjulia\u0026gt; push!(df2, [3.14, \u0026#34;pi\u0026#34;])\r2×2 DataFrame\r│ Row │ x │ y │\r│ │ Any │ String │\r├─────┼──────┼────────┤\r│ 1 │ 3 │ three │\r│ 2 │ 3.14 │ pi │ データフレームのタイプでストレスを感じたくない場合は、上記のようにAnyの空の配列を作ればいいです。タイプ指定と違って、データがうまく入ったことが確認できます。\n環境 OS: Windows julia: v1.6.2 ","id":2230,"permalink":"https://freshrimpsushi.github.io/jp/posts/2230/","tags":null,"title":"Juliaで空のデータフレームを作成する方法"},{"categories":"기하학","contents":"定義 1 ベクトル空間 $V$ の部分集合 $X$ の凸包Convex Hull $C$ とは、$X$ を含む全ての凸集合の共通部分を指し、数式では以下の通りになる。 $$ C = \\left\\{ \\sum_{k} t_{k} \\mathbf{x}_{k} : \\mathbf{x}_{k} \\in X, t_{k} \\ge 0 , \\sum_{k} t_{k} = 1 \\right\\} $$\n説明 実は、定義で出てきた数式は、そのものズバリの定義ではない。集合記号の中で条件付けで示した $$ \\sum_{k} t_{k} \\mathbf{x}_{k} $$ は$\\left\\{ \\mathbf{x}_{k} \\right\\}_{k}$ の凸組み合わせConvex Combinationと呼ばれる。\n言葉で説明すると難しく聞こえるかもしれないが、図で見るととても単純で2、凸包という用語自体が重要というよりは、幾何学、最適化理論、位相データ分析など、空間を簡単に扱いたいと思うコンテキストで唐突に現れる概念である。\n図で見た凸包は本当に単純だ。$X$ の全ての点を囲む最小の凸集合だ。数学的に全ての凸集合の共通部分について言及する理由は、「大きさ」が「小さい」といった表現が数学ではあまり直感的ではないからだ。\nMatousek. (2007). 線形プログラミングの理解と使用：p49.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSheffar. (2020). 位相データ分析入門. https://arxiv.org/abs/2004.04108v1\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2228,"permalink":"https://freshrimpsushi.github.io/jp/posts/2228/","tags":null,"title":"凸包の定義"},{"categories":"줄리아","contents":"説明 Clustering.jl パッケージの hclust() 関数を使えばいい。\nhclust(d::AbstractMatrix; [linkage], [uplo], [branchorder]) 距離行列 を入力として受け取り、階層的クラスタリング の結果を返す。クラスタ間の距離のデフォルトは 単一連結 だ。\nデンドログラム を描くには、Plots.jl ではなく StatsPlots.jl を使わなければならない。\nコード using StatsPlots using Clustering using Distances using Distributions\ra = rand(Uniform(-1,1), 2, 25)\rscatt = scatter(a[1,:], a[2,:], label=false)\rsavefig(scatt, \u0026#34;julia_hclust_scatter.png\u0026#34;) D_a = pairwise(Euclidean(), a, a)\rSL = hclust(D_a, linkage=:single)\rplot_SL = plot(SL)\rp = plot(scatt, plot_SL, size=(800,400))\rsavefig(p, \u0026#34;julia_hclust.png\u0026#34;) ","id":3259,"permalink":"https://freshrimpsushi.github.io/jp/posts/3259/","tags":null,"title":"ジュリアで階層的クラスタリングを行う方法"},{"categories":"기하학","contents":"概要 2階微分形式を定義した方法と同様に、微分多様体 $M$に対する $k$階形式 を定義する。\n微分多様体が難しいなら、$M = \\mathbb{R}^{n}$と思ってもいい。\nビルドアップ $M$を$n$次元微分多様体とする。$p \\in M$は$M$の点であり、$T_{p}M$は点$p$での$M$の接空間だ。$T_{p}^{\\ast}M$は接空間の双対空間である余接空間だ。$\\Lambda^{k} (T_{p}^{\\ast}M)$を次のように多重線形な交代関数の集合として定義しよう。\n$$ \\Lambda^{k} (T_{p}^{\\ast}M) := \\left\\{ \\varphi : \\underbrace{T_{p}M \\times \\cdots \\times T_{p}M}_{k \\text{ times}} \\to \\mathbb{R}\\ | \\ \\varphi \\text{ is k-linear alternating map} \\right\\} $$\n$\\varphi_{1}, \\dots, \\varphi_{k} \\in T_{p}^{\\ast}M$に対して、クサビ積 $\\wedge$を次のように定義すると、$(\\varphi_{1} \\wedge \\varphi_{2} \\wedge \\cdots \\wedge \\varphi_{k})$は$\\Lambda^{k} (T_{p}^{\\ast}M)$の要素になる。\n$$ (\\varphi_{1} \\wedge \\varphi_{2} \\wedge \\cdots \\wedge \\varphi_{k})(v_{1}, v_{2}, \\dots, v_{k}) = \\det \\left[ \\varphi_{i}(v_{j}) \\right],\\quad i,j=1,\\dots,k $$\n今、便宜上次のように表記しよう。\n$$ (dx_{i_{1}} \\wedge dx_{i_{2}} \\wedge \\cdots \\wedge dx_{i_{k}})_{p} \\overset{\\text{notation}}{=} (dx_{i_{1}})_{p} \\wedge (dx_{i_{2}})_{p} \\wedge \\cdots \\wedge (dx_{i_{k}})_{p} \\in \\Lambda^{k} (T_{p}^{\\ast}M) $$\nこの時、$i_{1}, i_{2}, \\dots, i_{k} = 1, \\dots, n$だ。すると、$\\Lambda^{k} (T_{p}^{\\ast}M)$はベクトル空間になる。\n定理 下の集合\n$$ \\mathcal{B} = \\left\\{ (dx_{i_{1}} \\wedge dx_{i_{2}} \\wedge \\cdots \\wedge dx_{i_{k}})_{p} : i_{1} \\lt i_{2} \\lt \\cdots \\lt i_{k},\\ i_{j}\\in \\left\\{ 1,\\dots,n \\right\\} \\right\\} $$\nは$\\Lambda^{k} (T_{p}^{\\ast}M)$の基底だ。\n証明 基底の定義により、$\\mathcal{B}$が線形独立であり、$\\Lambda^{k} (T_{p}^{\\ast}M)$を生成することを示せばよい。便宜上、$M$の接空間 $T_{p}M$の基底を次のように表す。\n$$ \\left\\{ e_{i} \\right\\} = \\left\\{ \\dfrac{\\partial }{\\partial x_{i}} \\right\\} $$ ​\nPart 1. 線形独立\n次の式の解が$a_{i_{1}\\dots i_{k}}$が全て$0$である場合のみであることを示せばよい。\n$$ \\sum \\limits_{i_{1} \\lt \\cdots \\lt i_{k} } a_{i_{1}\\dots i_{k}}dx_{i_{1}} \\wedge dx_{i_{2}} \\wedge \\cdots \\wedge dx_{i_{k}} = 0 $$\nここで\n$$ \\left(e_{j_{1}}, \\dots, e_{j_{k}} \\right),\\quad j_{1}\\lt \\cdots \\lt j_{k},\\ j_{\\ell} \\in \\left\\{ 1,\\dots, n \\right\\} $$\nを代入してみよう。\n$$ \\begin{align*} 0 =\u0026amp;\\ \\sum \\limits_{i_{1} \\lt \\cdots \\lt i_{k} } a_{i_{1}\\dots i_{k}}dx_{i_{1}} \\wedge dx_{i_{2}} \\wedge \\cdots \\wedge dx_{i_{k}}\\left(e_{j_{1}}, \\dots, e_{j_{k}} \\right) \\\\[1em] =\u0026amp;\\ \\sum \\limits_{i_{1} \\lt \\cdots \\lt i_{k} } a_{i_{1}\\dots i_{k}} \\begin{vmatrix} dx_{i_{1}}(e_{j_{1}}) \u0026amp; dx_{i_{1}}(e_{j_{2}}) \u0026amp; \\cdots \u0026amp; dx_{i_{1}}(e_{j_{k}}) \\\\[1em] dx_{i_{2}}(e_{j_{1}}) \u0026amp; dx_{i_{2}}(e_{j_{2}}) \u0026amp; \\cdots \u0026amp; dx_{i_{2}}(e_{j_{k}}) \\\\[1em] \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\[1em] dx_{i_{k}}(e_{j_{1}}) \u0026amp; dx_{i_{k}}(e_{j_{2}}) \u0026amp; \\cdots \u0026amp; dx_{i_{k}}(e_{j_{k}}) \\end{vmatrix} \\end{align*} $$\n行列式の最初の列を見てみよう。この列に$0$でない成分があるならば、$j_{1} \\in \\left\\{ i_{1}, \\dots i_{k} \\right\\}$でなければならない。この条件を次の列に順番に適用すると、次の結果を得る。\n$$ j_{1}, \\dots, j_{k} \\in \\left\\{ i_{1}, \\dots i_{k} \\right\\} $$\nしかし、$i$のインデックスと$j$のインデックスには$i_{1}\\lt \\cdots \\lt i_{k}$、$j_{1}\\lt \\cdots \\lt j_{k}$という条件があるため、$i_{\\ell} = j_{\\ell}$だ。\n$$ 0 = a_{j_{1}\\dots j_{k}} $$\n同じ論理で、全ての係数$a$が$0$でなければならないことがわかる。\nPart 2. 生成\nもし$f \\in \\Lambda^{k} (T_{p}^{\\ast}M)$なら、$f$が$\\mathcal{B}$の線形結合で表され、次の式が成り立つことを示せばよい。\n$$ f = \\sum \\limits_{i_{1} \\lt \\cdots \\lt i_{k} } a_{i_{1} \\dots i_{k}} dx_{i_{1}} \\wedge dx_{i_{2}} \\wedge \\cdots \\wedge dx_{i_{k}} $$\n$g$を次のように定義しよう。\n$$ g = \\sum \\limits_{i_{1} \\lt \\cdots \\lt i_{k} } f(e_{i_{1}},\\dots,e_{i_{k}}) dx_{i_{1}} \\wedge dx_{i_{2}} \\wedge \\cdots \\wedge dx_{i_{k}} $$\nすると、$g$がまさに$f$であることがわかる。両辺に$(e_{i_{1}},\\dots,e_{i_{k}})$を代入すると\n$$ \\begin{align*} g(e_{i_{1}},\\dots,e_{i_{k}}) =\u0026amp;\\ \\sum \\limits_{i_{1} \\lt \\cdots \\lt i_{k} } f(e_{i_{1}},\\dots,e_{i_{k}}) dx_{i_{1}} \\wedge dx_{i_{2}} \\wedge \\cdots \\wedge dx_{i_{k}}(e_{i_{1}},\\dots,e_{i_{k}}) \\\\ =\u0026amp;\\ f(e_{i_{1}},\\dots,e_{i_{k}}) \\end{align*} $$\nよって、$f(e_{i_{1}},\\dots,e_{i_{k}}) = a_{i_{1} \\dots i_{k}}$とすると、\n$$ f = g = \\sum \\limits_{i_{1} \\lt \\cdots \\lt i_{k} } a_{i_{1} \\dots i_{k}} dx_{i_{1}} \\wedge dx_{i_{2}} \\wedge \\cdots \\wedge dx_{i_{k}} $$\n■\n定義 点$p \\in M$を次のようにマッピングする関数$\\omega : M \\to \\Lambda^{k} (T_{p}^{\\ast}M)$を$M$でのk階形式exterior k-form と定義する。\n$$ \\omega (p) = \\sum \\limits_{i_{1} \\lt \\cdots \\lt i_{k} } a_{i_{1}\\dots i_{k}}(p)(dx_{i_{1}} \\wedge dx_{i_{2}} \\wedge \\cdots \\wedge dx_{i_{k}})_{p},\\quad i_{j} \\in \\left\\{ 1, \\dots, n \\right\\} $$\n$$ \\omega = \\sum \\limits_{i_{1} \\lt \\cdots \\lt i_{k} } a_{i_{1}\\dots i_{k}}dx_{i_{1}} \\wedge dx_{i_{2}} \\wedge \\cdots \\wedge dx_{i_{k}} $$\nこの時、$a_{i_{1}\\dots i_{k}} : M \\to \\mathbb{R}$だ。各$a_{i_{1}\\dots i_{k}}$が微分可能ならば、$\\omega$をk階微分形式differential k-formと言う。また、便宜上$I = (i_{1},\\dots,i_{k})$と言って次のように表記する。\n$$ \\omega = \\sum \\limits_{I} a_{I}dx_{I} $$\n■\n説明 定義により、$n$次元多様体では最大で$n$階形式まで存在する。また、$n$次元多様体での$k$階形式は$\\binom{n}{k}$個の項を持つ。したがって、$\\Lambda^{k} (T_{p}^{\\ast}M)$は$\\binom{n}{k}$次元ベクトル空間だ。特に、微分多様体$M$上の**$0$-形式**は$M$上で定義された関数$f : M \\to \\mathbb{R}$によって定義される。\n例えば、$\\mathbb{R}^{3}$では3階形式まで存在する。\n0階形式：$\\mathbb{R}^{3}$上の関数 1階形式：$a_{1}dx_{1} + a_{2}dx_{2} + a_{3}dx_{3}$ 2階形式：$a_{12}dx_{1}\\wedge dx_{2} + a_{13}dx_{1}\\wedge dx_{3} + a_{23} dx_{2} \\wedge dx_{3}$ 3階形式：$a_{123}dx_{1} \\wedge dx_{2} \\wedge dx_{3}$ 参照 余接空間と1階微分形式 2階微分形式 k階微分形式 ","id":3258,"permalink":"https://freshrimpsushi.github.io/jp/posts/3258/","tags":null,"title":"k次の微分形式"},{"categories":"줄리아","contents":"コード julia\u0026gt; findfirst(\u0026#34;li\u0026#34;, \u0026#34;multicolinearlity\u0026#34;)\r8:9\rjulia\u0026gt; findlast(\u0026#34;li\u0026#34;, \u0026#34;multicolinearlity\u0026#34;)\r14:15\rjulia\u0026gt; findnext(\u0026#34;l\u0026#34;, \u0026#34;multicolinearlity\u0026#34;, 1)\r3:3\rjulia\u0026gt; findnext(\u0026#34;l\u0026#34;, \u0026#34;multicolinearlity\u0026#34;, 4)\r8:8\rjulia\u0026gt; findnext(\u0026#34;l\u0026#34;, \u0026#34;multicolinearlity\u0026#34;, 9)\r14:14\rjulia\u0026gt; findfirst(r\u0026#34;t.+t\u0026#34;, \u0026#34;multicolinearlity\u0026#34;)\r4:16 findfirst(pattern, A)\n文字列Aでpatternに合致する区間をRangeでリターンする。 パターンには正規表現が入れることができる。最後の例では、最初のtから最後のtまでの区間を見つけてリターンした。 環境 OS: Windows julia: v1.6.2 ","id":2226,"permalink":"https://freshrimpsushi.github.io/jp/posts/2226/","tags":null,"title":"ジュリア文字列で特定のパターン位置を見つける方法"},{"categories":"줄리아","contents":"説明 与えられたデータをhclust()で階層的クラスタリングした後、plot()関数を使ってデンドログラムを描こうとすると、以下のようなエラーが発生する。\nusing Clustering using Distances using Plots\ra = rand(2, 10)\rD_a = pairwise(Euclidean(), a, a)\rSL = hclust(D_a, linkage=:single)\rdendrogram = plot(SL)\rERROR: LoadError: Cannot convert Hclust{Float64} to series data for plotting デンドログラムを描くにはPlots.jlではなくStatsPlots.jlを使用する必要がある。\nusing StatsPlots\rdendrogram = plot(SL)\rsavefig(dendrogram, \u0026#34;julia_dendrogram.png\u0026#34;) ","id":3257,"permalink":"https://freshrimpsushi.github.io/jp/posts/3257/","tags":null,"title":"ジュリアでデンドログラムを描く方法"},{"categories":"기하학","contents":"概要 二項演算 $\\wedge$を定義し、1次微分形式を定義した感覚で、微分多様体 $M$に対する2次形式を定義する。\n微分多様体が難しいと感じるなら、$M = \\mathbb{R}^{n}$と考えてもいい。\nビルドアップ1 1次形式 $\\omega$を考えてみよう。\n$$ \\begin{align*} \\omega : M \u0026amp;\\to T^{\\ast}M \\\\ p \u0026amp;\\mapsto \\omega_{p} \\end{align*} $$\nこれは、$n$次元の微分多様体 $M$の点 $p$を、余接空間の要素 $\\omega_{p} \\in T_{p}^{\\ast}M$へマッピングする。すると、$\\omega_{p}$は$T_{p}M$の双対空間の要素なので、以下のような汎関数である。\n$$ w_{p} : T_{p}M \\to \\mathbb{R} $$\nつまり、「1次」形式は点 $p$を、$p$における接ベクトル「1つ」を変数とする関数 $\\omega_{p}$へマッピングすると考えることができる。この感じで「2次」形式を定義することにする。\nウェッジ積 関数 $\\varphi : T_{p}M \\times T_{p}M \\to \\mathbb{R}$を双線形な交代関数としよう。\n$$ \\varphi (v_{1}, v_{2}) = - \\varphi (v_{2}, v_{1}),\\quad v_{i} \\in T_{p}M $$\nこのような$\\varphi$の集合を$\\Lambda^{2} (T_{p}^{\\ast}M)$と表記しよう。\n$$ \\Lambda^{2} (T_{p}^{\\ast}M) := \\left\\{ \\varphi : T_{p}M \\times T_{p}M \\to \\mathbb{R}\\ | \\ \\varphi \\text{ is bilinear and alternate} \\right\\} $$\nさて、$T_{p}^{\\ast}M$の2つの要素を$\\Lambda^{2} (T_{p}^{\\ast}M)$へ送る二項演算 $\\wedge : T_{p}^{\\ast}M \\times T_{p}^{\\ast}M \\to \\Lambda^{2} (T_{p}^{\\ast}M)$を定義してみよう。これは$\\Lambda^{2} (T_{p}^{\\ast}M)$の要素を$T_{p}^{\\ast}M$の要素で表現しようという意味だ。すると$\\varphi_{1}, \\varphi_{2} \\in T_{p}^{\\ast}M$とした場合、$\\Lambda^{2} (T_{p}^{\\ast}M)$は交代関数の集合なので、次が成立しなければならない。（ちなみに記号 $\\wedge$自体は[wedge]と読み、二項演算 $\\wedge$はウェッジ積または外積と呼ばれる。TeXコードは\\wedge）\n$$ \\varphi_{1} \\wedge \\varphi_{2} \\in \\Lambda^{2} (T_{p}^{\\ast}M) $$\n$$ (\\varphi_{1} \\wedge \\varphi_{2}) (v_{1}, v_{2}) = - (\\varphi_{1} \\wedge \\varphi_{2}) (v_{2}, v_{1}),\\quad v_{i} \\in T_{p}M $$\n$\\wedge$を以下のように定義すると、上記の条件を正確に満たす。\n$$ (\\varphi_{1} \\wedge \\varphi_{2})(v_{1}, v_{2}) := \\det \\left[ \\phi_{i}(v_{j}) \\right] $$\nこの時、$i$は行のインデックスを、$j$は列のインデックスを意味する。もちろんウェッジ積$\\wedge$自体も交代関数になる。\n交代性 $$ \\begin{align*} (\\varphi_{1} \\wedge \\varphi_{2})(v_{1}, v_{2}) =\u0026amp;\\ \\det \\left[ \\varphi_{i}(v_{j}) \\right] \\\\ =\u0026amp;\\ \\begin{vmatrix} \\varphi_{1}(v_{1}) \u0026amp; \\varphi_{1}(v_{2}) \\\\ \\varphi_{2}(v_{1}) \u0026amp; \\varphi_{2}(v_{2}) \\end{vmatrix} \\\\ =\u0026amp;\\ - \\begin{vmatrix} \\varphi_{1}(v_{2}) \u0026amp; \\varphi_{1}(v_{1}) \\\\ \\varphi_{2}(v_{2}) \u0026amp; \\varphi_{2}(v_{1}) \\end{vmatrix} \u0026amp; \\text{by property of determinant} \\\\ =\u0026amp;\\ - (\\varphi_{1} \\wedge \\varphi_{2})(v_{2}, v_{1}) \\end{align*} $$\n■\n線形性 $\\text{for } a\\in \\mathbb{R}$,\n$$ \\begin{align*} \u0026amp; (\\varphi_{1} \\wedge \\varphi_{2})(av_{1} + v_{2}, w) \\\\[1em] =\u0026amp;\\ \\begin{vmatrix} \\varphi_{1}(av_{1}+v_{2}) \u0026amp; \\varphi_{1}(w) \\\\ \\varphi_{2}(av_{1}+v_{2}) \u0026amp; \\varphi_{2}(w) \\end{vmatrix} \\\\[1em] =\u0026amp;\\ \\begin{vmatrix} a\\varphi_{1}(v_{1}) + \\varphi_{1}(v_{2}) \u0026amp; \\varphi_{1}(w) \\\\ a\\varphi_{2}(v_{1}) + \\varphi_{2}(v_{2}) \u0026amp; \\varphi_{2}(w) \\end{vmatrix} \u0026amp; \\text{by linearity of } \\varphi_{i} \\\\[1em] =\u0026amp;\\ \\begin{vmatrix} a\\varphi_{1}(v_{1}) \u0026amp; \\varphi_{1}(w) \\\\ a\\varphi_{2}(v_{1}) \u0026amp; \\varphi_{2}(w) \\end{vmatrix} + \\begin{vmatrix}\\varphi_{1}(v_{2}) \u0026amp; \\varphi_{1}(w) \\\\ \\varphi_{2}(v_{2}) \u0026amp; \\varphi_{2}(w) \\end{vmatrix} \u0026amp; \\text{by property of determinant} \\\\[1em] =\u0026amp;\\ a\\begin{vmatrix} \\varphi_{1}(v_{1}) \u0026amp; \\varphi_{1}(w) \\\\ \\varphi_{2}(v_{1}) \u0026amp; \\varphi_{2}(w) \\end{vmatrix} + \\begin{vmatrix} \\varphi_{1}(v_{2}) \u0026amp; \\varphi_{1}(w) \\\\ \\varphi_{2}(v_{2}) \u0026amp; \\varphi_{2}(w) \\end{vmatrix} \u0026amp; \\text{by property of determinant} \\\\[1em] =\u0026amp;\\ a(\\varphi_{1} \\wedge \\varphi_{2})(v_{1}, w) + (\\varphi_{1} \\wedge \\varphi_{2})(v_{2}, w) \\end{align*} $$\n■\n基底 これで、$T_{p}^{\\ast}M$の基底 $\\left\\{ (dx_{j})_{p} \\right\\}_{j}$たちのウェッジ積を考えてみよう。すぐに察しがつくかもしれないが、これらが$\\Lambda^{2} (T_{p}^{\\ast}M)$の基底になることが予想できるだろう。便宜上、以下のように表記しよう。\n$$ (dx_{i} \\wedge dx_{j})_{p} \\overset{\\text{notation}}{=} (dx_{i})_{p} \\wedge (dx_{j})_{p} \\in \\Lambda^{2} (T_{p}^{\\ast}M) $$\nすると、$\\left\\{ (dx_{i} \\wedge dx_{j})_{p} : i \\lt j \\right\\}$は実際に$\\Lambda^{2} (T_{p}^{\\ast}M)$の基底になり、次が成立する。\n$$ (dx_{i} \\wedge dx_{j})_{p} = - (dx_{j} \\wedge dx_{i})_{p},\\quad i \\ne j \\\\[1em] (dx_{i} \\wedge dx_{i})_{p} = 0 $$\nこれで2次形式を定義する準備ができた。\n定義 点 $p \\in M$を次のようにマッピングする関数 $\\omega : M \\to \\Lambda^{2} (T_{p}^{\\ast}M)$を$M$での2次形式exterior form of degree 2と定義する。\n$$ \\omega (p) = a_{12}(p)(dx_{1} \\wedge dx_{2})_{p} + a_{13}(p)(dx_{1} \\wedge dx_{3})_{p} + a_{23}(p)(dx_{2} \\wedge dx_{3})_{p} $$\n$\\omega$は単に以下のように表記する。\n$$ \\begin{align*} \\omega =\u0026amp;\\ a_{12}dx_{1} \\wedge dx_{2} + a_{13}dx_{1} \\wedge dx_{3} + a_{23}dx_{2} \\wedge dx_{3} \\\\ =\u0026amp;\\ a_{ij}dx_{i}\\wedge dx_{j} (i \\lt j) \u0026amp; \\text{by Einstein notation} \\end{align*} $$\nこの時、$a_{ij} : M \\to \\mathbb{R}$である。各$a_{ij}$が微分可能であれば、$\\omega$を2次微分形式differential form of degree 2と呼ぶ。\n参考 余接空間と1次微分形式 2次微分形式 k次微分形式 Manfredo P. Do Carmo, Differential Forms and Applications, p2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3256,"permalink":"https://freshrimpsushi.github.io/jp/posts/3256/","tags":null,"title":"二階微分形式"},{"categories":"줄리아","contents":"コード julia\u0026gt; contains(\u0026#34;qwerty\u0026#34;, \u0026#34;er\u0026#34;)\rtrue\rjulia\u0026gt; contains(\u0026#34;qwerty\u0026#34;, \u0026#34;et\u0026#34;)\rfalse\rjulia\u0026gt; contains(\u0026#34;qwerty\u0026#34;, r\u0026#34;q?\u0026#34;)\rtrue contains(haystack::AbstractString, needle)\nhaystackにneedleが含まれているかをブーリアンで返す。needleにはr\u0026quot;...\u0026quot;のような正規表現が入れられる。 ちなみに、\u0026lsquo;haystack\u0026rsquo;は干し草の山という意味で、「干し草の山から針を探す」という意味の\u0026quot;a needle in a haystack\u0026quot;という表現がある。\n環境 OS: Windows julia: v1.6.2 ","id":2224,"permalink":"https://freshrimpsushi.github.io/jp/posts/2224/","tags":null,"title":"Juliaで特定の文字列を含むかどうかを確認する方法"},{"categories":"최적화이론","contents":"定義 1 $$ \\begin{matrix} \\text{Maximize} \u0026amp; \\mathbf{c}^{T} \\mathbf{x} \\\\ \\text{subject to} \u0026amp; A \\mathbf{x} = \\mathbf{b} \\\\ \u0026amp; \\mathbf{x} \\ge \\mathbf{0} \\end{matrix} $$\n行列 $A \\in \\mathbb{R}^{m \\times n}$、$\\mathbf{b} \\in \\mathbb{R}^{m \\times 1}$、$\\mathbf{c} \\in \\mathbb{R}^{n}$ に対して、線型計画問題が方程式フォームで表されたとするとき、解 $\\mathbf{x} \\in \\mathbb{R}^{n}$ に対して、次の二つの条件を満たすとき、基数 $m$ の集合 $B \\subseteq [n]$ が存在し、$\\mathbf{x} = \\left( x_{1} , \\cdots , x_{n} \\right)$ を基底可能解Basic Feasible Solutionという。\n(i): $\\exists A_{B}^{-1}$ (ii): $x_{j} = 0 \\forall j \\notin B$ この場合、$x_{j}$を基底変数Basic Variable、集合 $B$ を基底Basisと呼ぶ。基底可能解である可能解は形容詞形を使用して、可能解がベーシックBasicであると言える。\n$\\mathbf{c}^{T}$は[転置]を意味する。 [可能解]とは、最適化とは無関係に、制約条件を満たす解のことである。 $[n] = \\left\\{ 1, \\cdots , n \\right\\}$は $1$ から $n$ までの自然数の集合である。 $A_{B}$は、行列 $A$ から集合 $B$ に列挙された列のみを取った正方行列である。 説明 定義は言葉が多くて難しそうに見えるけど、実はたいしたことないから、怖がらないでほしい。\n基底可能解は、とにかく可能解に関する議論であって、$\\mathbf{c} \\in \\mathbb{R}^{n}$は全く関係ない。\n例 $$ \\begin{align*} A =\u0026amp; \\begin{bmatrix} 1 \u0026amp; 5 \u0026amp; 3 \u0026amp; 4 \u0026amp; 6 \\\\ 0 \u0026amp; 1 \u0026amp; 3 \u0026amp; 5 \u0026amp; 6 \\end{bmatrix} \\\\ \\mathbf{b} =\u0026amp; \\begin{bmatrix} 14 \\\\ 7 \\end{bmatrix} \\end{align*} $$ とするとき、$\\mathbf{x} = (0,2,0,1,0)$は方程式フォームで二つの制約 $$ \\begin{align*} 1 x_{1} + 5 x_{2} + 3 x_{3} + 4 x_{4} + 6 x_{5} =\u0026amp; 14 \\\\ 0 x_{1} + 1 x_{2} + 3 x_{3} + 5 x_{4} + 6 x_{5} =\u0026amp; 7 \\end{align*} $$ を満たす可能解である。ここで、$\\mathbf{x}$の成分としては、$x_{2}, x_{4}$のみが使用された条件(ii)、かつ $$ A_{B} = \\begin{bmatrix} 5 \u0026amp; 4 \\\\ 1 \u0026amp; 5 \\end{bmatrix} $$ が非特異nonsingularである条件(i)ため、可能解$\\mathbf{x}$は基底可能解である。\n幾何学的説明 $A \\in \\mathbb{R}^{1 \\times 3}$の場合、つまり制約が$m=1$個で解空間の次元が$n = 3$の線型計画問題を考えてみよう。\nはっきり言って、基底可能解とは上の図の三角錐で$\\mathbf{0}$でない頂点Vertexのことを言う。現在、最適解ではなく可能解のみを考慮しているが、目的関数が非線形で曲がっていないのだから、最適解が辺Edgeの真ん中にあるとは思えないよね？もし最適解が存在するなら、それはその三つの頂点の中の一つにあるだろうし、これを抽象化して一般化したものが基底可能解の概念である。\n一方、この解空間はまさに単体であり、ここから単体法のアイデアが生まれる。\n代数的説明 $A_{B} \\in \\mathbb{R}^{m \\times m}$ の逆行列 $A_{B}^{-1}$ が存在するということは、正方行列 $A_{B}$ の列ベクトルが線形独立であることを意味し、これは$n$個のすべての変数ではなく、正確に必要な$m \\le n$個の変数だけを考えても十分であることを意味する。上の段落で幾何学的に見たとき、三つの頂点を表現するためにそれぞれ一つの次元だけを必要としたこととピッタリ合う。\nMatousek. (2007). Understanding and Using Linear Programming: p45.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2223,"permalink":"https://freshrimpsushi.github.io/jp/posts/2223/","tags":null,"title":"線形計画問題の基底解"},{"categories":"기하학","contents":"概要 余接空間と微分1形式を定義する。微分多様体が難しいなら、$M = \\mathbb{R}^{n}$だと思ってもいい。\nアインシュタイン表記を使う。\n余接空間1 $M$を$n$次元微分多様体とする。すると、点$p \\in M$での接空間$T_{p}M$は、$n$次元ベクトル空間（関数空間）になり、基底は$\\left\\{ \\mathbf{e}_{i} = \\left. \\frac{\\partial }{\\partial x_{i}}\\right|_{p} \\right\\}_{i}$である。\nこの時、接空間$T_{p} M$の双対空間$T_{p}^{\\ast}M$を余接空間と言う。\n$$ T_{p}^{\\ast}M := \\left\\{ \\psi : T_{p}M \\to \\mathbb{R}\\ |\\ \\psi \\text{ is continuous and linear} \\right\\} $$\n説明 双対空間の性質により、$\\dim T_{p}M = n = \\dim T_{p}^{\\ast}M$であり、双対基底$\\left\\{ (dx_{j})_{p} \\right\\}$は次のように定義される関数である。\n$$ (dx_{j})_{p} : T_{p}M \\to \\mathbb{R} $$\n$$ (dx_{j})_{p} \\left(\\textstyle \\left. \\frac{\\partial }{\\partial x_{k}}\\right|_{p} \\right) = \\delta_{jk} = \\begin{cases} 1, \u0026amp; j=k \\\\ 0, \u0026amp; j\\ne k \\end{cases} $$\n任意の$\\omega_{p} \\in T_{p}^{\\ast}M$は、基底$\\left\\{ (dx_{j})_{p} \\right\\}$に対して次のように表される。\n$$ \\begin{align*} \\omega_{p} =\u0026amp;\\ (a_{p}^{1}, a_{p}^{2}, a_{p}^{3}),\\quad a_{p}^{i} \\in \\mathbb{R} \\\\[1em] =\u0026amp;\\ a_{p}^{1}(dx_{1})_{p} + a_{p}^{2}(dx_{1})_{p} + a_{p}^{3}(dx_{3})_{p} \\end{align*} $$\nそれでは、それぞれの点$p \\in M$を$\\omega_{p} \\in T_{p}^{\\ast}M$にマッピングする関数$\\omega$を考えよう。\n微分1次形式 微分多様体$M$の上の各点$p\\in M$を余接空間の元$\\omega_{p} \\in T_{p}^{\\ast}M$にマッピングする$\\omega$を1-形式と言う。\n$$ \\begin{align*} \\omega : M \u0026amp;\\to T^{\\ast}M \\\\ p \u0026amp;\\mapsto \\omega_{p} \\end{align*} $$\nこの時$T^{\\ast}M = \\bigcup \\limits_{p \\in M} T_{p}^{\\ast}M$は余接[バンドル]と言う。\n説明 1-形式は1次形式とも呼ばれ、英語ではexterior form of degree 1, field of linear form等と呼ばれる。\n$a_{i}$を$a_{i} : M \\to \\mathbb{R}$であり$a_{i}(p) = a_{p}^{i}$の関数だとすれば、$\\omega_{p}$は次のように表せる。\n$$ \\begin{align*} \\omega_{p} = \\omega (p) =\u0026amp;\\ (a_{1}(p), a_{2}(p), a_{3}(p)) \\\\ =\u0026amp;\\ a_{1}(p)(dx_{1})_{p} + a_{2}(p)(dx_{1})_{p} + a_{3}(p)(dx_{3})_{p} \\end{align*} $$\nすると$\\omega$は次のようになる。アインシュタイン記法を使えば、\n$$ \\omega = a_{1}dx_{1} + a_{2}dx_{2} + a_{3}dx_{3} = a_{i}dx_{i} $$\nこの時、各$a_{i}$が微分可能な関数ならば、$\\omega$を1次微分形式と言う。\n$\\mathbb{R}^{n}$での$1$-形式 この抽象的な話では、これの意味を理解するのは難しいだろう。微分形式は、微分積分学で$dx$と$dy$を自由に扱うことに対する理論的なバックアップを提供する。ユークリッド空間の例を見よう。関数$f : \\mathbb{R}^{n} \\to \\mathbb{R}$を考える。すると、$f$の微分は$df_{p} : T_{p}\\mathbb{R}^{n} \\to T_{f(p)}\\mathbb{R}$である。$v \\in T_{p}\\mathbb{R}^{n}$とすると、\n$$ v = \\sum\\limits_{i} v_{i}\\dfrac{\\partial }{\\partial x_{i}} = v_{i}\\dfrac{\\partial }{\\partial x_{i}} = (v_{1}, \\dots, v_{n}) $$\n$\\mathbb{R}$の座標を$y$とすると、$T_{f(p)}\\mathbb{R}$の基底は$\\left\\{ \\dfrac{\\partial }{\\partial y} \\right\\}$で、微分に$v$を代入すると、\n$$ \\begin{align*} df_{p} (v) \u0026amp;= \\begin{bmatrix}\\dfrac{\\partial f}{\\partial x_{1}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial f}{\\partial x_{n}}\\end{bmatrix} \\begin{bmatrix}v_{1} \\\\ \\vdots \\\\ v_{n} \\end{bmatrix} \\\\ \u0026amp;= \\begin{bmatrix} v_{i} \\dfrac{\\partial f}{\\partial x_{i}}\\end{bmatrix} \\\\ \u0026amp;= \\left( v_{i} \\dfrac{\\partial f}{\\partial x_{i}} \\right) \\dfrac{\\partial }{\\partial y} \\end{align*} $$\n今、$\\mathbb{R}^{n}$での$1$-形式$\\omega_{p} = \\dfrac{\\partial f}{\\partial x_{i}}dx_{i}$を見よう。$v$を代入すると、\n$$ \\begin{align*} w_{p} (v) \u0026amp;= \\dfrac{\\partial f}{\\partial x_{i}}dx_{i} (v) \\\\ \u0026amp;= \\dfrac{\\partial f}{\\partial x_{i}}dx_{i} \\left( v_{j}\\dfrac{\\partial }{\\partial x_{j}} \\right) \\\\ \u0026amp;= \\dfrac{\\partial f}{\\partial x_{i}} v_{j}\\delta_{ij} \\\\ \u0026amp;= v_{i}\\dfrac{\\partial f}{\\partial x_{i}} \\end{align*} $$\nすると、この場合はどのみち$\\mathbb{R}$の次元が$1$なので、次が成り立つ。\n$$ df_{p}(v) = \\begin{bmatrix} v_{i} \\dfrac{\\partial f}{\\partial x_{i}}\\end{bmatrix} = v_{i}\\dfrac{\\partial f}{\\partial x_{i}} = \\omega_{p}(v) $$\n従って、$\\mathbb{R}^{n}$上の$1$形式$\\omega_{p}$と$\\mathbb{R}^{n}$上で定義された関数の微分$df_{p}$が同じであることが分かる。これは微分積分学でスカラー関数の完全微分$df$を次のように表したものの本質である。\n$$ df = \\dfrac{\\partial f}{\\partial x}dx + \\dfrac{\\partial f}{\\partial y}dy + \\dfrac{\\partial f}{\\partial z}dz $$\n例 $f(x,y) = x^{2} + y^{2}$ならば、\n$$ df = \\dfrac{\\partial f}{\\partial x}dx + \\dfrac{\\partial f}{\\partial y}dy = 2xdx + dydy $$\n$f(x,y) = e^{xy} + 3x$ならば、\n$$ df = (ye^{xy} + 3)dx + xe^{xy}dy $$\n関連項目 2次微分形式 k次微分形式 Manfredo P. Do Carmo, Differential Forms and Applications, p1-2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3254,"permalink":"https://freshrimpsushi.github.io/jp/posts/3254/","tags":null,"title":"余接空間と一階微分形式"},{"categories":"줄리아","contents":"概要 Primes.jlは、素数関連の関数や素因数分解を取り扱うパッケージだ。解析的整数論に関する関数の実装はまだ不足している。\nパッケージの全ての機能がまとめられているわけではなく、有用なものだけを選んだので、詳細はリポジトリをチェックしてくれ1。\nタイプ 素因数分解 Primes.Factorization julia\u0026gt; factor(12)\r2^2 * 3\rjulia\u0026gt; factor(12)[1]\r0\rjulia\u0026gt; factor(12)[2]\r2\rjulia\u0026gt; factor(12)[3]\r1\rjulia\u0026gt; factor(12)[4]\r0 素因数分解は、底と指数が区別され、独自のデータ型を使用する。インデックスとして底にアクセスすると、その指数を参照できる。\n関数 素数生成 prime(), primes() julia\u0026gt; using Primes\rjulia\u0026gt; prime(4)\r7\rjulia\u0026gt; primes(10)\r4-element Vector{Int64}:\r2\r3\r5\r7 prime(::Type{\u0026lt;:Integer}=Int, i::Integer)\ni番目の素数を返す。 primes([lo,] hi)\nhiまでの素数の配列を返す。 素数判定 isprime() julia\u0026gt; isprime(7)\rtrue\rjulia\u0026gt; isprime(8)\rfalse isprime(n::Integer)\nnが素数かどうかを判断した結果をブール値で返す。この関数の実装には、ミラーラビン判定法などが使用されている。 素因数分解 factor() julia\u0026gt; factor(24)\r2^3 * 3\rjulia\u0026gt; factor(Vector, 24)\r4-element Vector{Int64}:\r2\r2\r2\r3\rjulia\u0026gt; factor(Array, 24)\r4-element Vector{Int64}:\r2\r2\r2\r3\rjulia\u0026gt; factor(Set, 24)\rSet{Int64} with 2 elements:\r2\r3 factor(n::Integer) -\u0026gt; Primes.Factorization factor(ContainerType, n::Integer) -\u0026gt; ContainerType\nnの素因数分解を返す。この関数の実装には、ポラードの$p-1$ロー素因数分解アルゴリズムなどが使用されている。 ContainerTypeを指定すると、そのコンテナに合わせて結果を返し、別に指定しない場合は、自身のデータ型Primes.Factorizationで返す。 オイラーのφ関数 julia\u0026gt; totient(12)\r4 totient(n::Integer)\nn=$n$に対して、オイラーのφ関数$\\phi$を使用して、$\\displaystyle n \\prod_{p \\mid n} \\left( 1 - {{ 1 } \\over { p }} \\right)$を返す。 https://github.com/JuliaMath/Primes.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2222,"permalink":"https://freshrimpsushi.github.io/jp/posts/2222/","tags":null,"title":"ジュリアでの因数分解および素数関数の使用方法"},{"categories":"최적화이론","contents":"定義 1 行列 $A \\in \\mathbb{R}^{m \\times n}$、$\\mathbf{b} \\in \\mathbb{R}^{m \\times 1}$、そして $\\mathbf{c} \\in \\mathbb{R}^{n}$に関して、以下の線形計画問題を標準形Standard Formまたは方程式フォームEquational Formと呼ぶ。 $$ \\begin{matrix} \\text{Maximize} \u0026amp; \\mathbf{c}^{T} \\mathbf{x} \\\\ \\text{subject to} \u0026amp; A \\mathbf{x} = \\mathbf{b} \\\\ \u0026amp; \\mathbf{x} \\ge \\mathbf{0} \\end{matrix} $$\n$\\mathbf{c}^{T}$は転置を意味する。 最適化は最大化あるいは最小化を指す。 説明 標準形への変換 原則として、どんな線形計画問題も標準形に変換できる。例えば、以下のような最大化問題が与えられたとする。\n$$ \\begin{matrix} \\text{Maximize} \u0026amp; 3 x_{1} - 2 x_{2} \\\\ \\text{subject to} \u0026amp; 2 x_{1} - x_{2} \\le 4 \\\\ \u0026amp; x_{1} + 3 x_{2} \\ge 5 \\\\ \u0026amp; x_{2} \\ge 0 \\end{matrix} $$\n変換の基本的なアイディアは「不等式をまとめること」である。最初の制約 $$ 2 x_{1} - x_{2} \\le 4 $$ は実際、何らかの $x_{3} \\ge 0$について $$ 2 x_{1} - x_{2} + x_{3} = 4 $$ と表現しても全く問題ない。$2 x_{1} - x_{2}$が$4$より小さい量を$x_{3} \u0026gt; 0$が補う役割をするからである。このように方程式を緩める役割をする変数をスラック変数Slack Variableと呼ぶ。二番目の制約 $$ x_{1} + 3 x_{2} \\ge 5 $$ は両辺に$-1$を掛け、最初の制約で行ったように新しいスラック変数$x_{4}$を導入すれば $$ - x_{1} - 3 x_{2} + x_{4} = - 5 $$ である。ここまで見ると $$ \\begin{matrix} \\text{Maximize} \u0026amp; 3 x_{1} - 2 x_{2} \\\\ \\text{subject to} \u0026amp; 2 x_{1} - x_{2} + x_{3} = 4 \\\\ \u0026amp; - x_{1} - 3 x_{2} + x_{4} = - 5 \\\\ \u0026amp; x_{2}, x_{3}, x_{4} \\ge 0 \\end{matrix} $$ は方程式フォームの条件を満たしたように見えるが、まだ$x_{1} \\ge 0$という制約がないので、これを形式に合わせなければならない。与えられた問題で$x_{1}$は実数全体で許されているので、これを二つの正数の差$x_{1} = y_{1} - z_{1}$に分解すれば$x_{1}$は式から消え、次のような方程式フォームで表現できる。 $$ \\begin{matrix} \\text{Maximize} \u0026amp; 3 \\left( y_{1} - z_{1} \\right) - 2 x_{2} \\\\ \\text{subject to} \u0026amp; 2 \\left( y_{1} - z_{1} \\right) - x_{2} + x_{3} = 4 \\\\ \u0026amp; - \\left( y_{1} - z_{1} \\right) - 3 x_{2} + x_{4} = - 5 \\\\ \u0026amp; y_{1}, z_{1}, x_{2}, x_{3}, x_{4} \\ge 0 \\end{matrix} $$\n方程式フォームの幾何 方程式$A \\mathbf{x} = \\mathbf{b}$は一般的な平面、超平面を形成し、$\\mathbf{x} \\ge \\mathbf{0}$という条件によってユークリッド空間で円錐形の部分だけを取ることができるようになる。スラック変数が導入されることで次元は増えるが、最適化のために探索しなければならない空間自体は大幅に単純化される。\n一方、この解空間は他ならぬシンプレックスであり、ここからシンプレックスメソッドのアイデアにつながる。\nMatousek. (2007). 線形プログラミングを理解して使う: p41.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2221,"permalink":"https://freshrimpsushi.github.io/jp/posts/2221/","tags":null,"title":"線形計画問題の方程式フォーム"},{"categories":"줄리아","contents":"概要 Polynomials.jlは多項式関数の表現や計算を含むパッケージだ。数学的に単純な多項式だからコーディングも簡単に考えがちだが、実際に必要な機能を実装し始めると、結構面倒だ。もちろんものすごく難しいわけではないが、できればパッケージを使用しよう。\nパッケージの全ての機能をまとめたわけではなく、役に立ちそうなものだけをピックアップしたので、詳しくはリポジトリをチェックしてほしい1。\n一般的な多項式関数 係数で多項式関数を定義する Polynomial() julia\u0026gt; using Polynomials\rjulia\u0026gt; p = Polynomial([1,0,0,1])\rPolynomial(1 + x^3)\rjulia\u0026gt; q = Polynomial([1,1])\rPolynomial(1 + x)\rjulia\u0026gt; r = Polynomial([1,1], :t)\rPolynomial(1 + t)\rjulia\u0026gt; p(0)\r1\rjulia\u0026gt; p(2)\r9 Polynomial{T, X}(coeffs::AbstractVector{T}, [var = :x])\nそれ自体が多項式関数を返す。coeffsは係数の配列で、最初が定数項で、後ろに行くほど高次項になる。 varでは多項式関数の変数を与える。例のようにシンボル :tを入れるとtの多項式になる。 データで多項式関数を定義する fit() julia\u0026gt; fit([-1,0,1], [2,0,2])\rPolynomial(2.0*x^2) fit(::Type{RationalFunction}, xs::AbstractVector{S}, ys::AbstractVector{T}, m, n; var=:x)\n$x$の座標がxsで、$y$の座標がysの点を通る多項式関数を返す。 根で多項式関数を定義する roots() julia\u0026gt; fromroots([-2,2])\rPolynomial(-4 + x^2) fromroots(::AbstractVector{\u0026lt;:Number}; var=:x)\n与えられた配列の要素が根になるような多項式関数を返す。 演算 +, -, *, ÷ julia\u0026gt; p + 1\rPolynomial(2 + x^3)\rjulia\u0026gt; 2p\rPolynomial(2 + 2*x^3) スカラーを加えたり乗じたりすると、上のように直感的に演算される。\njulia\u0026gt; p + q\rPolynomial(2 + x + x^3)\rjulia\u0026gt; p - q\rPolynomial(-x + x^3)\rjulia\u0026gt; p * q\rPolynomial(1 + x + x^3 + x^4)\rjulia\u0026gt; p ÷ q\rPolynomial(1.0 - 1.0*x + 1.0*x^2) 多項式関数間の四則演算は、+, -, *, ÷でオーバーライドされる。\n根を求める roots() julia\u0026gt; roots(p)\r3-element Vector{ComplexF64}:\r-1.0 + 0.0im\r0.4999999999999998 - 0.8660254037844383im\r0.4999999999999998 + 0.8660254037844383im roots(f::AbstractPolynomial)\nfの根をベクトルとして返す。 微分 derivative() julia\u0026gt; derivative(p, 3)\rPolynomial(6) derivative(f::AbstractPolynomial, order::Int = 1)\nfのorder次の導関数を返す。 積分 integrate() julia\u0026gt; integrate(p, 7)\rPolynomial(7.0 + 1.0*x + 0.25*x^4) integrate(f::AbstractPolynomial, C = 0)\n積分定数がCのfの不定積分を返す。 特別な多項式関数 ローラン多項式関数 LaurentPolynomial() julia\u0026gt; LaurentPolynomial([4,3,2,1], -1)\rLaurentPolynomial(4*x⁻¹ + 3 + 2*x + x²) LaurentPolynomial{T,X}(coeffs::AbstractVector, [m::Integer = 0], [var = :x])\n次数が整数に拡張されたローラン多項式関数を返す。 最小項の次数はmで与えられる。 チェビシェフ多項式関数 ChebyshevT() julia\u0026gt; ChebyshevT([3,2,1])\rChebyshevT(3⋅T_0(x) + 2⋅T_1(x) + 1⋅T_2(x)) ChebyshevT{T, X}(coeffs::AbstractVector)\n第一種チェビシェフ多項式関数を返す。式のT_n(x)は$T_{n}(x) = \\cos \\left( n \\cos^{-1} x \\right)$を表す。 環境 OS: Windows julia: v1.6.2 https://juliamath.github.io/Polynomials.jl/stable/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2220,"permalink":"https://freshrimpsushi.github.io/jp/posts/2220/","tags":null,"title":"ジュリアで多項式を使用する方法"},{"categories":"확률분포론","contents":"定義 スケールパラメータ$\\lambda \u0026gt; 0$とシェイプパラメータ$k \u0026gt; 0$に関して、以下の確率密度関数を持つ確率分布をワイブル分布Weibull Distributionという。 $$ f(x) = {{ k } \\over { \\lambda }} \\left( {{ x } \\over { \\lambda }} \\right)^{k-1} e^{-(x/\\lambda)^{k}} \\qquad , x \\ge 0 $$\n定理 [1] 指数分布の一般化：$k=1$の場合、ワイブル分布は指数分布になる。 [2] レイリー分布の一般化：$k=2$の場合、ワイブル分布はレイリー分布になる。 説明 確率密度関数の数式的表現からもわかるように、ワイブル分布は最も一般的に見られる指数分布の一般化と見ることができる。その応用は非常に多岐にわたるが、最も代表的なものは指数分布と同様の生存分析であり、ただし指数分布のように失敗率Failure Rateが一定ではなく、$k$によって変化すると見ることができる：\n$k\u0026lt;1$の場合、失敗率が時間とともに徐々に小さくなると見る。 特定の期間を過ぎると急激に低下する現象、例えば乳児死亡率をよく説明できるとされる。 $k=1$の場合、失敗率が時間の経過とともに一定、つまり指数分布だ。 $k\u0026gt;1$の場合、失敗率が時間とともに徐々に増加すると見る。 特にウイルスの潜伏期間や回復期間を推定する研究論文で頻繁に登場する。 三パラメータの一般化 1 スケールパラメータ$\\alpha \u0026gt; 0$、ロケーションパラメータ$\\beta \u0026gt; 0$、シェイプパラメータ$\\gamma \u0026gt; 0$に関して、以下の確率密度関数を持つ確率分布を三パラメータワイブル分布Three-parameter Weibull Distributionという。 $$ f(x) = {{ \\gamma } \\over { \\alpha }} \\left( {{ x-\\beta } \\over { \\alpha }} \\right)^{\\gamma-1} e^{- \\left( (x - \\beta) / \\alpha \\right)^{\\gamma}} \\qquad , x \\ge \\beta $$ もし$X \\sim \\text{Weibull} (\\alpha, \\beta, \\gamma)$ならば、その平均と分散は以下の通りである。 $$ \\begin{align*} E(X) =\u0026amp; \\alpha \\Gamma \\left( 1 + {{ 1 } \\over { \\gamma }} \\right) + \\beta \\\\ \\text{Var} (X) =\u0026amp; \\alpha^{2} \\left[ \\Gamma \\left(1 + {{ 2 } \\over { \\gamma }} \\right) - \\left( \\Gamma \\left( 1 + {{ 1 } \\over { \\gamma }} \\right)^{2} \\right) \\right] \\end{align*} $$ もちろん、二つのパラメータに関して$X \\sim \\text{Weibull} (\\lambda, k)$ならば、次のようになる。 $$ \\begin{align*} E(X) =\u0026amp; \\lambda \\Gamma \\left( 1 + {{ 1 } \\over { k }} \\right) \\\\ \\text{Var} (X) =\u0026amp; \\lambda^{2} \\left[ \\Gamma \\left(1 + {{ 2 } \\over { k }} \\right) - \\left( \\Gamma \\left( 1 + {{ 1 } \\over { k }} \\right)^{2} \\right) \\right] \\end{align*} $$\nここで、$\\Gamma$はガンマ関数だ。 Miller. (2006). A Derivation of the Pythagorean Won-Loss Formula in Baseball. https://arxiv.org/abs/math/0509698v4\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2219,"permalink":"https://freshrimpsushi.github.io/jp/posts/2219/","tags":null,"title":"ワイブル分布"},{"categories":"줄리아","contents":"コード 文字列の連結 * julia\u0026gt; \u0026#34;oh\u0026#34; * \u0026#34;my\u0026#34; * \u0026#34;girl\u0026#34;\r\u0026#34;ohmygirl\u0026#34; Pythonの+に相当する。\n複数の文字列を連結する string() julia\u0026gt; string(\u0026#34;oh\u0026#34;,\u0026#34;my\u0026#34;, \u0026#34;girl\u0026#34;)\r\u0026#34;ohmygirl\u0026#34; Rのpaste0()に相当する。\n文字列のリストのアイテムとして連結する join() julia\u0026gt; OMG = [\u0026#34;oh\u0026#34;,\u0026#34;my\u0026#34;, \u0026#34;girl\u0026#34;]\r3-element Vector{String}:\r\u0026#34;oh\u0026#34;\r\u0026#34;my\u0026#34;\r\u0026#34;girl\u0026#34;\rjulia\u0026gt; join(OMG)\r\u0026#34;ohmygirl\u0026#34; Pythonのjoin()に相当する。\n同じ文字列を繰り返す ^ julia\u0026gt; \u0026#34;=-\u0026#34; ^ 10\r\u0026#34;=-=-=-=-=-=-=-=-=-=-\u0026#34; Pythonの*に相当する。繰り返しをべき乗で表現することは、偶然ではなく、Pythonで文字列を連結する二項演算が+(和)で、これを繰り返すことが*(積)であるように、Juliaでは連結する演算が*(積)で、これを繰り返すことが^(べき乗)になるのだ。\nなぜ？ なぜ、他の言語と同じように直感的に理解しやすい+ではなく*を使うのか？それは代数的、数学的観点から、文字列の結合は加法よりも乗法に近く、自然だからだ1。代数学の自由群というのを理解できれば最高だけど、そういう背景知識がなくても、数学でxとyの積をx * y = xyのように表すのは納得できるだろう。 $$ x \\ast y = xy $$ 今、\u0026quot;xy\u0026quot;という文字列に\u0026quot;litol\u0026quot;という文字列を付け足して、\u0026quot;xylitol\u0026quot;キシリトールを作ると考えてみよう。 $$ xy \\ast litol = xylitol $$ 納得できるだろう。今更\u0026quot;xy\u0026quot; + \u0026quot;litol\u0026quot;を考えると、何か変に感じるかもしれない。Juliaを作った人たちは、そういう数学的直感に真剣そのものだ。\n環境 OS: Windows julia: v1.6.2 https://discourse.julialang.org/t/why-are-there-all-these-strange-stumbling-blocks-in-julia/92644/40\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2218,"permalink":"https://freshrimpsushi.github.io/jp/posts/2218/","tags":null,"title":"ジュリアで文字列を結合する方法"},{"categories":"줄리아","contents":"コード 1 using Plots\rx = rand(30)\ry = rand(30)\rz = rand(30)\rplot(x)\rplot!(y)\rplot!(z)\rpng(\u0026#34;result1\u0026#34;) 上のように、特定のデータだけ凡例に表示させたくない場合がある。\nlabel = \u0026quot;\u0026quot; plot(x, label = \u0026#34;\u0026#34;)\rplot!(y)\rpng(\u0026#34;result2\u0026#34;) そんな時は、label = \u0026quot;\u0026quot;というオプションを使えばいい。図には最初のデータが表示されているけど、凡例には現れないのがわかる。\nprimary = false plot!(z, primary = false)\rpng(\u0026#34;result3\u0026#34;) もうひとつの方法として、primary = falseというオプションを使うこともできるらしい。見ての通り、最後のデータがオレンジ色でプロットされ、凡例からは隠されている。「primary」をオフにすることで生じる副作用なので、可能ならば「label」オプションだけ触るようにしよう。\n環境 OS: Windows julia: v1.6.2 https://github.com/JuliaPlots/Plots.jl/issues/1388#issuecomment-363940741\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2216,"permalink":"https://freshrimpsushi.github.io/jp/posts/2216/","tags":null,"title":"ジュリアプロットで特定のデータラベルを隠す方法"},{"categories":"줄리아","contents":"コード 1 annotate!()を使えばいいんだ。以下のコードはブラウン運動で最大点と最小点をマークした絵を描くコードだよ。\nusing Plots\rcd(@__DIR__)\rdata = cumsum(randn(100))\rplot(data, color = :black, legend = :none)\rannotate!(argmax(data), maximum(data), \u0026#34;max\\n\u0026#34;)\rannotate!(argmin(data), minimum(data), \u0026#34;\\nmin\u0026#34;)\rpng(\u0026#34;result\u0026#34;) 環境 OS: Windows julia: v1.7.0 https://discourse.julialang.org/t/plots-annotate/37784\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2214,"permalink":"https://freshrimpsushi.github.io/jp/posts/2214/","tags":null,"title":"ジュリアプロットにテキストを挿入する方法"},{"categories":"줄리아","contents":"環境 OS: Windows julia: v1.6.2 エラー julia\u0026gt; plot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;)\rGKS: glyph missing from current font: 48652\rGKS: glyph missing from current font: 46972\rGKS: glyph missing from current font: 50868\rGKS: glyph missing from current font: 47784\rGKS: glyph missing from current font: 49496 原因 韓国語フォントが見つからないためだ。\n解決法 二つの方法は特に理想的じゃないし、他にいい方法があれば、いつでも提案してほしい。Juliaを使用する上で韓国語があまり必要ないため、韓国語のサポートが不十分であることは事実だ。\ndefault(fontfamily = \u0026quot;\u0026quot;) 1 plot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;, fontfamily = \u0026#34;\u0026#34;)\rdefault(fontfamily = \u0026#34;\u0026#34;)\rplot(data, color = :red,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;) plot()のfontfamily = \u0026quot;\u0026quot;オプションやdefault(fontfamily = \u0026quot;\u0026quot;)を通じて韓国語を表示させることはできる。しかし、具体的なフォント名を変えても、うまく認識されず、保存する際には結局フォントを見つけられず、一枚目の画像のように文字化けすることなく、テキストが空白で表示される問題があることを確認した。\nPlots.plotly() 2 Plots.plotly()\rplot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;)\rsavefig(\u0026#34;result.html\u0026#34;) plotlyバックエンドを使用すると、韓国語の文字自体は表示される。しかし、*.pngに直接保存することはできず、*.htmlに出力した後、別途保存する必要がある。\nコード using Plots\r#Plots.gr()\rdata = cumsum(randn(100))\rplot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;)\rplot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;, fontfamily = \u0026#34;\u0026#34;)\rdefault(fontfamily = \u0026#34;\u0026#34;)\rplot(data, color = :red,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;)\rsavefig(\u0026#34;result.png\u0026#34;)\rPlots.plotly()\rplot(data, color = :black,\rlabel = \u0026#34;값\u0026#34;, title = \u0026#34;브라운모션\u0026#34;)\rsavefig(\u0026#34;result.html\u0026#34;) https://discourse.julialang.org/t/nice-fonts-with-plots-gr-and-latexstrings/60037\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://besixdouze.net/16\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2212,"permalink":"https://freshrimpsushi.github.io/jp/posts/2212/","tags":null,"title":"ジュリアプロットに韓国語テキストを挿入する方法"},{"categories":"최적화이론","contents":"定義 1 目的関数Objective Functionと制約Constraintが線形な最適化問題を線形計画問題Linear Programming Problem、略してLP問題という。簡単に言うと、線形問題とは、与えられた目的関数 $f: \\mathbb{R}^{n} \\to \\mathbb{R}$ がベクトル $\\mathbf{c} \\in \\mathbb{R}^{n}$ に対して $$ f \\left( \\mathbf{x} \\right) := \\mathbf{c}^{T} \\mathbf{x} $$ であり、与えられた行列 $A \\in \\mathbb{R}^{m \\times n}$ と $\\mathbf{b} \\in \\mathbb{R}^{m \\times 1}$ に対して $$ A \\mathbf{x} \\le \\mathbf{b} $$ を満たしながら $f$ の関数値が最適化される $\\mathbf{x} \\in \\mathbb{R}^{n}$ を見つける問題である。主に以下のように表される。 $$ \\begin{matrix} \\text{Optimize} \u0026amp; \\mathbf{c}^{T} \\mathbf{x} \\\\ \\text{subject to} \u0026amp; A \\mathbf{x} \\le \\mathbf{b} \\end{matrix} $$ このような問題では、まず、制約条件を満たす解を実行可能解Feasible Solutionとし、その中で目的関数を最大化または最小化する解を最適解Optimal Solutionという。\n$\\mathbf{c}^{T}$ は転置を意味する。 最適化は、最大化または最小化を指す。 説明 語り 線形とは、与えられた目的関数と制約条件が線形であるために付けられた言葉だ。式に$\\mathbf{x} = \\left( x_{1} , \\cdots , x_{n} \\right)$の二乗項やログなどの非線形関数型が入らず、制約も行列（不等）式の形で現れなければならない点から、適切な表現である。主に行列代数、線形代数でこのように等式で表された問題を解決することと対比される。 $$ A \\mathbf{x} = \\mathbf{b} $$\n計画は、我々が一般的に理解しているコンピュータのアプリケーションではなく、スケジュールやタスクを意味する。実際の線形計画法の応用では、各変数 $x_{1} , \\cdots , x_{n}$ は時間、エネルギー、資源など様々な要素を表すことができる。\n例えば、経済学や経営学では、これらの要素を最大化することは恐らく有用性、最小化することはコストに相当するだろう。出退勤時間と効率が異なる従業員をどの仕事に割り当てるかによる最適な勤務表を作成する場合、線形計画法が最も優先される方法となるだろう。\n例 2 $$ \\begin{matrix} \\text{Optimize} \u0026amp; x_{1} + x_{2} \\\\ \\text{subject to} \u0026amp; x_{1} \\ge 0 \\\\ \u0026amp; x_{2} \\ge 0 \\\\ \u0026amp; x_{2} - x_{1} \\le 1 \\\\ \u0026amp; x_{1} + 6 x_{2} \\le 15 \\\\ \u0026amp; 4 x_{1} - x_{2} \\le 10 \\end{matrix} $$\n上記のような線形問題を考えてみよう。条件が五つもあるため、一見複雑に見えるが、実際に扱う変数は$x_{1}$と$x_{2}$の二つだけなので、これを$2$次元の平面に表示すれば、以下の図のように条件を満たす領域を確認できる。\nこの領域に属する全ての点は、少なくとも制約条件を満たしている実行可能解である。ここから、目的関数$\\Lambda \\left( x_{1} , x_{2} \\right) = x_{1} + x_{2}$が最大になる点を見つければいい。幾何学的に言うと、実行可能解の領域と$x_{1} + x_{2} = \\lambda$が交差する部分の中で$\\lambda$が最も大きくなる場所、つまり直線の$y$切片が最も大きくなる点を見つければいいのだ。\n実際の例の解答は$\\lambda = 5$になるような$\\left( x_{1} , x_{2} \\right) = (3,2)$である。しかし、この解法はどこか見覚えがあるはずだ。一般的な教育課程を経ていれば、恐らく高校一年生ぐらいの時に教科書でこのような問題を見たことがあるだろう。このように基本的なアイデアは中学校を卒業したばかりの子供たちでも理解できるほどシンプルだ。元々「線形」が付いているということは、実際の解法がどうであれ、概念自体は簡単であるということだ。\nMatousek. (2007). Understanding and Using Linear Programming: p3.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMatousek. (2007). Understanding and Using Linear Programming: p1~2.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2207,"permalink":"https://freshrimpsushi.github.io/jp/posts/2207/","tags":null,"title":"線形計画問題の定義"},{"categories":"기하학","contents":"ガウス・ボンネの定理 $\\mathbf{x} : U \\to \\mathbb{R}^{3}$を単連結な測地線座標切片写像、$\\boldsymbol{\\gamma}(I) \\subset \\mathbf{x}(U)$である$\\boldsymbol{\\gamma}$を区間ごとに正則曲線としよう。そして、$\\boldsymbol{\\gamma}$がある領域$\\mathscr{R}$を囲むとする。すると、以下が成立する。\n$$ \\iint_{\\mathscr{R}} K dA + \\int_{\\boldsymbol{\\gamma}} \\kappa_{g} ds + \\sum \\alpha_{i} = 2\\pi $$\nここで$K$はガウス曲率、$\\kappa_{g}$は測地曲率、$\\alpha_{i}$は$\\boldsymbol{\\gamma}$の区間と区間の間の接する点juntion pointでの角度差jump anglesである。\n説明 $\\boldsymbol{\\gamma}$を区間ごとに正則な曲線と仮定したので、タンジェントの方向が突然大きく変わる点があるが、その場所での角度差を$\\alpha_{i}$とした。$\\boldsymbol{\\gamma}$が全体的に滑らかにつながる曲線なら、角度がジャンプする場所はないので、$\\alpha_{i}$は$0$である。(図(が))\n上の定理は$\\mathbf{x}$を測地線座標切片写像という強い条件を置いたときの結果である。より一般的な結果では、式にオイラー指標が登場し、次のようである。\n$$ \\iint_{\\mathscr{R}} K dA + \\int_{C_{i}}\\kappa_{g}ds + \\sum\\alpha_{i} = 2\\pi \\chi(\\mathscr{R}) $$\n証明 $\\mathbf{x}$が測地線座標切片写像であるので、第1基本形式の係数を次のようにしよう。\n$$ \\left[ g_{ij} \\right] = \\begin{bmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; h^{2} \\end{bmatrix} $$\nそして、$\\boldsymbol{\\gamma}(t) = \\mathbf{x}\\left( \\gamma^{1}(t), \\gamma^{2}(t) \\right)$としよう。今、$\\mathbf{x}_{1}$と$\\boldsymbol{\\gamma}$のタンジェント$T = \\boldsymbol{\\gamma}^{\\prime}$間の角度を$\\alpha$としよう。\n$$ \\alpha (t) := \\angle ( \\mathbf{x}_{1}, T) $$\n私たちは、$\\boldsymbol{\\gamma}$がパスに沿って一周するとき、$\\mathbf{x}_{1}$を基準にしたときの$T$の角度変化が$2 \\pi$であることを利用して、定理を証明するだろう。まず、$\\boldsymbol{\\gamma}$を単位速度曲線と仮定しよう。そして、$P$を次を満たす$\\boldsymbol{\\gamma}$に沿った平行なベクトル場としよう。(上の図(な)参照)\n$$ P(t) = \\text{parallel vector field starting from a juction point s.t. } \\dfrac{P \\times T}{\\left\\| P \\times T \\right\\|} = \\mathbf{n} $$\nそして、$\\phi$と$\\theta$をそれぞれ$\\mathbf{x_{1}}$と$P$および$P$と$T$間の角度としよう。\n$$ \\phi (t) = \\angle(\\mathbf{x}_{1}, P),\\quad \\theta (t) = \\angle(P, T) $$\n言い換えると、$\\left\\langle \\mathbf{x}_{1}, P(t) \\right\\rangle = \\cos\\phi (t)$であり、これを微分すると、\n$$ -\\sin \\phi (t) \\dfrac{d \\phi}{d t}(t) = \\left\\langle \\dfrac{d \\mathbf{x}_{1}(\\gamma^{1}(t), \\gamma^{2}(t))}{d t}, P(t) \\right\\rangle + \\left\\langle \\mathbf{x}_{1}, \\dfrac{d P}{d t}(t) \\right\\rangle $$\nこの時、$P$が$\\gamma$に沿って平行なベクトル場であるため、$\\dfrac{dP}{dt}$は定義により$M$と垂直である。$\\mathbf{x}_{1}$は$M$と接するので、後ろの項は$0$である。さらに計算すると、\n$$ \\begin{align*} \u0026amp;\\quad -\\sin \\phi (t) \\dfrac{d \\phi}{d t}(t) \\\\ \u0026amp;= \\left\\langle \\dfrac{d \\mathbf{x}_{1}(\\gamma^{1}(t), \\gamma^{2}(t))}{d t}, P(t) \\right\\rangle \\\\ \u0026amp;= \\Big[ \\mathbf{x}_{11}(\\gamma^{1}(t), \\gamma^{2}(t)) (\\gamma^{1})^{\\prime}(t) + \\mathbf{x}_{12}(\\gamma^{1}(t), \\gamma^{2}(t)) (\\gamma^{2})^{\\prime}(t) \\Big] \\cdot P(t) \\\\ \u0026amp;= \\Big[ \\left( L_{11}\\mathbf{n} + \\Gamma_{11}^{1}\\mathbf{x}_{1} + \\Gamma_{11}^{2}\\mathbf{x}_{2} \\right)(\\gamma^{1})^{\\prime}(t) + \\left( L_{12}\\mathbf{n} + \\Gamma_{12}^{1}\\mathbf{x}_{1} + \\Gamma_{12}^{2}\\mathbf{x}_{2} \\right)(\\gamma^{2})^{\\prime}(t) \\Big] \\cdot P(t) \\\\ \u0026amp;= \\Big[ \\left(\\Gamma_{11}^{1}\\mathbf{x}_{1} + \\Gamma_{11}^{2}\\mathbf{x}_{2} \\right)(\\gamma^{1})^{\\prime}(t) + \\left(\\Gamma_{12}^{1}\\mathbf{x}_{1} + \\Gamma_{12}^{2}\\mathbf{x}_{2} \\right)(\\gamma^{2})^{\\prime}(t) \\Big] \\cdot P(t) \\end{align*} $$\n二番目の等号は連鎖律により、三番目の等号は第2基本形式とクリストッフェル記号の定義によって成立する。四番目の等号は、$P$と$\\mathbf{n}$が互いに垂直であるために成立する。\n測地線座標切片写像のクリストッフェル記号\n下のもの以外はすべて$0$である。\n$$ \\Gamma_{22}^{1} = -hh_{1},\\quad \\Gamma_{12}^{2} = \\Gamma_{21}^{2} = \\dfrac{h_{1}}{h},\\quad \\Gamma_{22}^{2} = \\dfrac{h_{2}}{h} $$\nこれで、$0$になる項をすべて整理すると、以下のようになる。\n$$ -\\sin\\phi (t) \\phi^{\\prime}(t) = \\left\\langle \\dfrac{h_{1}}{h}(\\gamma^{2})^{\\prime}(t)\\mathbf{x}_{2}, P(t) \\right\\rangle = \\dfrac{h_{1}}{h}(\\gamma^{2})^{\\prime}(t) \\left\\langle \\mathbf{x}_{2}, P(t) \\right\\rangle\\tag{1} $$\n$g_{11} = \\left\\langle \\mathbf{x}_{1}, \\mathbf{x}_{1} \\right\\rangle = 1$であるため、$\\mathbf{x}_{1}$は単位ベクトルであり、$g_{12} = \\left\\langle \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\rangle = 0$であるため、$\\mathbf{x}_{1} \\perp \\mathbf{x}_{2}$である。したがって、$\\left\\{ \\mathbf{x}_{1}, \\dfrac{\\mathbf{x}_{2}}{\\left\\| \\mathbf{x}_{2} \\right\\|} \\right\\}$はタンジェント平面の正規直交基底となる。従って、タンジェント平面の要素$P$は、以下のように表される。\n$$ P = \\left\\langle \\mathbf{x}_{1}, P \\right\\rangle\\mathbf{x}_{1} + \\left\\langle \\dfrac{\\mathbf{x}_{2}}{\\left\\| \\mathbf{x}_{2} \\right\\|}, P \\right\\rangle \\dfrac{\\mathbf{x}_{2}}{\\left\\| \\mathbf{x}_{2} \\right\\|} = \\cos\\phi \\mathbf{x}_{1} + \\sin\\phi \\dfrac{\\mathbf{x}_{2}}{h} $$\nまた、$\\left\\langle \\mathbf{x}_{2}, P \\right\\rangle = \\left\\| x_{2} \\right\\|^{2} \\dfrac{\\sin \\phi}{h} = h\\sin \\phi$を$(1)$に代入すると、\n$$ \\phi^{\\prime}(t) = -h_{1}(\\gamma^{2})^{\\prime}(t) $$\nしたがって、$\\phi$の全角変動は\n$$ \\delta \\phi = \\int_{\\boldsymbol{\\gamma}} \\phi^{\\prime} dt = - \\int_{\\boldsymbol{\\gamma}}h_{1}(\\gamma^{2})^{\\prime}(t)dt = - \\int_{\\boldsymbol{\\gamma}}h_{1} d\\gamma^{2} = - \\int_{\\boldsymbol{\\gamma}}h_{1} du^{2} \\tag{2} $$\nさらに、以下の式が成立することを示す。\n$$ \\text{Claim: } \\theta^{\\prime} = k_{g} $$\n$\\theta (t) = \\angle(P, T)$としたので、$\\cos\\theta (t) = \\left\\langle P, T \\right\\rangle$であり、これを微分すると、\n$$ -\\sin\\theta (t)\\theta^{\\prime}(t) = \\left\\langle \\dfrac{d P}{d t}, T \\right\\rangle + \\left\\langle P, \\dfrac{d T}{d t} \\right\\rangle = \\left\\langle P, T^{\\prime} \\right\\rangle $$\n二番目の等号は、$dP/dt$が$\\mathbf{n}$と平行であるために成立する。測地曲率の定義により、示したいものを以下のように得る。\n$$ \\begin{align*} \\kappa_{g} = \\left\\langle \\mathbf{S}, T^{\\prime} \\right\\rangle \u0026amp;= \\left\\langle (\\mathbf{n} \\times T), T^{\\prime} \\right\\rangle \\\\ \u0026amp;= \\left\\langle \\mathbf{n}, (T \\times T^{\\prime}) \\right\\rangle \\\\ \u0026amp;= \\left\\langle \\dfrac{P \\times T}{\\sin \\theta}, (T\\times T^{\\prime}) \\right\\rangle \u0026amp; \\because \\dfrac{P \\times T}{\\left\\| P \\times T \\right\\|} = \\mathbf{n} \\\\ \u0026amp;= \\left\\langle \\dfrac{1}{\\sin\\theta} P, (T\\times (T\\times T^{\\prime})) \\right\\rangle \\\\ \u0026amp;= \\left\\langle \\dfrac{1}{\\sin\\theta} P, -T^{\\prime} \\right\\rangle \\\\ \u0026amp;= \\theta^{\\prime}(t) \\end{align*} $$\n三番目、五番目の等号はスカラー三重積が交換可能であるために成立する。したがって、以下を得る。\n$$ \\delta \\theta = \\int_{\\boldsymbol{\\gamma}} \\theta^{\\prime} dt = \\int_{\\boldsymbol{\\gamma}} k_{g}dt \\tag{3} $$\n$\\alpha = \\phi + \\theta$であるため、\n$$ \\int_{\\boldsymbol{\\gamma}} \\alpha^{\\prime}dt = \\int_{\\boldsymbol{\\gamma}} \\phi^{\\prime}dt + \\int_{\\boldsymbol{\\gamma}} \\theta^{\\prime}dt $$\n$(2)$と$(3)$により、以下を得る。\n$$ \\int_{\\boldsymbol{\\gamma}} \\alpha^{\\prime}dt + \\sum_{i}\\alpha_{i} = - \\int_{\\boldsymbol{\\gamma}}h_{1} du^{2} + \\int_{\\boldsymbol{\\gamma}} k_{g}dt + \\sum_{i}\\alpha_{i} $$\n$\\boldsymbol{\\gamma}$は$\\mathscr{R}$を囲むため、上の式の左辺は明らかに一周したときの角度変化、すなわち$2 \\pi$である。\n$$ {} - \\int_{\\boldsymbol{\\gamma}}h_{1} du^{2} + \\int_{\\boldsymbol{\\gamma}} k_{g}dt + \\sum_{i}\\alpha_{i} = 2 \\pi $$\nグリーンの定理\n$$ \\oint_{\\partial \\mathscr{R}} Pdx = - \\iint_{\\mathscr{R}} P_{y} dy dx $$\n測地線座標切片写像のガウス曲率\n$$ K = -\\dfrac{h_{11}}{h} $$\n曲面の面積要素\n$$ dA = \\sqrt{g} du^{1} du^{2} $$\n左辺の最初の項は、グリーンの定理を利用すると、以下のように変更できる。\n$$ \\begin{align*} {} - \\int_{\\boldsymbol{\\gamma}}h_{1} du^{2} \u0026amp;= - \\iint_{\\mathscr{R}}h_{11} du^{1}du^{2} \\\\ \u0026amp;= - \\iint_{\\mathscr{R}}\\dfrac{h_{11}}{h} h du^{1}du^{2} \\\\ \u0026amp;= - \\iint_{\\mathscr{R}}\\dfrac{h_{11}}{h} \\sqrt{g} du^{1}du^{2} \\\\ \u0026amp;= \\iint_{\\mathscr{R}} K dA \\end{align*} $$\n最後に、以下の結論を得る。\n$$ \\iint_{R} K dA + \\int_{\\gamma} \\kappa_{g} ds + \\sum \\alpha_{i} = 2\\pi $$\n","id":3238,"permalink":"https://freshrimpsushi.github.io/jp/posts/3238/","tags":null,"title":"ガウス・ボーネの定理"},{"categories":"줄리아","contents":"コード 1 2 3 julia\u0026gt; replace(\u0026#34;qwerty\u0026#34;, \u0026#34;q\u0026#34;=\u0026gt;\u0026#34;Q\u0026#34;)\r\u0026#34;Qwerty\u0026#34;\rjulia\u0026gt; join(\u0026#34;qwerty\u0026#34;, \u0026#34;,\u0026#34;)\r\u0026#34;q,w,e,r,t,y\u0026#34;\rjulia\u0026gt; split(\u0026#34;qwerty\u0026#34;, \u0026#34;\u0026#34;)\r6-element Vector{SubString{String}}:\r\u0026#34;q\u0026#34;\r\u0026#34;w\u0026#34;\r\u0026#34;e\u0026#34;\r\u0026#34;r\u0026#34;\r\u0026#34;t\u0026#34;\r\u0026#34;y\u0026#34; ジュリアは文字列処理に特出している言語ではないけど、そのせいか、Pythonをたくさん真似して、簡単にそして早く学べる。既に知っている機能のほとんどが実装されていて、モジュールかどうかの部分を除けば、使い方はほとんど似ている。ちなみに、replace()を使う時、\u0026quot;q\u0026quot;=\u0026gt;\u0026quot;Q\u0026quot;は何か独特の文法ではなくて、ペアを直接使ったものだ。\n## 環境 - OS: Windows - julia: v1.7.0 https://docs.julialang.org/en/v1/base/collections/#Base.replace-Tuple{Any,%20Vararg{Pair,%20N}%20where%20N}\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/strings/#Base.join\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/strings/#Base.split\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2205,"permalink":"https://freshrimpsushi.github.io/jp/posts/2205/","tags":null,"title":"PythonのようにJuliaで文字列を扱う方法"},{"categories":"기하학","contents":"定義 $\\mathscr{R}$を曲面 $M$のリージョンとする。$\\mathscr{R}$内の全ての閉曲線がヌル・ホモトピックであれば、$\\mathscr{R}$は単純連結simply connectedだと言われる。\n説明 $\\mathbb{R}^{2}$、ディスク$\\left\\{ x^{2} + y^{2} = r^{2} \\right\\}$、球体$\\mathbb{S}^{2}$のような簡単な例は、すぐに単純連結であると思われるだろう。しかし、下の図を見ると、トーラス $T^{2}$が単純連結ではないことが分かる。$\\gamma$と違って、$\\alpha$や$\\beta$は一点に収縮させることができない。\n","id":3236,"permalink":"https://freshrimpsushi.github.io/jp/posts/3236/","tags":null,"title":"単純連結領域"},{"categories":"줄리아","contents":"コード 比較演算子として$\\approx$を使えば、二つの値が十分に似ている時だけ真を返す。≈は$\\TeX$でと同じように、\\approxと入力してTabを押せば使える。\njulia\u0026gt; π ≈ 3.141592653\rtrue\rjulia\u0026gt; π ≈ 3.14159265\rtrue\rjulia\u0026gt; π ≈ 3.1415926\rfalse\rjulia\u0026gt; π ≈ 3.141592\rfalse 環境 OS: Windows julia: v1.7.0 ","id":2203,"permalink":"https://freshrimpsushi.github.io/jp/posts/2203/","tags":null,"title":"ジュリアで近似値をチェックする方法"},{"categories":"줄리아","contents":"コード 1 julia\u0026gt; d = Dict(\u0026#34;A\u0026#34;=\u0026gt;1, \u0026#34;B\u0026#34;=\u0026gt;2)\rDict{String, Int64} with 2 entries:\r\u0026#34;B\u0026#34; =\u0026gt; 2\r\u0026#34;A\u0026#34; =\u0026gt; 1\rjulia\u0026gt; push!(d,(\u0026#34;C\u0026#34;,3))\rERROR: MethodError: no method matching push!(::Dict{String, Int64}, ::Tuple{String, Int64})\rjulia\u0026gt; push!(d,\u0026#34;C\u0026#34; =\u0026gt; 3)\rDict{String, Int64} with 3 entries:\r\u0026#34;B\u0026#34; =\u0026gt; 2\r\u0026#34;A\u0026#34; =\u0026gt; 1\r\u0026#34;C\u0026#34; =\u0026gt; 3\rjulia\u0026gt; typeof(\u0026#34;C\u0026#34; =\u0026gt; 3)\rPair{String, Int64} ジュリアの辞書Dictionaryは、他のプログラミング言語でよく見られる、キーKeyと値Valueがペアになったデータ型だ。ジュリアの少し違う点は、辞書を各ペアPairの集まりと見なすことだ。提供された実行例で確認できるように、ペアは辞書を構成する要素だ。キーと値は右向きの矢印 =\u0026gt; を通して繋がれ、それ自体も Pair というデータ型を持つ。\n次の例はジュリアで文字列の一部を置換する方法を示している。\njulia\u0026gt; replace(\u0026#34;qwerty\u0026#34;, \u0026#34;q\u0026#34;=\u0026gt;\u0026#34;Q\u0026#34;)\r\u0026#34;Qwerty\u0026#34; Pythonと区別される特徴としては、ペアは辞書なしでもペア自体が存在できることだ。一つのペアだけを含む辞書ではなく、ペア自体をデータとして見るからに、ジュリアのコードはこのように新しい文法のようにペアを活用することもある。直接使うかどうかは別として、読むべきものだ。\n環境 OS: Windows julia: v1.6.3 https://docs.julialang.org/en/v1/base/collections/#Base.Dict\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2201,"permalink":"https://freshrimpsushi.github.io/jp/posts/2201/","tags":null,"title":"ジュリアから：辞書とペア"},{"categories":"줄리아","contents":"概要 JLD.jlは、Juliaを使用している間に発生する一時データを保存することができるパッケージだ1。純粋なJuliaプロジェクトを進行している際、データの入出力が面倒であれば役立つ。一方で、JLD.jlのインタフェースをより直感的に改善したJLD2.jlも利用可能である。2このポストに紹介された内容は、おおよそこのような機能があるということを認識し、可能な限りJLD2.jlを使用することをお勧めする。下位互換性も問題なくサポートされる。\n一方で、matファイルのようなものではなく、正確にマットラボのmatファイルを読み書きしたい場合は、MAT.jlパッケージを参照してください。\nコード using JLD\rcd(@__DIR__); pwd()\rnumpad = reshape(1:9, 3,3)\rcube = zeros(Int64, 3,3,3)\rsave(\u0026#34;mydata.jld\u0026#34;, \u0026#34;numpad\u0026#34;,numpad, \u0026#34;cube\u0026#34;,cube)\rmydata = load(\u0026#34;mydata.jld\u0026#34;)\rmydata[\u0026#34;numpad\u0026#34;]\rmydata[\u0026#34;cube\u0026#34;] 実行結果 julia\u0026gt; numpad = reshape(1:9, 3,3)\r3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\r1 4 7\r2 5 8\r3 6 9\rjulia\u0026gt; cube = zeros(Int64, 3,3,3)\r3×3×3 Array{Int64, 3}:\r[:, :, 1] =\r0 0 0\r0 0 0\r0 0 0\r[:, :, 2] =\r0 0 0\r0 0 0\r0 0 0\r[:, :, 3] =\r0 0 0\r0 0 0\r0 0 0\rjulia\u0026gt; save(\u0026#34;mydata.jld\u0026#34;, \u0026#34;numpad\u0026#34;,numpad, \u0026#34;cube\u0026#34;,cube)\r┌ Warning: JLD incorrectly extends FileIO functions (see FileIO documentation)\r└ @ FileIO C:\\Users\\rmsms\\.julia\\packages\\FileIO\\5JdlO\\src\\loadsave.jl:215 保存されるファイルの拡張子は*.jldでなければならない。保存される各データの名前を文字列で与え、割り当てられた変数を連続してつなげると、そのデータが一つにまとめて保存される。\njulia\u0026gt; mydata = load(\u0026#34;mydata.jld\u0026#34;)\r┌ Warning: JLD incorrectly extends FileIO functions (see FileIO documentation)\r└ @ FileIO C:\\Users\\rmsms\\.julia\\packages\\FileIO\\5JdlO\\src\\loadsave.jl:215 Dict{String, Any} with 7 entries:\r\u0026#34;_creator\\\\JULIA_PATCH\u0026#34; =\u0026gt; 0x00000001\r\u0026#34;cube\u0026#34; =\u0026gt; [0 0 0; 0 0 0; 0 0 0]…\r\u0026#34;_creator\\\\WORD_SIZE\u0026#34; =\u0026gt; 64\r\u0026#34;numpad\u0026#34; =\u0026gt; [1 4 7; 2 5 8; 3 6 9]\r\u0026#34;_creator\\\\JULIA_MINOR\u0026#34; =\u0026gt; 0x00000006\r\u0026#34;_creator\\\\ENDIAN_BOM\u0026#34; =\u0026gt; 0x04030201\r\u0026#34;_creator\\\\JULIA_MAJOR\u0026#34; =\u0026gt; 0x00000001 結果を見ると、辞書が返された。保存時に文字列で与えられた名前がキーKeyとして入り、実際のデータは値Valueにある。次のように、辞書として参照すればいい。\njulia\u0026gt; mydata[\u0026#34;numpad\u0026#34;]\r3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\r1 4 7\r2 5 8\r3 6 9\rjulia\u0026gt; mydata[\u0026#34;cube\u0026#34;]\r3×3×3 Array{Int64, 3}:\r[:, :, 1] =\r0 0 0\r0 0 0\r0 0 0\r[:, :, 2] =\r0 0 0\r0 0 0\r0 0 0\r[:, :, 3] =\r0 0 0\r0 0 0\r0 0 0 JLD2 例で、文字列で辞書を作る過程が不便だったが、JLD2.jlではネームドタプルを使って、同じ機能をより便利に利用することができる。\nhttps://github.com/JuliaIO/JLD.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/JuliaIO/JLD2.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2199,"permalink":"https://freshrimpsushi.github.io/jp/posts/2199/","tags":null,"title":"ジュリアで.matのようにデータを保存する方法"},{"categories":"기하학","contents":"リージョン1 曲面 $M$ の部分集合 $\\mathscr{R}$ を考えよう。$\\mathscr{R}$ が開集合であり、$\\mathscr{R}$ の任意の二点に対して、それらを含む $\\mathscr{R}$ 上の曲線が存在する場合、$\\mathscr{R}$ を $M$ のリージョンregion と呼ぶ。\n境界 曲面 $M$ のリージョン $\\mathscr{R}$ に対して、次の集合 $\\partial \\mathscr{R}$ を $\\mathscr{R}$ の境界boundary と呼ぶ。\n$$ \\partial \\mathscr{R} = \\left\\{ p \\notin \\mathscr{R} : \\exists \\left\\{ p_{j} \\right\\} \\subset \\mathscr{R} \\text{ such that } \\lim\\limits_{j \\to \\infty} p_{j} = p \\right\\} $$\nリージョンを囲む曲線 曲線 $\\boldsymbol{\\gamma}$ のイメージがリージョン $\\mathscr{R}$ の境界であり、$\\boldsymbol{\\gamma}$ の固有の法線 $\\mathbf{S}$ が $\\mathscr{R}$ の内側を向きつつ$-\\mathbf{S}$が外側を向くとき、$\\boldsymbol{\\gamma}$ がリージョン $\\mathscr{R}$ を囲むA curve $\\boldsymbol{\\gamma}$ bounds a region $\\mathscr{R}$と言う。\n説明 $\\mathbf{S}$ がリージョンの内側を向くということは、曲線が反時計回りに回転する必要があることを意味する。例えば、曲面 $M = \\mathbb{R}^{2}$ に対して、$\\mathscr{R} = \\left\\{ (x,y) \\in \\mathbb{R}^{2} : x^{2} + y^{2} \\lt 1 \\right\\}$ は $M$ のリージョンである。また、曲線 $\\boldsymbol{\\alpha}(\\theta) = (\\cos \\theta, \\sin \\theta)$ は $\\mathscr{R}$ の境界である。\n一方で、下の右の図のようなトーラス $T^{2}$を曲面 $M$ としよう。$\\boldsymbol{\\gamma}$ のイメージ以外のすべての点をリージョン $\\mathscr{R}$ とすると、$\\mathscr{R}$ の境界は $\\boldsymbol{\\gamma}$ のイメージになる。しかし、曲線 $\\boldsymbol{\\gamma}$ の $-\\mathbf{S}$ が $\\mathscr{R}$ の外側ではなく内側を向いているため、$\\boldsymbol{\\gamma}$ が $\\mathscr{R}$ を囲まないと言われる。\nRichard S. Millman and George D. Parker、Elements of Differential Geometry (1977)、p180-181\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3230,"permalink":"https://freshrimpsushi.github.io/jp/posts/3230/","tags":null,"title":"微分幾何学における曲面と領域の境界"},{"categories":"줄리아","contents":"コード 1 Base.Iterators.enumerate() は、Pythonのように配列のインデックスと値の両方を参照できるイテレーターIteratorを返す。\njulia\u0026gt; x = [3,5,4,1,2]\r5-element Vector{Int64}:\r3\r5\r4\r1\r2\rjulia\u0026gt; for (idx, value) in enumerate(x)\rprintln(\u0026#34;x[▷eq1◁value\u0026#34;)\rend\rx[1]: 3\rx[2]: 5\rx[3]: 4\rx[4]: 1\rx[5]: 2\rjulia\u0026gt; typeof(enumerate(x))\rBase.Iterators.Enumerate{Vector{Int64}} https://docs.julialang.org/en/v1/base/iterators/#Base.Iterators.enumerate\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2197,"permalink":"https://freshrimpsushi.github.io/jp/posts/2197/","tags":null,"title":"ジュリアのループでインデックスと値の両方を参照する方法"},{"categories":"줄리아","contents":"概要 Juliaに初めて接すると、戸惑うことも少なくないのがシンボルSymbolデータタイプである。シンボルは冒頭に:を付けて使用され、内部データなしにその名前そのもので機能する。主に名前やラベル、辞書のキーなどとして使われる1。\n説明 他のプログラミング言語では、関数にオプションを付ける時に数字を使ったり、意味を正確にするために文字列を使用することが多い。例えば、次の二つの関数がそうである。\njulia\u0026gt; function foo0(x, option = 0)\rif option == 0\rreturn string(x)\relseif option == 1\rreturn Int(x)\relse\rerror(\u0026#34;wrong\u0026#34;)\rend\rend\rfoo0 (generic function with 2 methods)\rjulia\u0026gt; foo0(3.0, 0)\r\u0026#34;3.0\u0026#34;\rjulia\u0026gt; foo0(3.0, 1)\r3\rjulia\u0026gt; function foo1(x, option = \u0026#34;string\u0026#34;)\rif option == \u0026#34;string\u0026#34;\rreturn string(x)\relseif option == \u0026#34;Int\u0026#34;\rreturn Int(x)\relse\rerror(\u0026#34;wrong\u0026#34;)\rend\rend\rfoo1 (generic function with 2 methods)\rjulia\u0026gt; foo1(3.0, \u0026#34;string\u0026#34;)\r\u0026#34;3.0\u0026#34;\rjulia\u0026gt; foo1(3.0, \u0026#34;Int\u0026#34;)\r3 その一方で、以下はシンボルを使用した定義である。一見すると、上記の二つの関数と違いがないように見える。\njulia\u0026gt; function foo2(x, option = :string)\rif option == :string\rreturn string(x)\relseif option == :Int\rreturn Int(x)\relse\rerror(\u0026#34;wrong\u0026#34;)\rend\rend\rfoo2 (generic function with 2 methods)\rjulia\u0026gt; foo2(3.0, :string)\r\u0026#34;3.0\u0026#34;\rjulia\u0026gt; foo2(3.0, :Int)\r3 シンボルを使用する理由は、プログラムの途中で変更されることがないから簡単に説明できる。時にはこの点が不便と感じることもあるが、整数や文字列とは異なり、予期せぬところで変化する可能性が全くない。\nまた、シンボルは真の意味での指定、命令である。インターフェース的な観点では、文字列とシンボルは似ているが、例えば\u0026quot;Int\u0026quot;という文字列を受け取り、その文字列が整数を返す意味であるのに対し、シンボルが:Intとして直接来た場合は、質問もせずに整数型で返すという程度の違いである。この違いが理解できなくても、無理に共感する必要はない。\nその他、シンボルを使用する場合には、データフレームのカラム名など、文字列で表現すると変数と区別しにくい、または区別したくない場合などがある。慣れない表記法のため難しく感じるかもしれないが、用途と違いを理解すれば、特に問題はないという点を押さえておけば良い。\nhttps://docs.julialang.org/en/v1/base/base/#Core.Symbol\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2195,"permalink":"https://freshrimpsushi.github.io/jp/posts/2195/","tags":null,"title":"ジュリアでのシンボル"},{"categories":"세이버메트릭스","contents":"定義 1 打者が打ったボールが有効エリアに落ちたもののうち、野手の選択とエラーを除いた打球の数をヒットHit、略してHと呼ぶ。ヒットはシングルSingles、ダブルDoubles、トリプルTriples、ホームランHome Runsの四種類がある。\n整理 [1]: 打席 PA と 打数 AB と ヒット H に関して、以下の不等式が成り立つ。 $$ \\begin{align*} H \\le AB \\le PA \\\\ 안타 \\le 타수 \\le 타석 \\end{align*} $$ 説明 野手の選択とエラー 野手の選択とは、本来なら打者がアウトになるべき場面で、野手が別の塁へ送球する判断をして打者が生き残る場合を指す。ヒットではないが打数にカウントされるため、打率や長打率を下げる記録となる。\nエラーは、守備によって通常なら打者がアウトになるはずが、野手のミスで出塁した場合を指す。\nセイバーメトリクス 打数は理解しにくいが、ヒットは理解しやすい。定義にもあるように、ただ四つの場合だけである。記録規則によると、データ分析家は「野手の選択」と「エラー」という単語を知る必要さえない。\n定義で言及されたように、野手の選択とエラーを除外するということは偶然の介入を最小限にするという意味であり、「とにかく出塁すれば良い」というわけではなく、「バットで作り出した得点貢献」をカウントすることになる。打者の打撃指標を算出する基本的な累積スタッツとして理解すればよい。\nhttps://www.mlb.com/glossary/standard-stats/hit\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2194,"permalink":"https://freshrimpsushi.github.io/jp/posts/2194/","tags":null,"title":"野球におけるヒットの定義"},{"categories":"줄리아","contents":"ガイド 1 julia\u0026gt; x = rand(\u0026#39;a\u0026#39;:\u0026#39;c\u0026#39;, 10)\r10-element Vector{Char}:\r\u0026#39;a\u0026#39;: ASCII/Unicode U+0061 (category Ll: Letter, lowercase)\r\u0026#39;a\u0026#39;: ASCII/Unicode U+0061 (category Ll: Letter, lowercase)\r\u0026#39;b\u0026#39;: ASCII/Unicode U+0062 (category Ll: Letter, lowercase)\r\u0026#39;c\u0026#39;: ASCII/Unicode U+0063 (category Ll: Letter, lowercase)\r\u0026#39;b\u0026#39;: ASCII/Unicode U+0062 (category Ll: Letter, lowercase)\r\u0026#39;c\u0026#39;: ASCII/Unicode U+0063 (category Ll: Letter, lowercase)\r\u0026#39;c\u0026#39;: ASCII/Unicode U+0063 (category Ll: Letter, lowercase)\r\u0026#39;c\u0026#39;: ASCII/Unicode U+0063 (category Ll: Letter, lowercase)\r\u0026#39;b\u0026#39;: ASCII/Unicode U+0062 (category Ll: Letter, lowercase)\r\u0026#39;b\u0026#39;: ASCII/Unicode U+0062 (category Ll: Letter, lowercase) 上に示されたような配列があるとしよう。例で、私たちの目標は'a' と 'b' の両方を選ぶことだとしよう。自然には包含演算子 $\\in$でブロードキャストすればいいと思うかもしれないが、結果は以下の通りだ。\njulia\u0026gt; x .∈ [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]\rERROR: DimensionMismatch(\u0026#34;arrays could not be broadcast to a common size; got a dimension with lengths 10 and 2\u0026#34;) DimensionMismatch エラーが発生した。これは配列 x とカテゴリー ['a', 'b'] の両方に同時にブロードキャストが行われたために起きたエラーだ。エラーメッセージを解釈すると、x の長さ10と ['a', 'b'] の長さ2が同時に入ってきて混乱しているということだ。\njulia\u0026gt; x .∈ Ref([\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;])\r10-element BitVector:\r1\r1\r1\r0\r1\r0\r0\r0\r1\r1 この場合は Ref() 関数を使ってブロードキャスト問題を解決できる。これにより ['a', 'b'] 内の 'a' と 'b' がスカラとして扱われ、このように二つのキャラクターがある場所だけを見つけることができた。\n注意事項 julia\u0026gt; y = rand(\u0026#39;a\u0026#39;:\u0026#39;c\u0026#39;, 1, 10)\r1×10 Matrix{Char}:\r\u0026#39;b\u0026#39; \u0026#39;a\u0026#39; \u0026#39;b\u0026#39; \u0026#39;b\u0026#39; \u0026#39;a\u0026#39; \u0026#39;c\u0026#39; \u0026#39;a\u0026#39; \u0026#39;b\u0026#39; \u0026#39;b\u0026#39; \u0026#39;c\u0026#39; 上に示されたような $1 \\times 10$ 行列の場合を考えてみよう。一見すると、上のガイドで見た場合と何も変わらないように思えるかもしれないが、.∈が全く異なる方法で使われている。\njulia\u0026gt; y .∈ [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]\r2×10 BitMatrix:\r0 1 0 0 1 0 1 0 0 0\r1 0 1 1 0 0 0 1 1 0 見ての通り、最初の行は 'a' の位置を、二行目は 'b' の位置を示している。これはベクトルか行列かという違いに由来するものだ。\njulia\u0026gt; y .∈ Ref([\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;])\r1×10 BitMatrix:\r1 1 1 1 1 0 1 1 1 0 Ref() を使う場合、一貫した結果を得ることができる。\n環境 OS: Windows julia: v1.7.0 https://stackoverflow.com/a/59978386/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2193,"permalink":"https://freshrimpsushi.github.io/jp/posts/2193/","tags":null,"title":"ジュリアで配列の要素がリストに属しているかを確認する方法"},{"categories":"세이버메트릭스","contents":"定義 1 バッターが\nヒットを打つか 野手の選択によって出塁するか エラーで出塁するか アウトされる(犠牲ヒットは除く) 回数を打数At Bat、略してABと呼ぶ。\n定理 $$ \\begin{align*} PA =\u0026amp; AB + (BB + HBP) + (SH + SF) + X \\\\ 타석 =\u0026amp; 타수 + 사사구 + 희생타 + X \\end{align*} $$ $$ \\begin{align*} H \\le AB \\le PA \\\\ 안타 \\le 타수 \\le 타석 \\end{align*} $$ 説明 野手の選択とエラー 野手の選択Fielder\u0026rsquo;s Choiceは本来はバッターをアウトにできたが野手の判断で別のベースへ送球してバッターが生き残った場合を指す。野手選択はヒットではないが打数に数えられるので打率や長打率は下がる記録だ。\nエラーErrorは本来は野手の守備によりバッターがアウトになるべきだったが、ミスにより出塁した場合を指す。\nセイバーメトリクス 打率や長打率、出塁率、さらにはOPSの算出まで影響を及ぼす重要な記録だ。野球に慣れていない立場からは野手の選択やエラーが何であるか理解するのが難しいため、定理1の計算で計算することが望ましいし、韓国リーグ(KBO)では打席を完成させた回数から四死球、犠牲打、打撃妨害/走者妨害での出塁を除いて定義している。2\n打席と打数の違い 野球をよく知らない人にはどちらも「打ㅅ」と見えてしばしば混乱を招く記録だ。簡単に言えば\n打席はバットと関係なく人が立っていた回数で、 打数はバットが物理的にボールに触れた回数だ。3 打席に立たなければバットを使うことがないので、打数が打席より多くなることはありえない。役立つヒットをヒットとした場合、定理2の不等式が成立する。 $$ 안타 \\le 타수 \\le 타석 $$\n特定の試合状況の例を用いてこの記録を外野に説明することの効果は明らかに低いことを知っている。代わりに、下記の記録を上記の不等式に従って理解しよう。\n(1) 4打席3打数3ヒット: 四回の機会を得て、三回機会を掴み、三回とも攻撃に貢献していい記録だ。 (2) 5打席1打数2ヒット: スイング一振りで最低2つボールが飛んだことを意味するが、存在できない記録だ。 (3) 5打席2打数1ヒット: 五回の機会を得て、その中で二回機会を掴み、その中で一回ヒットを打ち不等式を正しく成立させた記録だ。 (3) 3打席0打数0ヒット: 三回の機会をもらっても一回の打撃もできず、攻撃に貢献できなかった悪い記録だ。 https://www.mlb.com/glossary/standard-stats/at-bat\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.koreabaseball.com/About/Committee/RecordRule.aspx\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n물론 희생타는 제외되지만 타석과 타수의 차이라는 맥락에서는 중요하지 않다.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2192,"permalink":"https://freshrimpsushi.github.io/jp/posts/2192/","tags":null,"title":"野球における打数の定義"},{"categories":"줄리아","contents":"説明 1次元配列（ベクトル）は次のように定義される。\njulia\u0026gt; A = [1; 2; 3]\r3-element Vector{Int64}:\r1\r2\r3 ここで、;は第一次元を基準に次の要素に移る意味を持つ。これを一般化すると、;;は第二次元を基準に次の要素に移る意味を持つ。\njulia\u0026gt; A = [1; 2; 3;; 4; 5; 6]\r3×2 Matrix{Int64}:\r1 4\r2 5\r3 6 同じ方法で3次元以上の配列を定義することができる。ちなみにこのコードはジュリアバージョン1.7以降で可能である。\njulia\u0026gt; A = [1 2; 3 4;;; 5 6; 7 8]\r2×2×2 Array{Int64, 3}:\r[:, :, 1] =\r1 2\r3 4\r[:, :, 2] =\r5 6\r7 8\rjulia\u0026gt; A = [1 2; 3 4;;; 5 6; 7 8 ;;;; 9 10; 11 12;;; 13 14; 15 16]\r2×2×2×2 Array{Int64, 4}:\r[:, :, 1, 1] =\r1 2\r3 4\r[:, :, 2, 1] =\r5 6\r7 8\r[:, :, 1, 2] =\r9 10\r11 12\r[:, :, 2, 2] =\r13 14 環境 OS: Windows10 Version: Julia 1.7.1 ","id":3223,"permalink":"https://freshrimpsushi.github.io/jp/posts/3223/","tags":null,"title":"ジュリアにおいて多次元配列を直接定義する方法"},{"categories":"줄리아","contents":"ガイド while while文は他の言語と変わらない。\njulia\u0026gt; while x \u0026lt; 10\rx += 1\rprint(\u0026#34;▷eq1◁i - \u0026#34;)\rend\r1 - 2 - 3 - 4 - 5 - 6 - 7 - 8 - 9 - 10 -\rjulia\u0026gt; for i = 1:10\rprint(\u0026#34;▷eq2◁i - \u0026#34;)\rend\r1 - 2 - 3 - 4 - 5 - 6 - 7 - 8 - 9 - 10 - ジュリアで主に使われるループのスタイルは、上に示した3つがある。一番上はRやパイソンで使われる方法に似ているし、二番目はマトラブに似ている。最もエレガントな表現は三番目の集合の内包表記を使用した方法だ。\nネストしたループ 以下の2つのループは機能的に全く同じだ。\njulia\u0026gt; X = 1:4; Y = 8:(-1):5;\rjulia\u0026gt; for x ∈ X\rfor y ∈ Y\rprint(\u0026#34; (▷eq3◁y) = ▷eq4◁x + ▷eq5◁(x + y)\u0026#34;)\rif y == 5 println() end\rend\r(1 + 8) = 9 (1 + 7) = 8 (1 + 6) = 7 (1 + 5) = 6\r(2 + 8) = 10 (2 + 7) = 9 (2 + 6) = 8 (2 + 5) = 7\r(3 + 8) = 11 (3 + 7) = 10 (3 + 6) = 9 (3 + 5) = 8\r(4 + 8) = 12 (4 + 7) = 11 (4 + 6) = 10 (4 + 5) = 9 まるで擬似コードPseudo Codeを書くかのようにコードが書かれているのがわかる。注意事項としては、以下のように反復子Iteratorとしてタプルを与えた場合だ。\njulia\u0026gt; for (x,y) ∈ (X, Y)\rprint(\u0026#34; (▷eq3◁y) = ▷eq7◁x + ▷eq5◁(x + y)\u0026#34;)\rif y == 5 println() end\rend\r(1 + 8) = 9 (2 + 7) = 9 (3 + 6) = 9 (4 + 5) = 9 ","id":2191,"permalink":"https://freshrimpsushi.github.io/jp/posts/2191/","tags":null,"title":"ジュリアでエレガントなループを使用する方法"},{"categories":"줄리아","contents":"説明 Julia REPLで右の角括弧 ] を入力すると、パッケージ管理モードに切り替えることができる。パッケージ管理モードで利用可能なコマンドは以下の通りだ。\nコマンド 機能 add foo foo パッケージを追加する。 free foo パッケージの固定を解除する。 help, ? これらのコマンドを表示する。 pin foo foo パッケージのバージョンを固定する。 remove foo, rm foo foo パッケージを削除する。 test foo foo パッケージをテスト実行する。 status, st インストールされている全てのパッケージとそのバージョンを表示する。パッケージ名を入力すると、そのパッケージのバージョンだけが表示される。 undo 最近の実行内容を取り消す。 update, up 全てのパッケージを最新バージョンに更新する。パッケージ名を入力することで、特定のパッケージだけを更新される。 環境 OS: Windows10 Version: Julia 1.7.1 ","id":3217,"permalink":"https://freshrimpsushi.github.io/jp/posts/3217/","tags":null,"title":"ジュリアパッケージ管理モードで使用可能なコマンドのリスト"},{"categories":"줄리아","contents":"説明 この写真は、Pythonでファントム$f$のラドン変換$\\mathcal{R}f$を計算し、それを*.npyファイルとして保存する過程を撮影したものです。ジュリアでこのファイルを読み込む場合は、PyCall.jlパッケージを使用すれば良いです。\nusing PyCall\rnp = pyimport(\u0026#34;numpy\u0026#34;) このコードはPythonでimport numpy as npを実行するのと同じです。そうすることで、Pythonのnumpyで使っているコードをそのまま$f$と$\\mathcal{R}f$を読み込むことができます。\nf = np.load(\u0026#34;f.npy\u0026#34;)\rRf = np.load(\u0026#34;Rf.npy\u0026#34;) しっかり読み込めているか、ヒートマップで確認してみましょう。\np1 = heatmap(reverse(f, dims=1), color=:viridis)\rp2 = heatmap(reverse(Rf, dims=1), color=:viridis)\rplot(p1, p2, size=(728,250)) 環境 OS: Windows10 バージョン: Julia 1.6.2, PyCall 1.93.0 ","id":3215,"permalink":"https://freshrimpsushi.github.io/jp/posts/3215/","tags":null,"title":"ジュリアでnpyファイルを読み込む方法"},{"categories":"그래프이론","contents":"定義 1 次数分布がパレート分布に従うランダムネットワークをスケールフリーネットワークScale-free Networkという。\n説明 スケールフリー(SF)ネットワークという名前は、パレート分布が持つスケールフリー性から来ている。次数分布で定義されるネットワークであるため、その分布の性質を強く受け継いでいるのが特徴だ。数式で表すと、スケールフリーネットワーク$G$のノード$v \\in V(G)$の次数はあるパラメータ$\\gamma \u0026gt; 0$に対して次のように表すことができる。 $$ P \\left( \\deg v = k \\right) \\sim k^{-\\gamma} $$ SFネットワークはパレート分布のように科学界全般で頻繁に現れ、社会や自然現象を説明するにはとても良いモデルとして知られている。しかし、最近ではこのSFネットワーク万能論への警鐘を鳴らす論文2も発表されたので注意が必要だ。\nその代表的な応用は特定のダイナミクスの基盤構造を模倣することだ。数多くの実際のデータから導かれるので、人間関係、交通網、生態系など数学的に普遍的な構造が必要な場合に最初に考慮され、実際に広く使われている。SFネットワークを人工的に生成する方法としては、次のモデルが知られている。\nチョン・ル適合度モデル バラバシ・アルバートモデル 次はバラバシ・アルバートモデルで生成されるSFネットワークの視覚化だ。\n式で、次数分布のヒストグラムはログログ図を描いたときに直線を形成すべきだ。つまり、 $$ p(x) \\sim x^{-\\gamma} $$ の両辺にログをとったとき $$ \\log p (x) = - \\gamma \\log x $$ なので、傾きが$- \\gamma$の直線になる。実際に描いてみると、ハブノードが出現する領域を除いて、はっきりした直線を形成しているのが確認できる。\nAlbert. (2002). Statistical mechanics of complex networks. https://doi.org/10.1103/RevModPhys.74.47\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBroido. (20109). Scale-free networks are rare. https://doi.org/10.1038/s41467-019-08746-5\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2183,"permalink":"https://freshrimpsushi.github.io/jp/posts/2183/","tags":null,"title":"スケールフリーネットワーク"},{"categories":"줄리아","contents":"コード 例えば、$(5,5)$の配列のヒートマップの上に、$0$から$2\\pi$までのサイン曲線を描きたいとしよう。こんなコードを書きたくなるだろうが、図に見えるように、望んだ通りに出力されない。\nusing Plots\rA = rand(Bool, 5,5)\rheatmap(A, color=:greens)\rx = range(0, 2pi, length=100)\ry = sin.(x)\rplot!(x, y, color=:red, width=3) これは、配列$A$の横と縦の範囲が$1$から$5$までと認識されるからだ。これを解決するためには、下のようなコードで$A$の横と縦の範囲をそれぞれどこからどこまでか指定してやればいい。ちなみに、配列$A$の範囲を指定せずに、ヒートマップが表示される範囲だけを指定すると、下の(な)みたいに表示される。\nxₐ = range(0,2pi, length=5)\ryₐ = range(-1.5,1.5, length=5)\rp1 = heatmap(xₐ, yₐ, A, color=:greens, xlims=(0,2pi), ylims=(-1.5,1.5)) #그림 (가)\rp2 = heatmap(A, color=:greens, xlims=(0,2pi), ylims=(-1.5,1.5)) #그림 (나) これで、p1の上にサイン曲線を再度描くと、望んだ通りに正しく描かれる。\nplot!(x, y, color=:red, width=3) 環境 OS: Windows10 Version: Julia 1.7.1, Plots 1.25.3 ","id":3213,"permalink":"https://freshrimpsushi.github.io/jp/posts/3213/","tags":null,"title":"ジュリアでヒートマップにプロットを重ねて描く方法"},{"categories":"확률분포론","contents":"定義 1 スケールパラメータ$x_{0} \u0026gt; 0$とシェイプパラメータ$\\alpha \u0026gt; 0$の場合、以下の確率関数をパレート分布、パワー法則、またはスケールフリー分布という。\n連続型：定数$\\displaystyle \\int_{x_{0}}^{\\infty} p(x) dx = 1$を満たすための定数$C$について $$ p(x) = C x^{-\\alpha} \\qquad , x \u0026gt; x_{0} $$ 離散型：リーマンのゼータ関数$\\zeta$について $$ p_{k} = {{ 1 } \\over { \\zeta (\\alpha) }} k^{-\\alpha} \\qquad , k \\in \\mathbb{N} $$ 基本的な性質 [1] モーメント生成関数：パレート分布のモーメント生成関数は存在しない。 [2] 平均と分散：$X \\sim \\text{Pareto} \\left( x_{0}, \\alpha \\right)$の場合 $$ \\begin{align*} E (X) =\u0026amp; {{ \\alpha - 1 } \\over { \\alpha - 2 }} x_{0} \u0026amp; , \\alpha \u0026gt; 2 \\\\ \\text{Var} (X) =\u0026amp; {{ (\\alpha - 1) } \\over { \\left( \\alpha -2 \\right)^{2} (\\alpha - 3) }} x_{0}^{2} \u0026amp; , \\alpha \u0026gt; 3 \\end{align*} $$ 定理 [a] スケールフリー性：パレート分布は唯一のスケールフリー分布だ。つまり、すべての$b$についてある定数$\\alpha$が存在し、次が成立する。 $$ p(bx) = g(b) p(x) \\implies p(x) = p(1) x^{-\\alpha} $$ [b] $k$次のモーメント：$0 \u0026lt; k \u0026lt; \\alpha - 1$の場合、$k$次のモーメントが存在し $$ E X^{k} = {{ \\alpha - 1 } \\over { \\alpha - 1 - k }} x_{0}^{k} $$ 説明 パレート分布はこの実際の世界に蔓延っている不平等を説明する代表的な分布で、次の概念と非常に密接な関係がある。\nヒープスの法則とジップの法則：単語の頻度に関する経験則。 本の販売量 通信量 地震の強さ クレーターの直径 財産 引用数 確率密度関数の形を見ると、シェイプ$\\alpha$が大きいほど不平等が激しくなることが直感的に理解できる。経済的に言えば、富裕層はお金が無限にあり、貧しい人がたくさんいるということだ。\nパレート分布がスケールフリー性を持つという話は、文字通りスケールがないということだ。例えば、ポアソン分布に従う2つの確率変数のパラメータが$\\lambda_{1} = 10$、$\\lambda_{2} = 1000$であれば、見る場所によって大きな違いがあるが、パレート分布はどこを見ても本質的に違いがないということだ。数学的には$b$がどのような値で与えられても結論が同じであることに相当する。\n証明 [1] 確率変数のモーメント生成関数が存在することは、すべての$k \\in \\mathbb{N}$に対して$k$次のモーメントが存在することを意味する。しかし、定理[2]で示されたように、パレート分布の$1$次のモーメントは$\\alpha \u0026gt; 1$の場合にのみ存在するので、モーメント生成関数は存在しえない。\n■\n[2] 戦略：モーメントの公式[b]を利用する。\n$$ \\begin{align*} EX^{1} =\u0026amp; {{ \\alpha - 1 } \\over { \\alpha - 1 - 1 }} x_{0}^{1} \\\\ =\u0026amp; {{ \\alpha - 1 } \\over { \\alpha - 2 }} x_{0}^{1} \\end{align*} $$ であり、$\\displaystyle EX^{2} = {{ \\alpha - 1 } \\over { \\alpha - 3 }} x_{0}^{2}$なので $$ \\begin{align*} \\text{Var} X =\u0026amp; {{ \\alpha - 1 } \\over { \\alpha - 3 }} x_{0}^{2} - \\left[ {{ \\alpha - 1 } \\over { \\alpha - 2 }} x_{0}^{1} \\right]^{2} \\\\ =\u0026amp; \\left[ {{ 1 } \\over { \\alpha - 3 }} - {{ \\alpha - 1 } \\over { \\left( \\alpha - 2 \\right)^{2} }} \\right] (\\alpha - 1) x_{0}^{2} \\\\ =\u0026amp; \\left[ \\alpha^{2} - 4 \\alpha + 4 - \\alpha^{2} + 4 \\alpha - 3 \\right] {{ (\\alpha - 1) } \\over { (\\alpha - 3) \\left( \\alpha -2 \\right)^{2} }} x_{0}^{2} \\\\ =\u0026amp; {{ (\\alpha - 1) } \\over { \\left( \\alpha -2 \\right)^{2} (\\alpha - 3) }} x_{0}^{2} \\end{align*} $$\n■\n[a] すべての$b$に対してある関数$g$が存在し $$ p(bx) = g(b) p(x) $$ が成立すると仮定する。$x = 1$を代入してみると$p(b) = g(b) p(1)$であるため、$g(b) = p(b) / p(1)$であり $$ p(bx) = {{ p(b) p(x) } \\over { p(1) }} $$ $b$に対して微分してみると $$ x p '(bx) = {{ p ' (b) p(x) } \\over { p(1) }} $$ $b=1$を代入してみると対数関数の微分法を用いたトリックにより2 $$ \\begin{align*} \u0026amp; x p '(x) = {{ p ' (1) p(x) } \\over { p(1) }} \\\\ \\implies \u0026amp; {{ p '(x) } \\over { p(x) }} = {{ p '(1) } \\over { p(1) }} \\cdot {{ 1 } \\over { x }} \\\\ \\implies \u0026amp; {{ d \\log p(x) } \\over { dx }} = {{ p '(1) } \\over { p(1) }} \\cdot {{ 1 } \\over { x }} \\\\ \\implies \u0026amp; d \\log p(x) = {{ p '(1) } \\over { p(1) }} {{ 1 } \\over { x }} dx \\end{align*} $$ これは、ある定数$\\text{constant}$に対する単純な分離可能な1階微分方程式で、次を得る。 $$ \\log p(x) = {{ p '(1) } \\over { p(1) }} \\log x + \\text{constant} $$ $x = 1$を代入してみると$\\text{constant} = \\log p(1)$であることがわかる。$\\displaystyle \\alpha := - {{ p '(1) } \\over { p(1) }}$と定義すると、求めていた次の式を得る。 $$ \\begin{align*} \u0026amp; \\log p(x) = - \\alpha \\log x + \\log p(1) \\\\ \\implies \u0026amp; \\log p(x) = \\log x^{-\\alpha} + \\log p(1) \\\\ \\implies \u0026amp; \\log p(x) = \\log x^{-\\alpha} p(1) \\\\ \\implies \u0026amp; p(x) = p(1) x^{-\\alpha} \\end{align*} $$\n■\n[b] $0 \u0026lt; \\alpha -1$であるので、$\\displaystyle \\int_{x_{0}}^{\\infty} C x^{-\\alpha} dx = 1$から$C = \\left( \\alpha - 1 \\right) x_{0}^{\\alpha - 1}$を得る。したがって、 $$ \\begin{align*} E X^{k} =\u0026amp; \\int_{x_{0}}^{\\infty} x^{k} C x^{-\\alpha} dx \\\\ =\u0026amp; C \\int_{x_{0}}^{\\infty} x^{k-\\alpha} dx \\\\ =\u0026amp; \\left( \\alpha - 1 \\right) x_{0}^{\\alpha - 1} \\left[ {{ 1 } \\over { k - \\alpha + 1 }} x^{k - \\alpha + 1} \\right]_{x_{0}}^{\\infty} \\\\ =\u0026amp; \\left( \\alpha - 1 \\right) x_{0}^{\\alpha - 1} \\left( 0 - {{ 1 } \\over { k - \\alpha + 1 }} x_{0}^{k - \\alpha + 1} \\right) \\\\ =\u0026amp; {{ \\alpha - 1 } \\over { \\alpha - 1 - k }} x_{0}^{k} \\end{align*} $$\n■\n可視化 次に、パレート分布の確率密度関数をGIFで表示するJuliaのコードです。\n@time using LaTeXStrings @time using Distributions @time using Plots cd(@__DIR__) x = 1:0.1:10 A = collect(0.5:0.01:3.5); append!(A, reverse(A)) animation = @animate for α ∈ A plot(x, pdf.(Pareto(α), x), color = :black, label = \u0026#34;α = $(round(α, digits = 2))\u0026#34;, size = (400,300)) xlims!(0,5); ylims!(0,4); title!(L\u0026#34;\\mathrm{pdf\\,of\\,Pareto}(\\alpha)\u0026#34;) end gif(animation, \u0026#34;pdf.gif\u0026#34;) Newman. (2005). パワー則、パレート分布とジップの法則. https://doi.org/10.1080/00107510500052444\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://math.stackexchange.com/a/391311\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2181,"permalink":"https://freshrimpsushi.github.io/jp/posts/2181/","tags":null,"title":"パレート分布"},{"categories":"줄리아","contents":"コード 1 LaTeXStrings ライブラリを使うには、文字列の前に L を付けて、L\u0026quot;...\u0026quot; のように書く。\n@time using Plots\r@time using LaTeXStrings\rplot(0:0.1:2π, sin.(0:0.1:2π), xlabel = L\u0026#34;x\u0026#34;, ylabel = L\u0026#34;y\u0026#34;)\rtitle!(L\u0026#34;\\mathrm{TeX\\,representation:\\,} y = \\sin x , x \\in [0, 2 \\pi]\u0026#34;) 注意するべき点は、パッケージ名が正確に LaTeXStrings であり、通常のテキストには \\text{} が効かないため、代わりに \\mathrm{} を使う必要があること、スペースは \\, で行うことである。\n環境 OS: Windows julia: v1.7.0 https://discourse.julialang.org/t/latex-code-for-titles-labels-with-plots-jl/1967/18\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2180,"permalink":"https://freshrimpsushi.github.io/jp/posts/2180/","tags":null,"title":"ジュリアでプロットにTeXを使用する方法"},{"categories":"줄리아","contents":"説明 julia\u0026gt; x = [1 2 3]\r1×3 Matrix{Int64}:\r1 2 3\rjulia\u0026gt; y = [1 2 3 4]\r1×4 Matrix{Int64}:\r1 2 3 4\rjulia\u0026gt; x .+ y\rERROR: DimensionMismatch サイズが異なる二つのベクトルは、基本的に要素ごとの演算を行うことができない。これを直接実装するなら、二重for文を使う必要があるが、幸いにも一つを行ベクトル、もう一つを列ベクトルとして与えることで、次のように要素ごとに演算した二次元配列を返すことができる。これは、MATLABやPython NumPyでも可能だ。\nサイズが異なってもエラーにならない点で、意図しない計算になっているか注意が必要だ。\njulia\u0026gt; x\u0026#39; .+ y\r3×4 Matrix{Int64}:\r2 3 4 5\r3 4 5 6\r4 5 6 7\rjulia\u0026gt; x\u0026#39; .* y\r3×4 Matrix{Int64}:\r1 2 3 4\r2 4 6 8\r3 6 9 12\rjulia\u0026gt; x\u0026#39; ./ y\r3×4 Matrix{Float64}:\r1.0 0.5 0.333333 0.25\r2.0 1.0 0.666667 0.5\r3.0 1.5 1.0 0.75 もちろん、x' の代わりに transpose(x) を使ってもできる。\n環境 OS: Windows10 Version: Julia 1.6.2 ","id":3207,"permalink":"https://freshrimpsushi.github.io/jp/posts/3207/","tags":null,"title":"ジュリアで異なるサイズのベクトル成分ごとに操作する方法"},{"categories":"기하학","contents":"定義1 リーマン曲率テンソルの係数coefficients of Riemannian curvature tensor $R_{ijk}^{l}$は以下のように定義される。\n$$ R_{ijk}^{l} = \\dfrac{\\partial \\Gamma_{ik}^{l}}{\\partial u^{j}} - \\dfrac{\\partial \\Gamma_{ij}^{l}}{\\partial u^{k}} + \\sum_{p} \\left( \\Gamma_{ik}^{p} \\Gamma_{pj}^{l} - \\Gamma_{ij}^{p}\\Gamma_{pk}^{l}\\right) \\text{ for } 1 \\le i,j,k,l \\le 2 $$\nここで$\\Gamma_{ij}^{k}$はクリストッフェル記号である。\n説明 クリストッフェル記号が内在的であるため、リーマン曲率テンソルも内在的である。\n微分幾何学で登場する「係数」という名前が付いたものは、座標系に依存しないという特徴がある。これらを私達はテンソルtensorと呼ぶ。\nガウスの方程式は、第二基本形式とワインガルテン写像の観点から$R_{ijk}^{l}$の外在的な表現を提供する。\n定理 ガウスの方程式Gauss\u0026rsquo;s equations $$ R_{ijk}^{l} = L_{ik}L_{j}^{l} - L_{ij}L_{k}^{l} $$\nコダッチ-マイナルディの方程式Codazzi-Mainardi equations $$ \\dfrac{\\partial L_{ij}}{\\partial u^{k}_{}} - \\dfrac{\\partial L_{ik}}{\\partial u^{j}} = \\sum\\limits_{l} \\left( \\Gamma_{ik}^{l}L_{lj} - \\Gamma_{ij}^{l}L_{lk} \\right) $$\nここで、$L_{j}^{i}$はワインガルテン写像の行列表現の成分である。\n証明 2つの式を同時に証明する。$\\mathbf{x} : U \\to \\R^{3}$を座標片写像とする。$(u^{1}, u^{2})$を$U$の座標とする。\nガウスの公式\n$$ \\mathbf{x}_{ij} = L_{ij} \\mathbf{n} + \\sum \\limits_{l=1}^{2} \\Gamma_{ij}^{l} \\mathbf{x}_{l} $$\nまず、ガウスの公式により、次のものを得る。\n$$ \\begin{align*} \\mathbf{x}_{i j k} \u0026amp;= \\dfrac{\\partial}{\\partial u^{k}}\\left( L_{ij} \\mathbf{n} + \\sum \\limits_{l=1}^{2} \\Gamma_{ij}^{l} \\mathbf{x}_{l} \\right) \\\\ \u0026amp;= \\frac{\\partial L_{ij}}{\\partial u^{k}}\\mathbf{n} + L_{i j} \\mathbf{n}_{k}+\\sum\\limits_{l} \\frac{\\partial \\Gamma_{i j}^{l}}{\\partial u^{k}} \\mathbf{x}_{l}+\\sum\\limits_{l}\\Gamma_{i j}^{l} \\mathbf{x}_{l k} \\end{align*} $$\nここで$\\mathbf{n}_{k} = \\mathbf{x}_{k}\\mathbf{n} = - L(\\mathbf{x}_{k}) = -\\sum\\limits_{l}L_{k}^{l}\\mathbf{x}_{l}$であるため、第二項は$L_{ij}\\mathbf{n}_{k} = -\\sum\\limits_{l} L_{i j} L_{k}^{l} \\mathbf{x}_{l}$である。さらに、第四項にガウスの公式を再適用すると、\n$$ \\sum\\limits_{l} \\Gamma_{ij}^{l} \\mathbf{x}_{l k} = \\sum\\limits_{l} \\Gamma_{i j}^{l}L_{lk} \\mathbf{n} + \\sum\\limits_{l,m}\\Gamma_{i j}^{l}\\Gamma_{lk}^{m} \\mathbf{x}_{m} $$\nこれを代入すると、次を得る。\n$$ \\begin{align*} \\mathbf{x}_{i j k} \u0026amp;= \\frac{\\partial L_{ij}}{\\partial u^{k}}\\mathbf{n} -\\sum\\limits_{l} L_{i j} L_{k}^{l} \\mathbf{x}_{l} + \\sum\\limits_{l} \\frac{\\partial \\Gamma_{i j}^{l}}{\\partial u^{k}} \\mathbf{x}_{l} + \\sum\\limits_{l} \\Gamma_{i j}^{l}L_{lk} \\mathbf{n} + \\sum\\limits_{l,m}\\Gamma_{i j}^{l}\\Gamma_{lk}^{m} \\mathbf{x}_{m} \\\\ \u0026amp;= \\frac{\\partial L_{ij}}{\\partial u^{k}}\\mathbf{n} -\\sum\\limits_{l} L_{i j} L_{k}^{l} \\mathbf{x}_{l} + \\sum\\limits_{l} \\frac{\\partial \\Gamma_{i j}^{l}}{\\partial u^{k}} \\mathbf{x}_{l} + \\sum\\limits_{l} \\Gamma_{i j}^{l}L_{lk} \\mathbf{n} + \\sum\\limits_{p,l}\\Gamma_{i j}^{p}\\Gamma_{pk}^{l} \\mathbf{x}_{l} \\\\ \u0026amp;= \\left( \\frac{\\partial L_{ij}}{\\partial u^{k}} + \\sum\\limits_{l} \\Gamma_{i j}^{l}L_{lk} \\right)\\mathbf{n} + \\sum\\limits_{l} \\left(\\frac{\\partial \\Gamma_{i j}^{l}}{\\partial u^{k}} - L_{i j} L_{k}^{l} + \\sum\\limits_{p}\\Gamma_{i j}^{p}\\Gamma_{pk}^{l} \\right)\\mathbf{x}_{l} \\end{align*} $$\n$l,m$はダミーインデックスなので、最後の項のインデックスを$(l,m) \\to (p,l)$に変え、項をまとめた。同様に、次を得る。\n$$ \\mathbf{x}_{ikj} = \\left( \\frac{\\partial L_{ik}}{\\partial u^{j}} + \\sum\\limits_{l} \\Gamma_{i k}^{l}L_{lj} \\right)\\mathbf{n} + \\sum\\limits_{l} \\left(\\frac{\\partial \\Gamma_{i k}^{l}}{\\partial u^{j}} - L_{i k} L_{j}^{l} + \\sum\\limits_{p}\\Gamma_{i k}^{p}\\Gamma_{pj}^{l} \\right)\\mathbf{x}_{l} $$\nこの際、座標片写像$\\mathbf{x}$は十分に微分可能であると仮定すると、\n$$ \\mathbf{x}_{i j k}=\\frac{\\partial^{3} \\mathbf{x}}{\\partial u^{k} \\partial u^{j} \\partial u^{i}}=\\frac{\\partial^{3} \\mathbf{x}}{\\partial u^{j} \\partial u^{k} \\partial u^{i}}=\\mathbf{x}_{i k j} $$\n$\\left\\{ \\mathbf{x}_{1}, \\mathbf{x}_{2}, \\mathbf{n} \\right\\}$は$\\mathbb{R}^{3}$の基底であるため、$\\mathbf{x}_{ijk}$と$\\mathbf{x}_{ikj}$の各成分は同じでなければならない。したがって、次を得る。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\frac{\\partial L_{ij}}{\\partial u^{k}} + \\sum\\limits_{l} \\Gamma_{i j}^{l}L_{lk} \u0026amp;= \\frac{\\partial L_{ik}}{\\partial u^{j}} + \\sum\\limits_{l} \\Gamma_{i k}^{l}L_{lj} \\\\ \\implies \u0026amp;\u0026amp; \\frac{\\partial L_{ij}}{\\partial u^{k}} - \\frac{\\partial L_{ik}}{\\partial u^{j}} \u0026amp;= \\sum\\limits_{l} \\left( \\Gamma_{i k}^{l}L_{lj} - \\Gamma_{i j}^{l}L_{lk} \\right) \\end{align*} $$\nコダッチ-マイナルディの方程式が証明された。同じ論理で次の等式が成り立つ。\n$$ \\frac{\\partial \\Gamma_{i j}^{l}}{\\partial u^{k}} - L_{i j} L_{k}^{l} + \\sum\\limits_{p}\\Gamma_{i j}^{p}\\Gamma_{pk}^{l} = \\frac{\\partial \\Gamma_{i k}^{l}}{\\partial u^{j}} - L_{i k} L_{j}^{l} + \\sum\\limits_{p}\\Gamma_{i k}^{p}\\Gamma_{pj}^{l} $$\n整理すると、次を得る。\n$$ L_{i k} L_{j}^{l} - L_{i j} L_{k}^{l} = \\frac{\\partial \\Gamma_{i k}^{l}}{\\partial u^{j}} - \\frac{\\partial \\Gamma_{i j}^{l}}{\\partial u^{k}} + \\sum\\limits_{p} \\left( \\Gamma_{i k}^{p}\\Gamma_{pj}^{l} - \\Gamma_{i j}^{p}\\Gamma_{pk}^{l} \\right) = R_{ijk}^{l} $$\nガウスの方程式が証明された。\n■\n参照 微分多様体上のリーマン曲率テンソル Richard S. Millman and George D. Parker, Elements of Differential Geometry (1977), p141-142 $\\mathbf{x} : U \\to \\mathbb{R}^{3}$を曲面$M$上の座標片写像とする。$(u^{1}, u^{2})$を$U$の座標とする。$\\mathbf{x}$でクリストッフェル記号$\\Gamma_{ij}^{k}$と第二基本形式の係数$L_{ij}$が与えられたとする。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3206,"permalink":"https://freshrimpsushi.github.io/jp/posts/3206/","tags":null,"title":"微分幾何学におけるリーマン曲率テンソル、ガウスの方程式、コダッチ-マイナルディの方程式"},{"categories":"줄리아","contents":"コード 1 ブラウザがダークモードになっていれば、背景が透明になっているのをはっきりと確認できる。\nbackground_color オプションに :transparent シンボルを入れればいいんだ。*.pngとしてはちゃんと保存されるけど、*.pdfとしてはうまく保存されないそうだ。\nusing Plots\rplot(rand(10), background_color = :transparent)\rpng(\u0026#34;example\u0026#34;) オプション名から推測できるように、カラーシンボルを入れれば、その色で出力される。たとえば、黄色の :yellow で描いた絵はこんな感じだ。\n環境 OS: Windows julia: v1.6.2 https://discourse.julialang.org/t/save-figure-with-transparent-background-color-in-plots-jl/18808/3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2173,"permalink":"https://freshrimpsushi.github.io/jp/posts/2173/","tags":null,"title":"ジュリアでグラフィックスの背景を透明にする方法"},{"categories":"줄리아","contents":"特定の値まで塗る1 plot()の属性でfillrange=a、fillalpha=b、fillcolor=:colorを使うと、プロットされた曲線から値aまで:colorの色をbの透明度で塗る。fill=(a,b,:color)と書いても同じ機能をする。つまり、以下の2つのコードは同じだ。\nplot(x,y, fillrange=a, fillalpha=b, fillcolor=:color)\rplot(x,y, fill=(a,b,:color)) バグみたいだけど、fillrangeの値を$(0,1)$から選ぶと、塗りつぶしがされない。\nusing Plots\rrandom_walk = cumsum(rand(20).-.5)\rp1 = plot(random_walk,fill=(1,0.2,:lime), lw=3, legend=:bottomright)\rp2 = plot(random_walk,fill=(2,0.2,:tomato), lw=3, legend=:bottomright)\rplot(p1, p2) ２つの曲線の間を塗る fillalphaの値に片方の曲線の関数値を入れると、2つの曲線の間が色付けされる。\nrw = random_walk\rplot([rw rw.+1],fill=(rw.+1,0.2,:lime), lw=3, legend=:bottomright) 閉曲線の内部を塗る fillalphaの値を$(0,1)$だけではなく選ぶと、内部が塗りつぶされる。\ntheta = range(0,2pi, length=40)\rx = cos.(theta)\ry = sin.(theta)\rplot(x, y, fill=(1,0.2,:lime), xlim=(-3,3), ylim=(-1.5,1.5), size=(800,400), lw=3) 環境 OS: Windows10 Version: Julia 1.6.2, Plots 1.23.6 http://docs.juliaplots.org/latest/attributes/#fill\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3203,"permalink":"https://freshrimpsushi.github.io/jp/posts/3203/","tags":null,"title":"ジュリアでの曲線から特定の値まで/二つの曲線の間/閉曲線の内部の塗り方"},{"categories":"기하학","contents":"定義1 曲面 $M$ 上の点 $p$ における主曲率を $\\kappa_{1}, \\kappa_{2}$ としよう。$L$ を ワインガルテンマップ と称する。ガウス曲率Gaussian curvature $K$ を次のように定義する。\n$$ K := \\kappa_{1} \\kappa_{2} = \\det L = \\det ([{L^{i}}_{j}]) $$\nこの時、${L^{i}}_{j} = \\sum \\limits_{k} L_{kj}g^{ki}$ が成り立つ。\n公式 主曲率の積 $$ K = \\kappa_{1} \\kappa_{2} $$\nガウス曲率はリーマン曲率テンソルで表せる。 $$\n$$\nガウス曲率はクリストッフェル記号で表せる。 $$ K = $$\n点 $p$ におけるガウス曲率はガウスマップと領域の面積で示される。 $$ K = \\lim\\limits_{\\mathscr{R} \\to p} \\dfrac{A(\\nu (\\mathscr{R}))}{A(\\mathscr{R})} $$\n定理 $H^{2} \\ge K$ が成立する。\n$\\mathbf{X}, \\mathbf{Y}$ を点 $p$ における正規直交ベクトルとする。すると、次が成り立つ。\n$$ H = \\dfrac{1}{2}\\left( II(\\mathbf{X}, \\mathbf{X}) + II(\\mathbf{Y}, \\mathbf{Y}) \\right) $$\n$\\mathbf{Y} \\in T_{p}M$ を単位接ベクトル、$\\kappa_{n}$ を法曲率とし、$\\theta$ を主方向 $\\mathbf{X}_{1}$ と $\\mathbf{Y}$ の間の角度とする。すると、次が成り立つ。 $$ H = \\dfrac{1}{2\\pi}\\int_{0}^{2\\pi}\\kappa_{n}d\\theta $$\n証明 3. $\\kappa_{n} = II(\\mathbf{Y}, \\mathbf{Y})$ かつ、$II(\\mathbf{Y}, \\mathbf{Y}) = \\kappa_{1}\\cos^{2}\\theta + \\kappa_{2}\\sin^{2}\\theta$ ので、\n$$ \\begin{align*} \\dfrac{1}{2\\pi}\\int_{0}^{2\\pi}\\kappa_{n}d\\theta \u0026amp;= \\dfrac{1}{2\\pi}\\int_{0}^{2\\pi} \\kappa_{1}\\cos^{2}\\theta + \\kappa_{2}\\sin^{2}\\theta d\\theta \\\\ \u0026amp;= \\dfrac{1}{2\\pi} \\left( \\kappa_{1} \\int_{0}^{2\\pi} \\cos^{2} d\\theta + \\kappa_{2} \\int_{0}^{2\\pi}\\sin^{2}\\theta d\\theta \\right) \\\\ \u0026amp;= \\dfrac{1}{2\\pi} \\left( \\kappa_{1} \\pi + \\kappa_{2} \\pi \\right) \\\\ \u0026amp;= \\dfrac{\\kappa_{1} + \\kappa_{2}}{2} \\\\ \u0026amp;= H \\end{align*} $$\n(三角関数の積分表 $(2), (3)$ 参照)\n■\nRichard S. Millman and George D. Parker, Elements of Differential Geometry (1977), p130\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3200,"permalink":"https://freshrimpsushi.github.io/jp/posts/3200/","tags":null,"title":"ガウス曲率と平均曲率"},{"categories":"정수론","contents":"定義 1 $p \\ge 2$の自然数が$1$と$p$だけを約数に持つ場合、素数Prime Numberと言う。 $m \\ge 2$の自然数が素数ではない場合、合成数Composite Numberと言う。 説明 定義によると、$2$は明らかに素数だ。\n数論で扱う数は非常に広く有理数にまで及ぶが、実際その研究対象は「素数論」とも呼べるほどの関心が集まっている。合成数は他の素数の積で表すことができ、素数について何らかの性質が明らかになれば、その一般化は比較的簡単なので、整数論の多くの定理では条件として素数を求めている。\nSilverman. (2012). 数論へのやさしい入門 (第4版): p46.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2163,"permalink":"https://freshrimpsushi.github.io/jp/posts/2163/","tags":null,"title":"素数と合成数"},{"categories":"기하학","contents":"ビルドアップ1 曲面 $M$がどの方向に、どれだけ曲がっているかを知るためには、各方向の法曲率 $\\kappa_{n}$を知ればいい。つまり、点$p$での全ての$\\kappa_{n}$を知れば、$M$がどのように曲がっているかを知ることができる。これを知るための最初のステップとして、$\\kappa_{n}$の最大値と最小値について考えよう。単位接線曲線$\\boldsymbol{\\gamma}$に対して、次の定理が成立する。\n補助定理\n$\\mathbf{T}$が単位速度曲線$\\boldsymbol{\\gamma}$の接線場とすると、$\\kappa_{n} = II (\\mathbf{T}, \\mathbf{T})$が成立する。\nよって、私たちの目的は、接線ベクトル$\\mathbf{X} \\in T_{p}M$に対して$II(\\mathbf{X}, \\mathbf{X}) = \\kappa_{n}$の最大値と最小値を求めることである。ここで、$II$は第二基本形式である。\nこの問題は言い換えれば、制約条件が$\\left\\langle \\mathbf{X}, \\mathbf{X} \\right\\rangle = 1$の、$II(\\mathbf{X}, \\mathbf{X})$の最大化（最小化）問題である。このような問題はラグランジュの未定乗数法で解くことができる。それでは、私たちが解くべき問題は$II(\\mathbf{X}, \\mathbf{X})$の最大値（最小値）を求めることから、次のような$f$の最大値（最小値）を求めることに変わる。ワインガルテンマップ$L$に対して$II(\\mathbf{X}, \\mathbf{X}) = \\left\\langle L(\\mathbf{X}), \\mathbf{X} \\right\\rangle$なので、\n$$ \\begin{align*} f(\\mathbf{X}, \\lambda) \u0026amp;= II(\\mathbf{X}, \\mathbf{X}) - \\lambda (\\left\\langle \\mathbf{X}, \\mathbf{X} \\right\\rangle - 1) \\\\ \u0026amp;= \\left\\langle L(\\mathbf{X}), \\mathbf{X} \\right\\rangle - \\lambda\\left\\langle \\mathbf{X}, \\mathbf{X} \\right\\rangle + \\lambda\\\\ \u0026amp;= \\left\\langle L(\\mathbf{X}) - \\lambda \\mathbf{X}, \\mathbf{X} \\right\\rangle + \\lambda\\\\ \\end{align*} $$\nこれを座標極座標写像$\\mathbf{x}$に対して表現すると、$\\mathbf{X} = X^{1}\\mathbf{x}_{1} + \\mathbf{X}^{2}\\mathbf{x}_{2}$、$L(\\mathbf{x}_{k}) = \\sum\\limits_{l}{L^{l}}_{k}\\mathbf{x}_{l}$なので、\n$$ \\begin{align*} f(\\mathbf{X}, \\lambda) \u0026amp;= f(X^{1}, X^{2}, \\lambda) \\\\ \u0026amp;= \\lambda + \\left\\langle \\sum\\limits_{i,j} {L^{i}}_{j}X^{j}\\mathbf{x}_{i} - \\sum\\limits_{j}\\lambda X^{j}\\mathbf{x}_{j}, \\sum\\limits_{k}X^{k}\\mathbf{x}_{k} \\right\\rangle \\\\ \u0026amp;= \\lambda + \\left\\langle {L^{i}}_{j}X^{j}\\mathbf{x}_{i} - \\lambda X^{j}\\mathbf{x}_{j}, X^{k}\\mathbf{x}_{k} \\right\\rangle \u0026amp; \\text{by } \\href{https://freshrimpsushi.github.io/posts/einstein-notation}{\\text{Einstein notation}} \\\\ \u0026amp;= \\lambda + {L^{i}}_{j}X^{j}X^{k}\\left\\langle \\mathbf{x}_{i}, \\mathbf{x}_{k} \\right\\rangle - \\lambda X^{j}X^{k} \\left\\langle \\mathbf{x}_{j}, \\mathbf{x}_{k} \\right\\rangle \\\\ \u0026amp;= \\lambda + {L^{i}}_{j}X^{j}X^{k}g_{ik} - \\lambda X^{j}X^{k} g_{jk} \\\\ \u0026amp;= \\lambda + {L^{i}}_{j}X^{j}X^{k}g_{ik} - \\lambda X^{j}X^{k} \\delta_{ij}g_{ik} \\\\ \u0026amp;= \\lambda + ({L^{i}}_{j} - \\lambda \\delta_{ij}) X^{j}X^{k}g_{ik} \\end{align*} $$\n$\\delta$はクロネッカーのデルタである。ラグランジュの未定乗数法により$\\dfrac{\\partial f}{\\partial X^{l}} = 0$を得る。$L_{jk} = \\sum\\limits_{l}{L^{l}}_{k}g_{lj}$なので、\n$$ \\begin{align*} 0 = \\dfrac{\\partial f}{\\partial X^{l}} \u0026amp;= \\sum\\limits_{ijk} ({L^{i}}_{j} - \\lambda \\delta_{ij})\\delta_{jl}X^{k}g_{ik} + \\sum\\limits_{ijk} ({L^{i}}_{j} - \\lambda \\delta_{ij})\\delta_{kl}X^{j}g_{ik} \\\\ \u0026amp;= \\sum\\limits_{ik} ({L^{i}}_{l} - \\lambda \\delta_{il})X^{k}g_{ik} + \\sum\\limits_{ij} ({L^{i}}_{j} - \\lambda \\delta_{ij})X^{j}g_{il} \\\\ \u0026amp;= \\sum\\limits_{ik} {L^{i}}_{l}X^{k}g_{ik} - \\sum\\limits_{ik}\\lambda \\delta_{il}X^{k}g_{ik} + \\sum\\limits_{ij} {L^{i}}_{j}X^{j}g_{il} - \\sum\\limits_{ij}\\lambda \\delta_{ij}X^{j}g_{il} \\\\ \u0026amp;= \\sum\\limits_{k} L_{kl}X^{k} - \\sum\\limits_{k}\\lambda X^{k}g_{lk} + \\sum\\limits_{j} L_{lj}X^{j} - \\sum\\limits_{j}\\lambda X^{j}g_{jl} \\\\ \u0026amp;= \\sum\\limits_{j} \\left( L_{jl}X^{j} - \\lambda X^{j}g_{lj} + L_{lj}X^{j} - \\lambda X^{j}g_{jl} \\right) \\\\ \u0026amp;= 2\\sum\\limits_{j} \\left( L_{jl}X^{j} - \\lambda X^{j}g_{lj} \\right) = 2\\sum\\limits_{j}L_{jl}X^{j} - 2\\sum\\limits_{j}\\lambda X^{j}g_{lj} \\\\ \u0026amp;= 2\\sum\\limits_{j}L_{jl}X^{j} - 2\\sum\\limits_{j}\\lambda X^{j}g_{lj} \\\\ \u0026amp;= 2\\sum\\limits_{ij}{L^{i}}_{j}X^{j}g_{il} - 2\\sum\\limits_{ij}\\lambda X^{j}\\delta_{ij}g_{li} \\\\ \u0026amp;= 2\\sum\\limits_{ij}\\left( {L^{i}}_{j} - \\lambda\\delta_{ij} \\right)X^{j}g_{li} \\\\ \\end{align*} $$\n$$ \\implies \\sum\\limits_{ij}\\left( {L^{i}}_{j} - \\lambda\\delta_{ij} \\right)X^{j}g_{li} = 0 $$\nしたがって、全ての$Y^{l}$に対して、次を得る。\n$$ \\sum\\limits_{ijl}\\left( {L^{i}}_{j} - \\lambda\\delta_{ij} \\right)X^{j}Y^{l}g_{li} = 0 $$\nこれは次を意味する。$\\forall \\mathbf{Y}=\\sum\\limits_{l}Y^{l}\\mathbf{x}_{l}$、\n$$ \\begin{align*} \\left\\langle L(\\mathbf{X}) - \\lambda \\mathbf{X}, \\mathbf{Y} \\right\\rangle \u0026amp;= \\left\\langle L\\left( \\sum\\limits_{j}X^{j}\\mathbf{x}_{j} \\right) - \\sum\\limits_{i}\\lambda X^{i} \\mathbf{x}_{i}, \\sum\\limits_{l}Y^{l}\\mathbf{x}_{l} \\right\\rangle \\\\ \u0026amp;= \\left\\langle \\sum\\limits_{ij}{L^{i}}_{j}X^{j}\\mathbf{x}_{i} - \\sum\\limits_{ij}\\lambda \\delta_{ij} X^{j} \\mathbf{x}_{i}, \\sum\\limits_{l}Y^{l}\\mathbf{x}_{l} \\right\\rangle \\\\ \u0026amp;= \\sum\\limits_{ijl}{L^{i}}_{j}X^{j}Y^{l}\\left\\langle \\mathbf{x}_{i}, \\mathbf{x}_{l} \\right\\rangle - \\sum\\limits_{ijl}\\lambda \\delta_{ij}X^{j}Y^{l}\\left\\langle \\mathbf{x}_{i}, \\mathbf{x}_{l} \\right\\rangle \\\\ \u0026amp;= \\sum\\limits_{ijl}({L^{i}}_{j} - \\lambda \\delta_{ij} )X^{j}Y^{l}g_{il} \\\\ \u0026amp;= 0 \\end{align*} $$\nしたがって、次を得る。\n$$ \\dfrac{\\partial f}{\\partial X^{l}} = 0 \\implies \\left\\langle L(\\mathbf{X}) - \\lambda \\mathbf{X}, \\mathbf{Y} \\right\\rangle = 0\\quad \\forall \\mathbf{Y} \\implies L(\\mathbf{X}) = \\lambda \\mathbf{X} $$\nよって、$\\lambda$は$L$の固有値であり、$\\mathbf{X}$はそれに対応する固有ベクトルである。特に、$\\mathbf{X}$は制約条件$\\left\\langle \\mathbf{X}, \\mathbf{X} \\right\\rangle = 1$を満たさなければならないので単位固有ベクトルである。なので、2つの単位固有ベクトルに対して$II(\\mathbf{X}, \\mathbf{X})$は最大値（最小値）を取るという結論を得る。\nさらに、$B = \\left\\{ \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\}$とし、便宜上$L$の行列表示を$L$と重複して表記し、$L \\equiv \\left[ L \\right]_{B}$とすると、$\\lambda$は次の式の解である。\n$$ \\begin{equation} \\begin{aligned} \\det(L - \\lambda I) \u0026amp;= (\\lambda - {L^{1}}_{1})(\\lambda - {L^{2}}_{2}) - {L^{1}}_{2}{L^{2}}_{1} \\\\ \u0026amp;= \\lambda^{2} - ({L^{1}}_{1}{L^{2}}_{2})\\lambda + ({L^{1}}_{1}{L^{2}}_{2} - {L^{1}}_{2}{L^{2}}_{1}) \\\\ \u0026amp;= \\lambda^{2} - \\tr(L) \\lambda + \\det(L) \\\\ \u0026amp;= 0 \\end{aligned} \\label{1} \\end{equation} $$\n2つの解（固有値）を$\\kappa_{1}, \\kappa_{2}$（$\\kappa_{1} \\ge \\kappa_{2}$）と表記しよう。以下の定理は、これら2つの値が実際には$\\kappa_{n}$の最小値と最大値であることを述べている。\n定理 曲面$M$上の各点には、1.法曲率がそれぞれ最大、最小であり、2.互いに垂直な2つの方向が存在する。\n証明 $L$の2つの固有値はそれぞれ法曲率の最大値と最小値である。 上述の考察に従って、$L$の固有ベクトルの方向において、$M$上の点$p$での法曲率は最大値と最小値をとる。点$p$での$L$の2つの固有値を$\\kappa_{1}, \\kappa_{2}(\\kappa_{1} \\ge \\kappa_{2})$、それに対応する固有ベクトルを$\\mathbf{X}_{1}, \\mathbf{X}_{2}$としよう。すると、法曲率の最大値と最小値は以下の通りである。\n$$ \\kappa_{n} = II(\\mathbf{X}_{i}, \\mathbf{X}_{i}) = \\left\\langle L(\\mathbf{X}_{i}), \\mathbf{X}_{i} \\right\\rangle = \\left\\langle \\kappa_{i}\\mathbf{X}_{i}, \\mathbf{X}_{i} \\right\\rangle = \\kappa_{i}\\left\\langle \\mathbf{X}_{i}, \\mathbf{X}_{i} \\right\\rangle = \\kappa_{i} $$\nしたがって、大きい固有値$\\kappa_{1}$が最大法曲率、小さい値$\\kappa_{2}$が最小法曲率である。\n2つの固有ベクトルは互いに垂直である。 $\\kappa_{1} \\ne \\kappa_{2}$ この場合、$L$が自己随伴であるため、\n$$ \\kappa_{1} \\left\\langle \\mathbf{X}_{1}, \\mathbf{X}_{2} \\right\\rangle = \\left\\langle L(\\mathbf{X}_{1}), \\mathbf{X}_{2} \\right\\rangle = \\left\\langle \\mathbf{X}_{1}, L(\\mathbf{X}_{2}) \\right\\rangle = \\left\\langle \\mathbf{X}_{1}, \\kappa_{2} \\mathbf{X}_{2} \\right\\rangle = \\kappa_{2} \\left\\langle \\mathbf{X}_{1}, \\mathbf{X}_{2} \\right\\rangle \\\\ \\implies (\\kappa_{1} - \\kappa_{2}) \\left\\langle \\mathbf{X}_{1}, \\mathbf{X}_{2} \\right\\rangle = 0 $$\n仮定により、$\\left\\langle \\mathbf{X}_{1}, \\mathbf{X}_{2} \\right\\rangle = 0$\n$\\kappa_{1} = \\kappa_{2}$ 補助定理\n$\\lambda$、$\\mathbf{X}$が曲面$M$上の点$p$での$L$の固有値、固有ベクトルとする。単位接線ベクトル$\\mathbf{Y} \\in T_{p}M$が$\\left\\langle \\mathbf{X}, \\mathbf{Y} \\right\\rangle = 0$を満たすとする。すると$\\mathbf{Y}$も固有ベクトルである。\n証明\n仮定により$\\left\\{ \\mathbf{X}, \\mathbf{Y} \\right\\}$は$T_{p}M$の基底である。$L$が自己随伴であるため、\n$$ 0 = \\left\\langle \\lambda \\mathbf{X}, \\mathbf{Y} \\right\\rangle = \\left\\langle L(\\mathbf{X}), \\mathbf{Y} \\right\\rangle = \\left\\langle \\mathbf{X}, L(\\mathbf{Y}) \\right\\rangle = \\left\\langle \\mathbf{X}, a_{1} \\mathbf{X} + a_{2} \\mathbf{Y} \\right\\rangle $$\nよって、$a_{1}=0$が成立し、$L(\\mathbf{Y}) = a_{2}\\mathbf{Y}$であるため、$\\mathbf{Y}$も固有ベクトルである。\n■\n補助定理によって、$\\mathbf{X}_{1}$に垂直な単位ベクトルも固有ベクトルである。したがって、これを$\\mathbf{X}_{2}$として選ぶことができる。\n■\n定義 点$p\\in M$で定義されたワインガルテンマップ$L$の固有値$\\kappa_{1}, \\kappa_{2}$を、点$p$での曲面$M$の主曲率と呼ぶ。$L$の固有ベクトルを、点$p$での主方向と呼ぶ。\n2つの主曲率$\\kappa_{1}, \\kappa_{2}$が等しい点をアンビリックと呼ぶ。\n曲線の全ての点において、接線ベクトルがその点での曲面$M$上の主方向である場合、その曲線を曲面$M$上の曲率線と呼ぶ。\n説明 上述の考察により、主曲率の大きい値（小さい値）は、点$p$での法曲率の最大値（最小値）である。\n$S^{2}$と$\\mathbb{R}^{2}$の全ての点はアンビリックである。[逆も成立する。]\n$\\eqref{1}$では、根と係数の関係により、$\\kappa_{1} \\kappa_{2} = \\det L$が成立し、これをガウス曲率と呼ぶ。また、$\\dfrac{\\kappa_{1} + \\kappa_{2}}{2} = \\dfrac{\\tr{L}}{2}$を平均曲率と呼ぶ。\nRichard S. Millman and George D. Parker, Elements of Differential Geometry (1977), p127-129\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3194,"permalink":"https://freshrimpsushi.github.io/jp/posts/3194/","tags":null,"title":"主曲線の曲率"},{"categories":"프로그래밍","contents":"コード import pandas as pd\rdata = { \u0026#39;나이\u0026#39; : [26,23,22,22,21,21,20,20,20,20,18,17], \u0026#39;키\u0026#39; : [160, 163, 163, 162, 164, 163, 164, 150, 158, 162, 172, 173], \u0026#39;별명\u0026#39; : [\u0026#39;땡모\u0026#39;, \u0026#39;김쿠라\u0026#39;, \u0026#39;광배\u0026#39;, \u0026#39;오리\u0026#39;, \u0026#39;깃털\u0026#39;, \u0026#39;쌈무\u0026#39;, \u0026#39;밍구리\u0026#39;, \u0026#39;나부키 야코\u0026#39;, \u0026#39;월클토미\u0026#39;, \u0026#39;쪼율\u0026#39;, \u0026#39;안댕댕\u0026#39;, \u0026#39;워뇨\u0026#39;], \u0026#39;국적\u0026#39; : [\u0026#39;한국\u0026#39;, \u0026#39;일본\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;일본\u0026#39;, \u0026#39;일본\u0026#39;, \u0026#39;한국\u0026#39;, \u0026#39;한국\u0026#39;] }\rindexName = [\u0026#39;권은비\u0026#39;,\u0026#39;미야와키 사쿠라\u0026#39;,\u0026#39;강혜원\u0026#39;, \u0026#39;최예나\u0026#39;, \u0026#39;이채연\u0026#39;, \u0026#39;김채원\u0026#39;, \u0026#39;김민주\u0026#39;, \u0026#39;야부키 나코\u0026#39;, \u0026#39;혼다 히토미\u0026#39;, \u0026#39;조유리\u0026#39;, \u0026#39;안유진\u0026#39;, \u0026#39;장원영\u0026#39;]\rdf = pd.DataFrame(data, index = indexName) 列のラベルは .columns で取得する。\n\u0026gt;\u0026gt;\u0026gt; df.columns Index([\u0026#39;나이\u0026#39;, \u0026#39;키\u0026#39;, \u0026#39;별명\u0026#39;, \u0026#39;국적\u0026#39;], dtype=\u0026#39;object\u0026#39;) \u0026gt;\u0026gt;\u0026gt; df.columns[2] \u0026#39;별명\u0026#39; 行のラベルは .index で取得する。.rowsではないことに注意しよう。\n\u0026gt;\u0026gt;\u0026gt; df.index Index([\u0026#39;권은비\u0026#39;, \u0026#39;미야와키 사쿠라\u0026#39;, \u0026#39;강혜원\u0026#39;, \u0026#39;최예나\u0026#39;, \u0026#39;이채연\u0026#39;, \u0026#39;김채원\u0026#39;, \u0026#39;김민주\u0026#39;, \u0026#39;야부키 나코\u0026#39;, \u0026#39;혼다 히토미\u0026#39;, \u0026#39;조유리\u0026#39;, \u0026#39;안유진\u0026#39;, \u0026#39;장원영\u0026#39;], dtype=\u0026#39;object\u0026#39;) \u0026gt;\u0026gt;\u0026gt; df.index[10] \u0026#39;안유진\u0026#39; ","id":3189,"permalink":"https://freshrimpsushi.github.io/jp/posts/3189/","tags":null,"title":"Python Pandasデータフレームの列と行の名前を取得する方法"},{"categories":"줄리아","contents":"コード 1 ==は値が同じかどうかを比較し、===は比較する値が可変Mutableかどうかによって異なる動作をする。\nMutable: 二項が同じオブジェクトを参照しているか確認する。つまり、プログラム上で二つの変数が区別できるかどうかを返す。 Immutable: 二項のタイプが同じかどうかをチェックし、 二項のストラクチャーが同じかどうかをチェックし、 各要素が==で同じかどうかを再帰的にチェックする。 julia\u0026gt; X = 1; Y = 1;\rjulia\u0026gt; X == Y\rtrue\rjulia\u0026gt; X === Y\rtrue\rjulia\u0026gt; X = [1]; Y = [1];\rjulia\u0026gt; X == Y\rtrue\rjulia\u0026gt; X === Y\rfalse 例えば、Pythonでよく見られる上記の実行結果を見てみよう。整数1はImmutableでプログラム上で区別が付かないため、==と===は同じように真を返したが、1だけを含む配列と見た場合にはXに新しい要素が追加されたとしたらXとYが異なる可能性があるMutableため、単に値を比較した==は真を返し、オブジェクト自体を比較した===は偽を返した。\nオブジェクトが何かわからなくても、この用法だけ理解すれば十分だ。\n最適化 オブジェクト指向性が弱いJuliaでは、このような差はそれほど大きく感じられない。コード最適化の観点から見ると、==と===の比較では、シングルトンSingtoneの比較で以下のような性能差が出る。\nN = 10^7\rx = rand(0:9, N)\rjulia\u0026gt; @time for t ∈ 1:N\rx[t] == 0\rend\r1.292501 seconds (30.00 M allocations: 610.336 MiB, 2.63% gc time)\rjulia\u0026gt; @time for t ∈ 1:N\rx[t] === 0\rend\r1.016211 seconds (30.00 M allocations: 610.336 MiB, 2.77% gc time) 値は通常Immutableなので、==よりも===の方が早いと理解すればいい。\nこのような差はほとんどの場合、それほど大きくないかもしれない。当然ながら、イテラティブIterativeではない作業は、次のようなベクター演算の方がずっと速く、この際の速度差はほぼないか無意味である。\njulia\u0026gt; @time x .== 0;\r0.009509 seconds (6 allocations: 1.196 MiB)\rjulia\u0026gt; @time x .=== 0;\r0.009478 seconds (6 allocations: 1.196 MiB) 環境 OS: Windows julia: v1.6.1 https://stackoverflow.com/a/38638838/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2157,"permalink":"https://freshrimpsushi.github.io/jp/posts/2157/","tags":null,"title":"ジュリアにおける==と===の違い"},{"categories":"기하학","contents":"定義1 $M$を曲面、$p \\in M$を曲面上の点とする。次のように定義される写像$L : T_{p}M \\to \\mathbb{R}^{3}$をバインガルテン・マップと呼ぶ。\n$$ L (\\mathbf{X}) = - \\mathbf{X}\\mathbf{n} $$\nこの時、$\\mathbf{X} \\in T_{p}M$は接ベクトルで、$\\mathbf{n}$は単位法線、$\\mathbf{X}\\mathbf{n}$は$\\mathbf{n}$の方向微分である。\n性質 $L$は$L : T_{p}M \\to T_{p}M$の線形変換である。\n$\\left\\{ \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\}$が$T_{p}M$の基底であるから、$L(\\mathbf{x}_{k}) = \\sum\\limits_{l}{L^{l}}_{k}\\mathbf{x}_{l}$とすると、次が成り立つ。\n$${L^{l}}_{k} = \\sum_{i}L_{ik}g^{il} = \\sum_{i}L_{ki}g^{il}$$\nここで、$L_{ij}$は第二基本形式の係数、$[g^{kl}]$は第一基本形式係数行列の逆行列である。行列で表すと、\n$$ \\begin{bmatrix} {L^{l}}_{k} \\end{bmatrix} = \\begin{bmatrix} {L^{1}}_{1} \u0026amp; {L^{1}}_{2} \\\\ {L^{2}}_{1} \u0026amp; {L^{2}}_{2} \\end{bmatrix} = \\begin{bmatrix} g^{li} \\end{bmatrix} \\begin{bmatrix} L_{ik} \\end{bmatrix} $$\n説明 定義におけるマイナス記号は便宜のために存在する。\nバインガルテン・マップは、それぞれの点$p$において、各接ベクトル方向への$\\mathbf{n}$の変化率を測る作用素として理解できる。この理由で、形状作用素shape operatorとも呼ばれる。\n定義により$L$は$T_{p}M$を$\\mathbb{R}^{3}$へ送る写像と定めたが、実際は$T_{p}M$へ送る写像となることが確認できる。\nつまり、${L^{l}}_{k}$は$L(\\mathbf{x_{k}})$の$l$番目の基底の係数である。すなわち、基底$B = \\left\\{ \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\}$に対する座標ベクトルで表せば、以下のようになる。 $$ L(\\mathbf{x}_{k}) = {L^{1}}_{k}\\mathbf{x}_{1} + {L^{2}}_{k}\\mathbf{x}_{2} $$ $$ \\left[ L(\\mathbf{x}_{k}) \\right]_{B} = \\begin{bmatrix} {L^{1}}_{k} \\\\ {L^{2}}_{k} \\end{bmatrix} $$ したがって、$L$の行列表現は、以下のようになる。 $$ [L]_{B} = \\begin{bmatrix} {L^{1}}_{1} \u0026amp; {L^{1}}_{2} \\\\ {L^{2}}_{1} \u0026amp; {L^{2}}_{2} \\end{bmatrix} $$ また、第一基本形式の性質により、以下が成り立つ。 $$ L_{ij} = \\sum_{l}L_{il}\\delta_{lj} = \\sum\\limits_{l,k} L_{il}g^{lk}g_{kj} = \\sum\\limits_{l,k} L_{li}g^{lk}g_{kj} = \\sum\\limits_{k}{L^{k}}_{i}g_{kj} $$\n$L$が有限次元ベクトル空間間の線形変換であるため、$\\tr{L}$と$\\det(L)$は不変量であり、これをそれぞれ平均曲率、ガウス曲率と呼ぶ。\nRichard S. Millman and George D. Parker, Elements of Differential Geometry (1977), p125\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3188,"permalink":"https://freshrimpsushi.github.io/jp/posts/3188/","tags":null,"title":"バインガルテン・マップ"},{"categories":"확률미분방정식","contents":"モデル 1 $t$ 時点で $S_{t}$ を基礎資産 $1$単位の価格とし、$S_{t}$ が幾何ブラウン運動をすると仮定しよう。すなわち、標準ブラウン運動 $W_{t}$ とトレンドDrift $\\mu \\in \\mathbb{R}$ および拡散Diffusion $\\sigma^{2} \u0026gt; 0$ に対して、$S_{t}$ は次の確率微分方程式の解である。 $$ d S_{t} = S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) $$ 無リスク金利 $r \\in \\mathbb{R}$ が与えられたとき、$t$ 時点での派生商品 $1$単位の価格 $F = F \\left( t, S_{t} \\right)$ は次の偏微分方程式に従う。 $$ r F = {{ \\partial F } \\over { \\partial t }} + r S_{t} {{ \\partial F } \\over { \\partial S_{t} }} + {{ 1 } \\over { 2 }} \\sigma^{2} S_{t}^{2} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} $$\n変数 $F \\left( t, S_{t} \\right)$: 派生商品Derivativesは、先物、オプションなどの金融商品を指す。 $S_{t}$: 基礎資産Underlying Assetsは、通貨、債券、株式など、派生商品の取引対象となる商品を指す。 パラメータ $r \\in \\mathbb{R}$: 無リスク資産Risk-free Assetsの利率を示す。無リスク資産の代表的な例には預金がある。 $\\sigma^{2} \u0026gt; 0$: 市場のボラティリティVolatilityを示す。 説明 派生商品に対する一般的な誤解とは異なり、先物FutureやオプションOptionは、不確実な未来に対するヘッジEdgeの手段として作られた。不確実な未来に備えるために、ある程度の費用Premiumを支払ってもリスクを減らすための方法であった。問題は、その価格を設定する適切な方法がなかったことであり、取引者は経験に基づいて感覚的に派生商品を取引していた。ブラック-ショールズモデルは、そのような派生商品の価格を数学的に説明できるようにした方程式である。\n一般的に、ブラック-ショールズモデル(1973)の貢献者としては、フィッシャー・ブラックFischer Blackとマイロン・ショールズMyron Scholesのほか、本投稿の「ヘッジを用いた導出」を紹介したロバート・マートンRobert K. Mertonの3人が挙げられる。残念ながらブラックは1995年に亡くなり、ショールズとマートンは1997年にノーベル経済学賞を受賞した。ブラック-ショールズ-マートン方程式の発見以降、オプション市場は急速に発展し、学界には金融工学という新たな分野の誕生をもたらした。\nブラックの喜劇 ウィキペディア2によると、ブラックは博士課程の時に専攻を頻繁に変え、どこかに定着するのが苦手だったという。物理学から数学、コンピューター、人工知能へと変更したが、結局名を残した分野は経済学になった。\n信頼できるリファレンスは見つからなかったが、筆者がどこかで聞いた話によると、ブラックは物理学を専攻していた時に、周囲の狂った天才たちを見て「ここでは生き残れない」と思ったという。後に経済/金融を学んでみると、数学を積極的に使う先駆者がいなく、理工学の怪物たちがいない荒れ地で、数学を武器に自らが先駆者となったという。\nショールズの悲劇 ナムウィキ3によると、ショールズは1997年のノーベル経済学賞の記者会見で、賞金で株投資をすると答えてセンセーションを巻き起こしたという。当時、ショールズが運用していたヘッジファンドは過度の自信から過剰なレバレッジLeverageを使用し、1998年のロシアの債務不履行で破綻したという。危機を乗り越えた後、ショールズ は最終的に投資家に利益を返し、その後もファンドマネージャーとして活動を続けたが、サブプライムモーゲージ危機が起こる直前に引退したという。\n前提 本格的な導出に先立ち、いくつかの前提について確認しておこう。\n手数料、税金、配当などの言及されていない要素は考慮しない 物理学モデルで興味の対象でない抵抗や温度、気圧などを考慮しないのと同じ程度に受け止めればよい。そこに加えて、トレンド $\\mu$ と $\\sigma$ などは単純に定数と仮定する。\n派生商品は基礎資産と時点に依存している 派生商品の価格が基礎資産に独立していれば、派生、基礎という言葉を使う理由がない。基礎資産の価格が変わるにつれて派生商品の価格が変わるのが妥当である。また、時間の経過によって変わらない（定数である）ならば、派生商品の価格を検討する意味がない。したがって、$F$ の形を正確には言えないが、少なくとも二つの要素 $t$ と $S_{t}$ に対する関数であると仮定する。 $$ F = F \\left( t, S_{t} \\right) $$\n基礎資産は幾何ブラウン運動をする 幾何ブラウン運動 GBMの代表的な応用は、まさに株価などの基礎資産の価格変動を説明することである。人口の変動量が全体の人口に比例するように、資産の価格変動も資産の価格に比例し、上場廃止にならない限りマイナスになることはないなど、良い前提を多く持っている。 $$ d S_{t} = S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) $$\nある株式の価格 $p_{t}$ が GBMに従うと仮定しよう。$t$日目の終値を$t-1$日目の終値で割り、対数を取った $$ r_{t} = \\nabla \\log p_{t} = \\log {{ p_{t} } \\over { p_{t-1} }} $$ をリターン―リターンReturnと呼ぶが、株価の大きさに関係なく価格が上がれば正の値、下がれば負の値になり、直感と一致する。対数正規分布の項で説明したように、このリターンは正規分布に従い、単純な上下ではなく、株価の成長と逆成長その本質に関心を持つものと見ることができる。\n無リスク資産はメルサス成長をする メルサス成長モデルは、人口動態学Population Dynamicsで、資源の制限や介入などがない場合の集団の成長を説明する最も単純なモデルであり、経済/金融のセンスでは無リスク資産の増殖を説明する前提になる。無リスク収益率は$r$ 定数として仮定され、その金融収益は資産 $N_{t}$ の規模に比例するため、次のような常微分方程式で表現できる。 $$ {{ d V_{t} } \\over { d t }} = r V_{t} $$\n無裁定価格: ポートフォリオ間には価値の差がない ポートフォリオPortfolioに関する数式的な説明は、この証明でさらに詳しく説明する。 無裁定価格Arbitrage-free Pricingの前提とは、我々が考慮するすべてのポートフォリオが同じ価値でバランスを保っているということである。例えば、ポートフォリオ $A$ の価値が $B$ よりも高い場合、合理的な取引主体はより価値のある $A$ の比重を増やして裁定利益を得ることができるため、$B$ を考える理由がない。したがって、我々が考慮するポートフォリオは、このような裁定取引によってこれ以上利益を得ることができない状態であると仮定する。\n摩擦のない市場: 分割と空売りに制限がない 株をやったことがある人ならわかるが、この\n取引というのは最小限の値段単位があり、私が望む金額で取引できないし、空売りをしたくても日本の株式市場では貸借空売りが原則であり、制限がある。その取引単位を自由に分割でき、どんな制約もなく空売りできるということは、行動を妨げる摩擦がないと見ることができる。\n導出 Part 1. ポートフォリオの構成\n我々が保有できる資産は、次の3つの種類だけとしよう。\n基礎資産: $s$ 単位保有しているとしよう。 派生商品: $f$ 単位保有しているとしよう。 無リスク資産: 基礎資産でも派生商品でもない資産で、現金と考えても構わない。 $t$ 時点で我々が保有するすべての資産の価値を $V_{t}$ とすると、$S_{t}$ が基礎資産 $1$単位の価格であり、$F \\left( t , S_{t} \\right)$ が派生商品 $1$単位の価格であったので、次のように表せる。 $$ V_{t} = f F \\left( t, S_{t} \\right) + s S_{t} $$ ポートフォリオを構成するとは、この $f$ と $s$ の量を調整すること、つまりどのように投資するかについての戦略を立てることである。このようなポートフォリオ構成によって発生する取引量が多すぎて市場に影響を与えるという前提は非合理的であるため、基礎資産と派生商品の価格は、$f$ と $s$ の選択に関係なく一定と仮定しよう。つまり、$f$ と $s$ をどのように定めても、以下の数学的議論は変わらないということである。\n注意すべきは、$V_{t}$ は全資産の合計ではないということである。株式口座の残高だけを見ると考えればわかりやすい。ポートフォリオの例としてどのようなものがあるか考えてみよう：\n貯蓄 $V_{t} = 0$：株式口座を整理してすべて貯蓄し、利息だけを受け取る。数学しか知らない士人の目にはあまりにもトリビアルTrivialに見えるかもしれないが、暴落市場や不況に対処できる立派な戦略である。 アリAnt $V_{t} = 5 S_{t}$：個人ならば、派生商品には手を出さないようにしよう。個人が空売りが禁止されている国では、ほとんどの個人投資家はこのようなポートフォリオを持っている。例として、数式で $S_{t} = 81,200$ がサムスン電子の株価であれば、このポートフォリオはサムスン電子 $5$株を保有している私の友人「キム・スヒョン」の口座である。 ヘッジHedge $\\displaystyle V_{t} = 1 \\cdot F- {{ \\partial F } \\over { \\partial S_{t} }} \\cdot S_{t}$：コールオプションCall Optionを $1$だけ買い、その基礎資産を ${{ \\partial F } \\over { \\partial S_{t} }}$ だけ空売りしたとしよう。オプションの満期日に基礎資産の価格が大幅に上昇した場合、コールオプションが大きな利益をもたらし、基礎資産の価格がむしろ下落した場合は、空売りで既に利益を得ている。 ヘッジについての説明で触れたオプションは、ヨーロピアンオプションEuropean Optionであり、通常、私たちが知っている「満期日にのみ権利を行使できるオプション」である。アメリカンオプションAmerican Optionは満期前でも常に権利を行使できるが、大して知る必要はなく、ヨーロピアン、アメリカンという言葉に怖がることはないようにしよう。\n我々は最後の例、現物空売りで派生商品をヘッジするポートフォリオ $$ V_{t} = 1 \\cdot F \\left( t, S_{t} \\right) - {{ \\partial F } \\over { \\partial S_{t} }} \\cdot S_{t} $$ からブラック-ショールズ方程式を導出する。正確にヘッジしているので、このポートフォリオは無リスク資産であり、時間 $t$ に対する増分Incrementは $$ d V_{t} = d F - {{ \\partial F } \\over { \\partial S_{t} }} d S_{t} $$ である。ここで、$S_{t}$ は幾何ブラウン運動をすると仮定したので、$d S_{t}$ に $\\displaystyle S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right)$ を代入すると $$ d V_{t} = d F - {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) $$ である。一方、無裁定価格の前提を考えると、このポートフォリオと無リスク資産のポートフォリオの増分は同じでなければならない。もしポートフォリオ間に価格差があると仮定すると、他方のポートフォリオを処分して異なるポートフォリオに投資することで裁定利益を得ることができるためである。無リスク資産はメルサス成長をするという前提をしたので、無リスク金利 $r$ に対して、次のような常微分方程式で表される。 $$ {{ d V_{t} } \\over { d t }} = r V_{t} $$ これを整理して表すと $$ \\begin{align*} d V_{t} =\u0026amp; d F - {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) \\\\ d V_{t} =\u0026amp; r V_{t} dt \\end{align*} $$ であるため、 $$ \\begin{equation} r V_{t} dt = d F - {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\left( \\mu dt + \\sigma d W_{t} \\right) \\label{1} \\end{equation} $$ を得る。これで、$dF$ を求めるために伊藤積分を通じて計算してみよう。\nPart 2. 伊藤計算\n伊藤の公式: 伊藤過程 $\\left\\{ X_{t} \\right\\}_{t \\ge 0}$ が与えられているとする。 $$ d X_{t} = u dt + v d W_{t} $$ 関数 $V \\left( t, X_{t} \\right) = V \\in C^{2} \\left( [0,\\infty) \\times \\mathbb{R} \\right)$ に対して $Y_{t} := V \\left( t, X_{t} \\right)$ と置くと、$\\left\\{ Y_{t} \\right\\}$ も伊藤過程であり、次が成立する。 $$ \\begin{align*} d Y_{t} =\u0026amp; V_{t} dt + V_{x} d X_{t} + {{ 1 } \\over { 2 }} V_{xx} \\left( d X_{t} \\right)^{2} \\\\ =\u0026amp; \\left( V_{t} + V_{x} u + {{ 1 } \\over { 2 }} V_{xx} v^{2} \\right) dt + V_{x} v d W_{t} \\end{align*} $$\n幾何ブラウン運動で $S_{t}$ を分配法則に従って展開すると $$ d S_{t} = \\mu S_{t} dt + \\sigma S_{t} d W_{t} $$ であり、伊藤の公式で $u = \\mu S_{t}$ および $v = \\sigma S_{t}$ なので $$ d F = \\left( {{ \\partial F } \\over { \\partial t }} + {{ \\partial F } \\over { \\partial S_{t} }} \\mu S_{t} + {{ 1 } \\over { 2 }} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} \\sigma^{2} S_{t}^{2} \\right) dt + {{ \\partial F } \\over { \\partial S_{t} }} \\sigma S_{t} d W_{t} $$ を得る。$\\eqref{1}$ の $d F$ にこれを代入してみると $$ \\begin{align*} r V_{t} dt =\u0026amp; d F - {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\mu dt - {{ \\partial F } \\over { \\partial S_{t} }} \\sigma S_{t} d W_{t} \\\\ =\u0026amp; \\left( {{ \\partial F } \\over { \\partial t }} + {\\color{Red}{{ \\partial F } \\over { \\partial S_{t} }} \\mu S_{t}} + {{ 1 } \\over { 2 }} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} \\sigma^{2} S_{t}^{2} \\right) dt + {\\color{Blue}{{ \\partial F } \\over { \\partial S_{t} }} \\sigma S_{t} d W_{t}} \\\\ \u0026amp; - {\\color{Red}{{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\mu dt} - {\\color{Blue}{{ \\partial F } \\over { \\partial S_{t} }} \\sigma S_{t} d W_{t}} \\\\ =\u0026amp; {{ \\partial F } \\over { \\partial t }} dt + {{ 1 } \\over { 2 }} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} \\sigma^{2} S_{t}^{2} dt \\end{align*} $$ である。左辺のポートフォリオの価値 $V_{t}$ が $\\displaystyle V_{t} = F- {{ \\partial F } \\over { \\partial S_{t} }} S_{t}$ のように定義されていたので、これを代入すると $$ r \\left( F- {{ \\partial F } \\over { \\partial S_{t} }} S_{t} \\right) dt = {{ \\partial F } \\over { \\partial t }} dt + {{ 1 } \\over { 2 }} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} \\sigma^{2} S_{t}^{2} dt $$ である。$rF$ に対して式を整理すると、求めていた次の方程式を得る。 $$ r F = {{ \\partial F } \\over { \\partial t }} + r S_{t} {{ \\partial F } \\over { \\partial S_{t} }} + {{ 1 } \\over { 2 }} \\sigma^{2} S_{t}^{2} {{ \\partial^{2} F } \\over { \\partial S_{t}^{2} }} $$\n■\n崔炳善. (2012). ブラック-ショールズ式の様々な導出\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Fischer_Black\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://namu.wiki/w/%EB%B8%94%EB%9E%99-%EC%88%84%EC%A6%88%20%EB%AA%A8%ED%98%95#s-5\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2156,"permalink":"https://freshrimpsushi.github.io/jp/posts/2156/","tags":null,"title":"ブラック-ショールズモデルの導出"},{"categories":"확률론","contents":"定義 1 $\\mu \\in \\mathbb{R}$と$\\sigma^{2} \u0026gt; 0$によって与えられる確率微分方程式があるとしよう。 $$ d X_{t} = X_{t} \\left( \\mu dt + \\sigma d B_{t} \\right) $$ このSDEの解は初期値$X_{0}$に対して、次のような確率過程で求められ、「ジオメトリックブラウン運動」と呼ばれる。 $$ X_{t} = X_{0} \\exp \\left[ \\left( \\mu - {{ \\sigma^{2} } \\over { 2 }} \\right) t + \\sigma B_{t} \\right] $$\n解説 **ジオメトリックブラウニアンモーション(GBM)**は、金融や経済の分野で指数のトレンドを説明する基本的なモデルとして有名だ。ジオメトリックという表現は、指数的に増加する等比数列の和である幾何級数から来ているようで、幾何学とは関係がない。\nログ正規分布 応用を除外して単純に数学的性質だけを考えた場合、GBMの最も大きな特徴は、ログを取ったときに正規分布に従うこと―つまり、ログ正規分布に従うことだ。数式的に見れば、$\\exp$に正規分布に従うブラウニアンモーションが含まれているので、当然のことだ。\nダイナミクス 人口動態学の観点から、GBMで表されるシステムはマルサス成長モデル $N ' = r N$にノイズ項$X_{t} \\sigma d B_{t}$が追加されたものに過ぎない。確かにGBMを確率微分方程式の解として定義する必要は厳密にはないが、このように決定論的な常微分方程式の知られた性質をすぐに知ることができるので、理解に非常に役立つ。\n金融工学 GBMの代表的な応用は、株価などの基礎資産の価格変動を説明することだ。人口の変動量が全体人口に比例するように、資産の価格変動も資産の価格に比例し、上場廃止にならない限り負の数になることはないなど、多くの良い仮定を持っている。\nある株の価格$p_{t}$がGBMに従うと仮定する。$t$日目の終値を$t-1$日目の終値で割り、ログを取った $$ r_{t} = \\nabla \\log p_{t} = \\log {{ p_{t} } \\over { p_{t-1} }} $$ はリターン―リターンと呼ばれ、株価の大きさに関係なく、価格が上がれば正の数、下がれば負の数になり、私たちの直感と一致する。ログ正規分布の節で説明したように、このリターンは正規分布に従い、単純な上昇や下降ではなく、株価の成長と逆成長その本質に興味を持つことができる。\nStojkoski. (2020). Generalised Geometric Brownian Motion: Theory and Applications to Option Pricing. https://doi.org/10.3390/e22121432\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2154,"permalink":"https://freshrimpsushi.github.io/jp/posts/2154/","tags":null,"title":"幾何ブラウン運動"},{"categories":"줄리아","contents":"コード 1 すごく簡単なんだけど、否定演算子の ! と ~ を単項演算子じゃなくて関数として見てしまって、!. や ~. を使う間違いをよくするよ。.! や .~ と書けばいいんだ。\njulia\u0026gt; a = rand(1,10) .\u0026lt; 0.5\r1×10 BitMatrix:\r1 1 0 0 1 0 1 0 0 0\rjulia\u0026gt; .!(a)\r1×10 BitMatrix:\r0 0 1 1 0 1 0 1 1 1\rjulia\u0026gt; .~(a)\r1×10 BitMatrix:\r0 0 1 1 0 1 0 1 1 1 環境 OS: Windows julia: v1.6.0 https://discourse.julialang.org/t/negation-of-boolean-array/16159/2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2149,"permalink":"https://freshrimpsushi.github.io/jp/posts/2149/","tags":null,"title":"ジュリアでビット配列を反転させる方法"},{"categories":"줄리아","contents":"コード 1 using Gtk\rfile_name = open_dialog(\u0026#34;파일 열기\u0026#34;) 最初の引数として与えられる文字列は、ダイアログのタイトルだ。実行すると、こんな感じで「ファイルを開く」ダイアログがポップアップするのが確認できる。\n環境 OS: Windows julia: v1.6.0 https://discourse.julialang.org/t/choose-a-file-interactively/10910/3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2143,"permalink":"https://freshrimpsushi.github.io/jp/posts/2143/","tags":null,"title":"ジュリアでfile.choose()のようにダイアログボックスを開いてファイルを選択する方法"},{"categories":"기하학","contents":"曲線に沿ったベクトル場[^1] 定義 曲面 $M$と曲線 $\\alpha : \\left[ a, b \\right] \\to M$が与えられたとする。それぞれの$t \\in \\left[ a,b \\right]$を点$\\alpha (t)$上で曲面$M$に対する接ベクトルに対応させる関数$\\mathbf{X}$を曲線$\\alpha$に沿ったベクトル場vector field along a curve$\\alpha$という。\n$$ \\mathbf{X} : \\left[ a, b \\right] \\to \\mathbb{R}^{3} \\\\ \\mathbf{X}(t) \\in T_{\\alpha (t)}M $$\n説明 定義で言う接ベクトルは、曲線$\\alpha$の接ベクトルではなく、$T_{\\alpha (t)}M$の要素である点$\\alpha (t)$での接ベクトルであることに注意しよう。曲面上の各点$\\alpha (t)$での接ベクトルは一意ではないため、曲線$\\alpha$に沿ったベクトル場も一意ではない。接平面に無限のベクトルがあるため$\\mathbf{X}$も無限に存在する。\n簡単な例として$M$上の曲線$\\alpha (t)$が与えられた時、$\\alpha (t)$の接ベクトル場である$\\mathbf{T}(t)$は$\\alpha$に沿ったベクトル場となる。$\\mathbf{S} = \\mathbf{n} \\times \\mathbf{T}$もまた$\\alpha$ベクトル場である。\n$\\mathbf{S}$と$\\mathbf{T}$は接空間の基底となるため、全ての$\\alpha$ベクトル場$\\mathbf{X}$は次のような線形結合で表される。\n$$ \\mathbf{X}(t) = A(t)\\mathbf{T}(t) + B(t)\\mathbf{S}(t)\\quad \\text{for some } A,B:[a,b]\\to \\mathbb{R} $$\n微分可能なベクトル場 定義 $\\alpha (t)$に沿ったベクトル場$\\mathbf{X}(t)$が微分可能であるdifferentiableとは、関数$\\mathbf{X} : \\left[ a, b \\right] \\to \\mathbb{R}^{3}$が微分可能であることを意味する。\n説明 正確には'$\\mathbf{X}$が微分可能である'と言うべきだが、'$\\mathbf{X}(t)$が微分可能である'とも便宜上言う。\n平行なベクトル場 定義 微分可能な$\\alpha$ベクトル場$\\mathbf{X}(t)$が与えられたとする。$\\dfrac{d \\mathbf{X}}{dt}$が曲面$M$と垂直なら、$\\mathbf{X}(t)$が$\\alpha (t)$に沿って平行であるparallel along$\\alpha (t)$と定義する。\n説明 上で説明したように、$\\alpha$ベクトル場は本当に任意に選ぶことができるが、「微分可能な$\\alpha$ベクトル場」という条件は、「平行な線」という概念を話すために制限を設けるものである。\n曲面$M$と垂直であるということは、$\\dfrac{d \\mathbf{X}}{dt}$が接方向の成分は持たず、法線方向の成分のみを持つということと同じである。定義を見ただけではなぜこのようなベクトル場を平行であるというのか理解しにくいかもしれないので、次の例を見よう。\n例 2次元平面で $xy-$平面上の曲線$\\boldsymbol{\\gamma}(t) = \\left( a(t), b(t), 0 \\right)$を考えよう。そして$\\mathbf{X}(t) = \\left( A(t), B(t), 0 \\right)$を$\\boldsymbol{\\gamma}$に沿ったベクトル場としよう。そうすると、\n$$ \\dfrac{d \\mathbf{X}}{dt} = \\left( \\dfrac{d A}{dt}, \\dfrac{d B}{dt}, 0 \\right) $$\nこのベクトルが$xy-$平面と垂直であるためには、任意の全てのベクトル$(x,y,0)$との内積が$0$である必要があるので、次を得る。\n$$ \\dfrac{d A}{dt} = 0 = \\dfrac{d B}{dt} $$\nしたがって$A(t), B(t)$は定数である。これを図で表すと次のようになり、私たちが直感的に考える「曲線$\\boldsymbol{\\gamma}$に沿って平行なベクトルたち」によく合っている。\n球面上で $M$を単位球面としよう。${\\color{6699CC}\\boldsymbol{\\gamma}(t)}$を赤道線としよう。そして$\\boldsymbol{\\gamma}$に沿ったベクトル場${\\color{295F2E}\\mathbf{X}_{\\boldsymbol{\\gamma}}(t) = (0, 0, 1)}$を考えてみよう。そうすると$\\dfrac{d \\mathbf{X}_{\\boldsymbol{\\gamma}}}{dt} = (0,0,0)$であるため、常に▷\n","id":3174,"permalink":"https://freshrimpsushi.github.io/jp/posts/3174/","tags":null,"title":"曲面に沿った平行ベクトル場の定義"},{"categories":"기하학","contents":"定義1 $z$を与えられた軸の変数とし、$r\u0026gt;0$を$z-$軸からの距離とする。そうすると、下の図のような$rz-$平面上の曲線$\\alpha$を考えることができる。\n下の図のように、曲線$\\alpha$を$z-$軸に対して回転させて得られた曲面を回転面surface of revolutionと呼ぶ。\n回転面は次のように表される。\n$$ \\mathbf{x}(t, \\theta) = \\left( r(t)\\cos \\theta, r(t)\\sin \\theta, z(t) \\right) $$\n回転面の$t-$パラメトリック曲線を子午線meridian, 자오선と呼び、$\\theta-$パラメトリック曲線を緯度線circle of latitude, parallelと呼ぶ。\n説明 回転体と呼んでもコミュニケーションに大きな問題はないが、内部が空だから、厳密には回転体solid of revolutionではない。\nすべての子午線は測地線である。これとは異なり、緯度線が測地線になるためにはいくつかの条件が必要である。\n定理 曲線$\\alpha (t) = \\left( r(t), z(t) \\right)$が正則曲線であり、一対一である場合、$\\alpha$によって形成された回転面$\\mathbf{x}(t, \\theta) = \\left( r(t)\\cos \\theta, r(t)\\sin \\theta, z(t) \\right)$は単純曲面である。この時$\\theta$の条件は$-\\pi \\lt \\theta \\lt \\pi$である。\nRichard S. Millman and George D. Parker, Elements of Differential Geometry (1977), p86-87\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3170,"permalink":"https://freshrimpsushi.github.io/jp/posts/3170/","tags":null,"title":"微分幾何学における回転面"},{"categories":"정수론","contents":"定義 1 二つの整数 $n$ と $m \\ne 0$ について、次を満たす整数 $k$ が存在するなら、$n$ は $m$ で割り切れると言う。 $$ n = mk $$ この時、$n$ を $m$ の 倍数Multiple、$m$ を $n$ の 約数Divisorと言い、以下のように示す。 $$ m \\mid n $$ $m$ が $n$ を割り切れない場合は、取り消し線を引いて $m \\nmid n$ と表示する。 $0$ ではない二つの整数 $a$、$b$ が与えられているとする。両方を割る約数の中で最も大きな数を $a$ と $b$ の 最大公約数Greatest Common Divisorと言い、$\\gcd (a,b)$ のように表記する。 もし $\\gcd (a,b) = 1$ ならば、$a$ と $b$ は 互いに素Relatively Primeであると言う。 説明 最大公約数と互いに素という概念は、ほとんどの人が小学校の時から見てきた概念だ。\n互いに素の英語表現である Relatively Primeから分かるように、実際に多くの人の言葉の習慣とは異なり（このポストのタイトルでもスペース無しで互いに素と書いているように）互いに素は、相対的にRelatively素Primeという意味だ。英語で「お互いに」と表現するにはMutuallyという単語がもっと適しているかもしれないが、数学全般でMutuallyにはそれなりに重要な意味があるため、相対的に(Relatively)が使われる。ここでの相対的というのは、二つの数 $a$ と $b$ は、$1$ 以外の約数を共有していないため、お互いに対しては事実上素数と同じ扱いができるという意味だ。絶対的には素数ではないかもしれないが、お互いに対しては素数として扱うことができるという程度に受け取れば良い。\n併せて見る 代数学的一般化 一意分解整域にて一般化される。\nSilverman. (2012). A Friendly Introduction to Number Theory (4th Edition): p30.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2137,"permalink":"https://freshrimpsushi.github.io/jp/posts/2137/","tags":null,"title":"最大公約数と互いに素"},{"categories":"함수","contents":"定義1 集合 $X$に対して、以下の関数 $I_{X} : X \\to X$を恒等関数identity functionという。\n$$ I_{X}(x) = x,\\quad \\forall x \\in X $$\n説明 主に以下のような記法が使われる。\n$$ I,\\quad \\text{id},\\quad \\text{1} $$\n微分多様体上の接ベクトルは、$\\dfrac{d (f\\circ \\alpha)}{d t}$のように定義され、微分される関数を\n$$ f \\circ \\alpha = f \\circ I \\circ \\alpha = f \\circ \\mathbf{x} \\circ \\mathbf{x}^{-1} \\circ \\alpha $$\nとして分解することで、任意の座標系 $\\mathbf{x}$ に対して接ベクトルを表現しつつ、その座標系の選択に依存しないようにすることができる。\n例 恒等行列 $$ I_{n\\times n} = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \\end{bmatrix} $$\nYou-Feng Lin, (2011). 集合論(Set Theory: An Intuitive Approach, 李興全 訳) (2011), p165\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3167,"permalink":"https://freshrimpsushi.github.io/jp/posts/3167/","tags":null,"title":"同一関数"},{"categories":"줄리아","contents":"コード 実のところ、Juliaは文字列のフォーマットなどが特に便利な言語ではない。コンソールに出力する際に文字列自体の機能を使う方法もあるが、round()関数のデフォルトオプションであるdigitsを使用する方が便利なことが多いだろう。\njulia\u0026gt; for k in 0:8\rprintln(round(π, digits = k))\rend\r3.0\r3.1\r3.14\r3.142\r3.1416\r3.14159\r3.141593\r3.1415927\r3.14159265 環境 OS: Windows julia: v1.6.0 ","id":2133,"permalink":"https://freshrimpsushi.github.io/jp/posts/2133/","tags":null,"title":"ジュリアで小数点以下特定の桁で丸める方法"},{"categories":"기하학","contents":"要旨1 クリストッフェル記号 $\\Gamma_{ij}^{k}$は次の式を満たす。つまり、固有的である。\n$$ \\Gamma_{ij}^{k} = \\dfrac{1}{2} \\sum \\limits_{l=1}^{2} g^{lk} \\left( \\dfrac{\\partial g_{lj}}{\\partial u_{i}} - \\dfrac{\\partial g_{ij}}{\\partial u_{l}} + \\dfrac{\\partial g_{il}}{\\partial u_{j}} \\right) $$\n説明 ガウスが証明した。\nクリストッフェル記号はリーマン計量にのみ依存し、法線ベクトルとは無関係である。従って、クリストッフェル記号を使って、曲面を離れずに曲面の構造を理解することができる。\n証明 まず、各インデックスに対するリーマン計量係数の偏微分を求めると次のようになる。\n$$ \\dfrac{\\partial g_{il}}{\\partial u_{j}} = \\dfrac{\\partial}{\\partial u_{j}} \\left\\langle \\mathbf{x}_{i} , \\mathbf{x}_{l} \\right\\rangle = \\langle \\mathbf{x}_{ij} , \\mathbf{x}_{l} \\rangle + \\langle \\mathbf{x}_{i}, \\mathbf{x}_{lj} \\rangle $$\n$$ \\dfrac{\\partial g_{ij}}{\\partial u_{l}} = \\dfrac{\\partial}{\\partial u_{l}} \\left\\langle \\mathbf{x}_{i} , \\mathbf{x}_{j} \\right\\rangle = \\langle \\mathbf{x}_{il} , \\mathbf{x}_{j} \\rangle + \\langle \\mathbf{x}_{i}, \\mathbf{x}_{jl} \\rangle $$\n$$ \\dfrac{\\partial g_{lj}}{\\partial u_{i}} = \\dfrac{\\partial}{\\partial u_{i}} \\left\\langle \\mathbf{x}_{l} , \\mathbf{x}_{j} \\right\\rangle = \\langle \\mathbf{x}_{li} , \\mathbf{x}_{j} \\rangle + \\langle \\mathbf{x}_{l}, \\mathbf{x}_{ji} \\rangle $$\n$\\mathbf{\\mathbf{x}}_{ij} = \\mathbf{\\mathbf{x}}_{ji}$であるため、\n$$ \\begin{align*} \\dfrac{\\partial g_{il}}{\\partial u_{j}} - \\dfrac{\\partial g_{ij}}{\\partial u_{l}} + \\dfrac{\\partial g_{lj}}{\\partial u_{i}} =\u0026amp;\\ \\langle \\mathbf{x}_{ij} , \\mathbf{x}_{l} \\rangle + \\langle \\mathbf{x}_{i}, \\mathbf{x}_{lj} \\rangle - \\langle \\mathbf{x}_{il} , \\mathbf{x}_{j} \\rangle - \\langle \\mathbf{x}_{i}, \\mathbf{x}_{jl} \\rangle + \\langle \\mathbf{x}_{li} , \\mathbf{x}_{j} \\rangle + \\langle \\mathbf{x}_{l}, \\mathbf{x}_{ji} \\rangle \\\\ =\u0026amp;\\ \\langle \\mathbf{x}_{ij} , \\mathbf{x}_{l} \\rangle + \\langle \\mathbf{x}_{l}, \\mathbf{x}_{ji} \\rangle \\\\ =\u0026amp;\\ 2 \\langle \\mathbf{x}_{ij} , \\mathbf{x}_{l} \\rangle \\end{align*} $$\n従って、クリストッフェル記号は\n$$ \\Gamma_{ij}^{k} = \\sum \\limits_{l=1}^{2} \\left\\langle \\mathbf{x}_{ij}, \\mathbf{x}_{l} \\right\\rangle g^{lk} = \\dfrac{1}{2} \\sum \\limits_{l=1}^{2} \\left( \\dfrac{\\partial g_{lj}}{\\partial u_{i}} - \\dfrac{\\partial g_{ij}}{\\partial u_{l}} + \\dfrac{\\partial g_{il}}{\\partial u_{j}} \\right) g^{lk} $$\n例 $\\mathbf{x}(u_{1}, u_{2}) = \\left( u_{1}, u_{2}, f(u_{1}, u_{2}) \\right)$のようなモンジュパッチが与えられたとする。すると\n$$ \\mathbf{x}_{1} = \\partial_{u_{1}}\\mathbf{x} = (1,0,f_{1}) \\quad \\text{and} \\quad \\mathbf{x}_{2} = (0,1,f_{2}) $$\nこの時、$f_{i} = \\partial_{u_{i}}f$である。$\\Gamma_{11}^{1}$は以下の二つの方法で求めることができる。\n外的に計算する $$ \\mathbf{x}_{1} \\times \\mathbf{x}_{2} = (-f_{1}, -f_{2}, 1) $$\n単位法線は\n$$ \\mathbf{n} = \\dfrac{(-f_{1}, -f_{2}, 1)}{\\sqrt{(f_{1})^{2} + (f_{2})^{2} + 1}} $$\nガウスの公式\n$$ \\mathbf{x}_{ij} = L_{ij} \\mathbf{n} + \\sum \\limits_{k=1}^{2} \\Gamma_{ij}^{k} \\mathbf{x}_{k} $$\n$\\mathbf{x}$の二回微分はガウスの公式により次のようになる。\n$$ \\mathbf{x}_{11} = (0, 0, f_{11}) = L_{11}\\mathbf{n} + \\Gamma_{11}^{1}\\mathbf{x}_{1} + \\Gamma_{11}^{2}\\mathbf{x}_{2} $$\n従って、第二基本形式の係数は$L_{ij} = \\langle \\mathbf{x}_{ij}, \\mathbf{n} \\rangle$である\n$$ L_{11} = \\left\\langle (0,0,f_{11}), \\dfrac{(-f_{1}, -f_{2}, 1)}{\\sqrt{(f_{1})^{2} + (f_{2})^{2} + 1}}\\right\\rangle = \\dfrac{f_{11}}{\\sqrt{(f_{1})^{2} + (f_{2})^{2} + 1}} $$\n$\\mathbf{x}_{11}$を成分に分けて詳しく見ると次のようになる。\n$$ \\mathbf{x}_{11} = (0, 0, f_{11}) = \\dfrac{L_{11}}{\\sqrt{(f_{1})^{2} + (f_{2})^{2} + 1}}(-f_{1}, -f_{2}, 1) + \\Gamma_{11}^{1}(1,0,f_{1}) + \\Gamma_{11}^{2}(0,1,f_{2}) $$\n最初の成分を見ると、次の式を得る。\n$$ \\begin{align*} \u0026amp;\u0026amp; 0 =\u0026amp;\\ \\dfrac{L_{11}(-f_{1})}{\\sqrt{(f_{1})^{2} + (f_{2})^{2} + 1}} + \\Gamma_{11}^{1} \\\\[1em] \\implies\u0026amp;\u0026amp; \\Gamma_{11}^{1} =\u0026amp;\\ \\dfrac{L_{11}(f_{1})}{\\sqrt{(f_{1})^{2} + (f_{2})^{2} + 1}} \\\\[1em] \\implies\u0026amp;\u0026amp; \\Gamma_{11}^{1} =\u0026amp;\\ \\dfrac{f_{1} f_{11}}{(f_{1})^{2} + (f_{2})^{2} + 1} \\end{align*} $$\n■\n固有に計算する 上述の定理によると、$\\Gamma_{11}^{1}$は次のように計算できる。\n$$ \\begin{align*} \\Gamma_{11}^{1} =\u0026amp;\\ \\dfrac{1}{2} \\sum \\limits_{l=1}^{2} g^{l1} \\left( \\dfrac{\\partial g_{l1}}{\\partial u_{1}} - \\dfrac{\\partial g_{11}}{\\partial u_{l}} + \\dfrac{\\partial g_{1l}}{\\partial u_{1}} \\right) \\\\ =\u0026amp;\\ \\dfrac{1}{2} \\left[ g^{11} \\left( \\dfrac{\\partial g_{11}}{\\partial u_{1}} - \\dfrac{\\partial g_{11}}{\\partial u_{1}} + \\dfrac{\\partial g_{11}}{\\partial u_{1}} \\right) + g^{21} \\left( \\dfrac{\\partial g_{21}}{\\partial u_{1}} - \\dfrac{\\partial g_{11}}{\\partial u_{2}} + \\dfrac{\\partial g_{12}}{\\partial u_{1}} \\right) \\right] \\\\ =\u0026amp;\\ \\dfrac{1}{2} \\left[ g^{11} \\dfrac{\\partial g_{11}}{\\partial u_{1}} + 2g^{21}\\dfrac{\\partial g_{21}}{\\partial u_{1}} - g^{21}\\dfrac{\\partial g_{11}}{\\partial u_{2}} \\right] \\end{align*} $$\n第一基本形式の係数は$g_{ij} = \\left\\langle \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\rangle$である\n$$ \\left[ g_{ij} \\right] = \\begin{bmatrix} 1+(f_{1})^{2} \u0026amp; f_{1}f_{2} \\\\[1em] f_{1}f_{2} \u0026amp; 1 + (f_{2})^{2} \\end{bmatrix} $$\n逆行列は\n$$ \\left[ g_{ij} \\right]^{-1} = \\left[ g^{lk} \\right] = \\dfrac{1}{(f_{1})^{2} + (f_{2})^{2} + 1}\\begin{bmatrix} 1+(f_{2})^{2} \u0026amp; -f_{1}f_{2} \\\\[1em] -f_{1}f_{2} \u0026amp; 1 + (f_{1})^{2} \\end{bmatrix} $$\n必要なものを集めると次のようになる。\n$$ \\begin{align*} \\dfrac{\\partial g_{11}}{\\partial u_{1}} =\u0026amp;\\ \\dfrac{\\partial }{\\partial u_{1}}\\left( 1+ (f_{1})^{2} \\right) = 2f_{1}f_{11} \\\\ \\dfrac{\\partial g_{21}}{\\partial u_{1}} =\u0026amp;\\ \\dfrac{\\partial }{\\partial u_{1}}\\left( f_{1}f_{2} \\right) = f_{11}f_{2} + f_{1}f_{21} \\\\ \\dfrac{\\partial g_{11}}{\\partial u_{2}} =\u0026amp;\\ \\dfrac{\\partial }{\\partial u_{2}}\\left( 1+ (f_{1})^{2} \\right) = 2f_{1}f_{12} \\end{align*} $$\nそして\n$$ \\begin{align*} g^{11} =\u0026amp;\\ \\dfrac{1+(f_{2})^{2}}{(f_{1})^{2} + (f_{2})^{2} + 1} \\\\ g^{21} =\u0026amp;\\ \\dfrac{-f_{1}f_{2}}{(f_{1})^{2} + (f_{2})^{2} + 1} \\end{align*} $$\n代入してみると次のようになる。\n$$ \\begin{align*} \\Gamma_{11}^{1} =\u0026amp;\\ \\dfrac{1}{2} \\left[ g^{11} \\dfrac{\\partial g_{11}}{\\partial u_{1}} + 2g^{21}\\dfrac{\\partial g_{21}}{\\partial u_{1}} - g^{21}\\dfrac{\\partial g_{11}}{\\partial u_{2}} \\right] \\\\ =\u0026amp;\\ \\dfrac{1}{2} \\left[ \\dfrac{1+(f_{2})^{2}}{(f_{1})^{2} + (f_{2})^{2} + 1} 2f_{1}f_{11} + 2\\dfrac{-f_{1}f_{2}}{(f_{1})^{2} + (f_{2})^{2} + 1} \\left( f_{11}f_{2} + f_{1}f_{21} \\right)- \\dfrac{-f_{1}f_{2}}{(f_{1})^{2} + (f_{2})^{2} + 1}2f_{1}f_{12} \\right] \\\\ =\u0026amp;\\ \\dfrac{1}{(f_{1})^{2} + (f_{2})^{2} + 1}\\left[ \\left( 1+(f_{2})^{2} \\right)f_{1}f_{11} + \\left( -f_{1}f_{2} \\right) \\left( f_{11}f_{2} + f_{1}f_{21} \\right)- (-f_{1}f_{2})f_{1}f_{12} \\right] \\\\ =\u0026amp;\\ \\dfrac{1}{(f_{1})^{2} + (f_{2})^{2} + 1}\\left[ f_{1}f_{11} + f_{1}(f_{2})^{2}f_{11} - f_{1}(f_{2})^{2}f_{11} -(f_{1})^{2}f_{2}f_{21} + (f_{1})^{2}f_{2}f_{12} \\right] \\\\ =\u0026amp;\\ \\dfrac{1}{(f_{1})^{2} + (f_{2})^{2} + 1}\\left[ f_{1}f_{11} \\right] \\\\ =\u0026amp;\\ \\dfrac{f_{1}f_{11}}{(f_{1})^{2} + (f_{2})^{2} + 1} \\end{align*} $$\n固有の性質だけでクリストッフェル記号を計算することは、そうでない場合に比べて複雑であることが分かる。\n■\nRichard S. Millman and George D. Parker, Elements of Differential Geometry (1977), p105-106\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3164,"permalink":"https://freshrimpsushi.github.io/jp/posts/3164/","tags":null,"title":"クリストッフェル記号は内在的である"},{"categories":"다변수벡터해석","contents":"まとめ1 $f : \\mathbb{R}^{n} \\to \\mathbb{R}$を$C^{k}$ 関数、$\\mathbf{a} = (a_{1}, \\dots, a_{n}) \\in \\mathbb{R}^{n}$としよう。そしたら、次を満たす$C^{k-2}$関数$h_{ij}$が存在する。\n$$ f(\\mathbf{x}) = f(\\mathbf{a}) + \\sum_{i} (x_{i} - a_{i})\\dfrac{\\partial f}{\\partial x_{i}}(\\mathbf{a}) + \\sum_{i,j}h_{ij}(\\mathbf{x})(x_{i} - a_{i}) (x_{j} - a_{j}) $$\n説明 テイラーの定理を多変数関数に一般化したものだ。\nsecond-order\n$$ \\begin{align*} f(\\mathbf{x}) \u0026amp;= f(\\mathbf{a}) + \\sum\\limits_{i=1}^{n} (x_{i} - a_{i}) \\dfrac{\\partial f}{\\partial x_{i}}(\\mathbf{a}) + \\dfrac{1}{2!}\\sum\\limits_{i,j=1}^{n} (x_{i} - a_{i})^{2} \\dfrac{\\partial^{2} f}{\\partial x_{i} \\partial x_{j}}(\\mathbf{a}) + \\text{Remainder} \\\\ \u0026amp;= f(\\mathbf{a}) + (\\mathbf{x} - \\mathbf{a})^{T} \\nabla f (\\mathbf{a}) + \\dfrac{1}{2!}(\\mathbf{x} - \\mathbf{a})^{T} (H(\\mathbf{a})) (\\mathbf{x} - \\mathbf{a}) + \\text{Remainder} \\end{align*} $$\nここで、$\\nabla f$は$f$グラディエントで、$H$は$f$のヘシアンだ。\n残差項remainder termについては、以下の形も便利に使われる。\n$$ f(\\mathbf{x} + \\mathbf{p}) = f(\\mathbf{x}) + \\mathbf{p}^{T}\\nabla f(\\mathbf{x} + t \\mathbf{p}) \\quad \\text{for some } t \\in (0,1) $$ $$ f(\\mathbf{x} + \\mathbf{p}) = f(\\mathbf{x}) + \\mathbf{p}^{T}\\nabla f(\\mathbf{x}) + \\dfrac{1}{2!}\\mathbf{p}^{T} H(\\mathbf{x} + t \\mathbf{p}) \\mathbf{p} \\quad \\text{for some } t \\in (0,1) $$\n$$ f(\\mathbf{x} + \\mathbf{p}) = f(\\mathbf{x}) + \\int_{0}^{1}\\mathbf{p}^{T}\\nabla f (\\mathbf{x} + t\\mathbf{p})dt $$\n証明 $$ \\begin{align*} f(\\mathbf{x}) - f(\\mathbf{a}) =\u0026amp;\\ \\int_{0}^{1} \\dfrac{d}{dt} \\left[ f(t(\\mathbf{x} - \\mathbf{a}) + \\mathbf{a}) \\right]dt \\\\ =\u0026amp;\\ \\int_{0}^{1} \\left( \\sum_{i} \\dfrac{\\partial f}{\\partial x_{i}}\\left( t(\\mathbf{x} - \\mathbf{a}) + \\mathbf{a} \\right)(x_{i}-a_{i}) \\right) dt \u0026amp; \\text{by } \\href{https://freshrimpsushi.github.io/posts/3134}{\\text{chain rule}} \\\\ =\u0026amp;\\ \\sum_{i}(x_{i} - a_{i}) \\int_{0}^{1} \\left( \\dfrac{\\partial f}{\\partial x_{i}}\\left( t(\\mathbf{x} - \\mathbf{a}) + \\mathbf{a} \\right) \\right) dt \\end{align*} $$\n積分部分を$g_{i}(\\mathbf{x})$と表記しよう。$g_{i}(\\mathbf{x}) = \\displaystyle \\int_{0}^{1} \\left( \\dfrac{\\partial f}{\\partial x_{i}}\\left( t(\\mathbf{x} - \\mathbf{a}) + \\mathbf{a} \\right) \\right) dt$とすると、\n$$ \\begin{equation} f(\\mathbf{x}) - f(\\mathbf{a}) = \\sum_{i}(x_{i} - a_{i}) \\int_{0}^{1} \\left( \\dfrac{\\partial f}{\\partial x_{i}}\\left( t(\\mathbf{x} - \\mathbf{a}) + \\mathbf{a} \\right) \\right) dt = \\sum_{i} g_{i}(\\mathbf{x}) (x_{i} - a_{i}) \\end{equation} $$\n$g_{i}(\\mathbf{a})$の値は次の通り。\n$$ g_{i}(\\mathbf{a}) = \\int_{0}^{1} \\dfrac{\\partial f}{\\partial x_{i}} \\left(t(\\mathbf{a} - \\mathbf{a}) + \\mathbf{a} \\right) dt = \\int_{0}^{1} \\dfrac{\\partial f}{\\partial x_{i}}\\left( \\mathbf{a} \\right) dt = \\dfrac{\\partial f}{\\partial x_{i}}\\left( \\mathbf{a} \\right) $$\nそれならば、$(1)$を導出した時と同様の方法で、次の式を得られる。\n$$ g_{i}(\\mathbf{x}) - g_{i}(\\mathbf{a}) = \\sum_{j} h_{ij}(\\mathbf{x}) (x_{j}-a_{j}) $$\nこれで、まとめると\n$$ \\begin{align*} f(\\mathbf{x}) =\u0026amp;\\ f(\\mathbf{a}) + \\sum_{i}g_{i}(\\mathbf{x})(x_{i}-a_{i}) \\\\ =\u0026amp;\\ f(\\mathbf{a}) + \\sum_{i}\\left( g_{i}(\\mathbf{a}) + \\sum_{j} h_{ij}(\\mathbf{x}) (x_{j}-a_{j}) \\right)(x_{i}-a_{i}) \\\\ =\u0026amp;\\ f(\\mathbf{a}) + \\sum_{i} g_{i}(\\mathbf{a})(x_{i}-a_{i}) + \\sum_{i,j} h_{ij}(\\mathbf{x})(x_{i}-a_{i})(x_{j}-a_{j}) \\\\ =\u0026amp;\\ f(\\mathbf{a}) + \\sum_{i} \\dfrac{\\partial f}{\\partial x_{i}}\\left( \\mathbf{a} \\right)(x_{i}-a_{i}) + \\sum_{i,j} h_{ij}(\\mathbf{x})(x_{i}-a_{i})(x_{j}-a_{j}) \\end{align*} $$\n■\n一緒に見る テイラーの定理 Richard S. Millman and George D. Parker, Elements of Differential Geometry (1977), p213-214\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3163,"permalink":"https://freshrimpsushi.github.io/jp/posts/3163/","tags":null,"title":"多変数関数のテイラーの定理"},{"categories":"기하학","contents":"定義1 微分幾何学では、(単位ノーマル $\\mathbf{n}$には依存せずに) 第一基本形式の係数 $g_{ij}$のみに依存する関数を 内在的intrinsic, 本質的と言う。 Description2 3 If the coefficients of the Riemannian metric $g_{ij}$ are known, the length of curves on the surface and the area of the surface can be calculated without leaving the surface as follows.\n$$ \\text{length of } \\alpha = \\int_{a}^{b} \\sqrt{ g_{ij} \\alpha_{i}^{\\prime} \\alpha_{j}^{\\prime} } dt = \\int_{a}^{b} \\sqrt{ E\\left( \\dfrac{d u_{1}}{dt} \\right)^{2} + 2F\\dfrac{d u_{1}}{dt}\\dfrac{d u_{2}}{dt} + G\\left( \\dfrac{d u_{2}}{dt} \\right)^{2}} dt $$\n$$ \\text{area of } R = \\iint _{Q} \\sqrt{g} du_{1}du_{2} = \\iint _{Q} \\sqrt{EG-F^{2}} du_{1}du_{2} $$\nThis means that it is possible to calculate such information without using information about the exterior of the surface (for example, unit normal(../2110)) but only using information from the tangent plane (coefficients of the first fundamental form). Therefore, things that can be calculated in this manner are referred to as intrinsic.\nViewing a surface $M$ from an intrinsic point of view means considering $M$ as the entire space itself, and viewing it from an extrinsic point of view means considering it as a subspace of $\\R^{3}$, as in $M \\subset \\R^{3}$.\nExamples of intrinsic things are as follows.\nExamples Christoffel symbols Geodesic curvature See also Intrinsic spatial processes in spatial statistics Richard S. Millman and George D. Parker, 微分幾何学の要素 (1977), p106\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBarrett O\u0026rsquo;Neill, Elementary Differential Geometry (Revised 2nd Edition, 2006), p263\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nManfredo P. Do Carmo Curves \u0026amp; Surfacesの微分幾何学 (Revised \u0026amp; Updated 2nd Edition, 2016), p220-221\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3162,"permalink":"https://freshrimpsushi.github.io/jp/posts/3162/","tags":null,"title":"微分幾何学における内在的/本質的な定義"},{"categories":"기하학","contents":"整理1 $\\mathbf{x} : U \\to \\R^{3}$を座標パッチと言おう。$(u_{1}, u_{2})$を$U$の座標としよう。\n$\\mathbf{n}$を単位ノーマル、$L_{ij} = \\left\\langle \\mathbf{x}_{ij}, \\mathbf{n} \\right\\rangle$を第2基本形式の係数、$\\Gamma_{ij}^{k} = \\sum \\limits_{l=1}^{2} \\left\\langle \\mathbf{x}_{ij}, \\mathbf{x}_{l} \\right\\rangle g^{lk} = \\left\\langle \\mathbf{x}_{ij}, \\mathbf{x}_{l} \\right\\rangle g^{lk}$をクリストッフェル記号としよう。\nすると、次が成り立つ。\n(a) ガウスの公式Gauss\u0026rsquo;s formulas:\n$$ \\mathbf{x}_{ij} = L_{ij} \\mathbf{n} + \\sum \\limits_{k=1}^{2} \\Gamma_{ij}^{k} \\mathbf{x}_{k} $$\n(b) 任意の単位速度曲線$\\boldsymbol{\\gamma}(s) = \\mathbf{x}\\left( \\gamma^{1}(s), \\gamma^{2}(s) \\right)$に対して、\n$$ \\kappa_{n} = \\sum \\limits_{i=1}^{2} \\sum \\limits_{j=1}^{2} L_{ij} (\\gamma^{i})^{\\prime} (\\gamma^{j})^{\\prime} $$\nそして\n$$ \\kappa_{g}\\mathbf{S} = \\sum \\limits_{k=1}^{2} \\left[ u_{k}^{\\prime \\prime} + \\sum \\limits_{i,j=1}^{2} \\Gamma_{ij}^{k}(\\gamma^{i})^{\\prime}(\\gamma^{j})^{\\prime} \\right] \\mathbf{x}_{k} $$\nこの時、$\\kappa_{n}$は法曲率、$\\kappa_{g}$は測地曲率、そして$\\mathbf{S} = \\mathbf{n} \\times \\mathbf{T}$である。\n説明 事実、(a)は$L_{ij}$と$\\Gamma_{ij}^{k}$の定義そのものである。\n(a)の結果から、次の式を得る。\n$$ \\left\\langle \\mathbf{x}_{ij}, \\mathbf{x}_{l} \\right\\rangle = \\sum \\limits_{k=1}^{2}\\Gamma_{ij}^{k}g_{kl} $$\nこれを第1クリストッフェル記号と言う。\n証明 (a) 単位ノーマルは接空間に垂直であり、$\\left\\{ \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\}$は接空間の基底なので$\\left\\{ \\mathbf{n}, \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\}$は$\\R^{3}$の基底になる。したがって、$\\R^{3}$のすべてのベクトルはこれらの線形結合で表せる。今、$\\mathbf{x}_{ij}$を以下のように表そう。\n$$ \\mathbf{x}_{ij} = a_{ij}\\mathbf{n} + {b_{ij}}^{1}\\mathbf{x}_{1} + {b_{ij}}^{2}\\mathbf{x}_{2} $$\nしたがって、$\\left\\langle \\mathbf{x}_{i}, \\mathbf{n} \\right\\rangle=0$であるので、第2基本形式係数の定義により\n$$ L_{ij} = \\left\\langle \\mathbf{x}_{ij}, \\mathbf{n} \\right\\rangle = \\left\\langle a_{ij}\\mathbf{n} + {b_{ij}}^{1}\\mathbf{x}_{1} + {b_{ij}}^{2}\\mathbf{x}_{2}, \\mathbf{n} \\right\\rangle = a_{ij} $$\nまた、リーマンメトリックの係数は、以下のように定義される。\n$$ \\begin{align*} \\left\\langle \\mathbf{x}_{ij}, \\mathbf{x}_{l} \\right\\rangle =\u0026amp;\\ \\left\\langle a_{ij}\\mathbf{n} + {b_{ij}}^{1}\\mathbf{x}_{1} + {b_{ij}}^{2}\\mathbf{x}_{2}, \\mathbf{x}_{l} \\right\\rangle \\\\ \u0026amp;= {b_{ij}}^{1} \\left\\langle \\mathbf{x}_{1}, \\mathbf{x}_{l} \\right\\rangle + {b_{ij}}^{2} \\left\\langle \\mathbf{x}_{2}, \\mathbf{x}_{l} \\right\\rangle \\\\ \u0026amp;= {b_{ij}}^{1} g_{1l} + {b_{ij}}^{2} g_{2l} \\\\ \u0026amp;= \\sum \\limits_{m=1}^{2} {b_{ij}}^{m} g_{ml} \\\\ \u0026amp;= {b_{ij}}^{m} g_{ml} \\quad (\\text{Einstein notation}) \\end{align*} $$\nしたがって、$[g^{lk}]$は$[g_{ij}]$の逆行列であるので、次の式が成り立つ。\n$$ \\left\\langle \\mathbf{x}_{ij}, \\mathbf{x}_{l} \\right\\rangle g^{lk} = \\sum \\limits_{m=1}^{2} {b_{ij}}^{m} g_{ml}g^{lk} $$\n左辺をすべての$l$に対して足すとクリストッフェル記号である。リーマンメトリックに対して$g_{ik}g^{kj} = {\\delta_{i}}^{j}$が成り立つので、\n$$ \\Gamma_{ij}^{k} = \\sum \\limits_{l=1}^{2} \\left\\langle \\mathbf{x}_{ij}, \\mathbf{x}_{l} \\right\\rangle g^{lk} = \\sum \\limits_{l=1}^{2} \\sum \\limits_{m=1}^{2} {b_{ij}}^{m} g_{ml}g^{lk} = \\sum \\limits_{m=1}^{2} {b_{ij}}^{m} {\\delta_{m}}^{k} = {b_{ij}}^{k} $$\nしたがって、\n$$ \\begin{align*} \\mathbf{x}_{ij} =\u0026amp;\\ a_{ij}\\mathbf{n} + {b_{ij}}^{1}\\mathbf{x}_{1} + {b_{ij}}^{2}\\mathbf{x}_{2} \\\\ =\u0026amp;\\ L_{ij} \\mathbf{n} + {\\Gamma_{ij}}^{1} \\mathbf{x}_{1} + {\\Gamma_{ij}}^{2} \\mathbf{x}_{2} \\\\ =\u0026amp;\\ L_{ij} \\mathbf{n} + \\sum \\limits_{k=1}^{2} \\Gamma_{ij}^{k} \\mathbf{x}_{k} \\end{align*} $$\n■\n(b) Part 1. $\\boldsymbol{\\gamma}^{\\prime \\prime}$の計算\n$\\boldsymbol{\\gamma}(s) = \\mathbf{x}\\left( \\gamma^{1}(s), \\gamma^{2}(s) \\right)$の接ベクトルを計算すると、次のようになる。\n$$ \\begin{align*} T(s) =\u0026amp;\\ \\dfrac{d \\boldsymbol{\\gamma}}{d s} \\\\ =\u0026amp;\\ \\dfrac{d}{ds} \\mathbf{x}(\\gamma^{1}, \\gamma^{2}) \\\\ =\u0026amp;\\ \\dfrac{\\partial \\mathbf{x}}{\\partial \\gamma^{1}}\\dfrac{d \\gamma^{1}}{ds} + \\dfrac{\\partial \\mathbf{x}}{\\partial \\gamma^{2}}\\dfrac{d \\gamma^{2}}{ds}\u0026amp; \\text{by } \\href{https://freshrimpsushi.github.io/posts/derivative-of-three-dimentional-scalar-vector-function}{\\text{chain rule}} \\\\ =\u0026amp;\\ \\mathbf{x}_{1}(\\gamma^{1})^{\\prime} + \\mathbf{x}_{2}(\\gamma^{2})^{\\prime} \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{2}\\mathbf{x}_{i}(\\gamma^{i})^{\\prime} \\\\ =\u0026amp;\\ \\mathbf{x}_{i}(\\gamma^{i})^{\\prime} \\end{align*} $$\n最後の等号ではアインシュタインの表記法を使用した。加速度を計算してみよう。アインシュタインの記法と微分演算に慣れている人なら、以下のように一発で計算できる。\n$$ \\boldsymbol{\\gamma} ^{\\prime \\prime} = \\dfrac{d}{ds}\\left( \\mathbf{x}_{i}(\\gamma^{i})^{\\prime} \\right) = \\mathbf{x}_{ij}(\\gamma^{j})^{\\prime}(\\gamma^{i})^{\\prime} + \\mathbf{x}_{i}(\\gamma^{i})^{\\prime \\prime} = \\sum \\limits_{i=1}^{2}\\left( \\sum\\limits_{j=1}^{2}\\mathbf{x}_{ij}(\\gamma^{j})^{\\prime}(\\gamma^{i})^{\\prime} + \\mathbf{x}_{i}(\\gamma^{i})^{\\prime \\prime} \\right) $$\nアインシュタインの記法に慣れていない人のために、計算過程をできるだけ詳細に書くと、次のようになる。\n$$ \\begin{align*} \\boldsymbol{\\gamma}^{\\prime \\prime} =\u0026amp;\\ \\dfrac{d}{ds} \\left( \\mathbf{x}_{1}(\\gamma^{1})^{\\prime} + \\mathbf{x}_{2}(\\gamma^{2})^{\\prime} \\right) \\\\ =\u0026amp;\\ \\dfrac{d}{ds} \\left( \\mathbf{x}_{1}(\\gamma^{1})^{\\prime} \\right) +\\dfrac{d}{ds} \\left( \\mathbf{x}_{2}(\\gamma^{2})^{\\prime} \\right) \\\\ =\u0026amp;\\ \\dfrac{d}{ds} \\left( \\mathbf{x}_{1} \\right) (\\gamma^{1})^{\\prime} +\\mathbf{x}_{1} \\dfrac{d}{ds} \\left( (\\gamma^{1})^{\\prime} \\right) + \\dfrac{d}{ds} \\left( \\mathbf{x}_{2} \\right) (\\gamma^{2})^{\\prime} +\\mathbf{x}_{2} \\dfrac{d}{ds} \\left( (\\gamma^{2})^{\\prime} \\right) \\\\ =\u0026amp;\\ \\left( \\dfrac{\\partial \\mathbf{x}_{1}}{\\partial \\gamma^{1}}\\dfrac{d \\gamma^{1}}{ds} + \\dfrac{\\partial \\mathbf{x}_{1}}{\\partial \\gamma^{2}}\\dfrac{d \\gamma^{2}}{ds} \\right) (\\gamma^{1})^{\\prime} + \\mathbf{x}_{1} (\\gamma^{1})^{\\prime \\prime} + \\left( \\dfrac{\\partial \\mathbf{x}_{2}}{\\partial \\gamma^{1}}\\dfrac{d \\gamma^{1}}{ds} + \\dfrac{\\partial \\mathbf{x}_{2}}{\\partial \\gamma^{2}}\\dfrac{d \\gamma^{2}}{ds} \\right) (\\gamma^{2})^{\\prime} + \\mathbf{x}_{2} (\\gamma^{2})^{\\prime \\prime} \\\\ =\u0026amp;\\ \\left( \\mathbf{x}_{11}(\\gamma^{1})^{\\prime} + \\mathbf{x}_{12}(\\gamma^{2})^{\\prime} \\right)(\\gamma^{1})^{\\prime} + \\mathbf{x}_{1} (\\gamma^{1})^{\\prime \\prime} + \\left( \\mathbf{x}_{21}(\\gamma^{1})^{\\prime} + \\mathbf{x}_{22}(\\gamma^{2})^{\\prime} \\right)(\\gamma^{2})^{\\prime} + \\mathbf{x}_{2} (\\gamma^{2})^{\\prime \\prime} \\\\ =\u0026amp;\\ \\mathbf{x}_{11}(\\gamma^{1})^{\\prime}(\\gamma^{1})^{\\prime} + \\mathbf{x}_{12}(\\gamma^{2})^{\\prime}(\\gamma^{1})^{\\prime} + \\mathbf{x}_{1} (\\gamma^{1})^{\\prime \\prime} + \\mathbf{x}_{21}(\\gamma^{1})^{\\prime}(\\gamma^{2})^{\\prime} + \\mathbf{x}_{22}(\\gamma^{2})^{\\prime}(\\gamma^{2})^{\\prime} + \\mathbf{x}_{2} (\\gamma^{2})^{\\prime \\prime} \\\\ =\u0026amp;\\ \\left( \\mathbf{x}_{11}(\\gamma^{1})^{\\prime}(\\gamma^{1})^{\\prime} + \\mathbf{x}_{12}(\\gamma^{2})^{\\prime}(\\gamma^{1})^{\\prime} + \\mathbf{x}_{21}(\\gamma^{1})^{\\prime}(\\gamma^{2})^{\\prime} + \\mathbf{x}_{22}(\\gamma^{2})^{\\prime}(\\gamma^{2})^{\\prime} \\right) + \\mathbf{x}_{1} (\\gamma^{1})^{\\prime \\prime} + \\mathbf{x}_{2} (\\gamma^{2})^{\\prime \\prime} \\\\ =\u0026amp;\\ \\sum \\limits_{j=1}^{2}\\left( \\mathbf{x}_{1j}(\\gamma^{j})^{\\prime}(\\gamma^{1})^{\\prime} + \\mathbf{x}_{2j}(\\gamma^{j})^{\\prime}(\\gamma^{2})^{\\prime} \\right) + \\mathbf{x}_{1} u_{1}^{\\prime \\prime} + \\mathbf{x}_{2} (\\gamma^{2})^{\\prime \\prime} \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{2} \\left(\\sum \\limits_{j=1}^{2} \\mathbf{x}_{ij}(\\gamma^{j})^{\\prime}(\\gamma^{i})^{\\prime} + \\mathbf{x}_{i} (\\gamma^{i})^{\\prime \\prime} \\right) \\\\ =\u0026amp;\\ \\mathbf{x}_{ij}(\\gamma^{j})^{\\prime}(\\gamma^{i})^{\\prime} + \\mathbf{x}_{i} (\\gamma^{i})^{\\prime \\prime} \\end{align*} $$\nPart 2.\n$\\boldsymbol{\\gamma}^{\\prime \\prime}$にガウスの公式 (a)を代入すると、\n$$ \\begin{align*} \\boldsymbol{\\gamma}^{\\prime \\prime} (s) \u0026amp;= \\sum \\limits_{i=1}^{2} \\sum \\limits_{j=1}^{2} \\mathbf{x}_{ij} (\\gamma^{i})^{\\prime} (\\gamma^{j})^{\\prime} + \\sum \\limits_{k=1}^{2} \\mathbf{x}_{k} (\\gamma^{k})^{\\prime \\prime} \\\\ \u0026amp;= \\sum \\limits_{i=1}^{2} \\sum \\limits_{j=1}^{2} \\left( L_{ij} \\mathbf{n} + \\sum \\limits_{k=1}^{2} \\Gamma_{ij}^{k} \\mathbf{x}_{k} \\right) (\\gamma^{i})^{\\prime} (\\gamma^{j})^{\\prime} + \\sum \\limits_{k=1}^{2} \\mathbf{x}_{k} (\\gamma^{k})^{\\prime \\prime} \\\\ \u0026amp;= \\sum \\limits_{i=1}^{2} \\sum \\limits_{j=1}^{2} L_{ij} (\\gamma^{i})^{\\prime} (\\gamma^{j})^{\\prime} \\mathbf{n} + \\sum \\limits_{k=1}^{2}\\left( (\\gamma^{k})^{\\prime \\prime} + \\sum \\limits_{i=1}^{2} \\sum \\limits_{j=1}^{2} \\Gamma_{ij}^{k} (\\gamma^{i})^{\\prime} (\\gamma^{j})^{\\prime} \\right)\\mathbf{x}_{k} \\end{align*} $$\nしかし、$\\boldsymbol{\\gamma}^{\\prime \\prime} = \\kappa_{n}\\mathbf{n} + \\kappa_{g}\\mathbf{S}$のように表されるので\n$$ \\kappa_{n} = \\sum \\limits_{i=1}^{2} \\sum \\limits_{j=1}^{2} L_{ij} (\\gamma^{i})^{\\prime} (\\gamma^{j})^{\\prime} $$\n$$ \\kappa_{g}\\mathbf{S} = \\sum \\limits_{k=1}^{2}\\left( (\\gamma^{k})^{\\prime \\prime} + \\sum \\limits_{i=1}^{2} \\sum \\limits_{j=1}^{2} \\Gamma_{ij}^{k} (\\gamma^{i})^{\\prime} (\\gamma^{j})^{\\prime} \\right)\\mathbf{x}_{k} $$\n■\nRichard S. Millman and George D. Parker, Elements of Differential Geometry (1977), p104-105\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3160,"permalink":"https://freshrimpsushi.github.io/jp/posts/3160/","tags":null,"title":"微分幾何学におけるガウスの定理"},{"categories":"기하학","contents":"ビルドアップ $\\mathbf{x} : U \\to \\mathbb{R}^{3}$を座標写像とする。微分幾何学では、幾何学的な対象の特徴や性質を微分を通して説明する。したがって、座標切れ目$\\mathbf{x}$の導関数が色々な定理と公式で登場することになる。例えば、1次の導関数$\\left\\{ \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\}$は接空間$T_{p}M$の基底になる。したがって、任意の接線ベクトル$\\mathbf{X} \\in T_{p}M$は次のように表現することができる。\n$$ \\mathbf{X} = X^{1}\\mathbf{x}_{1} + X^{2}\\mathbf{x}_{2} $$\nそれでは、座標写像の2次の導関数$\\mathbf{x}_{ij} = \\dfrac{\\partial^{2} \\mathbf{x}}{\\partial u_{i} \\partial u_{j}}$について考えてみよう。これは$\\mathbb{R}^{3}$のベクトルなので、$\\mathbb{R}^{3}$の基底の線形組み合わせで表現することができる。でも、すでに$\\mathbb{R}^{3}$で互いに直交する3つのベクトルを知っているけど、それは1次の導関数と単位法線だ。\n$$ \\left\\{ \\mathbf{n}, \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\} $$\nすると、$\\mathbf{x}_{ij}$は次のように表される。\n$$ \\mathbf{x}_{ij} = a_{ij} \\mathbf{n} + b^{1}_{ij} \\mathbf{x}_{1} + b^{2}_{ij} \\mathbf{x}_{2} $$\nこれらの係数$b_{ij}^{1}, b_{ij}^{2}$をクリストッフェル記号という。さて、これらの係数を具体的に求めてみよう。第1基本形式の性質によって以下が成立する。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\left\\langle \\mathbf{x}_{ij}, \\mathbf{x}_{l} \\right\\rangle \u0026amp;=\\ b_{ij}^{1}\\left\\langle \\mathbf{x}_{1}, \\mathbf{x}_{l} \\right\\rangle + b_{ij}^{2}\\left\\langle \\mathbf{x}_{2}, \\mathbf{x}_{l} \\right\\rangle \\\\ \u0026amp;\u0026amp; \u0026amp;=\\ \\sum\\limits_{k^{\\prime}=1}^{2}b_{ij}^{k^{\\prime}}\\left\\langle \\mathbf{x}_{k^{\\prime}}, \\mathbf{x}_{l} \\right\\rangle \\\\ \u0026amp;\u0026amp; \u0026amp;=\\ \\sum\\limits_{k^{\\prime}=1}^{2}b_{ij}^{k^{\\prime}} g_{k^{\\prime}l} \\\\ \\implies \u0026amp;\u0026amp; \\sum\\limits_{l=1}^{2}\\left\\langle \\mathbf{x}_{ij}, \\mathbf{x}_{l} \\right\\rangle g^{lk} \u0026amp;=\\ \\sum\\limits_{l=1}^{2}\\sum\\limits_{k^{\\prime}=1}^{2}b_{ij}^{k^{\\prime}} g_{k^{\\prime}l}g^{lk} \\\\ \u0026amp;\u0026amp; \u0026amp;=\\ \\sum\\limits_{k^{\\prime}=1}^{2}b_{ij}^{k^{\\prime}} \\delta_{k^{\\prime}}^{k} \\\\ \u0026amp;\u0026amp; \u0026amp;=\\ b_{ij}^{k} \\end{align*} $$\nさて、これらの$b_{ij}^{k}$を$\\Gamma_{ij}^{k}$と表記して、次のように定義しよう。\n定義 以下のように定義される$\\Gamma_{ij}^{k}(1\\le i,j,k \\le 2)$をクリストッフェル記号という。\n$$ \\Gamma_{ij}^{k} := \\sum \\limits_{l=1}^{2} \\left\\langle \\mathbf{x}_{ij}, \\mathbf{x}_{l} \\right\\rangle g^{lk} = \\left\\langle \\mathbf{x}_{ij}, \\mathbf{x}_{l} \\right\\rangle g^{lk} $$\n$\\sum$の省略された式はアインシュタインの記法を使用している。\n説明 $\\mathbf{x}_{12} = \\mathbf{x}_{21}$であるから、$\\Gamma_{12}^{k} = \\Gamma_{21}^{k}$である。\n$\\mathbf{x}_{ij}$の接成分$b_{ij}^{k}$を$\\Gamma_{ij}^{k}$と表記し、クリストッフェル記号と呼び、$\\mathbf{x}_{ij}$の法成分$a_{ij}$を$L_{ij}$と表記し、第2基本形式の係数と呼ぶ。\nここで紹介したクリストッフェル記号は具体的には第2クリストッフェル記号である。第1クリストッフェル記号は以下のように定義される。\n$$ \\Gamma_{ij \\vert l} := \\sum \\limits_{k=1}^{2} \\Gamma_{ij}^{k}g_{kl} $$\n通常、クリストッフェル記号と言えば第2記号を指す。これらの記号を初めて使ったのはG.B.クリストッフェルで、当時は第2記号を$\\begin{Bmatrix} ij \\\\ k \\end{Bmatrix}$としていたという。\n参照 第1基本形式 第2基本形式 ","id":3158,"permalink":"https://freshrimpsushi.github.io/jp/posts/3158/","tags":null,"title":"微分幾何学におけるクリストッフェル記号"},{"categories":"줄리아","contents":"コード 1 ヒートマップを描く時、数値に応じて値のスケールが固定されないと困ることがある。基本のヒートマップ関数でclimオプションを通じて色の範囲を固定することができる。\nusing Plots\rcd(@__DIR__)\rheatmap(rand(4,4)); png(\u0026#34;1.png\u0026#34;)\rheatmap(rand(4,4), clim = (0,1)); png(\u0026#34;2.png\u0026#34;) 結果は以下の通りだ。最初のヒートマップは範囲がないが、二番目のヒートマップは0と1で範囲が固定されているのを確認できる。\n一方だけを制限 heatmap(rand(4,4), clim = (0,Inf))\rheatmap(rand(4,4), clim = (-Inf,1)) 上限だけを置くか、下限だけを置きたい場合は、上に示すようにInfを通じて上下限を開けることができる。\n環境 OS: Windows julia: v1.6.0 同じく見る Pythonのmatplotlib.pyplotで https://discourse.julialang.org/t/setting-min-and-max-values-in-a-heatmap-plots-jl/36496\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2126,"permalink":"https://freshrimpsushi.github.io/jp/posts/2126/","tags":null,"title":"ジュリアでヒートマップの色範囲を指定する方法"},{"categories":"확률미분방정식","contents":"定義 1 $$ d X(t) = f \\left( t, X(t) \\right) dt + g \\left( t, X(t) \\right) d W_{t} \\qquad , t \\in \\left[ t_{0} , T \\right], T \u0026gt; 0 $$\n上記の形式の方程式は、確率微分方程式、略してSDEと呼ばれる。ここで、$f$と$g$はそれぞれドリフトDrift、ディフュージョンDiffusionの係数関数と呼ばれる。初期条件$X_{0} := X \\left( t_{0} \\right)$に対する積分形は、次のように表される。\n$$ X(t) = X_{0} + \\int_{t_{0}}^{t} f \\left( s, X (s) \\right) ds + \\int_{t_{0}}^{t} g \\left( s, X (s) \\right) d W_{s} $$\n説明 $$ d X_{t} = f \\left( t, X_{t}\\right) dt + g \\left( t, X_{t} \\right) d W_{t} $$\nこの形が気にならなければ、伊藤の微積分学をよく勉強しているか、微分方程式をほとんど知らないか、のどちらかだと思う。微分方程式には慣れているがSDEには不慣れな人にとって、自然に$g d W_{t}$が目障りになるべきだ。SDEはODEと異なり、このような確率過程が含まれており、モデルに不確実性を加えている。この項を$0$として考え、つまり$g d W_{t} = 0$の非決定論的システムとして見れば、次のようになる。 $$ \\begin{align*} d X(t) =\u0026amp; f \\left( t, X(t) \\right) dt + g \\left( t, X(t) \\right) d W_{t} \\\\ =\u0026amp; f \\left( t, X(t) \\right) dt + 0 \\\\ =\u0026amp; f \\left( t, X(t) \\right) dt \\end{align*} $$ 両辺を$dt$で割ると $$ {{ d X (t) } \\over { dt }} = f \\left( t, X(t) \\right) $$ そのため、私たちがよく知る非自律系の様子を取り戻したことを確認できる。\nドリフト この説明で、時系列解析のドリフトを思い出すと、係数関数$f$をドリフトと呼ぶのはかなり自然である。後ろの項がどうであれ、システム自体をシステムとして扱うことができる原動力は$f dt$だからだ。\nディフュージョン それでは、$g$を拡散Diffusionと呼ぶことは、その役割や性質が広がること、散らばることを自然に連想させる。確率微分方程式では、これは白色雑音の概念を指し、このようなノイズにより伊藤の公式のような独特の結果が生じる。\nPanik. (2017). Stochastic Differential Equations: An Introduction with Applications in Population Dynamics Modeling: p133.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2125,"permalink":"https://freshrimpsushi.github.io/jp/posts/2125/","tags":null,"title":"確率微分方程式とは?"},{"categories":"기하학","contents":"ビルドアップ $\\mathbf{x} : U \\to \\mathbb{R}^{3}$を座標チャートと呼ぼう。微分幾何学では、幾何学的な対象の特徴や性質を微分を通じて説明する。だから、座標チャートの微分$\\mathbf{x}$が色んな定理や公式に登場する。例えば、1次の微分$\\left\\{ \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\}$は接空間$T_{p}M$の基底になる。したがって、任意の接ベクトル$\\mathbf{X} \\in T_{p}M$は以下のように表される。\n$$ \\mathbf{X} = X^{1}\\mathbf{x}_{1} + X^{2}\\mathbf{x}_{2} $$\nそれでは、座標チャートの2次の微分$\\mathbf{x}_{ij} = \\dfrac{\\partial^{2} \\mathbf{x}}{\\partial u_{i} \\partial u_{j}}$について考えてみよう。これは$\\mathbb{R}^{3}$のベクトルなので、$\\mathbb{R}^{3}$の基底の線形組み合わせで表すことができる。だが、私たちは既に$\\mathbb{R}^{3}$で互いに直交する3つのベクトルを知っている。それは1次の微分と単位法線だ。\n$$ \\left\\{ \\mathbf{n}, \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\} $$\nすると$\\mathbf{x}_{ij}$は次のように表すことができる。\n$$ \\mathbf{x}_{ij} = a_{ij} \\mathbf{n} + b^{1}_{ij} \\mathbf{x}_{1} + b^{2}_{ij} \\mathbf{x}_{2} $$\n$\\mathbf{x}_{ij}$の$\\mathbf{n}$項の係数$a_{ij} = \\left\\langle \\mathbf{x}_{ij}, \\mathbf{n} \\right\\rangle$を$\\mathbf{x}$の第2基本形式の係数coefficient of the second fundamental formと呼ぶ。\n定義 $\\mathbf{x}_{ij}$と単位法線$\\mathbf{n}$の内積を$L_{ij}$と表記し、第2基本形式の係数と呼ぶ。\n$$ L_{ij} := \\left\\langle \\mathbf{x}_{ij}, \\mathbf{n} \\right\\rangle $$\n$\\mathbf{X}, \\mathbf{Y}$を曲面$\\mathbf{x}$の接空間$T_{P}M$のベクトルとしよう。接空間の基底は$\\left\\{ \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\}$なので、次のように表すことができる。\n$$ \\mathbf{X} = X^{1}\\mathbf{x}_{1} + X^{2}\\mathbf{x}_{2} \\quad \\text{and} \\quad \\mathbf{Y} = Y^{1}\\mathbf{x}_{1} + Y^{2}\\mathbf{x}_{2} $$\n次のような双線形形式$II$を曲面$\\mathbf{x}$の第2基本形式the second fundamental formと定義する。\n$$ II (\\mathbf{X}, \\mathbf{Y}) = \\sum \\limits_{i=1}^{2} \\sum \\limits_{j=1}^{2} L_{ij}X^{i}Y^{j} = L_{ij}X^{i}Y^{j} = \\begin{bmatrix} X^{1} \u0026amp; X^{2}\\end{bmatrix} \\begin{bmatrix} L_{11} \u0026amp; L_{12} \\\\ L_{21} \u0026amp; L_{22} \\end{bmatrix} \\begin{bmatrix} Y^{1} \\\\ Y^{2}\\end{bmatrix} $$\n$\\sum$が省略された式はアインシュタインの記法を使用したものだ。\n説明 $\\mathbf{x}_{12} = \\mathbf{x}_{21}$なので、$L_{12} = L_{21}$だ。\n$\\mathbf{x}_{ij}$の法線成分normal component$a_{ij}$を$L_{ij}$と表記し、第2基本形式の係数と呼び、$\\mathbf{x}_{ij}$の接成分tangential components$b_{ij}^{k}$を$\\Gamma_{ij}^{k}$と表記し、クリストッフェル記号と呼ぶ。\n第1基本形式が曲面上の曲線の長さに関連する関数であったように、第2基本形式は曲面がどれほど曲がっているかの指標であり、ガウス曲率$\\kappa_{n}$と関連している。\n第1基本形式はリーマン計量という別の名前でも呼ばれるが、第2基本形式は単に第2基本形式と呼ばれる。\n同時参照 第1基本形式 クリストッフェル記号 ","id":3156,"permalink":"https://freshrimpsushi.github.io/jp/posts/3156/","tags":null,"title":"微分幾何学における第2基本形式"},{"categories":"줄리아","contents":"概要 1 Pythonでは、zfill()は文字列クラスのメソッドとして、左側を0で埋める機能を持っている。しかし、Juliaではもっと汎用的で使い勝手の良い組み込み関数としてlpad()を提供している。zfill()はゼロで埋めるって意味で、lpad()は左のパディングって意味だ。\nコード julia\u0026gt; lpad(\u0026#34;12\u0026#34;, 4, \u0026#34;0\u0026#34;)\r\u0026#34;0012\u0026#34;\rjulia\u0026gt; lpad(12, 4, \u0026#34;0\u0026#34;)\r\u0026#34;0012\u0026#34; 概要での説明を続けると、Juliaでのlpad()はzfill()に比べてもっとジェネリックだ。文字列のメソッドではないから、文字列を引数にしても数字を引数にしても、勝手に文字列にして返してくれる。\njulia\u0026gt; lpad(12, 4)\r\u0026#34; 12\u0026#34;\rjulia\u0026gt; lpad(12, 4, \u0026#34;_\u0026#34;)\r\u0026#34;__12\u0026#34;\rjulia\u0026gt; lpad(12, 4, \u0026#34;_!\u0026#34;)\r\u0026#34;_!12\u0026#34;\rjulia\u0026gt; lpad(12, 4, \u0026#34;_?!\u0026#34;)\r\u0026#34;_?12\u0026#34;\rjulia\u0026gt; lpad(12, 7, \u0026#34;_?!\u0026#34;)\r\u0026#34;_?!_?12\u0026#34; こういう関数を使う一般的な理由は、出力をきれいに整えるためで、必ずしも0が必要だからではない。埋める文字を何も与えなければ、スペースを入れて、キャラクターかストリングを与えれば、上に示したように賢く埋めてくれる。\njulia\u0026gt; rpad(\u0026#34;left\u0026#34;, 6, \u0026#39;0\u0026#39;)\r\u0026#34;left00\u0026#34; もちろん、rpad()関数もある。基本的な機能は同じで、右のパディングという点だけが異なる。\n環境 OS: Windows julia: v1.6.0 https://docs.julialang.org/en/v1/base/strings/#Base.lpad\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2124,"permalink":"https://freshrimpsushi.github.io/jp/posts/2124/","tags":null,"title":"ジュリアでzfill()を使う方法"},{"categories":"기하학","contents":"ビルドアップ1 $$ \\left\\{ T(s), N(s), B(s), \\kappa (s), \\tau (s) \\right\\} $$\n曲線を分析するとき、フレネ・セレ装置を使ったのを思い出してほしい。曲面について学ぶときも、同様のものを考えることになる。$\\boldsymbol{\\alpha}$が単位速度曲線のとき、曲線の曲率は加速度の大きさ$\\kappa = \\left| T^{\\prime} \\right| = \\left| \\boldsymbol{\\alpha}^{\\prime \\prime} \\right|$で定義されたんだ。曲面がどれくらい曲がっているかを知るためには、曲面上の曲線がどれくらい曲がっているかを見ることは自然な考え方だ。\n$\\mathbf{x} : U\\subset \\R^{2} \\to M$として与えられた曲面を考えてみよう。$\\boldsymbol{\\alpha}(s)$を単純曲面$\\mathbf{x}$上の単位速度曲線としよう。それでは$\\boldsymbol{\\alpha}$のためのフレネ・セレ装置を次のように表記しよう。\n$$ \\left\\{ \\mathbf{T}, \\mathbf{N}, \\mathbf{B}, \\kappa, \\tau \\right\\} $$\n点$p \\in M$での単位法線を$\\mathbf{n}$としよう。点$p$で$M$に垂直なすべてのベクトルの集合を$N_{p}M$としよう。\n$$ N_{p}M := \\left\\{ r \\mathbf{n} : r \\in \\R \\right\\} = \\left\\{ \\text{all vectors perpendicular to } M \\text{ at } p \\right\\} $$\nそのため、接平面の定義により、$T_{p}M$は$N_{p}M$の直交補空間だ。\n$$ N_{p}M ^{\\perp} = T_{p}M $$\nしたがって、$\\R^{3}$は次のように直交分解され、$\\boldsymbol{\\alpha}^{\\prime \\prime}$は2つの空間のベクトルの線形結合で表すことができる。\n$$ \\R^{3} = N_{p}M \\oplus T_{p}M \\quad \\text{and} \\quad \\boldsymbol{\\alpha}^{\\prime \\prime}(s) = n_{1}\\mathbf{n}(s) + n_{2}\\mathbf{n}^{\\perp}(s)\\quad (\\mathbf{n}\\in N_{p}M,\\ \\mathbf{n}^{\\perp}\\in T_{p}M) $$\n$\\mathbf{T} = \\boldsymbol{\\alpha}^{\\prime}$を接ベクトルとしよう。$\\boldsymbol{\\alpha}$は単位速度ベクトルなので、次の式が成り立つ。\n$$ \\left| \\boldsymbol{\\alpha}^{\\prime}(s) \\right|^{2} = \\left| \\mathbf{T}(s) \\right|^{2} = \\left\\langle \\mathbf{T}, \\mathbf{T} \\right\\rangle = 1 $$\n両辺を微分すると、内積の微分法により次を得る。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\left\\langle \\mathbf{T}, \\mathbf{T} \\right\\rangle^{\\prime} =\u0026amp;\\ 0 \\\\ \\implies \u0026amp;\u0026amp; \\left\\langle \\mathbf{T}^{\\prime}, \\mathbf{T} \\right\\rangle =\u0026amp;\\ 0 \\\\ \\implies \u0026amp;\u0026amp; \\left\\langle \\boldsymbol{\\alpha}^{\\prime \\prime}, \\mathbf{T} \\right\\rangle =\u0026amp;\\ 0 \\end{align*} $$\nしたがって、$\\boldsymbol{\\alpha}^{\\prime \\prime}$は$\\mathbf{T}$と垂直だ。$\\boldsymbol{\\alpha}^{\\prime \\prime}$を分けて書いてみると、$\\mathbf{n}$と$\\mathbf{T}$は互いに垂直なので、次を得る。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\left\\langle \\boldsymbol{\\alpha}^{\\prime \\prime}, \\mathbf{T} \\right\\rangle =\u0026amp;\\ 0 \\\\ \\implies \u0026amp;\u0026amp; \\left\\langle n_{1}\\mathbf{n} + n_{2}\\mathbf{n}^{\\perp}, \\mathbf{T} \\right\\rangle =\u0026amp;\\ 0 \\\\ \\implies \u0026amp;\u0026amp; \\left\\langle n_{1}\\mathbf{n}, \\mathbf{T} \\right\\rangle + \\left\\langle n_{2}\\mathbf{n}^{\\perp}, \\mathbf{T} \\right\\rangle =\u0026amp;\\ 0 \\\\ \\implies \u0026amp;\u0026amp; \\left\\langle n_{2}\\mathbf{n}^{\\perp}, \\mathbf{T} \\right\\rangle =\u0026amp;\\ 0 \\end{align*} $$\nしたがって、$\\mathbf{n}^{\\perp}$は$\\mathbf{n}$と$\\mathbf{T}$の両方と垂直なベクトルであることがわかる。そこで、ベクトル$\\mathbf{S}$を次のように定義しよう。\n$$ \\mathbf{S} := \\mathbf{n}\\times \\mathbf{T} \\quad \\text{and} \\quad \\boldsymbol{\\alpha}^{\\prime \\prime} = n_{1}\\mathbf{n} + s\\mathbf{S} $$\n$\\mathbf{S}$を$\\boldsymbol{\\alpha}$の内在的法線と呼ぶ。\n定義 $\\mathbf{n}$の成分$n_{1}$を単位速度曲線$\\boldsymbol{\\alpha}$の法曲率と呼び、$\\kappa_{n}$と表記する。\n$$ \\kappa_{n} := \\left\\langle \\boldsymbol{\\alpha}^{\\prime \\prime}, \\mathbf{n} \\right\\rangle $$\n$\\mathbf{S}$の成分$s$を単位速度曲線$\\boldsymbol{\\alpha}$の測地曲率と呼び、$\\kappa_{g}$と表記する。\n$$ \\kappa_{g} := \\left\\langle \\boldsymbol{\\alpha}^{\\prime \\prime}, \\mathbf{S} \\right\\rangle $$\nしたがって、次の式が成り立つ。\n$$ \\kappa (s) \\mathbf{N}(s) = \\mathbf{T}^{\\prime}(s) = \\boldsymbol{\\alpha}^{\\prime \\prime}(s) = \\kappa_{n}(s)\\mathbf{n}(s) + \\kappa_{g}(s)\\mathbf{S}(s) $$\n説明 法曲率$\\kappa_{n}$は曲面$M$が$\\R^{3}$でどれくらい曲がっているかを測るのに使われる。測地曲率$\\kappa_{g}$は曲線$\\boldsymbol{\\alpha}$が曲面$M$でどれくらい曲がっているかを測るのに使われる。例えば、測地曲率$\\kappa_{g}$が$0$である曲線は、曲面上の直線、つまり測地線を意味することになる。\n$\\mathbf{n}, \\mathbf{S}$が単位ベクトルであるので、上の定義により次の式が成り立つ。\n$$ \\kappa^{2} = \\kappa_{n}^{2} + \\kappa_{g}^{2} $$\nRichard S. Millman and George D. parker, Elements of Differential Geometry (1977), p102-104\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3154,"permalink":"https://freshrimpsushi.github.io/jp/posts/3154/","tags":null,"title":"ガウス曲率と測地曲率"},{"categories":"확률미분방정식","contents":"まとめ 1 イットウ過程 $\\left\\{ X_{t} \\right\\}_{t \\ge 0}$ が与えられているとする。 $$ d X_{t} = u dt + v d W_{t} $$ 関数 $V \\left( t, X_{t} \\right) = V \\in C^{2} \\left( [0,\\infty) \\times \\mathbb{R} \\right)$ に対して $Y_{t} := V \\left( t, X_{t} \\right)$ としよう。すると $\\left\\{ Y_{t} \\right\\}$ もまたイットウ過程であり、次が成り立つ。 $$ \\begin{align*} d Y_{t} =\u0026amp; V_{t} dt + V_{x} d X_{t} + {{ 1 } \\over { 2 }} V_{xx} \\left( d X_{t} \\right)^{2} \\\\ =\u0026amp; \\left( V_{t} + V_{x} u + {{ 1 } \\over { 2 }} V_{xx} v^{2} \\right) dt + V_{x} v d W_{t} \\end{align*} $$\n$C^{2}$ は二度微分可能で、その導関数が連続である関数のクラスだ。 $\\displaystyle V_{t} = {{ \\partial V } \\over { \\partial t }}$、$\\displaystyle V_{x} = {{ \\partial V } \\over { \\partial X_{t} }}$、$\\displaystyle V_{xx} = {{ \\partial^{2} V } \\over { \\partial X_{t}^{2} }}$ である。 $\\left( d X_{t} \\right)^{2} = d X_{t} \\cdot d X_{t}$ は次のイットウ乗算テーブルに従って計算される。 $$ \\begin{align*} \\left( dt \\right)^{2} =\u0026amp; 0 \\\\ dt d W_{t} =\u0026amp; 0 \\\\ d W_{t} dt =\u0026amp; 0 \\\\ \\left( d W_{t} \\right)^{2} =\u0026amp; dt \\end{align*} $$ したがって、 $$ \\begin{align*} \\left( d X_{t} \\right)^{2} =\u0026amp; \\left( u dt + v d W_{t} \\right) \\left( u dt + v d W_{t} \\right) \\\\ =\u0026amp; u^{2} \\left( dt \\right)^{2} + 2 uv dt d W_{t} + v^{2} \\left( d W_{t} \\right)^ {2} \\\\ =\u0026amp; u^{2} \\cdot 0 + 2 \\cdot 0 + v^{2} dt \\\\ =\u0026amp; v^{2} dt \\end{align*} $$ を得る。 説明 イットウ公式は、イットウ補題またはイットウ連鎖則とも呼ばれ、確率微分方程式全般で非常に重要に使われる定理である。ほとんどすべての計算で日常的に登場するため、連鎖則と呼ばれる価値は十分にある。\n証明はここでは省略されるが、ただ多変数テーラー定理を適用して、高次の項を無視する方法で行われる。\n例 ウィーナー過程でウィーナー過程を積分することは直感的に理解しにくい。形式的にはリーマン積分を思い浮かべた常識通り$\\displaystyle \\int_{0}^{t} W_{s} d W_{s} = {{ 1 } \\over { 2 }} W_{t}^{2}$ のような結果が自然と思われるが、実際に計算してみよう。\nイットウ公式を使う前に、与えられたイットウ過程を $u = 0$、$v = 1$ として $$ d X_{t} = 0 dt + 1 d W_{t} $$ のようにセットすると $X_{t} = W_{t}$ である。ここで、$\\displaystyle Y_{t} := V \\left( t , X_{t} \\right) = {{ X_{t}^{2} } \\over { 2 }}$ とすると $$ \\begin{align*} V_{t} =\u0026amp; {{ \\partial } \\over { \\partial t }} \\left( {{ 1 } \\over { 2 }} W_{t}^{2} \\right) = 0 \\\\ V_{x} =\u0026amp; {{ \\partial } \\over { \\partial W_{t} }} \\left( {{ 1 } \\over { 2 }} W_{t}^{2} \\right) = W_{t} \\\\ V_{xx} =\u0026amp; {{ \\partial^{2} } \\over { \\partial W_{t}^{2} }} \\left( {{ 1 } \\over { 2 }} W_{t}^{2} \\right) = {{ \\partial } \\over { \\partial W_{t} }} W_{t} = 1 \\end{align*} $$ であるため、$u = 0$、$v = 1$ から $$ \\begin{align*} d \\left( {{ W_{t}^{2} } \\over { 2 }} \\right) =\u0026amp; \\left( V_{t} + V_{x} u + {{ 1 } \\over { 2 }} V_{xx} v^{2} \\right) dt + V_{x} v d W_{t} \\\\ =\u0026amp; \\left( 0 + W_{t} \\cdot 0 + {{ 1 } \\over { 2 }} \\cdot 1 \\cdot 1^{2} \\right) dt + W_{t} \\cdot 1 \\cdot d W_{t} \\\\ =\u0026amp; {{ 1 } \\over { 2 }} dt + W_{t} d W_{t} \\end{align*} $$ 微分形を積分形に変えると $$ {{ W_{t}^{2} } \\over { 2 }} = {{ 1 } \\over { 2 }} t + \\int_{0}^{t} W_{s} d W_{s} $$ を整理すると、次を得る。 $$ \\int_{0}^{t} W_{s} d W_{s} = {{ 1 } \\over { 2 }} \\left( W_{t}^{2} - t \\right) $$ 一見すると、リーマン積分にはなかった$1$巻く $-t /2$ が生じ、ごちゃごちゃして不便に見えるかもしれない。しかし、ここで期待値を取ると考えると $$ t = \\text{Var} \\left( W_{t} \\right) = E \\left( W_{t}^{2} \\right) - 0^{2} $$ よって、 $$ E \\left( \\int_{0}^{t} W_{s} d W_{s} \\right) = {{ 1 } \\over { 2 }} \\left( t - t \\right) = 0 $$ のようにきれいに消えることが確認できる。$1$項は単に計算方法の違いで生じるゴミ項ではなく、それなりの意味があるものだ。ウィーナー過程をウィーナー過程で積分したときの期待値は$0$ になるべきで、それに同意するなら、上の結論を直感的に受け入れることができるだろう。\n確率積分 2 $a \u0026lt; b$ であり、$c$ は定数で、$t \u0026gt; 0$ としよう。\n$$ \\begin{align*} \\int_{0}^{t} d W_{s} =\u0026amp; W_{t} \\\\ \\int_{a}^{b} c d W_{s} =\u0026amp; c \\left[ W_{b} - W_{a} \\right] \\end{align*} $$\n上記の2つのケースは通常のリーマン積分と同じ結果を出すが、次はイットウ積分特有の結果を出す。\n$$ \\begin{align*} \\int_{0}^{t} W_{s} d W_{s} =\u0026amp; {{ 1 } \\over { 2 }} W_{t}^{2} - {{ 1 } \\over { 2 }} t \\\\ \\int_{a}^{b} W_{s} d W_{s} =\u0026amp; {{ 1 } \\over { 2 }} \\left[ W_{b}^{2} - W_{a}^{2} \\right] - {{ 1 } \\over { 2 }} (b-a) \\\\ \\int_{0}^{t} s d W_{s} =\u0026amp; t W_{t} - \\int_{0}^{t} W_{s} ds = (t-1) W_{t} \\\\ \\int_{0}^{t} W_{s}^{2} d W_{s} =\u0026amp; {{ 1 } \\over { 3 }} W_{t}^{3} - \\int_{0}^{t} W_{s} ds \\\\ \\int_{0}^{t} e^{W_{s}} d W_{s} =\u0026amp; e^{W_{t}} - 1 - {{ 1 } \\over { 2 }} \\int_{0}^{t} e^{W_{s}} ds \\\\ \\int_{0}^{t} W_{t} e^{W_{s}} d W_{s} =\u0026amp; 1 + W_{t} e^{W_{t}} - e^{W_{t}} - {{ 1 } \\over { 2 }} \\int_{0}^{t} e^{W_{s}} \\left( 1 + W_{s} \\right) d W_{s} \\\\ \\int_{0}^{t} s W_{s} d W_{s} =\u0026amp; {{ t } \\over { 2 }} \\left( W_{t}^{2} - {{ t } \\over { 2 }} \\right) - {{ 1 } \\over { 2 }} \\int_{0}^{t} W_{s}^{2} ds \\\\ \\int_{0}^{t} \\left( W_{s}^{2} - s \\right) d W_{s} =\u0026amp; {{ 1 } \\over { 3 }} W_{t}^{3} - t W_{t} \\\\ \\int_{0}^{t} e^{-s/2 + W_{s}} d W_{s} =\u0026amp; e^{-t/2 + W_{t}} - 1 \\\\ \\int_{0}^{t} \\sin W_{s} d W_{s} =\u0026amp; 1 - \\cos W_{t} - {{ 1 } \\over { 2 }} \\int_{0}^{t} \\cos W_{s} ds \\\\ \\int_{0}^{t} \\cos W_{s} d W_{s} =\u0026amp; \\sin W_{t} + {{ 1 } \\over { 2 }} \\int_{0}^{t} \\sin W_{s} ds \\end{align*} $$\n特に期待値と分散に関しては、次の等式が知られている。\n$$ \\begin{align*} E \\left( \\int_{0}^{t} d W_{s} \\right) =\u0026amp; 0 \\\\ E \\left( \\int_{0}^{t} W_{s} d W_{s} \\right) =\u0026amp; 0 \\\\ \\text{Var} \\left( \\int_{0}^{t} W_{s} d W_{s} \\right) =\u0026amp; {{ t^{2} } \\over { 2 }} \\end{align*} $$\nØksendal. (2003). Stochastic Differential Equations: An Introduction with Applications: p48.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPanik. (2017). Stochastic Differential Equations: An Introduction with Applications in Population Dynamics Modeling: p125.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2121,"permalink":"https://freshrimpsushi.github.io/jp/posts/2121/","tags":null,"title":"伊藤の公式"},{"categories":"기하학","contents":"定義1 2 $\\mathbf{x} : U \\to \\R^{3}$を単純な表面としよう。$U$の座標を$(u, v)$としよう。ある点$(u_{0}, v_{0})$において、以下の曲線を$v = v_{0}$での$\\mathbf{x}$の**$u-$パラメータ曲線**$u-$parameter curveとする。\n$$ u \\mapsto \\mathbf{x}(u, v_{0}) $$\n以下のような曲線を$u = u_{0}$での$\\mathbf{x}$の**$v-$パラメータ曲線**$v-$parameter curveとする。\n$$ v \\mapsto \\mathbf{x}(u_{0}, v) $$\n点$(u_{0}, v_{0})$での二つのパラメータ曲線の速度ベクトル$\\dfrac{\\partial \\mathbf{x}}{\\partial u} = \\mathbf{x}_{u}=\\mathbf{x}_{1}$、$\\dfrac{\\partial \\mathbf{x}}{\\partial v} = \\mathbf{x}_{v}=\\mathbf{x}_{2}$を$(u_{0}, v_{0})$での$\\mathbf{x}$の偏速度ベクトルpartial velocity vectorとする。\n説明 $U$の座標は$(u^{1}, u^{2})$としてもよく書かれるので、上で述べた二つの曲線をそれぞれ$u^{1}$曲線、$u^{2}$曲線ともいう。\n定義により、表面$\\mathbf{x}$はそのようなパラメータ曲線のファミリーによってカバーされていることがわかる。\nこれら二つのパラメータ曲線によって形成される格子を曲線座標系curvilinear coordinate systemという。球座標系や円筒座標系がこれに該当する。\nBarrett O\u0026rsquo;Neill, Elementary Differential Geometry (Revised 2nd Edition, 2006), p139-141\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRichard S. Millman and George D. Parker, Elements of Differential Geometry (1977), p84\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3152,"permalink":"https://freshrimpsushi.github.io/jp/posts/3152/","tags":null,"title":"単純曲面上の媒介変数曲線"},{"categories":"줄리아","contents":"コード propertynames() propertynames()関数で確認するといい1。Juliaにはクラスがなく、構造体だけが存在するから2、この関数で返されるすべてのシンボルは、正確にプロパティだけの名前ということになる。\n次はGraphsパッケージでエルデシュ-レニィネットワークを生成し、ノードの数と各ノードのネイバーフッドを確認するコードである。このネットワークにpropertynames()関数を適用し、:neとfadjlistというプロパティがシンボルとして返された。\njulia\u0026gt; using Graphs\rjulia\u0026gt; G_nm = erdos_renyi(50,200)\r{50, 200} undirected simple Int64 graph\rjulia\u0026gt; propertynames(G_nm)\r(:ne, :fadjlist)\rjulia\u0026gt; G_nm.ne\r200\rjulia\u0026gt; G_nm.fadjlist\r50-element Vector{Vector{Int64}}:\r[3, 4, 11, 25, 26, 27, 33, 40, 44, 48]\r[11, 17, 20, 23, 24, 38, 45, 50]\r[1, 4, 15, 24, 29, 30, 34, 42, 46]\r[1, 3, 18, 24, 30, 32, 43, 45]\r[6, 7, 8, 24, 26, 29, 37, 39, 50]\r⋮\r[3, 13, 17, 28, 29, 32, 39, 44, 47, 49]\r[10, 14, 18, 26, 32, 36, 41, 44, 46]\r[1, 21, 23, 24, 25, 32, 41, 44, 45]\r[9, 13, 14, 17, 21, 31, 43, 46, 50]\r[2, 5, 13, 28, 31, 32, 35, 42, 44, 49] fieldnames() 次は少し難しい話だけど、具体的に知らなくてもJuliaのプログラミングには何の問題もない。\npropertynames(x)は根本的にfieldnames(typeof(x))と同じだと言われている3。実際に使う関数としては大きな意味はないが、これを通じて分かる事実は、Juliaでは構造体StructureのインスタンスInstanceをオブジェクトObjectと呼び、構造体そのものが持っている属性AttributeはフィールドField、そしてそのインスタンスとして実際に存在するオブジェクトの属性はプロパティPropertyと呼ばれることである。\n環境 OS: Windows julia: v1.6.0 https://docs.julialang.org/en/v1/base/base/#Base.propertynames\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://stackoverflow.com/a/56352954\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.julialang.org/en/v1/base/base/#Base.fieldnames\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2120,"permalink":"https://freshrimpsushi.github.io/jp/posts/2120/","tags":null,"title":"ジュリアで構造体の属性を確認する方法"},{"categories":"편미분방정식","contents":"定義 開集合$\\Omega$で定義された偏微分方程式が与えられたとしよう。$\\Omega$の境界である$\\partial \\Omega$で未知数$u$の値が与えられたとする。これを境界条件boundary conditionという。偏微分方程式と境界条件を合わせて境界値問題boundary value problemという。\n説明 略称のBVPがよく使われる。\n境界値問題を解くとは、与えられた偏微分方程式で境界条件を満たす解$u$を見つけることを意味する。\n例 ディリクレ境界条件\n$$ u = 0 \\quad \\text{on } \\partial \\Omega $$\nノイマン境界条件\n$$ \\dfrac{\\partial u}{\\partial \\nu} = 0 \\quad \\text{on } \\partial \\Omega $$\nこの時、$\\nu$は外向き単位法線ベクトルだ。\n混合ディリクレ-ノイマン境界条件mixed boundary conditions1\n$\\partial \\Omega$が二つの異なる閉集合$\\Gamma_{1}$、$\\Gamma_{2}$を含む場合、\n$$ \\begin{align*} u = 0\u0026amp; \\quad \\text{on } \\partial \\Gamma_{1} \\\\ \\dfrac{\\partial u}{\\partial \\nu} = 0\u0026amp; \\quad \\text{on } \\partial \\Gamma_{2} \\end{align*} $$\nロビン境界条件1\n$$ u + \\dfrac{\\partial u}{\\partial \\nu} = 0 \\quad \\text{on } \\partial \\Omega $$\n関連項目 初期値問題(IVP) Lawrence C. Evans, Partial Differential Equations (2nd Edition, 2010), p366\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3151,"permalink":"https://freshrimpsushi.github.io/jp/posts/3151/","tags":null,"title":"偏微分方程式における境界値問題"},{"categories":"확률미분방정식","contents":"定義 1 確率空間 $( \\Omega , \\mathcal{F} , P)$ と フィルトレーション $\\left\\{ \\mathcal{F}_{t} \\right\\}_{t \\ge 0}$ が与えられて、ウィーナープロセス $\\left\\{ W_{t} \\right\\}_{t \\ge 0}$ が $\\mathcal{F}_{t}$-適応していて、$f \\in \\mathcal{L}^{1} [0 , \\infty)$ と $g \\in \\mathcal{L}^{2} [0 , \\infty)$ に対して、以下のような $1$次元連続 $\\mathcal{F}_{t}$-適応 確率過程 $\\left\\{ X_{t} \\right\\}_{t \\ge 0}$ を$1$ 次元 イートープロセスという。 $$ X (t) := X_{0} + \\int_{0}^{t} f(s) ds + \\int_{0}^{t} g(s) d W_{s} $$\n$\\mathcal{L}^{p} (E)$ は、定義域が $E$ である関数たちを集めたルベーグ空間だ。 説明 普通、上の定義そのままでは、積分記号が多くて使いづらいので、確率微分を使って次のように表されることが多い。 $$ d X(t) = f(t) dt + g(t) d W_{t} $$\n一般化 2 $i \\ne j \\implies W_{i} (t) \\perp W_{j}$ 次元ブラウニアン モーション $\\left\\{ \\mathbf{W}_{t} \\right\\}_{t \\ge 0} := \\left( W_{1} (t) , \\cdots , W_{m} (t) \\right)$ が $\\mathcal{F}_{t}$-適応していて $$ \\begin{align*} \\mathbf{f} (t) = \\left( f_{1} (t) , \\cdots , f_{d} (t) \\right) \\in \u0026amp; \\mathcal{L}^{1} \\left( [0, \\infty)^{d} \\right) \\\\ \\mathbf{g} (t) = \\begin{bmatrix} g_{11} (t) \u0026amp; \\cdots \u0026amp; g_{1m} (t) \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ g_{d1} (t) \u0026amp; \\cdots \u0026amp; g_{dm} (t) \\end{bmatrix} \\in \u0026amp; \\mathcal{L}^{2} \\left( [0, \\infty)^{d \\times m} \\right) \\end{align*} $$ ベクトル関数 $\\mathbf{f} : [0, \\infty) \\to \\mathbb{R}^{d}$ と 行列関数 $\\mathbf{g} : [0, \\infty) \\to \\mathbb{R}^{d \\times m}$ に対して、以下のような $d$次元連続 $\\mathcal{F}_{t}$-適応 確率過程 $\\left\\{ \\mathbf{X}_{t} \\right\\}_{t \\ge 0}$ を$d$ 次元 イートープロセスという。 $$ \\mathbf{X} (t) := \\mathbf{X}_{0} + \\int_{0}^{t} \\mathbf{f}(s) ds + \\int_{0}^{t} \\mathbf{g}(s) d \\mathbf{W}_{s} $$ もちろん、これも以下のような確率微分形で書ける。 $$ d \\mathbf{X}(t) = \\mathbf{f}(t) dt + \\mathbf{g}(t) d \\mathbf{W}_{t} $$\nPanik. (2017). Stochastic Differential Equations: An Introduction with Applications in Population Dynamics Modeling: p120.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPanik. (2017). Stochastic Differential Equations: An Introduction with Applications in Population Dynamics Modeling: p127.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2119,"permalink":"https://freshrimpsushi.github.io/jp/posts/2119/","tags":null,"title":"伊藤過程"},{"categories":"그래프이론","contents":"定義 1 2 簡単な定義 リンクがそれぞれ独立に確率 $p \\in [0,1]$ に従って接続されるシンプルネットワークのランダムネットワークを、ギルバートモデルGilbert Model $\\mathbb{G}_{n,p}$ と呼ぶ。\n難しい定義 確率空間 $( \\Omega , \\mathcal{F} , P)$ が与えられ、$n$ 個のラベル付けされたLabeledノードを持つネットワークのプロパティ $2^{\\binom{n}{2}} \\subseteq 2^{\\binom{n}{2}}$ があるとする。リンクの数 $0 \\le m \\le \\binom{n}{2}$ に対して、次の確率質量関数を持つ$\\mathcal{F}$-可測関数 $\\mathbb{G}_{n,p} : \\Omega \\to 2^{\\binom{n}{2}}$ をギルバートモデルと呼ぶ。 $$ P \\left( \\mathbb{G}_{n,p} = G \\right) = p^{m} \\left( 1 - p \\right)^{\\binom{n}{2} - m} \\qquad , \\forall G \\in \\mathscr{G}_{n,m} \\subset 2^{\\binom{n}{2}} $$\n説明 ギルバートモデルはエルデシュ–レーニモデル (ERモデル)とともに多くの文献で言及されているが、実際にはERモデルと言いながらギルバートモデルを使用していることが非常に多い。両モデル間に違いがないわけではない。実際に深く掘り下げると、完全に違うモデルで、分厚い本一冊が出るほどだが、特に応用ネットワーク理論の文脈ではこれらをERモデルとしてひとくくりにする傾向が強い。一般的には使われない表現だが、確率分布が二項分布であるという意味で二項ランダムネットワークBinomial Random Networkとも呼べる。\n$\\mathbb{G}_{n,p}$ は大体 $\\displaystyle m = \\binom{n}{2} p \\approx {{ n^{2} p } \\over { 2 }}$ 個くらいのリンクを持つと予想される。ERモデルがちょうど $m$ 個のリンクを持つのと違い、ギルバートモデルはネットワークの大きさに係数 $p$ に比例するだけでリンクの数が定まっているわけではない。\n定義は難しく書かれているから難しそうに見えるが、アルゴリズムを読んでみると実は簡単だ。ギルバートモデルでネットワークをサンプリングするアルゴリズムは以下の通り。\nアルゴリズム 入力\nノードの数$n \\in \\mathbb{N}$とリンク確率$p \\in \\left( 0, 1 \\right)$が与えられているとする。\nステップ1. 初期化\n$n$個のノードを持つヌルグラフ $G$ を作る。\nステップ2. ベルヌーイ試行\nfor $i = 1 , \\cdots , n$\nfor $j = 1 , \\cdots , n$\nif $i \\ne j$\n$p$ の確率で次のようにネットワーク $G$ にリンク $ij$ を追加する。$$G \\gets G + ij$$\n出力\nノードが$n$個でリンクが$m \\approx n^{2} p / 2$個くらいあるシンプルグラフ$G \\in 2^{\\binom{n}{2}}$を得る。\n性質 [1]: ギルバートモデル $\\mathbb{G}_{n,p}$ からサンプリングされたネットワークのリンク数が正確に $m$ である必要がある場合は、$\\mathbb{G}_{n,m}$ からサンプリングされたものと同じである。 [2] 度数分布: $\\lambda = np$ のとき、$n \\to \\infty$ で各ノードの度数はポアソン分布 $\\displaystyle \\text{Poi} \\left( np \\right)$ に収束する。 $$ P \\left( \\deg v = k \\right) \\to {{ e^{-\\lambda} \\lambda^{k} } \\over { k! }} \\qquad , \\lambda \\approx np $$ 証明 [1] エルデシュ-レーニモデル: 次のような確率質量関数を持つランダムグラフをエルデシュ–レーニグラフErdős–Rényi Graph $\\mathbb{G}_{n,m}$ と呼ぶ。 $$ P \\left( \\mathbb{G}_{n,m} = G \\right) = \\binom{\\binom{n}{2}}{m}^{-1} \\qquad , \\forall G \\in \\mathscr{G}_{n,m} $$\nClaim: 任意の$G_{0} \\in \\mathscr{G}_{n,m}$を考えた時、次を示せば良い。 $$ P \\left( \\mathbb{G}_{n,p} = G_{0} | \\mathbb{G}_{n,p} \\in \\mathscr{G}_{n,m} \\right) = \\binom{\\binom{n}{2}}{m}^{-1} $$\n$\\mathbb{G}_{n,p}$が正確に$G_{0}$であるよりも$\\mathbb{G}_{n,p} \\in \\mathscr{G}_{n,m}$である可能性、つまりギルバートモデルでサンプリングしたけれども、リンクが全部で$m$個である可能性の方が大きい。これを事件として表すと $$ \\begin{align*} \\left\\{ \\mathbb{G}_{n,p} = G_{0} \\right\\} \u0026amp; \\subset \\left\\{ \\mathbb{G}_{n,p} \\in \\mathscr{G}_{n,m} \\right\\} \\\\ \\implies \\left\\{ \\mathbb{G}_{n,p} = G_{0} \\right\\} \u0026amp; \\cap \\left\\{ \\mathbb{G}_{n,p} \\in \\mathscr{G}_{n,m} \\right\\} = \\left\\{ \\mathbb{G}_{n,p} = G_{0} \\right\\} \\qquad \\cdots (\\star) \\end{align*} $$ ここで、各確率を計算すると$\\displaystyle P \\left( \\mathbb{G}_{n,p} = G_{0} \\right) = p^{m} \\left( 1 - p \\right)^{\\binom{n}{2} - m}$であるため、 $$ \\begin{align*} \u0026amp; P \\left( \\mathbb{G}_{n,p} \\in \\mathscr{G}_{n,m} \\right) \\\\ =\u0026amp; P \\left( \\bigcup_{G \\in \\mathscr{G}_{n,m}} \\left\\{ \\mathbb{G}_{n,p} = G \\right\\} \\right) \\\\ =\u0026amp; \\sum_{G \\in \\mathscr{G}_{n,m}} P \\left( \\mathbb{G}_{n,p} = G \\right) \\\\ =\u0026amp; \\binom{\\binom{n}{2}}{m} p^{m} \\left( 1 - p \\right)^{\\binom{n}{2} - m} \\end{align*} $$ である。これで条件付き確率を計算すると、 $$ \\begin{align*} P \\left( \\mathbb{G}_{n,p} = G_{0} | \\mathbb{G}_{n,p} \\in \\mathscr{G}_{n,m} \\right) =\u0026amp; {{ P \\left( \\mathbb{G}_{n,p} = G_{0} \\land \\mathbb{G}_{n,p} \\in \\mathscr{G}_{n,m} \\right) } \\over { P \\left( \\mathbb{G}_{n,p} \\in \\mathscr{G}_{n,m} \\right) }} \\\\ =\u0026amp; {{ P \\left( \\mathbb{G}_{n,p} = G_{0} \\right) } \\over { P \\left( \\mathbb{G}_{n,p} \\in \\mathscr{G}_{n,m} \\right) }} \u0026amp; \\because (\\star) \\\\ =\u0026amp; {{ p^{m} \\left( 1 - p \\right)^{\\binom{n}{2} - m} } \\over { \\binom{\\binom{n}{2}}{m} p^{m} \\left( 1 - p \\right)^{\\binom{n}{2} - m} }} \\\\ =\u0026amp; \\binom{\\binom{n}{2}}{m}^{-1} \\end{align*} $$ を示すことができる。ここで、$\\land$は論理積である。\n[2] $n$個のノードが与えられ、各ノードが接続される確率$p$が与えられており、ノードの度数は残り$(n-1)$個の他のノードと接続されているかどうかに依るので、二項分布 $B(n-1,p)$に従うことになる。数式できれいに書くと、次のようになる。 $$ P \\left( \\deg v = k \\right) = \\binom{n-1}{k} p^{k} \\left( 1 - p \\right)^{n-1-k} $$\n二項分布の極限分布としてのポアソン分布: $X_{n} \\sim B(n,p)$とする。\n$\\lambda \\approx np$ならば、 $$ X_{n} \\overset{D}{\\to} \\text{Poi} (\\lambda) $$\nこの補題により、十分に大きいギルバートネットワークのノードの度数はポアソン分布に従う。\n■\n可視化 $n = 50$, $p = 12\\%$ の時にサンプリングされたERネットワークとそのノードの度数のヒストグラムである。\nコード JuliaのGraphs, GraphRecipes パッケージを使用したコード。\ncd(@__DIR__); pwd()\r@time using Graphs\r@time using Plots\r@time using GraphRecipes\rn = 50\rG_np = erdos_renyi(n, 6/n, seed = 0)\rplot_G_np = graphplot(G_np, title = \u0026#34;n = 50, p = 12%\u0026#34;, curves=false, size = (300,300), node_weights = degree(G_np).^2,\rnode_shape = :rect, nodecolor = :black, nodestrokealpha = 0.5, edgecolor = :black)\rpng(plot_G_np, \u0026#34;plot_G_np.png\u0026#34;)\rhistogram_G_np = histogram(degree(G_np), title = \u0026#34;Histogram of Degree\u0026#34;,\rcolor = :black, alpha = 0.8, label = :none, size = (300,300))\rpng(histogram_G_np, \u0026#34;histogram_G_np.png\u0026#34;) 一般化 チョン-ルーフィットネスモデル Gilbert. (1959). Random Graphs\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFrieze. (2015). Introduction to Random Graphs: p3.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2118,"permalink":"https://freshrimpsushi.github.io/jp/posts/2118/","tags":["줄리아"],"title":"ギルバートモデル"},{"categories":"기하학","contents":"ビルドアップ リーマン計量は、表面上の曲線の長さを計算する過程から出てくる概念であり、その過程は次の通りです。\n$\\boldsymbol{\\alpha}(t)$を単純曲面$\\mathbf{x} : U \\to \\mathbb{R}^{3}$上を動く正則曲線としよう。$(u_{1}, u_{2})$を$U$の座標としよう。すると、$\\boldsymbol{\\alpha}$は次のように表される。\n$$ \\boldsymbol{\\alpha}(t) = \\mathbf{x}(u_{1}(t), u_{2}(t)) $$\nこの時点で、$a \\le t \\le b$での$\\boldsymbol{\\alpha}$の長さは次のように定義される。\n$$ \\int_{a}^{b} \\left| \\dfrac{d \\boldsymbol{\\alpha}}{d t} \\right| dt $$\n被積分関数を展開すると、次のようになる。\n$$ \\begin{align*} \\left| \\dfrac{d \\boldsymbol{\\alpha}}{d t} \\right| =\u0026amp;\\ \\sqrt{\\left\\langle \\dfrac{d \\boldsymbol{\\alpha}}{d t} , \\dfrac{d \\boldsymbol{\\alpha}}{d t} \\right\\rangle} \\\\ =\u0026amp;\\ \\sqrt{\\left\\langle \\dfrac{d \\mathbf{x}(u_{1}, u_{2})}{d t} , \\dfrac{d \\mathbf{x}(u_{1}, u_{2})}{d t} \\right\\rangle} \\end{align*} $$\n連鎖律により、\n$$ \\begin{align*} \\left| \\dfrac{d \\boldsymbol{\\alpha}}{d t} \\right| =\u0026amp;\\ \\sqrt{\\left\\langle \\dfrac{\\partial \\mathbf{x}}{\\partial u_{1}}\\dfrac{d u_{1}}{dt} + \\dfrac{\\partial \\mathbf{x}}{\\partial u_{2}}\\dfrac{d u_{2}}{dt}, \\dfrac{\\partial \\mathbf{x}}{\\partial u_{1}}\\dfrac{d u_{1}}{dt} + \\dfrac{\\partial \\mathbf{x}}{\\partial u_{2}}\\dfrac{d u_{2}}{dt} \\right\\rangle} \\\\ =\u0026amp;\\ \\sqrt{\\left\\langle \\mathbf{x}_{1}\\dfrac{d u_{1}}{dt} + \\mathbf{x}_{2}\\dfrac{d u_{2}}{dt}, \\mathbf{x}_{1}\\dfrac{d u_{1}}{dt} + \\mathbf{x}_{2}\\dfrac{d u_{2}}{dt} \\right\\rangle} \\end{align*} $$\nこの時点で、$\\mathbf{x}_{1} := \\dfrac{\\partial \\mathbf{x}}{\\partial u_{1}}, \\mathbf{x}_{2} := \\dfrac{\\partial \\mathbf{x}}{\\partial u_{2}}$だ。内積を展開して整理すると、\n$$ \\begin{align*} \u0026amp; \\left| \\dfrac{d \\boldsymbol{\\alpha}}{d t} \\right| \\\\ =\u0026amp;\\ \\sqrt{\\left\\langle \\mathbf{x}_{1}\\dfrac{d u_{1}}{dt} + \\mathbf{x}_{2}\\dfrac{d u_{2}}{dt}, \\mathbf{x}_{1}\\dfrac{d u_{1}}{dt} + \\mathbf{x}_{2}\\dfrac{d u_{2}}{dt} \\right\\rangle} \\\\ =\u0026amp;\\ \\sqrt{\\left( \\dfrac{d u_{1}}{dt} \\right)^{2} \\left\\langle \\mathbf{x}_{1}, \\mathbf{x}_{1} \\right\\rangle + \\dfrac{d u_{1}}{dt}\\dfrac{d u_{2}}{dt} \\left\\langle \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\rangle + \\dfrac{d u_{2}}{dt}\\dfrac{d u_{1}}{dt} \\left\\langle \\mathbf{x}_{2}, \\mathbf{x}_{1} \\right\\rangle + \\left( \\dfrac{d u_{2}}{dt} \\right)^{2} \\left\\langle \\mathbf{x}_{2}, \\mathbf{x}_{2} \\right\\rangle} \\end{align*} $$\nここで、上記の内積を$g_{ij} = \\left\\langle \\mathbf{x}_{i}, \\mathbf{x}_{j} \\right\\rangle$と表し、$\\sum$と整理すると、次のようになる。\n$$ \\begin{align*} \\left| \\dfrac{d \\boldsymbol{\\alpha}}{d t} \\right| =\u0026amp;\\ \\sqrt{ \\sum \\limits_{i=1}^{2}\\sum \\limits_{j=1}^{2} g_{ij} \\dfrac{d u_{i}}{dt}\\dfrac{d u_{j}}{dt}} \\\\ =\u0026amp;\\ \\sqrt{ g_{ij} \\dfrac{d u_{i}}{dt}\\dfrac{d u_{j}}{dt}} \\end{align*} $$\n二番目の等号で、アインシュタインの記法を使用して和記号を省略した。\n定義1 $g_{ij} = \\left\\langle \\mathbf{x}_{i}, \\mathbf{x}_{j} \\right\\rangle$は、リーマン計量の係数the coefficient of the Riemannian metricまたは第一基本形式the first fundamental formの係数と呼ばれる。\n$M$を$\\mathbb{R}^{3}$での曲面、$p \\in M$としよう。$\\mathbf{X}, \\mathbf{Y}$を$p$での接ベクトルとしよう。すると、$M$の固有切断写像$\\mathbf{x} : U \\to \\mathbb{R}^{3}$に対して次のように表される。\n$$ \\mathbf{X} = X^{1}\\mathbf{x}_{1} + X^{2}\\mathbf{x}_{2} \\quad \\text{and} \\quad \\mathbf{Y} = Y^{1}\\mathbf{x}_{1} + Y^{2}\\mathbf{x}_{2} $$\n次のような双線形形式$I$を曲面$\\mathbf{x}$のリーマン計量Riemannian metricまたは第一基本形式the first fundamental formと定義する。\n$$ I : T_{p}M \\times T_{p}M \\to \\mathbb{R} $$\n$$ I (\\mathbf{X}, \\mathbf{Y}) = \\sum \\limits_{i=1}^{2} \\sum \\limits_{j=1}^{2} g_{ij}X^{i}Y^{j} = g_{ij}X^{i}Y^{j} = \\begin{bmatrix} X^{1} \u0026amp; X^{2}\\end{bmatrix} \\begin{bmatrix} g_{11} \u0026amp; g_{12} \\\\ g_{21} \u0026amp; g_{22} \\end{bmatrix} \\begin{bmatrix} Y^{1} \\\\ Y^{2}\\end{bmatrix} $$\n係数の行列$\\left[ g_{ij} \\right]$の行列式を$g$と表記する。\n$$ g := \\det (\\left[ g_{ij} \\right]) = \\begin{vmatrix} g_{11} \u0026amp; g_{12} \\\\ g_{21} \u0026amp; g_{22}\\end{vmatrix} = g_{11}g_{22} - g_{12}g_{21} $$\n行列$\\left[ g_{ij} \\right]$の逆行列の$(k,l)$成分を$g^{kl}$と表記する。\n$$ \\begin{align*} \\begin{pmatrix} g_{11} \u0026amp; g_{12} \\\\ g_{21} \u0026amp; g_{22} \\end{pmatrix} ^{-1} =\u0026amp;\\ \\dfrac{1}{\\det \\left[ g_{ij} \\right]} \\begin{pmatrix} g_{22} \u0026amp; - g_{21} \\\\ g_{12} \u0026amp; g_{22} \\end{pmatrix} = \\dfrac{1}{g} \\begin{pmatrix} g_{22} \u0026amp; - g_{21} \\\\ g_{12} \u0026amp; g_{22} \\end{pmatrix} \\\\[1em] =\u0026amp;\\ \\begin{pmatrix}\\dfrac{g_{22}}{g} \u0026amp; - \\dfrac{g_{21}}{g} \\\\[1em] -\\dfrac{g_{12}}{g} \u0026amp; \\dfrac{g_{11}}{g} \\end{pmatrix} \\\\[1em] =\u0026amp;\\ \\begin{pmatrix} g^{11} \u0026amp; g^{12} \\\\[1em] g^{21} \u0026amp; g^{22} \\end{pmatrix} \\end{align*} $$\n説明 最近は第一基本形式という言葉はほとんど使われず、リーマン計量という言葉だけが主に使われるという。計量という名前がついたのは、ビルドアップで見たように、表面上の曲線の長さを測るために使うからである。\n$E = g_{11}$、$F=g_{21}=g_{12}$、$G=g_{22}$のような表記も多く使われる。\n曲線理論では登場しなかったリーマン計量という概念が出てくる理由は、接空間の基底$\\left\\{ \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\}$が一般的に正規直交基底ではないからである。正規直交基底ならば$g_{ij} = \\delta_{ij}$であり、意味がない。ここで、$\\delta$はクロネッカーデルタである。リーマン計量とアインシュタインの記法を使用して、表面上の曲線$\\boldsymbol{\\alpha}$の長さを表すと、次のようになる。\n$$ \\begin{align*} L (\\boldsymbol{\\alpha}) =\u0026amp;\\ \\text{length of } \\boldsymbol{\\alpha} \\\\ =\u0026amp;\\ \\int_{a}^{b} \\sqrt{ g_{ij} \\dfrac{d u_{i}}{dt}\\dfrac{d u_{j}}{dt}} dt \\\\ =\u0026amp;\\ \\int_{a}^{b} \\sqrt{ g_{ij} \\alpha_{i}^{\\prime} \\alpha_{j}^{\\prime} } dt \\\\ =\u0026amp;\\ \\int_{a}^{b} \\sqrt{ E\\left( \\dfrac{d u_{1}}{dt} \\right)^{2} + 2F\\dfrac{d u_{1}}{dt}\\dfrac{d u_{2}}{dt} + G\\left( \\dfrac{d u_{2}}{dt} \\right)^{2}} dt \\end{align*} $$\n表面の面積も、リーマン計量の積分によって定義される。\n単純曲面$\\mathbf{x}$上のある領域$R$について、$Q = \\mathbf{x}^{-1}(R)$とする。つまり、$Q \\subset U \\subset \\R^{2}$である。すると、$R$の面積は次のようになる。 $$ \\text{area of } R = \\iint _{Q} \\sqrt{g} du_{1}du_{2} = \\iint _{Q} \\left| \\mathbf{x}_{1} \\times \\mathbf{x}_{2} \\right| du_{1}du_{2} = \\iint _{Q} \\sqrt{EG-F^{2}} du_{1}du_{2} $$\n性質 単純曲面$\\mathbf{x} : U \\to \\mathbb{R}^{3}$に対して、\n(a) $g = \\left| \\mathbf{x}_{1} \\times \\mathbf{x}_{2} \\right|^{2}$\n(b) $g^{11} = \\dfrac{g_{22}}{g} \\quad \\text{and} \\quad g^{12} = g^{21} = -\\dfrac{g_{12}}{g} \\quad \\text{and} \\quad g^{22} = \\dfrac{g_{11}}{g}$\n(c) $\\forall i,j$、$\\sum \\limits_{k=1}^{2} g_{ik}g^{kj} = {\\delta_{i}}^{j}$\nここで、$\\delta$はクロネッカーデルタである。\n証明 (a) 外積の性質とリーマン計量の定義により、次が成立する。\n$$ \\begin{align*} \\left| \\mathbf{x}_{1} \\times \\mathbf{x}_{2} \\right|^{2} =\u0026amp;\\ \\left| \\mathbf{x}_{1} \\right|^{2} \\left| \\mathbf{x}_{2} \\right|^{2} \\sin ^{2} \\theta \\\\ =\u0026amp;\\ \\left| \\mathbf{x}_{1} \\right|^{2} \\left| \\mathbf{x}_{2} \\right|^{2}\\left(1- \\cos ^{2} \\theta \\right) \\\\ =\u0026amp;\\ \\left| \\mathbf{x}_{1} \\right|^{2} \\left| \\mathbf{x}_{2} \\right|^{2}\\left(1- \\dfrac{\\mathbf{x}_{1} \\cdot \\mathbf{x}_{2}}{\\left| \\mathbf{x}_{1} \\right| \\left| \\mathbf{x}_{2} \\right| } \\right) \\\\ =\u0026amp;\\ \\left| \\mathbf{x}_{1} \\right|^{2} \\left| \\mathbf{x}_{2} \\right|^{2} - \\left( \\mathbf{x}_{1} \\cdot \\mathbf{x}_{2} \\right)^{2} \\\\ =\u0026amp;\\ g_{11}g_{22} - g_{12}g_{21} \\\\ =\u0026amp;\\ \\det( [g_{ij}] ) \\\\ =\u0026amp;\\ g \\end{align*} $$\n■\n(b) 定義どおりだ。\n■\n(c) $[g^{kl}]$が$[g_{ij}]$の逆行列であるので、自然に成立する。\n$$ \\begin{align*} \\begin{pmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{pmatrix} =\u0026amp;\\ \\begin{pmatrix} g_{11} \u0026amp; g_{12} \\\\ g_{21} \u0026amp; g_{22} \\end{pmatrix} \\begin{pmatrix} g^{11} \u0026amp; g^{12} \\\\ g^{21} \u0026amp; g^{22} \\end{pmatrix} \\\\[1em] =\u0026amp;\\ \\begin{pmatrix} g_{11}g^{11}+g_{12}g^{21} \u0026amp; g_{11}g^{12} + g_{12}g^{22} \\\\[1em] g_{21}g^{11} + g_{22}g^{21} \u0026amp; g_{21}g^{12} + g_{22}g^{22} \\end{pmatrix} \\end{align*} $$\n■\n関連項目 第二基本形式 クリストッフェル記号 リーマン計量 Richard S. Millman and George D. Parker, Elements of Differential Geometry (1977), p93-96\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3148,"permalink":"https://freshrimpsushi.github.io/jp/posts/3148/","tags":null,"title":"第1 基本形式、リーマン計量"},{"categories":"함수","contents":"定義 符号関数sign function $\\mathrm{sgn} : \\mathbb{R} \\to \\mathbb{R}$は以下のように定義される。\n$$ \\mathrm{sgn}(x) :=\\begin{cases} 1 \u0026amp; x\u0026gt;0 \\\\ 0 \u0026amp; x=0 \\\\ -1 \u0026amp; x\u0026lt;0 \\end{cases} $$\n説明 式や定義を簡単に記述するために主に使用される。$\\mathrm{sign}$とも書く。\n関連項目 複素数の符号 ","id":3147,"permalink":"https://freshrimpsushi.github.io/jp/posts/3147/","tags":null,"title":"符号関数"},{"categories":"기하학","contents":"定義1 $M \\subset \\R^{3}$という$P \\in M$の全ての点に対して、イメージ$\\mathbf{x}(U)$が$P$のある$\\epsilon-$近傍$N_{p}$を含むようにする$C^{k}$ 微分同相写像$\\mathbf{x} : U \\subset \\R^{2} \\to M$が存在する場合、$M$を $\\R^{3}$の$C^{k}$ 曲面surfaceと呼ぶ。\nさらに、そのような二つの微分同相写像 $\\mathbf{x} : U \\to \\R^{3}$と$\\mathbf{y} : V \\to \\R^{3}$に対して、\n$$ \\mathbf{y}^{-1} \\circ \\mathbf{x} : \\mathbf{x}^{-1}\\left( \\mathbf{x}(U) \\cap \\mathbf{y}(V) \\right) \\to \\mathbf{y}^{-1}\\left( \\mathbf{x}(U) \\cap \\mathbf{y}(V) \\right) $$\nは$C^{k}$ 座標変換である。\n説明 $\\R^{3}$の曲面とは、端的に言えば、単純曲面のイメージをうまく合わせたものだ。\n多くの定義がそうであるように、曲面かどうかを定義だけで判断することは簡単ではない。曲面を判定するにあたって、以下のような定理がある。\n定理2 微分可能な関数$g : \\R^{3} \\to \\R$と定数$c \\in \\R$が与えられたとする。集合$M = \\left\\{ (x,y,z) : g(x,y,z) = c \\right\\}$に対して、$M$のある点で\n$$ dg = \\dfrac{\\partial g}{\\partial x}dx + \\dfrac{\\partial g}{\\partial y}dy + \\dfrac{\\partial g}{\\partial z}dz \\ne 0 $$\nが成り立つなら、$M$は曲面である。\nRichard S. Millman and George D. Parker, Elements of Differential Geometry (1977), p89\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBarrett O\u0026rsquo;Neill, Elementary Differential Geometry (Revised 2nd Edition, 2006), p133-134\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3146,"permalink":"https://freshrimpsushi.github.io/jp/posts/3146/","tags":null,"title":"微分幾何学における曲面の定義"},{"categories":"그래프이론","contents":"定義 簡単な定義 非決定論的な手続きで作られるか、ある確率分布に従って表現されるグラフをランダムグラフRandom Graphという。\n難しい定義 確率空間 $( \\Omega , \\mathcal{F} , P)$ が与えられ、$2^{\\binom{n}{2}}$ が $n$ 個の頂点を持つラベル付けされたグラフをすべて集めたグラフファミリーを表すとする。\n$\\mathcal{F}$-可測な関数 $\\mathbb{G} : \\Omega \\to 2^{\\binom{n}{2}}$ をランダムグラフRandom Graphという。つまり、$\\mathbb{G}$ はすべての(グラフの)ボレルセット $B \\in 2^{\\binom{n}{2}}$ に対して $\\mathbb{G}^{-1} \\left( B \\right) \\in \\mathcal{F}$ を満たす関数である。\n説明 21世紀に入りネットワーク理論がますます多くの分野に応用される中で、ランダムネットワークRandom Networkに関する議論も活発になっている。統計学で確率変数がデータを意味するように、応用数学ではランダムネットワークは、我々が望むかどうかにかかわらず、今手にしているあるデータの構造を意味することになる。\n例えば、学科で各学生をノードとし、SNSのフォロー状態をリンクとしてネットワークを作ると、まさにランダムネットワークの良い例になる。ノードを繋ぐルールは明確だが、実際に調査(サンプリング)をする前にはこのネットワークの形を正確に知ることはできない。典型的な構造はあるだろうが、学科を変えて調査するたびに、偶然によって異なるネットワークを得ることになるだろう。\nしかし、この説明は数学的に考えるにはあまりにもナイーブNaiveである。難しい定義では、ランダムグラフは確率変数が定義されるのと正確に同じ方法で、測度論を使用する確率論で定義されている。結局のところ、Random Variableは現実の中の実験や試行のような$\\omega \\in \\Omega$ をVariableにマッピングする関数であり、Random GraphはそれをGraphにマッピングする関数である。\n手で直接書いてみると、もっと理解しやすい。特定のグラフ $G_{0}$ があるとして、ランダムに作られたグラフが $G_{0}$ である確率が $3/4$ だとすれば、次のように書けばいい。 $$ P \\left( \\mathbb{G} = G_{0} \\right) = {{ 3 } \\over { 4 }} $$ 上記の数式での事象は $\\left\\{ \\mathbb{G} = G_{0} \\right\\} \\in \\mathcal{F}$ である。確率という関数の中に入るものはいつも事象である点を覚えておこう。ランダムグラフは、一般的な確率論でのただのランダムエレメントRandom Elementであり、値域がグラフファミリーであることだけが違う。\n例 $n$ 個のラベル付けされた頂点と$m$ 個のエッジを持つシンプルグラフというプロパティ $\\mathscr{G}_{n,m} \\subset 2^{\\binom{n}{2}}$ を考えよう。\n正確に$m$ 個のリンクを持つランダムグラフは$\\mathbb{G}_{n, m} : \\Omega \\to \\mathscr{G}_{n,m}$ のように表せる。このように作られるグラフは、$n$ 個のノードと$m$ 個のリンクを持っていれば、誰がどのような確率で作ったとしても、最終的には関係ない。つまり、まだ確率分布が与えられていない。ここで私たちが最も簡単に考えることができる確率分布は、全員が同じ確率を持つ均一分布である。\nシンプルグラフは異なるノードを結ばなければならないので、$n$ 個のノードの中から$2$ 個を選んで$\\binom{n}{2}$ 個のリンクを持つことができ、その中でも特に$m$ 個のリンクを選ぶと$\\displaystyle \\left| \\mathscr{G}_{n,m} \\right| = \\binom{\\binom{n}{2}}{m}$ になる。したがって、すべての$G \\in \\mathscr{G}_{n,m}$ に対して $$ P \\left( \\mathbb{G}_{n,m} = G \\right) = \\binom{\\binom{n}{2}}{m}^{-1} $$ とすれば、$n$ 個のノードと$m$ 個のリンクを持つすべてのグラフが選ばれる確率が等しくなる。このような確率分布を持つランダムグラフ$\\mathbb{G}_{n, m}$ がその名も有名なエルデシュ・レーニモデルである。\n","id":2114,"permalink":"https://freshrimpsushi.github.io/jp/posts/2114/","tags":null,"title":"ランダムグラフ"},{"categories":"논문작성","contents":"アルファ $\\Alpha, \\alpha$ アルファalphaと読む。TeXコードはそれぞれ\\Alpha、\\alpha\nギリシャ文字の最初の文字で、「アルファでありオメガ」は「始まりであり終わり」という意味である。\nインデックス集合のインデックス $\\alpha$ 微分幾何学での曲線 $\\alpha$ 微分多様体上の接ベクトルを定義するときに使われる曲線 $\\alpha$ ベータ $\\Beta, \\beta$ ベータbetaと読む。TeXコードはそれぞれ\\Beta、\\beta\nベータ関数 $B$ ガンマ $\\Gamma, \\gamma$ ガンマgammaと読む。TeXコードはそれぞれ\\Gamma、\\gamma\nガンマ関数 $\\Gamma$ 微分幾何学での曲線 $\\gamma$ クリストッフェル記号 $\\Gamma_{ij}^{k}$ デルタ $\\Delta, \\delta$ デルタdeltaと読む。TeXコードはそれぞれ\\Delta、\\delta\n微積分学で$x$の非常に小さな変化量 $\\Delta x$ 物理学でのラプラシアン $\\Delta$ 偏微分方程式でのラプラシアン $\\Delta$ 数学で非常に小さい正数 $\\delta$ : $\\epsilon - \\delta$ 論法 ディラックのデルタ関数 $\\delta$ クロネッカーのデルタ $\\delta_{ij}$ イプシロン $\\Epsilon, \\epsilon, \\varepsilon$ イプシロンepsilonと読む。TeXコードはそれぞれ\\Epsilon、\\epsilon、\\varepsilon\nイプシロンが正しい発音だが、エプシロンと読まれることが多い。\n数学で非常に小さい正数 $\\epsilon$ : $\\epsilon - \\delta$ 論法 電磁気学での誘電率 $\\epsilon$ レビ-チビタ記号 $\\epsilon_{ijk}$ ゼータ $\\Zeta, \\zeta$ ゼータzetaと読む。TeXコードはそれぞれ\\Zeta、\\zeta\nあまり使われない。変数が足りないときによく使われる。\nリーマンゼータ関数 $\\zeta$ エータ $\\Eta, \\eta$ エータetaと読む。TeXコードはそれぞれ\\Eta、\\eta\nゼータと同様に、適切な変数がないときに使われることがある。\n粒子物理学でバリオン $\\eta$ シータ $\\Theta, \\theta, \\vartheta$ シータthetaと読む。TeXコードはそれぞれ\\Theta、\\theta、\\vartheta\nほとんどの場合、角度を意味すると見なせる。\n角度 $\\theta$ 変数分離時の関数 $\\theta$に対する変数 $\\Theta (\\theta)$ イオタ $\\Iota, \\iota$ イオタiotaと読む。TeXコードはそれぞれ\\Iota、\\iota\nあまり使われない。\nカッパ $\\Kappa, \\kappa$ カッパkappaと読む。TeXコードはそれぞれ\\Kappa、\\kappa\n微分幾何学での曲率 $\\kappa$ :\n法曲率 $\\kappa _{n}$、測地曲率 $\\kappa_{g}$、[主曲率 $\\kappa$] 量子力学で負のエネルギーの置換定数 $\\kappa$ ラムダ $\\Lambda, \\lambda$ ラムダlambdaと読む。TeXコードはそれぞれ\\Lambda、\\lambda\n固有値 $\\lambda$ 物理学での波長 $\\lambda$ ミュー $\\Mu, \\mu$ ミューmuと読む。TeXコードはそれぞれ\\Mu、\\mu\n測度 $\\mu$ 電磁気学での透磁率 $\\mu$ 粒子物理学でミュオン $\\mu$ 統計学での平均 $\\mu$ ニュー $\\Nu, \\nu$ ニューnuと読む。TeXコードはそれぞれ\\Nu、\\nu\n物理学での振動数 $\\nu$ 粒子物理学でニュートリノ $\\nu$ クシー $\\Xi, \\xi$ クシーxiと読む。クサイ、ザイとも読む。ザイ アパートのザイがこれだ。TeXコードはそれぞれ\\Xi、\\xi\n変数が足りないときによく使われる。\n$x$に対するフーリエ変換の変数 $\\xi$ リーマンザイ関数 $\\xi$この場合はザイと読む。 オミクロン $\\Omicron, \\omicron$ オミクロンOmicronと読むが、アルファベット$o$と形がほとんど同じで、ほとんど使われない。TeXコードはそれぞれ\\Omicron、\\omicron\nパイ $\\Pi, \\pi$ パイpiと読む。円周率を意味する。TeXコードはそれぞれ\\Pi、\\pi\n積記号 $\\Pi$ 円周率 $\\pi$ 粒子物理学でパイオン中間子 $\\pi$ ロー $\\Rho, \\rho$ ローrhoと読む。TeXコードはそれぞれ\\Rho、\\rho\n円柱座標系の半径変数 $\\rho$ 物理学で密度 $\\rho$\n体積電荷密度 $\\rho$ シグマ $\\Sigma, \\sigma$ シグマsigmaと読む。TeXコードはそれぞれ\\Sigma、\\sigma\n和記号 $\\Sigma$ 統計学での分散 $\\sigma^{2}$ 電磁気学での表面電荷密度 $\\sigma$ 熱力学で[衝突断面積 $\\sigma$] タウ $\\Tau, \\tau$ 力学でのトルク $\\tau$ : $N$としてもよく使われる。 時間に対する変数として$t$の代わりに使われることがある。 円周率の2倍の数 $\\tau = 2\\pi$ ウプシロン $\\Upsilon, \\upsilon, \\varUpsilon$ ウプシロンupsilonと読む。TeXコードはそれぞれ\\Upsilon、\\upsilon、\\varUpsilon\n粒子物理学でウプシロン中間子 $\\varUpsilon$ カイ $\\Chi, \\chi$ カイchiと読む。先生が$x$を$\\chi$として使わないようにと言うのは、これが実際にはエックス$x$ではなくカイ$\\chi$だからである。是非ともこのように書かないでほしい。TeXコードはそれぞれ\\Chi、\\chi\n特性関数 $\\chi$ プサイ $\\Psi, \\psi$ プサイpsiと読む。TeXコードは\\Psi、\\psi\n$\\phi$とともに、任意の関数を表す際によく使われる。\n量子力学での波動関数 $\\psi$ ファイ $\\Phi, \\phi ,\\varphi$ ファイphiまたはパイと読む。経験上、物理学ではパイ、数学ではファイと読むことが多い。TeXコードはそれぞれ\\Phi、\\phi、\\varphi\n$\\psi$とともに、任意の関数を表す際によく使われる。\n円柱座標系の変数 $\\phi$ 球座標系の変数 $\\phi$ 量子力学での波動関数 $\\phi$ オメガ $\\Omega, \\omega$ オメガomegaと読む。TeXコードはそれぞれ\\Omega、\\omega\nギリシャ文字の最後の文字で、「アルファでありオメガ」という言葉は「始まりであり終わり」、「全て」という意味である。\n偏微分方程式、関数解析学での開集合 $\\Omega$ : $U$とともによく使われる記法である。 物理学で抵抗の単位 $\\Omega$ 球の立体角 $\\Omega$ 物理学での角振動数 $\\omega$ $t$に対するフーリエ変換の変数 $\\omega$ 多項式の複素数根 $\\omega$ ","id":3145,"permalink":"https://freshrimpsushi.github.io/jp/posts/3145/","tags":null,"title":"ギリシャ文字の読み方・書き方と数学・科学における意味"},{"categories":"확률미분방정식","contents":"ビルドアップ 確率的積分を考える前に、非常に重要な確率過程である初等過程Elementary Processを定義したい。初等過程は測度論でルベーグ積分を定義するために必要だった単純関数と似た役割を果たす。\n$$ a = t_{0} \u0026lt; t_{1} \u0026lt; \\cdots \u0026lt; t_{k} = b $$ ナチュラルドメイン $[a,b]$ で上のような分割を考えてみよう。指示関数 $\\chi$ および$\\mathcal{F}_{t_{j}}$-可測関数（確率変数）である$e_{j}$ について、次のように表示される$\\phi \\in m^{2}[a,b]$ を初等過程という。 $$ \\phi (t,\\omega) := \\sum_{j=0}^{k-1} e_{j} (\\omega) \\chi_{ \\left[ t_{j} , t_{j+1} \\right] } (t) $$ この関数を ウィーナー過程 $W(t)$ で積分するというのは区分求積法のアイデアそのままに、次のように考えることができる。 $$ \\int_{a}^{b} \\phi (t,\\omega) d W_{t} (\\omega) = \\sum_{j=0}^{k-1} e_{j} (\\omega) \\left[ W_{t_{j+1}} - W_{t_{j}} \\right] ( \\omega ) $$ これにより、次のような確率的積分Stochastic Integralを定義する。\n定義 1 $f \\in m^{2} [a,b]$ のイット積分Itô Integralを次のように定義する。 $$ \\int_{a}^{b} f (t,\\omega) d W_{t} (\\omega) := \\lim_{n \\to \\infty} \\int_{a}^{b} \\phi_{n} (t,\\omega) d W_{t} (\\omega) $$ ここで、シーケンス $\\left\\{ \\phi_{n} \\right\\}_{n \\in \\mathbb{N}}$ は次を満たす初等過程のシーケンスである。 $$ \\lim_{n \\to \\infty} E \\left[ \\int_{a}^{b} \\left( f (t,\\omega) - \\phi_{n} (t,\\omega) \\right)^{2} dt \\right] = 0 $$\n説明 定義では、$\\left\\{ \\phi_{n} \\right\\}_{n \\in \\mathbb{N}}$ は条件 $E \\int [f-\\phi_{n}]^{2} dt \\to 0$ を満たすなら、具体的にどのように選択されても構わない。\n基本性質 2 $f, g \\in m^{2} [a,b]$ であり、フィルトレーション $\\left\\{ \\mathcal{F}_{t} \\right\\}_{t \\ge 0}$ が与えられているとする。\n[1] 可測性: $\\displaystyle \\int_{a}^{b} f d W_{t} = \\left( \\int_{a}^{b} f d W_{t} \\right) (\\omega)$ は $\\mathcal{F}_{b}$-可測である。 [2] 線形性: 定数 $c$ に対して $$ \\int_{a}^{b} \\left( c f + g \\right) d W_{t} = \\int_{a}^{b} c f d W_{t} + \\int_{a}^{b} g d W_{t} $$ [3] 加法性: $a \u0026lt; c \u0026lt; b$ に対して $$ \\int_{a}^{b} f d W_{t} = \\int_{a}^{c} f d W_{t} + \\int_{c}^{b} f d W_{t} $$ [4] 正規性: $f$ が $\\omega \\in \\Omega$ と独立Independentであり、言い換えると$f$ が決定論的Deterministicであれば $$ \\int_{a}^{b} f d W_{t} \\sim N \\left( 0, \\int_{a}^{b} \\left( f \\right)^{2} dt \\right) $$ [5] 有界確率変数: $Z$ が $\\mathcal{F}_{b}$-可測であれば、$Z f \\in m^{2}[a,b]$ が成立し、次が成り立つ。 $$ \\int_{a}^{b} Z f (t) d W_{t} = Z \\int_{a}^{b} f (t) d W_{t} $$ [6] 期待値: サブシグマフィールド$\\mathcal{F}_{a}$に対して $$ E \\left[ \\int_{a}^{b} f d W_{t} \\right] = E \\left[ \\int_{a}^{b} f d W_{t} | \\mathcal{F}_{a} \\right] = 0 $$ であり、$f,g$ に対して次が成り立つ。 $$ E \\left( \\int_{a}^{b} f(t) d W_{t} \\int_{a}^{b} g(t) d W_{t} \\right) = E \\left( \\int_{a}^{b} f(t) g(t) d W_{t} \\right) $$ [7] イット等長等式: $$ E \\left( \\left| \\int_{a}^{b} f W_{t} \\right|^{2} \\right) = E \\left( \\int_{a}^{b} \\left| f \\right|^{2} W_{t} \\right) $$ これは条件付き期待値にも同様に適用され、次が成立する。 $$ E \\left( \\left| \\int_{a}^{b} f d W_{t} \\right|^{2} | \\mathcal{F}_{a} \\right) = E \\left( \\int_{a}^{b} \\left| f \\right|^{2} d W_{t} | \\mathcal{F}_{a} \\right) = \\int_{a}^{b} E \\left( \\left| f \\right|^{2} | \\mathcal{F}_{a} \\right) d W_{t} $$ [8] $f \\in m^2$ であり、$\\left\\{ f_{n} \\right\\}_{n \\in \\mathbb{N}} \\subset m^{2}$ とする。もし$n \\to \\infty$ のとき $$ E \\left[ \\int_{a}^{b} \\left( f_{n} - f \\right)^{2} dt \\right] \\to 0 $$ なら、$n \\to \\infty$ のとき$\\mathcal{L}_{2}$ 収束と同様にする。 $$ \\int_{a}^{b} f_{n} W_{t} \\to \\int_{a}^{b} f W_{t} $$ $\\mathcal{F}_{t}$ が$\\mathcal{F}$ のサブシグマフィールドであることは、両方が$\\Omega$ の[シグマフィールド]であり、$\\mathcal{F}_{t} \\subset \\mathcal{F}$が適用されることを意味する。 $f$ が$\\mathcal{F}_{t}$-可測関数であることは、すべてのボレル集合 $B \\in \\mathcal{B}([0,\\infty))$ に対して$f^{-1} (B) \\in \\mathcal{F}_{t}$であることを意味する。 Øksendal. (2003). Stochastic Differential Equations: An Introduction with Applications: p29.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPanik. (2017). Stochastic Differential Equations: An Introduction with Applications in Population Dynamics Modeling: p118.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2111,"permalink":"https://freshrimpsushi.github.io/jp/posts/2111/","tags":null,"title":"伊藤積分"},{"categories":"기하학","contents":"定義1 座標パッチ $\\mathbf{x} : U \\to \\mathbb{R}^{3}$ 上の点 $p = \\mathbf{x}(a,b)$ を考えよう。ベクトル $\\mathbf{X}$ が $p$ を通るある曲線 $\\mathbf{x}(U)$ 上の$p$での速度ベクトルならば、$\\mathbf{X}$ を単純な曲面 $\\mathbf{x}$ に対する接ベクトルtangent vectorと定義する。\nつまり、任意の$\\epsilon \u0026gt; 0$に対して、適当に短い曲線 $\\boldsymbol{\\alpha} : (-\\epsilon, \\epsilon) \\to \\mathbf{x}(U) \\subset \\mathbb{R}^{3}$ が存在して、次の条件\n$$ \\boldsymbol{\\alpha}(0) = p \\quad \\text{and} \\quad \\boldsymbol{\\alpha}^{\\prime}(0) = \\left. \\dfrac{d \\boldsymbol{\\alpha}}{d t}\\right|_{t=0}= \\mathbf{X} \\quad \\text{and} \\quad \\boldsymbol{\\alpha} (t) = \\mathbf{x}\\left( \\alpha_{1}(t), \\alpha_{2}(t) \\right) $$\nを満たすならば、$\\mathbf{X}$ を単純な曲面 $\\mathbf{x}$ に対する接ベクトルtangent vectorという。\n説明 上述のように定義された接ベクトルの集合は、下記の定理によりベクタースペースになり、これは実際には接平面と同じである。したがって、接平面は接空間tangent spaceと呼ばれる。\n曲面 $M$ 上の点 $p \\in M$ での $M$ に対する全ての接ベクトルの集合を $T_{p}M$ と記し、接空間tangent spaceと呼ぶ。 $$ T_{p}M = \\left\\{ \\text{all vectors tangent to } M \\text{ at } p \\right\\} $$\nこの定義の方法は微分多様体上の接ベクトルを定義するときにもそのまま使用される。最初この定義を見たとき、そんな曲線 $\\boldsymbol{\\alpha}$ を考えながら定義する理由がすぐには理解しづらいかもしれないが、微分幾何を続けて学んだり、多様体への一般化に接したりすると、自然と受け入れられるようになるだろう。\nRichard S. Millman and George D. Parker, Elements of Differential Geometry (1977), p83\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3142,"permalink":"https://freshrimpsushi.github.io/jp/posts/3142/","tags":null,"title":"単純な曲面上の接ベクトル"},{"categories":"기하학","contents":"定義1 $2$次元のユークリッド空間の部分集合$U \\subset \\mathbb{R}^{2}$が座標$u_{1}$、$u_{2}$を持っているとしよう、$\\mathbf{x}_{1}$、$\\mathbf{x}_{2}$をシンプルな曲面$\\mathbf{x} : U \\to \\mathbb{R}^{3}$での方向偏微分としよう。\n$$ \\begin{align*} \\mathbf{x}_{1} := {{ \\partial \\mathbf{x} } \\over { \\partial u_{1} }} \u0026amp; , \u0026amp; \\mathbf{x}_{2} := {{ \\partial \\mathbf{x} } \\over { \\partial u_{2} }} \\end{align*} $$\n点$p = \\mathbf{x} (a,b)$での$\\mathbf{x}_{1} \\times \\mathbf{x}_{2}$と垂直な平面を$p$での接平面Tangent Planeという。 次のように定義された$\\mathbf{n}$を$p$の単位法線Unit Normalという。 $$ \\mathbf{n}(a,b) := {{ \\mathbf{x}_{1} \\times \\mathbf{x}_{2} } \\over { \\left| \\mathbf{x}_{1} \\times \\mathbf{x}_{2} \\right| }} $$ 説明 曲線を語る時に接線を考えたのと同じように、曲面での接平面を考えることは非常に自然なことだ。$p$での接平面は、$p$の周りで曲面を最もよく近似する平面だ。\nシンプルな曲面の定義から$\\mathbf{x}_{1} \\times \\mathbf{x}_{2} \\ne 0$であるため、法線$\\mathbf{n}$の存在は常に保証されている。\n次の定理から、接平面は接ベクトルの集合と同じであり、ベクトル空間になることが分かる。このため、接平面は接空間tangent spaceと呼ばれる。曲面$M$の点$p$上の接空間を$T_{p}M$と表示する。\n定理2 シンプルな曲面$\\mathbf{x} : U \\to \\mathbb{R}^{3}$の点$p = \\mathbf{x}(a,b)$での全ての接ベクトルの集合は、基底が$\\left\\{ \\mathbf{x}_{1}(a,b), \\mathbf{x}_{2}(a,b) \\right\\}$である$2$次元のベクトル空間である。また$p$での接平面は、$\\mathbb{R}^{3}$の何らかの原点を通る直線と平行である。\n証明 点$p$での接ベクトル$\\mathbf{x}_{1}, \\mathbf{x}_{2}$は線形独立である。（$\\mathbf{x}_{1} \\times \\mathbf{x}_{2} \\ne \\mathbf{0}$であるので）$p$での全ての接ベクトルの集合はベクトル空間であるため、これは少なくとも$2$次元以上のベクトル空間である。このベクトル空間が$2$次元であることを示すためには、$\\left\\{ \\mathbf{x}_{1}, \\mathbf{x}_{2} \\right\\}$がこれを生成することを示せば良い。\n$\\mathbf{X}$を点$p$での接ベクトルとしよう。そして$\\boldsymbol{\\gamma}$を、$\\boldsymbol{\\gamma}(0) = p, \\dot{\\boldsymbol{\\gamma}}(0) = \\mathbf{X}$である$\\mathbf{x}(U)$上の曲線とする。そして、$\\boldsymbol{\\gamma}(t)$を次のように表現しよう。\n$$ \\boldsymbol{\\gamma}(t) = \\mathbf{x}\\left( \\gamma^{1}(t), \\gamma^{2}(t) \\right) $$\nそれから、連鎖規則によって、\n$$ \\dfrac{d \\boldsymbol{\\gamma}}{d t} = \\dfrac{\\partial \\mathbf{x}}{\\partial u^{1}}\\dfrac{d \\gamma^{1}}{d t} + \\dfrac{\\partial \\mathbf{x}}{\\partial u^{2}}\\dfrac{d \\gamma^{2}}{d t} = \\sum_{i}\\dfrac{d \\gamma^{i}}{d t}\\mathbf{x}_{i} $$\n$$ \\implies \\mathbf{X} = \\dfrac{d \\boldsymbol{\\gamma}}{d t}(0) = \\sum_{i}\\dfrac{d \\gamma^{i}}{d t}(0)\\mathbf{x}_{i}(a,b) $$\n任意の接ベクトル$\\mathbf{X}$が$\\left\\{ \\mathbf{x}_{i} \\right\\}$たちの線形結合で示されるので、$\\left\\{ \\mathbf{x}_{i} \\right\\}$は$p$での全ての接ベクトルの集合を生成する。したがって、$p=\\mathbf{x}(a,b)$での全ての接ベクトルの集合は、基底が$\\left\\{ \\mathbf{x}_{1}(a,b), \\mathbf{x}_{2}(a,b) \\right\\}$である$2$次元のベクトル空間である。\nRichard S. Millman and George D. Parker、Elements of Differential Geometry (1977)、p81\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRichard S. Millman and George D. Parker、Elements of Differential Geometry (1977)、p84-85\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2110,"permalink":"https://freshrimpsushi.github.io/jp/posts/2110/","tags":null,"title":"平面と法線ベクトルの交点"},{"categories":"확률미분방정식","contents":"定義 1 2 確率空間 $( \\Omega , \\mathcal{F} , P)$ が与えられているとする。\n$\\mathcal{F}$ のサブシグマフィールドのシーケンス $\\left\\{ \\mathcal{F}_{t} \\right\\}_{t \\ge 0}$ が以下を満たすとき、フィルトレーションFiltrationと呼ぶ。 $$ \\forall s \u0026lt; t, \\mathcal{F}_{s} \\subset \\mathcal{F}_{t} $$ 確率過程 $g(t,\\omega) : [0,\\infty) \\times \\Omega \\to \\mathbb{R}^{n}$ がすべての $t \\ge 0$ で $\\omega \\mapsto g (t,\\omega)$ が $\\mathcal{F}_{t}$-可測であれば $\\mathcal{F}_{t}$-適応$\\mathcal{F}_t$-Adaptedという。 区間 $I := [a,b]$ について、以下の三つの条件を満たす関数たちの集まりを $m^{2} = m^{2} [a,b]$ と表示する。特にこの $I$ を伊藤積分のナチュラルドメインNatural Domainと呼ぶ。 (i): $\\mathcal{B}$ が $[0, \\infty)$ のボレルシグマフィールドに対して$(t, \\omega) \\mapsto f(t, \\omega)$ が $\\mathcal{B} \\times \\mathcal{F}$-可測である。 (ii): $f (t,\\omega)$ が $\\mathcal{F}_{t}$-適応である。 (iii): ヒルベルト空間の構造である。つまり、 $$ \\left\\| f \\right\\|_{2}^{2} \\left( [a,b] \\right) = E \\left( \\int_{a}^{b} \\left| f(t,\\omega) \\right|^{2} dt \\right) \u0026lt; \\infty $$ $\\mathcal{F}_{t}$ が $\\mathcal{F}$ のサブシグマフィールドであることは、両方が $\\Omega$ のシグマフィールドであるが、$\\mathcal{F}_{t} \\subset \\mathcal{F}$ であることを意味する。 $f$ が $\\mathcal{F}_{t}$-可測関数であることは、すべてのボレルセット $B \\in \\mathcal{B}([0,\\infty))$ に対して $f^{-1} (B) \\in \\mathcal{F}_{t}$ であるという意味だ。 説明 フィルトレーションが与えられた場合、確率過程 $f$ が $\\mathcal{F}_{t}$-可測であることは、時点 $t$ までの歴史や情報を持っているとも見なすことができる。フィルトレーションは、時間とともに情報が増えるという概念と一致する、より大きなサブシグマフィールドのシーケンスだ。\n$m^{2}$ 空間という命名は、条件(iii)で見るように、$L^{2}$ 空間から来ているのは自然なことだ。\nØksendal. (2003). Stochastic Differential Equations: An Introduction with Applications: p25.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPanik. (2017). Stochastic Differential Equations: An Introduction with Applications in Population Dynamics Modeling: p116.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2109,"permalink":"https://freshrimpsushi.github.io/jp/posts/2109/","tags":null,"title":"m2 空間"},{"categories":"기하학","contents":"定義1 1 座標 $u_{1}$、$u_{2}$ を持つ $2$次元 ユークリッド空間の部分集合 $U \\subset \\mathbb{R}^{2}$が 開集合だとしよう。すべての $p \\in U$に対して以下を満たす $C^{k}$ 単射関数 $\\mathbf{x} : U \\to \\mathbb{R}^{3}$ が存在するなら、それを単純曲面Simple Surfaceと呼ぶ。\n$$ {{ \\partial \\mathbf{x} } \\over { \\partial u_{1} }} (p) \\times {{ \\partial \\mathbf{x} } \\over { \\partial u_{2} }} (p) \\ne \\mathbf{0} $$\n説明 定義において、開集合 $U$ は $2$次元空間から選ばれ、それが平らであれ曲がっていれ、重なる部分なしに(単射であるため)$3$次元空間へマッピングされる。この意味で、単純曲面は $2$次元の平らな片を$3$次元空間で滑らかに接続することと想像できる。この関数としての曲面の定義を幾何学的に理解するのが最善だが、すぐには思い浮かばなくても、時間をかけて慣れること。\n曲面をこのように2次元空間から3次元空間への写像として定義する理由は、曲面は局所的に見た時、平面のように扱えるためである。実際地球は球体に近い形だが、私たちは地表を上から見た時、2次元平面のように感じる。$U$を世界地図、$\\mathbf{x}(U)$を地球儀と例えることができる。\n一方、定義における数式で与えられた条件は、正則曲線が $\\displaystyle {{ d \\mathbf{x} } \\over { d u }} (p) \\ne 0$ のような条件を満たさなければならなかった曲線理論と似ている。直感的には、とがったり、奇妙にねじれた部分はすぐに排除するということである。$\\dfrac{ \\partial \\mathbf{x} }{ \\partial u_{1} } (p) \\times \\dfrac{\\partial \\mathbf{x} }{ \\partial u_{2} } (p) \\ne \\mathbf{0}$ を満たすとは、あらゆる方向の偏微分がシンギュラーでない($0$ではない)ことを意味し、ある意味で、二つの線形独立した（曲線）軸を見て、その幾何を考える意図を読み取ることができる。\nもし単純曲面が具体的な座標とグラフで表されていれば、それはモンジュ・パッチMonge Patchとも呼ばれる。例えば、単純曲面$f$が$f(x,y) = x^{2} + y^{2}$であれば、そのグラフは $$ \\left\\{ \\left( x, y , x^{2} + y^{2} \\right) : (x,y) \\in \\mathbb{R}^{2} \\right\\} $$ であり、モンジュ・パッチと呼ぶことができる。\n定義2 2 座標 $u_{1}$、$u_{2}$を持つ $2$次元 ユークリッド空間の部分集合 $U \\subset \\mathbb{R}^{2}$が 開集合だとしよう。写像 $\\mathbf{x} : U \\to \\mathbb{R}^{3}$が一対一で正則なら、$\\mathbf{x}$を座標片coordinate patchと言う。\n説明 3 $\\mathbf{x} : U \\to \\mathbb{R}^{3}$が正則であるとは、$\\mathbf{x}$のヤコビ行列のランクが$2$と同じということである。$\\mathbf{x}(u,v) = (x_{1}(u,v), x_{2}(u,v), x_{3}(u,v))$とすると、$\\mathbf{x}$のヤコビ行列は次のようになる。\n$$ J = \\begin{bmatrix} \\dfrac{\\partial x_{1}}{\\partial u} \u0026amp; \\dfrac{\\partial x_{1}}{\\partial v} \\\\[1em] \\dfrac{\\partial x_{2}}{\\partial u} \u0026amp; \\dfrac{\\partial x_{2}}{\\partial v} \\\\[1em] \\dfrac{\\partial x_{3}}{\\partial u} \u0026amp; \\dfrac{\\partial x_{3}}{\\partial v} \\end{bmatrix} $$\nこの行列のランクが$2$であるとは、列空間の次元が$2$であるということであり、$\\mathbf{x}_{u} = \\left( \\dfrac{\\partial x_{1}}{\\partial u}, \\dfrac{\\partial x_{2}}{\\partial u}, \\dfrac{\\partial x_{3}}{\\partial u} \\right)$と$\\mathbf{x}_{v} = \\left( \\dfrac{\\partial x_{1}}{\\partial v}, \\dfrac{\\partial x_{2}}{\\partial v}, \\dfrac{\\partial x_{3}}{\\partial v} \\right)$が線形独立であることを意味する。したがって、この二つの外積は$\\mathbf{0}$ではない。\n$$ \\mathbf{x}_{u} \\times \\mathbf{x}_{v} \\ne \\mathbf{0} $$\nそうすると、上記の二つの定義は同値であることがわかる。\nMillman. (1977). Elements of Differential Geometry: p77.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBarrett O\u0026rsquo;Neill, Elementary Differential Geometry (Revised 2nd Edition, 2006), p130-131\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBarrett O\u0026rsquo;Neill, Elementary Differential Geometry (Revised 2nd Edition, 2006), p142\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2106,"permalink":"https://freshrimpsushi.github.io/jp/posts/2106/","tags":null,"title":"単純曲面、座標写像"},{"categories":"기하학","contents":"まとめ1 $M_{1}^{n}, M_{2}^{m}$をそれぞれ$m, n$次元の微分多様体としよう。$\\varphi : M_{1} \\to M_{2}$を微分可能な関数としよう。そして、全ての点$p \\in M_{1}$と接ベクトル$v \\in T_{p}M$に対して、微分可能な曲線\n$$\\alpha : (-\\epsilon, \\epsilon) \\to M_{1} \\text{ with } \\alpha (0) = p,\\ \\alpha^{\\prime}(0)=v$$\nを選ぼう。そして$\\beta = \\varphi \\circ \\alpha$としよう。すると次のマッピング\n$$ d\\varphi_{p} : T_{p}M_{1} \\to T_{\\varphi(p)}M_{2} \\\\[1em] d\\varphi_{p}(v) = \\beta^{\\prime}(0) $$\nは$\\alpha$の選択に関係なく線形変換である。\n定義 上記の定理のように定義されたマッピング$d\\varphi_{p}$を$p$から$\\varphi$への微分differential of $\\varphi$ at $p$という。\n説明 微分係数ではなく、微分である。\n接ベクトルは微分多様体上で定義された関数に作用する関数なので、微分$d\\varphi_{p}$は関数空間から関数空間へのマッピングである。そして、証明の結論から$d\\phi_{p}$は関数$\\mathbf{y}^{-1} \\circ \\phi \\circ \\mathbf{x}$に対するヤコビアンであることがわかる。\n$$ \\text{differential} = \\text{Jacobian} $$\nヤコビアンの役割を思い出してみよう。例えば、$\\mathbb{R}^{2}$で定義された関数の積分が次のように与えられたとする。\n$$ \\int \\int f(x,y) dx dy $$\nこの積分を極座標$(r,\\theta)$へ座標変換するとき、ヤコビアンの行列式$\\displaystyle \\begin{vmatrix}\\dfrac{ \\partial x}{ \\partial r} \u0026amp; \\dfrac{ \\partial x}{ \\partial \\theta} \\\\ \\textstyle{} \\\\ \\dfrac{ \\partial y}{ \\partial r} \u0026amp; \\dfrac{ \\partial y}{ \\partial \\theta} \\end{vmatrix}=r$をかける必要がある。\n$$ \\int \\int f(x,y) dx dy = \\int \\int f(r, \\theta) rdr d\\theta $$\nだから、$\\phi : M_{1} \\to M_{2}$の微分$d\\phi_{p}$は微分多様体$M_{1}$と$M_{2}$の間の座標変換を、それぞれの座標系$\\mathbf{x}, \\mathbf{y}$を通して行うと考えることができる。\n合成のヤコビアンはヤコビアンの積と等しいので、$\\phi : M \\to N, \\psi : N \\to L$と$p\\in M$に対して次が成り立つ。\n$$ d(\\psi \\circ \\phi)_{p} = d(\\psi)_{\\phi (p)} d(\\phi)_{p} $$\n接ベクトルの定義と意味をよく理解していないと、当該ドキュメントの内容を理解するのが非常に難しいので、接ベクトルについて十分に理解してから読むようにしよう。\n証明 $M_{1}$の点$p \\in M_{1}$における座標系を$\\mathbf{x} : U \\subset \\mathbb{R}^{n} \\to M_{1}$としよう。$\\mathbb{R}^{n}$の座標を$(r_{1}, \\dots, r_{n}) \\in \\mathbb{R}^{n}$としよう。\n$$ \\mathbf{x}(r_{1}, \\dots, r_{n}) = p \\quad \\text{and} \\quad \\mathbf{x}^{-1}(p) = \\left( x_{1}(p), \\dots, x_{n}(p) \\right) $$\nそして$M_{2}$の点$\\phi (p) \\in M_{2}$における座標系を$\\mathbf{y} : V \\subset \\mathbb{R}^{m} \\to M_{2}$としよう。$\\mathbb{R}^{m}$の座標を$(s_{1}, \\dots, s_{m}) \\in \\mathbb{R}^{m}$としよう。\n$$ \\mathbf{y}(s_{1}, \\dots, s_{m}) = \\phi (p) \\quad \\text{and} \\quad \\mathbf{y}^{-1}(\\phi (p)) = \\Big( y_{1}(\\phi (p)), \\dots, y_{m}(\\phi (p)) \\Big) $$\n接ベクトルの定義により、$M_{2}$の点$\\phi (p)$における接ベクトル$\\beta^{\\prime}(0)$は次のようになる。$M_{2}$上で定義される微分可能な関数$g : M_{2} \\to \\mathbb{R}$に対して、\n$$ \\begin{align*} \\beta^{\\prime}(0) g =\u0026amp;\\ \\dfrac{d}{dt}(g \\circ \\beta)(0) = \\dfrac{d}{dt}(g \\circ \\mathbf{y} \\circ \\mathbf{y}^{-1} \\circ \\beta)(0) \\\\ =\u0026amp;\\ \\dfrac{d}{dt}\\big( (g \\circ \\mathbf{y}) \\circ (\\mathbf{y}^{-1} \\circ \\beta)\\big)(0) \\\\ =\u0026amp;\\ \\sum \\limits_{j=1}^{m} \\left.\\dfrac{\\partial (g\\circ \\mathbf{y})}{\\partial s_{j}}\\right|_{t=0} \\dfrac{d (\\mathbf{y}^{-1} \\circ \\beta)_{j}}{d t}(0) \u0026amp; \\text{by } \\href{https://freshrimpsushi.github.io/posts/chaine-rule-for-multivariable-vector-valued-funtion}{\\text{chain rule}} \\\\ =\u0026amp;\\ \\sum \\limits_{j=1}^{m} y_{j}^{\\prime}(0) \\left.\\dfrac{\\partial (g\\circ \\mathbf{y})}{\\partial s_{j}}\\right|_{t=0} \\\\ =\u0026amp;\\ \\sum_{j=1}^{m} y_{j}^{\\prime}(0) \\left.\\dfrac{\\partial g}{\\partial y_{j}}\\right|_{t=0} \\end{align*} $$\nこの時、オペレーター$\\left.\\dfrac{\\partial }{\\partial y_{j}}\\right|_{t=0}$の意味は、$M_{2}$上で定義されて微分できない関数$g$の定義域を$\\mathbf{y}$との合成を通して$\\mathbb{R}^{m}$へ引き寄せて微分するという意味である。では$y_{j}^{\\prime}$を求めよう。\n$$ y_{j}^{\\prime} = \\dfrac{d}{dt} (\\mathbf{y}^{-1} \\circ \\beta)_{j} $$\n接ベクトルを計算する時と同様に、$\\mathbf{y}^{-1} \\circ \\beta$を以下のように分解して考えよう。\n$$ \\mathbf{y}^{-1} \\circ \\beta = \\mathbf{y}^{-1} \\circ \\phi \\circ \\alpha = \\mathbf{y}^{-1} \\circ \\phi \\circ \\mathbf{x} \\circ \\mathbf{x}^{-1} \\circ \\alpha $$\nそして、上記の式を二つの関数$\\mathbf{y}^{-1} \\circ \\phi \\circ \\mathbf{x}$と$\\mathbf{x}^{-1} \\circ \\alpha$の合成として扱おう。\nパート1. $\\mathbf{y}^{-1} \\circ \\phi \\circ \\mathbf{x}$\n$\\mathbf{y}^{-1} \\circ \\phi \\circ \\mathbf{x} : \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$であるから、\n$$ \\begin{equation} \\mathbf{y}^{-1}\\left( \\phi (\\mathbf{x}(r_{1}, \\dots, r_{n})) \\right) = \\left( y_{1}(\\phi (\\mathbf{x}(r_{1}, \\dots, r_{n}))), \\dots, y_{m}(\\phi (\\mathbf{x}(r_{1}, \\dots, r_{n}))) \\right) \\end{equation} $$\n簡単に表現すると、\n$$ \\mathbf{y}^{-1} \\circ \\phi \\circ \\mathbf{x} (r_{1}, \\dots, r_{n}) = \\left( y_{1}, \\dots, y_{m}\\right) $$\nここで、各$y_{j}$は厳密に言うと$(1)$と同様に$\\phi (\\mathbf{x}(r_{1}, \\dots, r_{n}))$に関する関数だが、表記が複雑になるため便宜上$(r_{1}, \\dots, r_{n})$に関する関数として表記しよう。\n$$ y_{j} = y_{j}(r_{1}, \\dots, r_{n}),\\quad 1\\le j \\le m $$\nパート2. $\\mathbf{x}^{-1} \\circ \\alpha$\n$\\mathbf{x}^{-1} \\circ \\alpha : \\mathbb{R} \\to \\mathbb{R}^{n}$であるから、\n$$ \\mathbf{x}^{-1} (\\alpha (t)) = \\left( x_{i}(\\alpha (t)), \\dots, x_{n}(\\alpha (t)) \\right) $$\nここでも同様に、便宜上各$x_{i}$を$t$に関する関数として表記しよう。\n$$ \\mathbf{x} \\circ \\alpha (t) = ( x_{i}(t), \\dots, x_{n}(t) ) $$\nこれで、$(\\mathbf{y}^{-1} \\circ \\phi \\circ \\mathbf{x}) \\circ (\\mathbf{x}^{-1} \\circ \\alpha)$は$\\mathbb{R} \\to \\mathbb{R}^{n}$である関数と$\\mathbb{R}^{n} \\to \\mathbb{R}^{m}$である関数の合成であるから、連鎖律によって次を得る。\n$$ \\begin{align*} \\dfrac{d}{dt} (\\mathbf{y}^{-1} \\circ \\beta)(0) =\u0026amp;\\ \\dfrac{d}{dt}(\\mathbf{y}^{-1} \\circ \\phi \\circ \\mathbf{x}) \\circ (\\mathbf{x}^{-1} \\circ \\alpha)(0) \\\\ =\u0026amp;\\ \\begin{bmatrix} \\sum \\limits_{i=1}^{n} \\left.\\dfrac{\\partial (\\mathbf{y}^{-1} \\circ \\phi \\circ \\mathbf{x})_{1}}{\\partial x_{i}}\\right|_{t=0} \\dfrac{d (\\mathbf{x}^{-1} \\circ \\alpha)_{i}}{d t}(0) \\\\ \\vdots \\\\ \\sum \\limits_{i=1}^{n} \\left.\\dfrac{\\partial (\\mathbf{y}^{-1} \\circ \\phi \\circ \\mathbf{x})_{m}}{\\partial x_{i}}\\right|_{t=0} \\dfrac{d (\\mathbf{x}^{-1} \\circ \\alpha)_{i}}{d t}(0) \\end{bmatrix} \\\\ =\u0026amp;\\ \\begin{bmatrix} \\sum \\limits_{i=1}^{n} \\left.\\dfrac{\\partial y_{1}}{\\partial x_{i}}\\right|_{t=0} \\dfrac{d x_{i}}{d t}(0) \\\\ \\vdots \\\\ \\sum \\limits_{i=1}^{n} \\left.\\dfrac{\\partial y_{m}}{\\partial x_{i}}\\right|_{t=0} \\dfrac{d x_{i}}{d t}(0) \\end{bmatrix} \\\\ =\u0026amp;\\ \\begin{bmatrix} \\sum \\limits_{i=1}^{n} \\dfrac{\\partial y_{1}}{\\partial x_{i}} x_{i}^{\\prime}(0) \\\\ \\vdots \\\\ \\sum \\limits_{i=1}^{n} \\dfrac{\\partial y_{m}}{\\partial x_{i}} x_{i}^{\\prime}(0) \\end{bmatrix} \\end{align*} $$\n故に\n$$ y_{j}^{\\prime}(0) = \\dfrac{d}{dt} (\\mathbf{y}^{-1} \\circ \\beta)_{j}(0) = \\sum \\limits_{i=1}^{n} \\dfrac{\\partial y_{j}}{\\partial x_{i}} x_{i}^{\\prime}(0),\\quad 1\\le j \\le m $$\nしたがって、もし$\\beta^{\\prime}(0)$を基底$\\left\\{ \\left.\\dfrac{\\partial }{\\partial y_{j}}\\right|_{t=0} \\right\\}$に対する座標ベクトルとして表現すれば、次のようになる。\n$$ \\beta^{\\prime}(0) = \\sum_{j=1}^{m} y_{j}^{\\prime}(0) \\left.\\dfrac{\\partial}{\\partial y_{j}}\\right|_{t=0} = \\begin{bmatrix} y_{1}^{\\prime}(0) \\\\ \\vdots \\\\ y_{m}^{\\prime}(0) \\end{bmatrix} = \\begin{bmatrix} \\sum \\limits_{i=1}^{n} \\dfrac{\\partial y_{1}}{\\partial x_{i}} x_{i}^{\\prime}(0)\\\\ \\vdots \\\\ \\sum \\limits_{i=1}^{n} \\dfrac{\\partial y_{m}}{\\partial x_{i}} x_{i}^{\\prime}(0) \\end{bmatrix} $$\nしたがって、$\\beta^{\\prime}(0)$が$\\alpha$に依存しないことがわかる。\n一方で、$\\alpha^{\\prime}(0) = v$に対して次が成立する。\n$$ v = \\alpha^{\\prime}(0) = \\sum \\limits_{i=1}^{n}x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0} = \\begin{bmatrix} x_{1}^{\\prime}(0) \\\\ \\vdots \\\\ x_{n}^{\\prime}(0) \\end{bmatrix} $$\nそこで、$\\beta^{\\prime}(0) = d\\phi_{p}(v)$を整理すると次のようになる。\n$$ \\begin{align*} \\beta^{\\prime}(0) =\u0026amp;\\ \\begin{bmatrix} \\sum \\limits_{i=1}^{n} \\dfrac{\\partial y_{1}}{\\partial x_{i}} x_{i}^{\\prime}(0)\\\\ \\vdots \\\\ \\sum \\limits_{i=1}^{n} \\dfrac{\\partial y_{m}}{\\partial x_{i}} x_{i}^{\\prime}(0) \\end{bmatrix} \\\\ =\u0026amp;\\ \\begin{bmatrix} \\dfrac{\\partial y_{1}}{\\partial x_{1}} \u0026amp; \\dfrac{\\partial y_{1}}{\\partial x_{2}} \u0026amp; \\dots \u0026amp; \\dfrac{\\partial y_{1}}{\\partial x_{n}} \\\\[1em] \\dfrac{\\partial y_{2}}{\\partial x_{1}} \u0026amp; \\dfrac{\\partial y_{2}}{\\partial x_{2}} \u0026amp; \\dots \u0026amp; \\dfrac{\\partial y_{2}}{\\partial x_{n}} \\\\[1ex] \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\[1ex] \\dfrac{\\partial y_{m}}{\\partial x_{1}} \u0026amp; \\dfrac{\\partial y_{m}}{\\partial x_{2}} \u0026amp; \\dots \u0026amp; \\dfrac{\\partial y_{m}}{\\partial x_{n}} \\end{bmatrix} \\begin{bmatrix} x_{1}^{\\prime}(0) \\\\ \\vdots \\\\ x_{n}^{\\prime}(0) \\end{bmatrix} \\\\ =\u0026amp;\\ \\begin{bmatrix} \\dfrac{\\partial y_{1}}{\\partial x_{1}} \u0026amp; \\dfrac{\\partial y_{1}}{\\partial x_{2}} \u0026amp; \\dots \u0026amp; \\dfrac{\\partial y_{1}}{\\partial x_{n}} \\\\[1em] \\dfrac{\\partial y_{2}}{\\partial x_{1}} \u0026amp; \\dfrac{\\partial y_{2}}{\\partial x_{2}} \u0026amp; \\dots \u0026amp; \\dfrac{\\partial y_{2}}{\\partial x_{n}} \\\\[1ex] \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\[1ex] \\dfrac{\\partial y_{m}}{\\partial x_{1}} \u0026amp; \\dfrac{\\partial y_{m}}{\\partial x_{2}} \u0026amp; \\dots \u0026amp; \\dfrac{\\partial y_{m}}{\\partial x_{n}} \\end{bmatrix} v \\end{align*} $$\nしたがって、$d_{p}\\phi$は次のような行列で表される線形変換である。\n$$ d_{p}\\phi = \\begin{bmatrix} \\dfrac{\\partial y_{1}}{\\partial x_{1}} \u0026amp; \\dfrac{\\partial y_{1}}{\\partial x_{2}} \u0026amp; \\dots \u0026amp; \\dfrac{\\partial y_{1}}{\\partial x_{n}} \\\\[1em] \\dfrac{\\partial y_{2}}{\\partial x_{1}} \u0026amp; \\dfrac{\\partial y_{2}}{\\partial x_{2}} \u0026amp; \\dots \u0026amp; \\dfrac{\\partial y_{2}}{\\partial x_{n}} \\\\[1ex] \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\[1ex] \\dfrac{\\partial y_{m}}{\\partial x_{1}} \u0026amp; \\dfrac{\\partial y_{m}}{\\partial x_{2}} \u0026amp; \\dots \u0026amp; \\dfrac{\\partial y_{m}}{\\partial x_{n}} \\end{bmatrix} $$\nこれはまた、関数$\\mathbf{y}^{-1} \\circ \\phi \\circ \\mathbf{x}$のヤコビアンでもある。\n$$ d\\phi_{p} = \\text{Jacobian of } \\mathbf{y}^{-1} \\circ \\phi \\circ \\mathbf{x} = \\dfrac{\\partial (y_{1}, \\dots, y_{m})}{\\partial (x_{1}, \\dots, x_{n})} $$\n■\nManfredo P. Do Carmo, Riemannian Geometry (Eng Edition, 1992), p9-10\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3136,"permalink":"https://freshrimpsushi.github.io/jp/posts/3136/","tags":null,"title":"微分多様体上で定義された関数の微分"},{"categories":"줄리아","contents":"概要1 名前はCalculus.jlだけど、積分はサポートしない。\n機械学習などで話される自動微分が必要ならZygote.jlパッケージを参照してほしい。\n一変数関数の微分 導関数 derivative() $f : \\R \\to \\R$の導関数を求めてくれる。\nderivative(f)またはderivative(f, :x): 導関数$f^{\\prime}$を返す。 derivative(f, a): 微分係数$f^{\\prime}(a)$を返す。 julia\u0026gt; f(x) = 1 + 2x + 3x^2\rf (generic function with 1 method)\rjulia\u0026gt; g(x) = sin(x)\rg (generic function with 1 method)\rjulia\u0026gt; derivative(f)\r#1 (generic function with 1 method)\rjulia\u0026gt; derivative(f, 1)\r7.99999999996842\rjulia\u0026gt; Df = derivative(f)\r#1 (generic function with 1 method)\rjulia\u0026gt; Dg = derivative(g)\r#1 (generic function with 1 method)\r#f\u0026#39;(x) = 2 + 6x\rjulia\u0026gt; Df(1)\r7.99999999996842\r#g\u0026#39;(x) = cos x\rjulia\u0026gt; Dg(pi)\r-0.9999999999441258 合成関数も微分することができる。\n#f∘g(x) = (2 + 6 sin x)cos x\rjulia\u0026gt; derivative(f∘g)\r#1 (generic function with 1 method)\rjulia\u0026gt; derivative(f∘g, pi/4)\r4.414213562300037\rjulia\u0026gt; (2+6sin(pi/4))cos(pi/4)\r4.414213562373095 二階導関数 second_derivative() $f : \\R \\to \\R$の二階導関数を求めてくれる。\nderivative()で返される関数は整数を入力値に使えるが、second_derivative()は整数型を使えない。無理数型も使えない。\njulia\u0026gt; derivative(f, 1)\r7.99999999996842\rjulia\u0026gt; second_derivative(f, 1)\rERROR: MethodError: no method matching eps(::Type{Int64})\rClosest candidates are:\reps() at float.jl:764\reps(::AbstractFloat) at float.jl:760\reps(::Type{Float16}) at float.jl:761\rjulia\u0026gt; second_derivative(f, 1.)\r5.9999956492003905\rjulia\u0026gt; second_derivative(g, pi)\rERROR: MethodError: no method matching eps(::Type{Irrational{:π}})\rClosest candidates are:\reps() at float.jl:764\reps(::AbstractFloat) at float.jl:760\reps(::Type{Float16}) at float.jl:761\rjulia\u0026gt; second_derivative(g, convert(Float64, pi))\r-1.3553766145945872e-7\rjulia\u0026gt; second_derivative(g, 1pi)\r-1.3553766145945872e-7 多変数関数の微分 グラディエント gradient() $f : \\mathbb{R}^{n} \\to \\mathbb{R}$のグラディエントを返す。\n注意するべき点は、多変数関数を定義するとき、実際に変数が複数ある関数として定義してはいけないということだ。ベクトルを入力として受け取る一変数関数として定義しなければならない。ベクトルを入力として受け取る関数でなければ、微分はできても、値を計算することができないということだ。例えば、$f_{1}$のように定義してはいけないが、$f_{2}$のように定義する必要があるということだ。\njulia\u0026gt; f₁(x,y,z) = x*y + z^2\rf₁ (generic function with 1 method)\rjulia\u0026gt; Calculus.gradient(f₁)\r#2 (generic function with 1 method)\rjulia\u0026gt; ∇f₁ = Calculus.gradient(f₁)\r#2 (generic function with 1 method)\rjulia\u0026gt; ∇f₁(1,1,1)\rERROR: MethodError: no method matching (::Calculus.var\u0026#34;#2#4\u0026#34;{typeof(f₁), Symbol})(::Int64, ::Int64, ::Int64)\rjulia\u0026gt; ∇f₁([1,1,1])\rERROR: MethodError: no method matching f₁(::Vector{Float64})\rjulia\u0026gt; Calculus.gradient(f₁, 1,1,1)\rERROR: MethodError: no method matching gradient(::typeof(f₁), ::Int64, ::Int64, ::Int64)\rjulia\u0026gt; Calculus.gradient(f₁, [1,1,1])\rERROR: MethodError: no method matching f₁(::Vector{Float64})\rjulia\u0026gt; f₂(x) = x[1]*x[2] + x[3]^2\rf₂ (generic function with 1 method)\rjulia\u0026gt; Calculus.gradient(f₂, [1,1,1])\r3-element Vector{Float64}:\r1.0000000000235538\r1.0000000000235538\r1.9999999999737708\rjulia\u0026gt; ∇f₂ = Calculus.gradient(f₂)\r#2 (generic function with 1 method)\rjulia\u0026gt; ∇f₂(1,1,1)\rERROR: MethodError: no method matching (::Calculus.var\u0026#34;#2#4\u0026#34;{typeof(f₂), Symbol})(::Int64, ::Int64, ::Int64)\rjulia\u0026gt; ∇f₂([1,1,1])\r3-element Vector{Float64}:\r1.0000000000235538\r1.0000000000235538\r1.9999999999737708 ヘッシアン hessian() $f : \\mathbb{R}^{n} \\to \\mathbb{R}$のヘッシアンを返す。\nsecond_derivative()と同様に、Floatデータ型のみ入力を受け付ける。gradient()と同様に、ベクトルを入力として受け取る関数に対してのみ値を返すことができる。\njulia\u0026gt; hessian(f₂, [1.,1.,1.])\r3×3 Matrix{Float64}:\r3.88008e-7 1.0 0.0\r1.0 3.88008e-7 0.0\r0.0 0.0 2.0\rjulia\u0026gt; H = hessian(f₂)\r#7 (generic function with 1 method)\rjulia\u0026gt; H([1.,1.,1.])\r3×3 Matrix{Float64}:\r3.88008e-7 1.0 0.0\r1.0 3.88008e-7 0.0\r0.0 0.0 2.0 ヤコビアン jacobian() $f : \\mathbb{R}^{m} \\to \\mathbb{R}^{n}$のヤコビアンを返す。\nsecond_derivative()と同様に、Floatデータ型のみ入力を受け付ける。gradient()と同様に、ベクトルを入力として受け取る関数に対してのみ値を返すことができる。\n他の関数とは異なり、jacobian(f, [x, y, z])のように使うことはできない。\njulia\u0026gt; h(x) = [x[1], x[1]*x[2], x[1]*x[3]^2]\rh (generic function with 2 methods)\rjulia\u0026gt; jacobian(h, [1.,1.,1.])\rERROR: MethodError: no method matching jacobian(::typeof(h), ::Vector{Float64})\rjulia\u0026gt; Jh = jacobian(h)\r(::Calculus.var\u0026#34;#g#5\u0026#34;{typeof(h), Symbol}) (generic function with 1 method)\rjulia\u0026gt; Jh([1.,1.,1.])\r3×3 Matrix{Float64}:\r1.0 0.0 0.0\r1.0 1.0 0.0\r1.0 0.0 2.0 記号微分 記号微分はSymEngine.jlパッケージでも使用できる。\ndifferentiate() 記号微分を実行する。\n定数項と$x$はキレイに返すが、$ax$や$x^{n}$のような場合は、積の微分法の形で返す。例えば$3x^{2}$を微分すると、それを$3$と$x^{2}$の積と見て$\\dfrac{d 3}{dx} x^{2} + 3\\dfrac{d x^{2}}{dx}$のように返すということだ。さらに$x^{2}$も$1$と$x^{2}$の積と見る。\njulia\u0026gt; differentiate(\u0026#34;1\u0026#34;, :x)\r0\rjulia\u0026gt; differentiate(\u0026#34;1 + x\u0026#34;, :x)\r1\rjulia\u0026gt; differentiate(\u0026#34;x^2\u0026#34;, :x)\r:(2 * 1 * x ^ (2 - 1))\rjulia\u0026gt; differentiate(\u0026#34;x^3\u0026#34;, :x)\r:(3 * 1 * x ^ (3 - 1))\rjulia\u0026gt; differentiate(\u0026#34;1 + x + x^2\u0026#34;, :x)\r:(1 + 2 * 1 * x ^ (2 - 1))\rjulia\u0026gt; differentiate(\u0026#34;1 + 2x + 3x^2 + 4x^3\u0026#34;, :x)\r:((0 * x + 2 * 1) + (0 * x ^ 2 + 3 * (2 * 1 * x ^ (2 - 1))) + (0 * x ^ 3 + 4 * (3 * 1 * x ^ (3 - 1))))\rjulia\u0026gt; differentiate(\u0026#34;x^2 * sin(x) + exp(x) * cos(x)\u0026#34;, :x)\r:(((2 * 1 * x ^ (2 - 1)) * sin(x) + x ^ 2 * (1 * cos(x))) + ((1 * exp(x)) * cos(x) + exp(x) * (1 * -(sin(x))))) 入力された記号でない文字は定数として扱い、二つ以上の記号を入力した場合は、それぞれの記号に対する微分を返す。ただし、\u0026quot;3yx\u0026quot;と書くとxy自体を一つの変数と見てしまうので、必ず掛け算記号を入れて3x*yのように表さなければならない。\njulia\u0026gt; differentiate(\u0026#34;1 + x + 3yx + y^2\u0026#34;, :x)\r:(1 + (0yx + 3 * 0))\rjulia\u0026gt; differentiate(\u0026#34;1 + x + 3y*x + y^2\u0026#34;, :x)\r:(1 + ((0y + 3 * 0) * x + (3y) * 1))\rjulia\u0026gt; differentiate(\u0026#34;1 + x + 3yx + y^2\u0026#34;, [:x, :y])\r2-element Vector{Any}:\r:(1 + (0yx + 3 * 0))\r:((0yx + 3 * 0) + 2 * 1 * y ^ (2 - 1))\rjulia\u0026gt; differentiate(\u0026#34;1 + x + 3x*y + y^2\u0026#34;, [:x, :y])\r2-element Vector{Any}:\r:(1 + ((0 * x + 3 * 1) * y + (3x) * 0))\r:(((0 * x + 3 * 0) * y + (3x) * 1) + 2 * 1 * y ^ (2 - 1)) simplify() differentiate()の返り値は読みにくいが、simplify()はこれをきれいに整理してくれる。ただし、ベクトルを入力として使うときはうまく実行されない。\njulia\u0026gt; simplify(differentiate(\u0026#34;1 + 2x + 3x^2 + 4x^3\u0026#34;, :x))\r:(2 + 3 * (2x) + 4 * (3 * x ^ 2))\rjulia\u0026gt; simplify(differentiate(\u0026#34;1 + x + 3x*y + y^2\u0026#34;, :x))\r:(1 + 3y)\rjulia\u0026gt; simplify(differentiate(\u0026#34;1 + x + 3x*y + y^2\u0026#34;, :y))\r:(3x + 2y)\rjulia\u0026gt; simplify(differentiate(\u0026#34;1 + x + 3x*y + y^2\u0026#34;, [:x, :y]))\r2-element Vector{Any}:\r:(1 + ((0 * x + 3 * 1) * y + (3x) * 0))\r:(((0 * x + 3 * 0) * y + (3x) * 1) + 2 * 1 * y ^ (2 - 1)) deparse() 記号微分の返り値を文字列に変換する。\njulia\u0026gt; a = differentiate(\u0026#34;1 + x + 3x*y + y^2\u0026#34;, :x)\r:(1 + ((0 * x + 3 * 1) * y + (3x) * 0))\rjulia\u0026gt; deparse(a)\r\u0026#34;1 + ((0 * x + 3 * 1) * y + (3 * x) * 0)\u0026#34;\rjulia\u0026gt; deparse(simplify(a))\r\u0026#34;1 + 3 * y\u0026#34; 検算 check_derivative() derivative()で得た導関数が本当の導関数とどれくらい違うかを確認することができる。jacobian()を除いた他の4つの導関数に対して実装されている。\ncheck_derivative(f, Df, a): derivative(f, a)-Df(a)の絶対値を返す。 julia\u0026gt; f(x) = 1 + x^2\rf (generic function with 1 method)\rjulia\u0026gt; Df(x) = 2x\rDf (generic function with 1 method)\rjulia\u0026gt; Calculus.check_derivative(f, Df, 1)\r2.6229241001374248e-11 応用 Polynomials.jl Polynomials.jl自体にもderivativeが実装されているが、Calculus.derivative()でも導関数を求めることができる。\njulia\u0026gt; p = Polynomial([1,2,4,1])\rPolynomial(1 + 2*x + 4*x^2 + x^3)\rjulia\u0026gt; Polynomials.derivative(p)\rPolynomial(2 + 8*x + 3*x^2)\rjulia\u0026gt; Calculus.derivative(p)\r#1 (generic function with 1 method)\rjulia\u0026gt; Polynomials.derivative(p,2)\rPolynomial(8 + 6*x)\rjulia\u0026gt; Calculus.second_derivative(p)\r#6 (generic function with 1 method) 環境 OS: Windows10 Version: Julia 1.6.2, Calculus 0.5.1 https://github.com/JuliaMath/Calculus.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3135,"permalink":"https://freshrimpsushi.github.io/jp/posts/3135/","tags":null,"title":"ジュリアでの微分の求め方"},{"categories":"다변수벡터해석","contents":"要約 二つの関数 $\\mathbf{g} : D \\subset \\mathbb{R}^{m} \\to \\mathbb{R}^{k}$、$\\mathbf{f} : \\mathbf{g}(\\mathbb{R}^{k}) \\subset \\mathbb{R}^{k} \\to \\mathbb{R}^{n}$が微分可能だとしよう。すると、これら二つの関数の合成 $\\mathbf{F} = \\mathbf{f} \\circ \\mathbf{g} : \\mathbb{R}^{m} \\to \\mathbb{R}^{n}$も微分可能であり、$\\mathbf{F}$の(全)導関数は次を満たす。\n$$ \\mathbf{F}^{\\prime}(\\mathbf{x}) = \\mathbf{f}^{\\prime}\\left( \\mathbf{g}(\\mathbf{x}) \\right) \\mathbf{g}^{\\prime}(\\mathbf{x}) $$\n解説 これを連鎖律と呼ぶ。\n$\\mathbf{x} = (x_{1}, \\dots, x_{m})$、$\\mathbf{g}(\\mathbf{x}) = (g_{1}, \\dots, g_{k})$、$\\mathbf{f}(g_{1}, \\dots, g_{k}) = (f_{1}, \\dots, f_{n})$とした場合、公式の具体的な形は全導関数の定義から次のような$n \\times m$行列である。\n$$ \\begin{align*} \\mathbf{F}^{\\prime} (\\mathbf{x}) =\u0026amp;\\ \\begin{bmatrix} \\dfrac{\\partial f_{1}(\\mathbf{g}(\\mathbf{x}))}{\\partial g_{1}} \u0026amp; \\dfrac{\\partial f_{1}}{\\partial g_{2}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial f_{1}}{\\partial g_{k}} \\\\[1em] \\dfrac{\\partial f_{2}}{\\partial g_{1}} \u0026amp; \\dfrac{\\partial f_{2}}{\\partial g_{2}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial f_{2}}{\\partial g_{k}} \\\\[1em] \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\[1em] \\dfrac{\\partial f_{n}}{\\partial g_{1}} \u0026amp; \\dfrac{\\partial f_{n}}{\\partial g_{2}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial f_{n}}{\\partial g_{k}} \\end{bmatrix} \\begin{bmatrix} \\dfrac{\\partial g_{1}(\\mathbf{x})}{\\partial x_{1}} \u0026amp; \\dfrac{\\partial g_{1}}{\\partial x_{2}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial g_{1}}{\\partial x_{m}} \\\\[1em] \\dfrac{\\partial g_{2}}{\\partial x_{1}} \u0026amp; \\dfrac{\\partial g_{2}}{\\partial x_{2}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial g_{2}}{\\partial x_{m}} \\\\[1em] \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\[1em] \\dfrac{\\partial g_{k}}{\\partial x_{1}} \u0026amp; \\dfrac{\\partial g_{k}}{\\partial x_{2}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial g_{k}}{\\partial x_{m}} \\end{bmatrix} \\\\[1em] =\u0026amp;\\ \\begin{bmatrix} \\dfrac{\\partial f_{1}}{\\partial g_{1}} \\dfrac{\\partial g_{1}}{\\partial x_{1}}+\\dfrac{\\partial f_{1}}{\\partial g_{2}}\\dfrac{\\partial g_{2}}{\\partial x_{1}} + \\cdots + \\dfrac{\\partial f_{1}}{\\partial g_{k}} \\dfrac{\\partial g_{k}}{\\partial x_{1}} \u0026amp; \\dots \u0026amp; \\dfrac{\\partial f_{1}}{\\partial g_{1}} \\dfrac{\\partial g_{1}}{\\partial x_{1}}+\\dfrac{\\partial f_{1}}{\\partial g_{2}}\\dfrac{\\partial g_{2}}{\\partial x_{m}} + \\cdots + \\dfrac{\\partial f_{1}}{\\partial g_{k}} \\dfrac{\\partial g_{k}}{\\partial x_{m}} \\\\[1em] \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\[1em] \\dfrac{\\partial f_{n}}{\\partial g_{1}} \\dfrac{\\partial g_{1}}{\\partial x_{1}}+\\dfrac{\\partial f_{n}}{\\partial g_{2}}\\dfrac{\\partial g_{2}}{\\partial x_{1}} + \\cdots + \\dfrac{\\partial f_{n}}{\\partial g_{k}} \\dfrac{\\partial g_{k}}{\\partial x_{1}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial f_{n}}{\\partial g_{1}} \\dfrac{\\partial g_{1}}{\\partial x_{1}}+\\dfrac{\\partial f_{n}}{\\partial g_{2}}\\dfrac{\\partial g_{2}}{\\partial x_{m}} + \\cdots + \\dfrac{\\partial f_{n}}{\\partial g_{m}} \\dfrac{\\partial g_{k}}{\\partial x_{m}} \\end{bmatrix} \\\\[1em] =\u0026amp;\\ \\begin{bmatrix} \\displaystyle \\sum\\limits_{\\ell =1}^{k} \\dfrac{\\partial f_{1}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{1}} \u0026amp; \\dots \u0026amp; \\displaystyle \\sum\\limits_{\\ell=1}^{k} \\dfrac{\\partial f_{1}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{m}} \\\\[1em] \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\[1em] \\displaystyle \\sum\\limits_{\\ell=1}^{k} \\dfrac{\\partial f_{n}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{1}} \u0026amp; \\dots \u0026amp; \\displaystyle \\sum\\limits_{\\ell=1}^{k} \\dfrac{\\partial f_{n}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{m}} \\end{bmatrix} \\end{align*} $$\nアインシュタインの記法で簡単に表すと、$1 \\le i \\le n$、$1 \\le j \\le m$に対して\n$$ \\mathbf{F}^{\\prime} = \\left[ F_{ij}^{\\prime} \\right] = \\begin{bmatrix} \\dfrac{\\partial f_{1}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{1}} \u0026amp; \\dots \u0026amp; \\dfrac{\\partial f_{1}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{m}} \\\\[1em] \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\[1em] \\dfrac{\\partial f_{n}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{1}} \u0026amp; \\dots \u0026amp; \\dfrac{\\partial f_{n}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{m}} \\end{bmatrix} $$\n$$ F_{ij}^{\\prime} = \\dfrac{\\partial f_{i}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{j}} $$\nこれは最も一般化された形なので、$k, m, n$に従って、さまざまな具体的な公式を得ることができる。\n公式 ケース 1. $g : \\mathbb{R} \\to \\mathbb{R}$、$f : \\mathbb{R} \\to \\mathbb{R}$、$F = f \\circ g : \\mathbb{R} \\to \\mathbb{R}$\n$x \\in \\mathbb{R}$、$g = g(x)$、$f = f(g(x))$のとき、\n$$ F^{\\prime} = \\dfrac{d F}{d x} = \\dfrac{d f}{d g} \\dfrac{d g}{d x} $$\n証明\nケース 2. $\\mathbf{g} : \\mathbb{R} \\to \\mathbb{R}^{k}$、$f : \\mathbb{R}^{k} \\to \\mathbb{R}$、$F = f \\circ \\mathbf{g} : \\mathbb{R} \\to \\mathbb{R}$\n$x \\in \\mathbb{R}$、$\\mathbf{g}(x) = (g_{1}, \\dots, g_{k})$、$f = f(g_{1}, \\dots ,g_{k})$のとき、\n$$ F^{\\prime} = \\dfrac{d F}{d x} = \\sum \\limits_{\\ell=1}^{k}\\dfrac{\\partial f}{\\partial g_{\\ell}} \\dfrac{d g_{\\ell}}{d x} $$\nケース 3. $g : \\mathbb{R}^{m} \\to \\mathbb{R}$、$f : \\mathbb{R} \\to \\mathbb{R}$、$F = f \\circ g : \\mathbb{R}^{m} \\to \\mathbb{R}$\n$\\mathbf{x} = (x_{1}, \\dots, x_{n}) \\in \\mathbb{R}^{n}$、$g = g(\\mathbf{x})$、$f = f(g(\\mathbf{x}))$のとき、\n$$ F^{\\prime} = \\dfrac{d F}{d \\mathbf{x}} = \\begin{bmatrix} \\dfrac{d f}{d g} \\dfrac{\\partial g}{\\partial x_{1}} \u0026amp; \\dots \u0026amp; \\dfrac{d f}{d g} \\dfrac{\\partial g}{\\partial x_{m}} \\end{bmatrix} $$\n$$ F_{j}^{\\prime} = \\dfrac{d f}{d g} \\dfrac{\\partial g}{\\partial x_{j}},\\quad 1 \\le j \\le m $$\nケース 4. $\\mathbf{g} : \\mathbb{R}^{m} \\to \\mathbb{R}^{k}$、$f : \\mathbb{R}^{k} \\to \\mathbb{R}$、$F = f \\circ \\mathbf{g} : \\mathbb{R}^{m} \\to \\mathbb{R}$\n$\\mathbf{x} = (x_{1}, \\dots, x_{n}) \\in \\mathbb{R}^{n}$、$\\mathbf{g}(x) = (g_{1}, \\dots, g_{k})$、$f = f(g_{1}, \\dots, g_{k})$のとき、\n$$ F^{\\prime} = \\dfrac{d F}{d \\mathbf{x}} = \\begin{bmatrix} \\sum \\limits_{\\ell=1}^{k} \\dfrac{\\partial f}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{1}} \u0026amp; \\dots \u0026amp; \\sum \\limits_{\\ell=1}^{k} \\dfrac{\\partial f}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{m}} \\end{bmatrix} $$\n$$ F_{j}^{\\prime} = \\sum \\limits_{\\ell=1}^{k} \\dfrac{\\partial f}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{j}},\\quad 1 \\le j \\le m $$\nケース 5. $g : \\mathbb{R} \\to \\mathbb{R}$、$\\mathbf{f} : \\mathbb{R} \\to \\mathbb{R}^{n}$、$\\mathbf{F} = \\mathbf{f} \\circ g : \\mathbb{R} \\to \\mathbb{R}^{n}$\n$x \\in \\mathbb{R}$、$g = g(x)$、$\\mathbf{f}(g(x)) = (f_{1}, \\dots, f_{n})$のとき、\n$$ \\mathbf{F}^{\\prime} = \\dfrac{d \\mathbf{F}}{d x} = \\begin{bmatrix} \\dfrac{d f_{1}}{d g} \\dfrac{d g}{d x} \\\\[1em] \\vdots \\\\[1em] \\dfrac{d f_{n}}{d g} \\dfrac{d g}{d x} \\end{bmatrix} $$\n$$ F_{i}^{\\prime} = \\dfrac{d f_{i}}{d g} \\dfrac{d g}{d x},\\quad 1\\le i \\le n $$\nケース 6. $\\mathbf{g} : \\mathbb{R} \\to \\mathbb{R}^{k}$、$\\mathbf{f} : \\mathbb{R}^{k} \\to \\mathbb{R}^{n}$、$\\mathbf{F} = \\mathbf{f} \\circ \\mathbf{g} : \\mathbb{R} \\to \\mathbb{R}^{n}$\n$x \\in \\mathbb{R}$、$\\mathbf{g}(x) = (g_{1}, \\dots, g_{k})$、$\\mathbf{f}(g_{1}, \\dots ,g_{k}) = (f_{1}, \\dots, f_{n})$のとき、\n$$ \\mathbf{F}^{\\prime} = \\dfrac{d \\mathbf{F}}{d x} = \\begin{bmatrix} \\sum \\limits_{\\ell=1}^{k} \\dfrac{\\partial f_{1}}{\\partial g_{\\ell}} \\dfrac{d g_{\\ell}}{d x} \\\\[1em] \\vdots \\\\[1em] \\sum \\limits_{\\ell=1}^{k} \\dfrac{\\partial f_{n}}{\\partial g_{\\ell}} \\dfrac{d g_{\\ell}}{d x} \\end{bmatrix} $$\n$$ F_{i}^{\\prime} = \\sum \\limits_{\\ell=1}^{k} \\dfrac{\\partial f_{i}}{\\partial g_{\\ell}} \\dfrac{d g_{\\ell}}{d x},\\quad 1\\le i \\le n $$\nケース 7. $g : \\mathbb{R}^{m} \\to \\mathbb{R}$、$\\mathbf{f} : \\mathbb{R} \\to \\mathbb{R}^{n}$、$\\mathbf{F} = \\mathbf{f} \\circ g : \\mathbb{R}^{m} \\to \\mathbb{R}^{n}$\n$\\mathbf{x} = (x_{1}, \\dots, x_{n}) \\in \\mathbb{R}^{n}$、$g = g(\\mathbf{x})$、$\\mathbf{f}(g(\\mathbf{x})) = (f_{1}, \\dots, f_{n})$のとき、\n$$ \\mathbf{F}^{\\prime} = \\dfrac{d \\mathbf{F}}{d \\mathbf{x}} = \\begin{bmatrix} \\dfrac{d f_{1}}{d g} \\dfrac{\\partial g}{\\partial x_{1}} \u0026amp; \\dots \u0026amp; \\dfrac{d f_{1}}{d g} \\dfrac{\\partial g}{\\partial x_{m}} \\\\[1em] \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\[1em] \\dfrac{d f_{n}}{d g} \\dfrac{\\partial g}{\\partial x_{1}} \u0026amp; \\dots \u0026amp; \\dfrac{d f_{n}}{d g} \\dfrac{\\partial g}{\\partial x_{m}} \\end{bmatrix} $$\n$$ F_{ij}^{\\prime} = \\dfrac{d f_{i}}{d g} \\dfrac{\\partial g}{\\partial x_{j}},\\quad 1\\le i \\le n, 1 \\le j \\le m $$\nケース 8. $\\mathbf{g} : \\mathbb{R}^{m} \\to \\mathbb{R}^{k}$、$\\mathbf{f} : \\mathbb{R}^{k} \\to \\mathbb{R}^{n}$、$\\mathbf{F} = \\mathbf{f} \\circ \\mathbf{g} : \\mathbb{R}^{m} \\to \\mathbb{R}^{n}$\n$\\mathbf{x} = (x_{1}, \\dots, x_{n}) \\in \\mathbb{R}^{n}$、$g(\\mathbf{x}) = (g_{1}, \\dots, g_{k})$、$\\mathbf{f}(g_{1}, \\dots, g_{k}) = (f_{1}, \\dots, f_{n})$のとき、\n$$ \\mathbf{F}^{\\prime} = \\dfrac{d \\mathbf{F}}{d \\mathbf{x}} = \\begin{bmatrix} \\sum \\limits_{\\ell=1}^{k} \\dfrac{\\partial f_{1}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{1}} \u0026amp; \\dots \u0026amp; \\sum \\limits_{\\ell=1}^{k} \\dfrac{\\partial f_{1}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{m}} \\\\[1em] \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\[1em] \\sum \\limits_{\\ell=1}^{k} \\dfrac{\\partial f_{n}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{1}} \u0026amp; \\dots \u0026amp; \\sum \\limits_{\\ell=1}^{k} \\dfrac{\\partial f_{n}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{m}} \\end{bmatrix} $$\n$$ F_{ij}^{\\prime} = \\sum \\limits_{\\ell=1}^{k} \\dfrac{\\partial f_{i}}{\\partial g_{\\ell}} \\dfrac{\\partial g_{\\ell}}{\\partial x_{j}},\\quad 1\\le i \\le n, 1 \\le j \\le m $$\n証明 一般化された証明を参照。\n■\n","id":3134,"permalink":"https://freshrimpsushi.github.io/jp/posts/3134/","tags":null,"title":"多変数ベクトル関数の連鎖律"},{"categories":"줄리아","contents":"コード fill() 関数を使えばいい。Rの rep() 関数と似た機能をする。\n","id":2101,"permalink":"https://freshrimpsushi.github.io/jp/posts/2101/","tags":null,"title":"ジュリアで特定の値で埋めた配列を作る方法"},{"categories":"기하학","contents":"ビルドアップ1 微分多様体 $M$ の各点で接ベクトルを定義しようとしている。微分可能な曲線 $\\alpha : (-\\epsilon , \\epsilon) \\to M$が与えられたとする。これから、微分幾何学でのように、$\\alpha$の$t=0$での微分係数$\\dfrac{d \\alpha}{dt}(0)$を接ベクトルと定義したいが、$\\alpha$の値域が$M$であるため（距離空間とは限らないため）、$\\alpha$の導関数を言及することができない。このため、多様体上の接ベクトルを関数、つまりオペレーターとして定義することになる。微分幾何学を学んだなら、ベクトルをオペレーターとして扱うことに慣れているはずだ。次の説明を見てみよう。\n方向微分\n$\\mathbf{X} \\in T_{p}M$を曲面$M$の点$p$での接ベクトル、$\\alpha (t)$を$M$上の曲線とする。この時、$\\alpha : (-\\epsilon, \\epsilon) \\to M$であり、$\\alpha (0) = p$を満たす。つまり、$\\mathbf{X} = \\dfrac{d \\alpha}{d t} (0)$である。ここで関数$f$を曲面$M$上の点$p \\in M$のある近傍で定義された微分可能な関数とする。すると$\\mathbf{X}$方向への$f$の方向微分directional derivative$\\mathbf{X}f$を次のように定義する。\n$$ \\mathbf{X} : \\mathcal{D} \\to \\mathbb{R}, \\quad \\text{where } \\mathcal{D} \\text{ is set of all differentiable functions near } p $$\n$$ \\mathbf{X} f := \\dfrac{d}{dt_{}} (f \\circ \\alpha) (0) $$\n上の定義から見て、固定された接ベクトル$\\mathbf{X}$があれば、$f$が与えられるたびに$\\mathbf{X}f$が決定される。したがって、接ベクトルはそれ自体がオペレーターとして扱われる。$\\mathbf{X}f$のような記法も、オペレーターの観点から見るために使われる。微分多様体上の接ベクトルも同様に、$M$上で微分可能な関数$f$が与えられるたびに、$f$とある曲線$\\alpha$との合成を通じて実数空間をマッピングする関数として定義される。\n定義 $M$を$n$次元の微分多様体とする。微分可能な関数 $\\alpha : (-\\epsilon , \\epsilon) \\to M$を**$M$で微分可能な曲線**とする。$\\alpha (0)=p\\in M$と仮定する。そして集合$\\mathcal{D}$を次のように$p$で微分可能な関数の集合として定義する。\n$$ \\mathcal{D} := \\left\\{ f : M \\to \\mathbb{R} | \\text{functions on } M \\text{that are differentiable at } p \\right\\} $$\nすると、$\\alpha (0) = p$での接ベクトル$\\alpha^{\\prime}(0) : \\mathcal{D} \\to \\mathbb{R}$を次のような関数として定義する。\n$$ \\alpha^{\\prime} (0) f = \\dfrac{d}{dt} (f\\circ \\alpha)(0),\\quad f\\in \\mathcal{D} $$\n点$p\\in M$でのすべての接ベクトルの集合を接空間tangent spaceと呼び、$T_{p}M$のように表す。\n説明 $f : M \\to \\mathbb{R}$と$\\alpha : (-\\epsilon, \\epsilon) \\to M$はそれぞれ定義域と値域が距離空間であることが保証されていないため、古典的な意味で微分できないが、これらの合成である$f \\circ \\alpha : (-\\epsilon, \\epsilon) \\to \\mathbb{R}$は微分可能である。\nある微分可能な曲線$\\alpha$が与えられるたびに接ベクトルが決定されるので、微分可能な曲線が存在する限り接ベクトルが存在すると考えることができる。また、二つの接ベクトル$\\mathbf{X}, \\mathbf{Y}$が異なる二つの曲線$\\alpha$、$\\beta$によって決定されたとしても、すべての$f \\in \\mathcal{D}$に対して$\\mathbf{X}f = \\mathbf{Y}f$が成立する場合、$\\mathbf{X}$と$\\mathbf{Y}$を同じ接ベクトルとして扱う。\n接ベクトルの集合$T_{p}M$を接空間と呼ぶ理由は、これが実際に$n$次元のベクトル空間であるからである。\n以下に紹介する定理から、点$p$での接ベクトルの関数値$\\alpha^{\\prime}(0)f$は、任意の座標系$\\mathbf{x} : U \\to M$を一つ選択することでこれに対して表すことができ、この値は$\\mathbf{x}$の選択に依存しないことがわかる。\n例 $T_{p}\\mathbb{R}^{3}$を考えよう。ある微分可能な曲線$\\alpha : (-\\epsilon, \\epsilon) \\to \\mathbb{R}^{3}$が決定されると、3次元ベクトル$\\alpha^{\\prime}(0) = \\mathbf{v} = (v_{1}, v_{2}, v_{3}) \\in \\mathbb{R}^{3}$が決定される。定義により、接ベクトルは次のようになる。$f : \\mathbb{R}^{3} \\to \\mathbb{R}$に対して、\n$$ \\mathbf{X}f = \\dfrac{d (f\\circ \\alpha)}{d t}(0) = \\sum \\limits_{i} \\dfrac{\\partial f}{\\partial x_{i}}\\dfrac{d \\alpha_{i}}{d t}(0) = \\sum\\limits_{i} v_{i} \\dfrac{\\partial f}{\\partial x_{i}} $$\nこれはユークリッド空間での方向微分と同じである。\n$$ \\mathbf{v}[f] = \\nabla _{\\mathbf{v}}f = \\mathbf{v} \\cdot \\nabla f = \\sum \\limits_{i} v_{i} \\dfrac{\\partial f}{\\partial v_{i}} $$\n方向微分はベクトルをオペレーターとして扱ったものであり、実質的にベクトルと同じである。したがって、$\\mathbf{X}$は$\\mathbb{R}^{3}$の要素として扱うことができ、次が成立する。\n$$ T_{p}\\mathbb{R}^{3} \\approxeq \\mathbb{R}^{3} $$\n定理 $\\alpha (0) = p$である微分可能な曲線と点$p$での座標系$\\mathbf{x} : U \\to M$が与えられたとする。$(u_{1}, \\dots, u_{n})$は$\\mathbb{R}^{n}$の座標であり、\n$$ (x_{1}(p), \\dots, x_{n}(p)) = \\mathbf{x}^{-1}(p) $$\nとする。すると、次の式が成立する。\n$$ \\begin{align*} \\alpha ^{\\prime} (0) f =\u0026amp;\\ \\sum \\limits_{i=1}^{n}x_{i}^{\\prime}(p) \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{p} \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{n}x_{i}^{\\prime}(\\alpha (0)) \\left.\\dfrac{\\partial f}{\\partial x_{i}}\\right|_{t=0} \\end{align*} $$\nこの時、単純に$x_{i}^{\\prime}(0) = x_{i}^{\\prime}(\\alpha (0))$と表記する。したがって、$\\alpha^{\\prime}(0)$は次のような微分オペレーターである。\n$$ \\begin{equation} \\alpha ^{\\prime} (0) = \\sum \\limits_{i=1}^{n}x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0} \\end{equation} $$\n基底$\\left\\{ \\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0} \\right\\}$に対して座標ベクトルで表記すると、次のようになる。\n$$ \\alpha ^{\\prime} (0) = \\begin{bmatrix} x_{1}^{\\prime}(0) \\\\ \\vdots \\\\ x_{n}^{\\prime}(0) \\end{bmatrix} $$\n証明 $p = \\mathbf{x}(0)$となるような$M$の座標系 $\\mathbf{x} : U \\subset \\mathbb{R}^{n} \\to M$を一つ選ぼう。接ベクトルを座標系で表現できるように$f\\circ \\alpha = f \\circ \\mathbf{x} \\circ \\mathbf{x}^{-1} \\circ \\alpha$のように考える。すると、$\\mathbf{x} \\circ \\mathbf{x}^{-1} = I$は恒等関数であるため、どの座標系を選んでも関係ないことがわかる。これから、$f \\circ \\mathbf{x}$と$\\mathbf{x}^{-1} \\circ \\alpha$をそれぞれ全体として一つの関数とみなし、$f \\circ \\alpha$をこれら二つの合成関数と考える。\n$$ f \\circ \\alpha = (f \\circ \\mathbf{x}) \\circ (\\mathbf{x}^{-1} \\circ \\alpha) $$\nまず$f \\circ \\mathbf{x}$を考える。$f \\circ \\mathbf{x} : \\mathbb{R}^{n} \\to \\mathbb{R}$であるため、次のように表現され、古典的な意味で微分が可能である。\n$$ f \\circ \\mathbf{x} = f \\circ \\mathbf{x} (u) = f \\circ \\mathbf{x} (u_{1}, u_{2}, \\dots, u_{n}),\\quad u=(u_{1},\\dots,u_{n}) \\in \\mathbb{R}^{n} $$\n$\\mathbf{x}^{-1} \\circ \\alpha$もまた、$\\mathbf{x}^{-1} \\circ \\alpha : \\mathbb{R} \\to \\mathbb{R}^{n}$であるため、次のように表現され、古典的な意味で微分が可能である。\n$$ \\begin{align*} \\mathbf{x}^{-1} \\circ \\alpha (t) =\u0026amp;\\ (x_{1}(\\alpha (t)), x_{2}(\\alpha (t)), \\dots, x_{n}(\\alpha (t))) \\\\ =\u0026amp;\\ (x_{1}(t), x_{2}(t), \\dots, x_{n}(t)) \\end{align*} $$\nこの時、$x_{i}$は$x_{i} : M \\to \\mathbb{R}$である関数であり、$x_{i}(t)$は$x_{i}(\\alpha (t))$を簡単に表記したものであることに注意する。\nこのように考えると、$f \\circ \\alpha$は二つの関数の合成であり、$\\mathbb{R} \\overset{\\mathbf{x}^{-1} \\circ \\alpha}{\\longrightarrow} \\mathbb{R}^{n} \\overset{f\\circ \\mathbf{x}}{\\longrightarrow} \\mathbb{R}$のようにマッピングされる。したがって、連鎖律により、次が成立する。\n$$ \\dfrac{d}{d t}(f \\circ \\alpha) = \\dfrac{d}{dt} \\left( (f\\circ \\mathbf{x}) \\circ (\\mathbf{x}^{-1} \\circ \\alpha) \\right) = \\sum \\limits_{i=1}^{n}\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}} \\dfrac{d (\\mathbf{x}^{-1} \\circ \\alpha )_{i}}{d t} = \\sum \\limits_{i=1}^{n}\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}} \\dfrac{d x_{i}}{d t} $$\nしたがって、次を得る。\n$$ \\begin{align*} \\alpha^{\\prime}(0) f :=\u0026amp;\\ \\dfrac{d}{dt} (f\\circ \\alpha)(0) \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{n} \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{t=0} \\dfrac{d x_{i}}{d t}(0) \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{n} \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{t=0} x_{i}^{\\prime}(0) \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{n} x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{t=0} \\end{align*} $$\nここで、$\\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0}$を次のようなオペレーターとして定義しよう。\n$$ \\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0} f := \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{t=0} $$\n$\\dfrac{\\partial f}{\\partial x_{i}}$の意味をまとめると、次のようになる。\n$f$は定義域が$M$であるため微分できない。したがって、座標系$\\mathbf{x} : \\mathbb{R}^{n} \\to M$との合成を通じて$f\\circ \\mathbf{x}$を考える。これは$\\mathbb{R}^{n}$を$\\mathbb{R}$にマッピングするため、古典的な意味で微分が可能である。したがって、$\\dfrac{\\partial f}{\\partial x_{i}}$は$f$を$\\mathbf{x}$と合成した後、これをユークリッド空間$\\mathbb{R}^{n}$の$i$番目の変数$u_{i}$に対して微分したものとして定義する。\n最終的に次を得る。\n$$ \\begin{align*} \\alpha^{\\prime}(0) f =\u0026amp;\\ \\sum \\limits_{i=1}^{n} x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial (f\\circ \\mathbf{x})}{\\partial u_{i}}\\right|_{t=0} \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{n} x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0}f = \\ \\sum \\limits_{i=1}^{n} x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial f}{\\partial x_{i}}\\right|_{t=0} \\end{align*} $$\n$$ \\implies \\alpha^{\\prime}(0) = \\sum \\limits_{i=1}^{n} x_{i}^{\\prime}(0) \\left.\\dfrac{\\partial }{\\partial x_{i}}\\right|_{t=0} $$\n■\n関連項目 解析学での方向微分 微分幾何学での方向微分 Manfredo P. Do Carmo, Riemannian Geometry (英語版, 1992), p6-8\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3132,"permalink":"https://freshrimpsushi.github.io/jp/posts/3132/","tags":null,"title":"微分可能多様体上の接線ベクトル"},{"categories":"기하학","contents":"定義1 $M_{1}, M_{2}$をそれぞれ$n, m$次元の微分多様体とする。マッピング$\\varphi : M_{1} \\to M_{2}$が以下の条件を満たせば、$p \\in M_{1}$で微分可能differentiable at $p$と定義される。\n$\\varphi(p)$で座標系$\\mathbf{y} : V \\subset \\mathbb{R}^{m} \\to M_{2}$が与えられた時、$p$で座標系$\\mathbf{x} : U \\subset \\mathbb{R}^{n} \\to M_{1}$が存在し、$\\varphi\\left( \\mathbf{x}(U) \\right) \\subset \\mathbf{y}(V)$が成立する。\nマッピング$\\mathbf{y}^{-1} \\circ \\varphi \\circ \\mathbf{x} : U \\subset \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$が$\\mathbf{x}^{-1}(p)$で微分可能である。\n説明 微分可能な多様体を定義するときと同様に、座標系$\\mathbf{x}, \\mathbf{y}$を通じて微分を定義する。\n条件1.は一見難しそうだが、よく見ると$\\epsilon -\\delta$の方法の定義や位相数学での連続性を定義するセンスと完全に一致している。\n条件2.における$\\mathbf{y}^{-1} \\circ \\varphi \\circ \\mathbf{x}$は、ユークリッド空間からユークリッド空間への関数なので、古典的なセンスで微分可能である。このマッピングは、座標系$\\mathbf{x}$と$\\mathbf{y}$での$\\varphi$のexpressionと呼ばれる。\nManfredo P. Do Carmo, Riemannian Geometry (Eng Edition, 1992), p5-6\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3130,"permalink":"https://freshrimpsushi.github.io/jp/posts/3130/","tags":null,"title":"微分可能多様体から微分可能多様体への微分可能関数"},{"categories":"줄리아","contents":"コード XsDB_주거인구_100M_TM.shpというshpファイルを読み込むコードは以下の通りだ。\nusing Shapefile\rcd(@__DIR__)\rpath = \u0026#34;XsDB_주거인구_100M_TM.shp\u0026#34;\rtable = Shapefile.Table(path)\rusing DataFrames\rdf = DataFrame(table) もちろん、ファイルを読み込むだけではできることが限られており、データを調べるためにはデータフレームに変換する必要がある。\n実行結果 959660×16 DataFrame\rRow │ geometry MEGA_NM MEGA_CD CTY_NM CTY_CD X_AXIS Y_AXIS HOUS POP POP_10 POP_20 POP_30 POP_40 POP_50 POP_60_O \\xb9\\xe8\\xc6\\xf7ó │ Point…? String String String String Int64 Int64 Float64 Float64 Float64 Float64 Float64 Float64 Float64 Float64 String\r────────┼─────────────────────────────────────────────────────────────────────────────────────────────────\r1 │ Point(254298.0, 4.26549e5) \\xb0\\xe6\\xb1\\u2d75 41 \\xbf\\xa9\\xc1ֱ\\xba 41730 365950 526450 10.08 24.56 4.64 1.92 1.84 4.72 4.32 7.12 biz-gis.com\r2 │ Point(2.59622e5, 4.24405e5) \\xb0\\xe6\\xb1\\u2d75 41 \\xbf\\xa9\\xc1ֱ\\xba 41730 371250 524250 1.42 3.47 0.81 0.29 0.52 0.52 0.59 0.74 biz-gis.com\r3 │ Point(2.61134e5, 423221.0) \\xb0\\xe6\\xb1\\u2d75 41 \\xbf\\xa9\\xc1ֱ\\xba 41730 372750 523050 1.26 3.08 0.68 0.28 0.35 0.49 0.45 0.83 biz-gis.com\r4 │ Point(2.50311e5, 4.15806e5) \\xb0\\xe6\\xb1\\u2d75 41 \\xbf\\xa9\\xc1ֱ\\xba 41730 361850 515750 10.08 25.2 3.68 2.96 1.68 4.4 6.0 6.48 biz-gis.com\r⋮ │ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮\r959658 │ Point(2.09955e5, 2.46768e5) \\xc0\\xfc\\xb6\\xf3\\xbaϵ\\xb5 45 \\xbf\\xcf\\xc1ֱ\\xba 45710 319750 347150 1.83 4.53 1.92 0.24 0.45 0.72 0.69 0.51 biz-gis.com\r959659 │ Point(215588.0, 4.55344e5) \\xb0\\xe6\\xb1\\u2d75 41 \\xb3\\xb2\\xbe\\xe7\\xc1ֽ\\xc3 41360 327550 555650 2.38 5.31 0.91 0.74 0.57 0.97 1.05 1.07 biz-gis.com\r959660 │ Point(2.54717e5, 4.24754e5) \\xb0\\xe6\\xb1\\u2d75 41 \\xbf\\xa9\\xc1ֱ\\xba 41730 366350 524650 1.26 3.07 0.58 0.24 0.23 0.59 0.54 0.89 biz-gis.com\r959653 rows omitted 環境 OS: Windows julia: v1.5.0 ","id":2097,"permalink":"https://freshrimpsushi.github.io/jp/posts/2097/","tags":null,"title":"ジュリアでSHPファイルを読む方法"},{"categories":"줄리아","contents":"## 概要 `trunc`関数を使うには、第一引数に`Int`を入れるだけだ。 ## コード julia\u0026gt; @time for t in 1:10^8 Int64(ceil(t/1000)) end 0.189653 seconds\njulia\u0026gt; @time for t in 1:10^8 trunc(Int64, ceil(t/1000)) end 0.128472 seconds\n二つのループは全く同じ機能をするけど、1.5倍ほどの速度差がある。上は`ceil`で小数点以下を切り捨てて`Int64`に型キャストしたんだけど、下は`trunc`関数の内蔵機能を使ってネイティブに整数を返すから、より速いんだ。 他の言語を使ってた人には、上のループみたいなストレートな命令が自然に感じるだろうけど、Juliaにはデータ型を第一引数に入れて結果を返すような内蔵関数が多いから、下のループみたいな使い方の方が慣れると思うよ。 ## 環境 - OS: Windows - julia: v1.5.0 ","id":2095,"permalink":"https://freshrimpsushi.github.io/jp/posts/2095/","tags":null,"title":"ジュリアで小数点以下を切り捨てて整数に変換する方法"},{"categories":"기하학","contents":"定義 1 正則曲線 $\\beta (t)$が単純Simpleであるとは、$\\beta$が単射関数であるか、あるいは何らかの整数$n \\in \\mathbb{Z}$に対して次を満たす周期$a \u0026gt; 0$の閉曲線であることを意味する。 $$ \\beta \\left( t_{1} \\right) = \\beta \\left( t_{2} \\right) \\iff t_{1} - t_{2} = na $$\n例 上のように単射関数として表現できないけれども単純曲線とされる場合は、\n閉曲線であり、ねじれた部分がない必要がある。ねじれた部分があると、その点で数式条件を満たすことができない。\nMillman. (1977). Elements of Differential Geometry: p54.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2094,"permalink":"https://freshrimpsushi.github.io/jp/posts/2094/","tags":null,"title":"単純曲線の定義"},{"categories":"줄리아","contents":"概要 rename!() 関数を使って変更するといい1。\n文字列のリストを与えて一度に変更する方法もあるし、個別に変更する方法もある。\nコード using DataFrames\rdf = DataFrame(rand(1:9, 10, 3), :auto)\rrename!(df, [\u0026#34;X\u0026#34;, \u0026#34;Y\u0026#34;, \u0026#34;Z\u0026#34;])\rrename!(df, :X =\u0026gt; :A) 実行すると、最初に次のようなデータフレームが生成される。\njulia\u0026gt; df = DataFrame(rand(1:9, 10, 3), :auto)\r10×3 DataFrame\rRow │ x1 x2 x3 │ Int64 Int64 Int64 ─────┼─────────────────────\r1 │ 2 3 6\r2 │ 9 2 4\r3 │ 3 3 4\r4 │ 3 3 3\r5 │ 9 1 6\r6 │ 3 1 5\r7 │ 4 8 4\r8 │ 9 8 4\r9 │ 4 6 1\r10 │ 1 9 7 一度に変更する方法 julia\u0026gt; rename!(df, [\u0026#34;X\u0026#34;, \u0026#34;Y\u0026#34;, \u0026#34;Z\u0026#34;])\r10×3 DataFrame\rRow │ X Y Z │ Int64 Int64 Int64 ─────┼─────────────────────\r1 │ 2 3 6\r2 │ 9 2 4\r3 │ 3 3 4\r4 │ 3 3 3\r5 │ 9 1 6\r6 │ 3 1 5\r7 │ 4 8 4\r8 │ 9 8 4\r9 │ 4 6 1\r10 │ 1 9 7 文字列のリストを与えればいい。\n一つずつ変更する方法 julia\u0026gt; rename!(df, :X =\u0026gt; :A)\r10×3 DataFrame\rRow │ A Y Z │ Int64 Int64 Int64 ─────┼─────────────────────\r1 │ 2 3 6\r2 │ 9 2 4\r3 │ 3 3 4\r4 │ 3 3 3\r5 │ 9 1 6\r6 │ 3 1 5\r7 │ 4 8 4\r8 │ 9 8 4\r9 │ 4 6 1\r10 │ 1 9 7 他の言語ではあまり見られない方法だが、列名の前に : を付けて、=\u0026gt; でマッピングする。ジュリアで : で始まる変数はシンボルSymbolだ。\n環境 OS: Windows julia: v1.6.2 https://discourse.julialang.org/t/change-column-names-of-a-dataframe-previous-methods-dont-work/48026/2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2093,"permalink":"https://freshrimpsushi.github.io/jp/posts/2093/","tags":null,"title":"ジュリアでデータフレームの列名を変更する方法"},{"categories":"기하학","contents":"定義 1 正則曲線 $\\beta (t)$ が閉曲線Closed Curveであることは、$\\beta$ 周期関数であることと等価だ。\n公式: 閉曲線の長さ $\\alpha (s)$ が周期 $a\u0026gt;0$ の閉曲線 $\\beta (t)$ に対する弦の長さの再パラメータ化である場合、$\\alpha$は周期 $L = \\int_{0}^{a} |d \\beta / dt| dt$ を持つ閉曲線だ。言い換えれば、閉曲線 $\\beta$ の長さは $L$ である。\n導出 $$ \\begin{align*} s(t+a) =\u0026amp; \\int_{0}^{t+a}\\left|\\frac{d \\beta}{d t}\\right| d t \\\\ =\u0026amp; \\int_{0}^{a}\\left|\\frac{d \\beta}{d t}\\right| d t+\\int_{a}^{t+a}\\left|\\frac{d \\beta}{d t}\\right| d t \\\\ =\u0026amp; L+\\int_{0}^{t}\\left|\\frac{d \\beta}{d t}\\right| d t \\\\ =\u0026amp; L+s(t) \\end{align*} $$ 要約すると、$s \\left( t + a \\right) = s(t) + L$、 $$ \\begin{align*} \\alpha (s + L) =\u0026amp; \\alpha \\left( s (t) + L \\right) \\\\ =\u0026amp; \\alpha \\left( s (t + a) \\right) \\\\ =\u0026amp; \\beta (t + a) \\\\ =\u0026amp; \\beta (t) \\\\ =\u0026amp; \\alpha \\left( s(t) \\right) \\\\ =\u0026amp; \\alpha (s) \\end{align*} $$ だから、$\\alpha (s)$ は閉曲線だ。$a \u0026gt; 0$ が $$ \\beta (t + a) = \\beta (t) \\qquad , \\forall t $$ を満たす最小の正の数であるため、$L\u0026gt;0$ もまた $$ \\alpha (s + L) = \\alpha (s) \\qquad , \\forall s $$ を満たす最小の正の数でなければならない。言い換えれば、$\\beta$ の長さは $L$ である。\n■\nMillman. (1977). Elements of Differential Geometry: p53.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2092,"permalink":"https://freshrimpsushi.github.io/jp/posts/2092/","tags":null,"title":"閉曲線の定義"},{"categories":"함수","contents":"定義1 関数 $f : X \\to Y$が与えられたとしよう。$U \\subset X \\subset V$が成立するとしよう。\n縮小写像 次を満たす$f |_{U} \\to Y$を$f$の縮小写像Uへの$f$の制限と言う。\n$$ f|_{U} : U \\to Y \\quad \\text{and} \\quad f|_{U}(x) = f (x),\\quad \\forall x \\in U $$\n拡張 次を満たす$\\tilde{f} \\to Y$を$f$の拡張Vへの$f$の拡張と言う。\n$$ \\tilde{f} : V \\to Y \\quad \\text{and} \\quad \\tilde{f}(x) = f (x),\\quad \\forall x \\in X $$\n説明 通常、縮小写像(制限とも言われる)、拡張という翻訳語より、そのまま英語読みの[リストラクション]、[エクステンション]と言うことが多い。\n簡単に言えば、関数の形をそのまま保ちながら、定義域を狭めたり広げたりすることである。\n定義によって、明らかに$f$は$\\tilde{f}$のリストラクションであり、$f|_{U}$のエクステンションである。\nErwin Kreyszig, Introductory Functional Analysis with Applications (1989), p99\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3123,"permalink":"https://freshrimpsushi.github.io/jp/posts/3123/","tags":null,"title":"関数の拡張と縮小"},{"categories":"편미분방정식","contents":"定義 以下の偏微分方程式をヘルムホルツ方程式Helmholtz equationという。\n$$ \\nabla^{2}u(x) + k^{2} u(x) = \\Delta u(x) + k^{2} u(x) = (\\Delta + k^{2} )u(x) = 0,\\quad x \\in \\mathbb{R}^{n} $$\nここで、$\\nabla ^{2} = \\Delta$はラプラシアンである。\n説明 $-\\Delta u = \\lambda u$のような形で表現することもできる。このため、ラプラス作用素の固有値方程式1とも呼ばれることがある。\n波動方程式から導出できるため、reduced wave equation2とも呼ばれる。\n波動方程式には時間と空間に関する微分が含まれているが、ヘルムホルツ方程式は時間に関する項が消え、時間に無関係な空間変数のみに依存する偏微分方程式である。\n導出 波動方程式は以下の通りである。\n$$ \\Delta u(x,t) - \\dfrac{1}{c^{2}}\\dfrac{\\partial^{2} u(x,t)}{\\partial t^{2}} = 0 $$\nこの時、$c$は波の速度を意味する。\n方法1 波動方程式の解、つまり波動関数は以下の通りである。\n$$ u (x,t) = u(x)u(t) = e^{ikx} e^{-i\\omega t} = e^{i(kx - \\omega t)} $$\nこの時、$x , t$はそれぞれ空間と時間、$k, \\omega$は波数と角周波数を意味する。波の速度が$c$の時、以下の関係が成り立つ。\n$$ k = \\dfrac{\\omega}{c} $$\nしたがって、$u_{tt}(x,t)$を求めると以下の通りである。\n$$ u_{tt}(x,t) = \\dfrac{\\partial ^{2}}{\\partial t^{2}}e^{i(kx - \\omega t)} = (-i \\omega)^{2}e^{i(kx - \\omega t)} = -\\omega^{2}e^{i(kx - \\omega t)} $$\nこれを波動方程式に代入すれば、ヘルムホルツ方程式を得る。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\Delta u - \\dfrac{1}{c^{2}}\\dfrac{\\partial^{2} u}{\\partial t^{2}} =\u0026amp;\\ 0 \\\\[1em] \\implies \u0026amp;\u0026amp; \\Delta e^{i(kx - \\omega t)} + \\dfrac{\\omega^{2}}{c^{2}} e^{i(kx - \\omega t)} =\u0026amp;\\ 0 \\\\[1em] \\implies \u0026amp;\u0026amp; \\left( \\Delta e^{ikx} + k^{2} e^{ikx} \\right) e^{-i\\omega t}=\u0026amp;\\ 0 \\\\[1em] \\implies \u0026amp;\u0026amp; \\Delta e^{ikx} + k^{2} e^{ikx}=\u0026amp;\\ 0 \\\\[1em] \\implies \u0026amp;\u0026amp; \\Delta u(x) + k^{2} u(x) =\u0026amp;\\ 0 \\end{align*} $$\n■\n方法2 波動方程式を$t$に対してフーリエ変換すると、以下を得る。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\Delta u(x,t) - \\dfrac{1}{c^{2}}u_{tt}(x,t) =\u0026amp;\\ 0 \\\\ \\implies \u0026amp;\u0026amp; \\widehat{\\Delta u}(x,\\omega) - \\dfrac{1}{c^{2}} \\widehat{u_{tt}}(x,\\omega) =\u0026amp;\\ 0 \\end{align*} $$\nここで、二番目の項にフーリエ変換の性質$\\widehat{u^{\\prime \\prime}}(\\omega) = - \\omega^{2} \\widehat{u}(\\omega)$を用いると、以下を得る。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\widehat{\\Delta u}(x,\\omega) + \\dfrac{\\omega^{2}}{c^{2}} \\widehat{u}(x,\\omega) =\u0026amp;\\ 0 \\\\[1em] \\implies \u0026amp;\u0026amp; \\widehat{\\Delta u}(x,\\omega) + k^{2} \\widehat{u}(x,\\omega) =\u0026amp;\\ 0 \\\\[1em] \\implies \u0026amp;\u0026amp; \\Delta u(x, t) + k^{2} u(x, t) =\u0026amp;\\ 0 \\end{align*} $$\n$u(x,t)$が変数分離されると仮定すると、\n$$ \\begin{align*} \u0026amp;\u0026amp; \\Delta u(x, t) + k^{2} u(x, t) =\u0026amp;\\ 0 \\\\[1em] \\implies \u0026amp;\u0026amp; \\Delta u(x) u(t) + k^{2} u(x) u(t) =\u0026amp;\\ 0 \\\\[1em] \\implies \u0026amp;\u0026amp; \\Delta u(x) + k^{2} u(x) =\u0026amp;\\ 0 \\end{align*} $$\n■\n方法3 $u(x, t) = u(x)v(t)$と同様に変数分離されると仮定して、次のように式を整理しよう。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\dfrac{\\partial^{2} u(x, t)}{\\partial x^{2}} =\u0026amp;\\ \\dfrac{1}{c^{2}}\\dfrac{\\partial^{2} u(x, t)}{\\partial t^{2}} \\\\[1em] \\implies \u0026amp;\u0026amp; \\dfrac{d^{2} u}{d x^{2}} v =\u0026amp;\\ \\dfrac{1}{c^{2}}\\dfrac{d^{2} v }{d t^{2}} u \\\\[1em] \\implies \u0026amp;\u0026amp; \\dfrac{1}{u}\\dfrac{d^{2} u}{d x^{2}} =\u0026amp;\\ \\dfrac{1}{c^{2}} \\dfrac{1}{v}\\dfrac{d^{2} v }{d t^{2}} \\end{align*} $$\nすると、左辺は$t$に無関係で、右辺は$x$に無関係であるため、両辺は$x$と$t$に対して定数であることが分かる。その定数を$-k^{2}$としよう。すると、以下を得る。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\dfrac{1}{u}\\dfrac{d^{2} u}{d x^{2}} =\u0026amp;\\ -k^{2} \\\\[1em] \\implies \u0026amp;\u0026amp; \\dfrac{d^{2} u}{d x^{2}} =\u0026amp;\\ -k^{2}u \\\\[1em] \\implies \u0026amp;\u0026amp; \\dfrac{d^{2} u}{d x^{2}} + k^{2}u =\u0026amp;\\ 0 \\end{align*} $$\n■\nLawrence C. Evans, Partial Differential Equations (2nd Edition, 2010), p3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDavid Colton and Rainer Kress, Inverse Acoustic and Electromagnetic Scattering Theory (4th Edition, 2019), p15\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3122,"permalink":"https://freshrimpsushi.github.io/jp/posts/3122/","tags":null,"title":"ヘルムホルツ方程式"},{"categories":"줄리아","contents":"概要 $n$個の座標同士の距離を計算するにあたり、行列を作る必要はなく、単に距離を計算する場合、多次元検索に有利なデータ構造であるk-dツリー1を使用して速度を上げることができます。NearestNeighbors.jlに関連するアルゴリズムがすべて実装されているので、公式GitHubページを参照してください。\n速度比較 pairwise()関数で距離行列の計算に最適化された技術と比較してみましょう。\nusing Distances\rusing StatsBase\rusing Random\rusing NearestNeighbors\rRandom.seed!(0);\rε = 0.01\rN = 10^4\rcoordinate = rand(2, N);\rstate = sample([\u0026#39;S\u0026#39;, \u0026#39;I\u0026#39;], Weights([0.1, 0.9]), N);\rS = coordinate[:, state .== \u0026#39;S\u0026#39;]\rI = coordinate[:, state .== \u0026#39;I\u0026#39;]\r@time sum(pairwise(Euclidean(),S,I) .\u0026lt; ε, dims = 1)\r@time kdtree = KDTree(S); contact = length.(inrange(kdtree, I, ε, true)) 上記のコードを実行した結果は以下の通りです。一番下の二つのコマンドラインは同じ作業を行いますが、速度差は約500倍になります。実際、k-dツリーで検索する際の時間複雑度は$\\log n$で非常に効率的です。\njulia\u0026gt; @time sum(pairwise(Euclidean(),S,I) .\u0026lt; ε, dims = 1)\r0.098394 seconds (14 allocations: 69.639 MiB, 8.23% gc Time)\r1×9004 Array{Int64,2}:\r0 0 1 0 0 0 0 … 1 0 0 0 0 0\rjulia\u0026gt; @time kdtree = KDTree(S); contact = length.(inrange(kdtree, I, ε, true))\r0.000213 seconds (22 allocations: 51.609 KiB)\r9004-element Array{Int64,1}:\r0\r0\r1\r0\r⋮\r0\r0\r0 単純な速度の問題を置いておいても、k-dツリーを使用した方法では1次元の配列を返すため、結果物を使用する側面でもさらに便利です。\n環境 OS: Windows julia: v1.6.2 https://en.wikipedia.org/wiki/K-d_tree\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2088,"permalink":"https://freshrimpsushi.github.io/jp/posts/2088/","tags":null,"title":"JuliaでNearstNeighbors.jlを使用して距離を素早く計算する方法"},{"categories":"기하학","contents":"定義1 $M$を任意の集合、$U_{\\alpha} \\subset \\mathbb{R}^{n}$を開集合とする。関数$1-1$ $\\mathbf{x}_{\\alpha} : U_{\\alpha} \\to M$に対して、以下の条件を満たす順序対$\\left( M, \\left\\{ \\mathbf{x}_{\\alpha} \\right\\}_{\\alpha\\in \\mathscr{A}} \\right)$、または簡単に$M$を**$n$次元の微分可能多様体**dimension $n$の differentiable manifoldと定義する。\n$\\bigcup \\limits_{\\alpha} \\mathbf{x}_{\\alpha} \\left( U_{\\alpha} \\right) = M$ $\\varnothing \\ne W = \\mathbf{x}_{\\alpha}\\left( U_{\\alpha} \\right) \\cap \\mathbf{x}_{\\beta}\\left( U_{\\beta} \\right)$に対して、写像$\\mathbf{x}_{\\beta}^{-1} \\circ \\mathbf{x}_{\\alpha} : \\mathbf{x}_{\\alpha}^{-1}(W) \\to \\mathbf{x}_{\\beta}^{-1}(W)$が微分可能であること。 条件1および2を満たす可能なすべての$\\alpha$に対して、指数族$\\left\\{ \\left( U_{\\alpha}, \\mathbf{x}_{\\alpha} \\right) \\right\\}$を構成する。 説明 単に微分多様体または滑らかな多様体とも言う。$n$次元の微分多様体は、時に$M^{n}$と表記する。\n$p \\in \\mathbf{x}_{\\alpha}(U_{\\alpha})$のとき、$\\left( U_{\\alpha}, \\mathbf{x}_{\\alpha} \\right)$または単に$\\mathbf{x}_{\\alpha}$を$M$の$p$における座標系$M$ at $p$の system of coordinates、局所座標系、またはパラメータ化と呼ぶ。\n$\\mathbf{x}_{\\alpha}(U_{\\alpha})$を$p \\in M$における座標近傍と呼ぶ。\n条件3.の指数族$\\left\\{ \\left( U_{\\alpha}, \\mathbf{x}_{\\alpha} \\right) \\right\\}$を$M$上の微分可能構造と呼ぶ。\n$p \\in M$に対して、$\\mathbf{x}_{\\alpha}^{-1}(p) = \\left( x_{1}(p), \\dots, x_{n}(p) \\right)$を満たす$x_{i}$を座標関数と呼ぶ。\n$M$は完全に任意の集合として与えられる(つまり、通常は距離空間ではない)ので、$\\mathbf{x}_{\\alpha}$が微分可能かどうかについての議論ができない。さらに、$M$は様々なイメージの合併であるため、各交差点$W = \\mathbf{x}_{\\alpha}\\left( U_{\\alpha} \\right) \\cap \\mathbf{x}_{\\beta}\\left( U_{\\beta} \\right)$で適切に良い条件が必要であり、ここではそれを微分可能であるという条件で与えられている。\n写像$\\mathbf{x}_{\\beta}^{-1} \\circ \\mathbf{x}_{\\alpha}$の条件によって、多様体が様々な名前で呼ばれることになる。例えば、微分の代わりに連続であるという条件が与えられた場合、$M$は位相多様体になる。正則という条件が与えられた場合、$M$は複素多様体になる。また、$\\mathbf{x}_{\\beta}^{-1} \\circ \\mathbf{x}_{\\alpha} \\in C^{k}$の場合、$M$は$C^{k}$多様体と呼ばれる。微分幾何学では、微分という道具を使って幾何学を説明したいため、微分可能な多様体が扱われる。\nこの内容は技術的な部分であり、二つの微分可能構造が同じか違うか等の話を避けるために存在する条件である。1および2を満たすそのようなものすべてが集められていると仮定した方が良い。「こんなのはどうだ?」「これも含まれるか?」といったタックルをかけないで欲しいという意味である。\n例 ユークリッド空間 $\\mathbb{R}^{n}$ $$ \\mathbb{R}^{n} = \\left\\{ (x_{1}, x_{2}, \\dots, x_{n}) : x_{i} \\in \\mathbb{R} \\right\\} $$\n多様体が局所的にユークリッド空間と似ているため、$\\mathbb{R}^{n}$が微分可能な多様体であることは自然なことである。${\\rm id}$を恒等作用素とする。\n微分可能構造を$\\left\\{ \\left( U_{\\alpha}, {\\rm id} \\right) | U_{\\alpha} \\subset \\mathbb{R}^{n} \\text{ is open.} \\right\\}$のようにすると成立する。\n恒等作用素は微分可能なので成立する。\nこのようなすべての順序対に対して、指数族$\\left\\{ \\left( U_{\\alpha}, {\\rm id} \\right)\\right\\}$を構成する。\nしたがって、$\\left( \\mathbb{R}^{n}, \\left\\{ {\\rm id} \\right\\} \\right)$は微分可能な多様体である。\n2次元球面 $\\mathbb{S}^{2}$ $$ \\mathbb{S}^{2} = \\left\\{ p \\in \\mathbb{R}^{3} : \\left\\| p \\right\\|=1 \\right\\} $$\n2次元球面は、以下のように6つの座標パッチで表現できる。$(u,v) \\in U = \\left\\{ (u,v) : u^{2} + v^{2} \\lt 1 \\right\\}$について、\n座標パッチ 定義 逆 $\\mathbf{x}_{1} = \\mathbf{x}_{(0,0,1)} : U \\to \\R^{3}$ $\\mathbf{x}_{(0,0,1)}(u, v) = \\left( u, v , \\sqrt{1- u^{2} -v^{2} } \\right)$ $\\mathbf{x}_{(0,0,1)}^{-1}(x, y, z) = (x,y)$ $\\mathbf{x}_{2} = \\mathbf{x}_{(0,0,-1)} : U \\to \\R^{3}$ $\\mathbf{x}_{(0,0,-1)}(u, v) = \\left( u, v , -\\sqrt{1- u^{2} -v^{2} } \\right)$ $\\mathbf{x}_{(0,0,-1)}^{-1}(x, y, z) = (x,y)$ $\\mathbf{x}_{3} = \\mathbf{x}_{(0,1,0)} : U \\to \\R^{3}$ $\\mathbf{x}_{(0,1,0)}(u, v) = \\left( u, \\sqrt{1- u^{2} -v^{2}}, v \\right)$ $\\mathbf{x}_{(0,1,0)}^{-1}(x, y, z) = (x,z)$ $\\mathbf{x}_{4} = \\mathbf{x}_{(0,-1,0)} : U \\to \\R^{3}$ $\\mathbf{x}_{(0,-1,0)}(u, v) = \\left( u, -\\sqrt{1- u^{2} -v^{2}}, v \\right)$ $\\mathbf{x}_{(0,-1,0)}^{-1}(x, y, z) = (x,z)$ $\\mathbf{x}_{5} = \\mathbf{x}_{(1,0,0)} : U \\to \\R^{3}$ $\\mathbf{x}_{(1,0,0)}(u, v) = \\left( \\sqrt{1- u^{2} -v^{2}}, u, v \\right)$ $\\mathbf{x}_{(1,0,0)}^{-1}(x, y, z) = (y,z)$ $\\mathbf{x}_{6} = \\mathbf{x}_{(-1,0,0)} : U \\to \\R^{3}$ $\\mathbf{x}_{(-1,0,0)}(u, v) = \\left( -\\sqrt{1- u^{2} -v^{2}}, u, v \\right)$ $\\mathbf{x}_{(-1,0,0)}^{-1}(x, y, z) = (y,z)$ $\\bigcup \\limits_{i=1}^6 \\mathbf{x}_{i} = \\mathbb{S}^{2}$が成立する。\n$\\mathbf{x}_{(0,0,1)}^{-1} \\circ \\mathbf{x}_{(1,0,0)}$は次のようになるため、微分可能である。\n$$\\mathbf{x}_{(0,0,1)}^{-1} \\circ \\mathbf{x}_{(1,0,0)}(u,v) = \\mathbf{x}_{(0,0,1)}^{-1} \\left( \\sqrt{1- u^{2} -v^{2}}, u, v \\right) = \\left( \\sqrt{1- u^{2} -v^{2}}, u \\right) \\in C^{\\infty}$$\nこの方法で1, 2を満たすすべての順序対を集め、指数族$\\left\\{ \\left( U_{\\alpha}, \\mathbf{x}_{\\alpha} \\right) \\right\\}$を構成する。 したがって、$\\left( \\mathbb{S}^{2} , \\left\\{ \\mathbf{x}_{\\alpha} \\right\\} \\right)$は微分可能な多様体である。\nManfredo P. Do Carmo, Riemannian Geometry (英語版, 1992), p2-3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3116,"permalink":"https://freshrimpsushi.github.io/jp/posts/3116/","tags":null,"title":"微分可能な多様体"},{"categories":"수리통계학","contents":"要約 random sample $X_{1} , \\cdots , X_{n}$ がパラメータ $\\theta \\in \\Theta$ に対して同じ確率質量/密度関数$f \\left( x ; \\theta \\right)$ を持つとしよう。統計量 $Y = u_{1} \\left( X_{1} , \\cdots , X_{n} \\right)$ が $\\theta$ の 十分統計量 であるのは、以下を満たす非負の関数 $k_{1} , k_{2} \\ge 0$ が存在する場合である。 $$ f \\left( x_{1} ; \\theta \\right) \\cdots f \\left( x_{n} ; \\theta \\right) = k_{1} \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) ; \\theta \\right] k_{2} \\left( x_{1} , \\cdots , x_{n} \\right) $$ ただし、$k_{2}$ は $\\theta$ に依存してはならない。\n証明 十分統計量の定義：$\\theta \\in \\Theta$ に依存しない $H \\left( x_{1} , \\cdots , x_{n} \\right)$ に対して $$ {{ f \\left( x_{1} ; \\theta \\right) \\cdots f \\left( x_{n} ; \\theta \\right) } \\over { f_{Y_{1}} \\left( u_{1} \\left( x_{1} , \\cdots, x_{n} \\right) ; \\theta \\right) }} = H \\left( x_{1} , \\cdots , x_{n} \\right) $$ これが真である場合、$Y_{1}$ を$\\theta$ のための 十分統計量Sufficient Statistic と呼ぶ。\n連続確率分布に対してのみ証明する。離散確率分布に対する証明はCasellaを参照されたい。\n$(\\Rightarrow)$\n十分統計量の定義から $f_{Y_{1}}$ は$k_{1}$ に、$H$ は $f_{2}$ に該当するので自明である。\n$(\\Leftarrow)$\n$$ \\begin{align*} y_{1} \u0026amp;:= u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) \\\\ y_{2} \u0026amp;:= u_{2} \\left( x_{1} , \\cdots , x_{n} \\right) \\\\ \u0026amp;\\vdots \\\\ y_{n} \u0026amp;:= u_{n} \\left( x_{1} , \\cdots , x_{n} \\right) \\end{align*} $$\n便宜上、上記の関数の逆関数を以下のように置き、ヤコビアンを $J$ と表す。\n$$ \\begin{align*} x_{1} \u0026amp;:= w_{1} \\left( y_{1} , \\cdots , y_{n} \\right) \\\\ x_{2} \u0026amp;:= w_{2} \\left( y_{1} , \\cdots , y_{n} \\right) \\\\ \u0026amp;\\vdots \\\\ x_{n} \u0026amp;:= w_{n} \\left( y_{1} , \\cdots , y_{n} \\right) \\end{align*} $$\nすると、$Y_{1} , \\cdots , Y_{n}$ の 結合確率密度関数 $g$ は $w_{i} = w_{i} \\left( y_{1} , \\cdots , y_{n} \\right)$ に対して $$ g \\left( y_{1} , \\cdots , y_{n} ; \\theta \\right) = k_{1} \\left( y_{1} ; \\theta \\right) k_{2} \\left( w_{1} , \\cdots , w_{n} \\right) \\left| J \\right| $$ であり、$Y_{1}$ の 周辺確率密度関数 $f_{Y_{1}}$ は $$ \\begin{align*} f_{Y_{1}} \\left( y_{1} ; \\theta \\right) =\u0026amp; \\int_{-\\infty}^{\\infty} \\cdots \\int_{-\\infty}^{\\infty} g \\left( y_{1} , \\dots , y_{n} ; \\theta \\right) d y_{2} \\cdots d y_{n} \\\\ =\u0026amp; k_{1} \\left( y_{1} ; \\theta \\right) \\int_{-\\infty}^{\\infty} \\cdots \\int_{-\\infty}^{\\infty} \\left| J \\right| k_{2} \\left( w_{1} , \\dots , w_{n} \\right) d y_{2} \\cdots d y_{n} \\end{align*} $$ $k_{2}$ は $\\theta$ に依存しない関数であり、$J$ も $\\theta$ を含まないため、右辺の積分は $y_{1}$ のみの関数として表すことができる。これを仮に $m \\left( y_{1} \\right)$ と表すと $$ f_{Y_{1}} \\left( y_{1} ; \\theta \\right) = k_{1} \\left( y_{1} ; \\theta \\right) m \\left( y_{1} \\right) $$ ここで $m \\left( y_{1} \\right) = 0$ であれば自明に $f_{Y_{1}} \\left( y_{1} ; \\theta \\right) = 0$ である。今、$m \\left( y_{1} \\right) \u0026gt; 0$ と仮定してみると、次のように書くことができる。 $$ k_{1} \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) ; \\theta \\right] = {{ f_{Y_{1}} \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) ; \\theta \\right] } \\over { m \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) \\right] }} $$ 与えられた式に代入すると $$ \\begin{align*} f \\left( x_{1} ; \\theta \\right) \\cdots f \\left( x_{n} ; \\theta \\right) =\u0026amp; k_{1} \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) ; \\theta \\right] k_{2} \\left( x_{1} , \\cdots , x_{n} \\right) \\\\ =\u0026amp; {{ f_{Y_{1}} \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) ; \\theta \\right] } \\over { m \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) \\right] }} k_{2} \\left( x_{1} , \\cdots , x_{n} \\right) \\\\ =\u0026amp; f_{Y_{1}} \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) ; \\theta \\right] {{ k_{2} \\left( x_{1} , \\cdots , x_{n} \\right) } \\over { m \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) \\right] }} \\end{align*} $$ $k_{2}$ と $m$ はどちらも $\\theta$ に依存しないので、定義により$Y_{1}$ は $\\theta$ の十分統計量である。\n■\n","id":2084,"permalink":"https://freshrimpsushi.github.io/jp/posts/2084/","tags":null,"title":"ノイマン因数分解定理の証明"},{"categories":"복소해석","contents":"定義 1 $$ g(t) := p(t) + i q(t) \\qquad , t \\in [a,b] $$\n実関数 $p, q : [a,b] \\to \\mathbb{R}$ に対し、複素関数 $g : [a,b] \\to \\mathbb{C}$ が上記のように示されるとする。区間 $[a,b]$ から $g$ までの定積分は次のように定義される。 $$ \\int_{a}^{b} g(t) dt = \\int_{a}^{b} p(t) dt + i \\int_{a}^{b} q(t) dt $$ $t \\in [a,b]$ に対して、パス $\\mathscr{C} : z(t) = x(t) + i y(t)$ に沿った複素路線積分を次のように定義する。 $$ \\int_{\\mathscr{C}} f(z) dz = \\int_{a}^{b} f \\left( z(t) \\right) z\u0026rsquo;(t) dt $$\n説明 アークArc、あるいはカーブCurve $\\mathscr{C} : z(t)$ の定義は、幾何学では重要かもしれないが、複素解析自体ではそこまで正確にする必要はないので、少し無視して進もう。以下の説明にこだわるよりは、微分幾何等でカーブをしっかり学ぶか、とりあえず目の前の $\\mathscr{C}$ については直感的な概念だけを受け入れて進んでも十分だ。\n$\\mathscr{C}$ に重なる部分がない場合、つまり次を満たす場合、シンプルSimple あるいは ジョルダンJordanという。 $$ z \\left( t_{1} \\right) = z \\left( t_{2} \\right) \\implies t_{1} = t_{2} \\qquad , \\forall t_{1}, t_{2} \\in [a,b] $$\nどこでも微分可能で、微分係数が $0$ でない場合、つまり次を満たす場合、スムースSmoothという。 $$ \\exists z\u0026rsquo;(t) \\ne 0 \\qquad , \\forall t \\in [a,b] $$\n有限個の（シンプルな）スムースなアークの端と端を繋いだものを（シンプルな）コントゥアContourという。コントゥアを翻訳すると等高線となるが、意味がうまく伝わらないこともあり、複素解析の文脈ではほとんどがコントゥアを反時計回りAnticlockwiseに沿って積分する場合に使われるため、$\\mathscr{C}$ をパスとして簡化することもできる。\n$a \\to b$ の方向に進む場合は $\\mathscr{C}$ ならば、$b \\to a$ の方向に進む場合は $-\\mathscr{C}$ と表される。パラメーターによると、$\\mathscr{C} : z(t) , a \\le t \\le b$ の時は次のようになる。 $$ -\\mathscr{C} : z(-t) , -b \\le t \\le -a $$\n正確に両端の位置が同じだけなら、つまり $z(a) = z(b)$ は 閉じたコントゥアClosed Contourという。\nもう一度強調する。この説明が優れた数学者には気に入らないかもしれないが、スキップして進もう。以下の性質を直感的に受け入れられるかが遥かに重要だ。\n基本性質 $f,g$ が $\\mathscr{C}$ で区分的に連続であるとする。\n[1]: すべての $\\alpha , \\beta \\in \\mathbb{C}$ において $$ \\int_{\\mathscr{C}} \\left( \\alpha f(z) + \\beta g(z) \\right) dz = \\alpha \\int_{\\mathscr{C}} f(z) dz + \\beta \\int_{\\mathscr{C}} g(z) dz $$ [2]: $\\mathscr{C}$ の方向が $a \\to b \\to c$ であり、$a \\to b$ の方向の $\\mathscr{C}_{1}$ と $b \\to c$ の方向の $\\mathscr{C}_{2}$ から成る場合 $$ \\int_{\\mathscr{C}} f(z) dz = \\int_{\\mathscr{C}_{1}} f(z) dz + \\int_{\\mathscr{C}_{2}} f(z) dz $$ $$ \\int_{ - \\mathscr{C}} f(z) dz = - \\int_{\\mathscr{C}} f(z) dz $$ Osborne (1999). 複素変数 その応用: p69~71.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2082,"permalink":"https://freshrimpsushi.github.io/jp/posts/2082/","tags":null,"title":"複素関数の積分"},{"categories":"복소해석","contents":"定義 1 $\\alpha \\in \\mathbb{C}$が関数$f : \\mathbb{C} \\to \\mathbb{C}$の**$n$次のゼロ**Zero of Order $n$であることは、$\\displaystyle \\lim_{z \\to \\alpha} g(z) \\ne 0$のある関数$g$に対して、$f$が次のように表されることと同等である。 $$ f(z) = (z-\\alpha)^{n} g(z) $$\n定理 ゼロは孤立している：\nゼロの周りに他のゼロが存在しないような半径を取ることができる。 $f$のゼロ$\\alpha$には、$z \\in \\mathcal{N} (\\alpha) \\setminus \\left\\{ \\alpha \\right\\}$から$f (z) \\ne 0$の間の近傍$\\mathcal{N} (\\alpha)$が存在する。 証明 一般性を失わずに、$g$が$f$の$n$次のゼロ$\\alpha$で解析的であると仮定して$g(\\alpha) = 2 \\beta \\ne 0$とする。\n$g$が$\\alpha$で連続であるため、全ての$\\beta$に対して次を満たす$\\delta \u0026gt; 0$が存在しなければならない。 $$ | z - \\alpha | \u0026lt; \\delta \\implies \\left| g(z) - g(\\alpha) \\right| \u0026lt; |\\beta| $$ 先に$g(\\alpha) = 2 \\beta$としたので、三角不等式によって $$ | z - \\alpha | \u0026lt; \\delta \\implies |g(z)| \\ge \\left| |g(\\alpha)| - \\left| g(z) - g(\\alpha) \\right| \\right| \u0026gt; |\\beta| $$ $|z-\\alpha| \u0026lt; \\delta$から$|g(z)| \u0026gt; |\\beta|$となるので、$\\alpha$は$g$のゼロになることはできない。$f(z) = (z-\\alpha)^{n} g(z)$としたので、具体的にこのオープンボール$B \\left( \\alpha , \\delta \\right)$内では、$\\alpha$だけが$f$のゼロになる。 $$ f(z) \\begin{cases} = 0 \u0026amp; , \\text{if } z = \\alpha \\\\ \\ne 0 \u0026amp; , \\text{if } z \\in B \\left( \\alpha , \\delta \\right) \\setminus \\left\\{ \\alpha \\right\\} \\end{cases} $$\n■\n参考文献 抽象代数学でのゼロ Osborne (1999). Complex variables and their applications: p66.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2080,"permalink":"https://freshrimpsushi.github.io/jp/posts/2080/","tags":null,"title":"複素解析における零点"},{"categories":"해석개론","contents":"定義 関数$f$が無限に微分可能ならば、$f$をスムーズな関数という。\n関数$f$が微分可能であり、$f^{\\prime}$が連続している場合、$f$をスムーズな関数という。\n説明 $y= \\left| x \\right|$は、$x=0$で尖った形をしているため、$x=0$での微分が不可能である。だから、全ての点で微分がうまく行われ、微分した関数も尖っていない時、スムーズだと表現するのだ。\n解析学、関数解析学等では、スムーズという言葉は主に最初の定義を指す可能性が高い。特に関数空間について話している場合、スムーズな関数は$C^{\\infty}$空間の元を指す。\n一方で、微積分学、幾何学等では、スムーズという言葉は二番目の定義を指す可能性が高い。ここでは、関数は特に無限に微分可能である必要はなく、特に微積分学では無限に微分可能であることを定義し、扱いたいわけではないので、$C^{1}$程度でスムーズであれば、スムーズな関数と言う。「連続的に微分可能である」との定義が同じである。\n","id":3110,"permalink":"https://freshrimpsushi.github.io/jp/posts/3110/","tags":null,"title":"滑らかな関数の定義"},{"categories":"함수","contents":"定義 1 関数 $\\phi (x,y)$ が領域 $\\mathscr{R}$ で連続な二階微分を持ち、ラプラスの方程式の解であれば、ハーモニックであると言う。言い換えると、ハーモニック関数は次を満たす関数である。 $$ \\Delta \\phi = \\nabla^{2} \\phi = \\phi_{xx} + \\phi_{yy} = 0 $$ 特に、関数 $u(x,y), v(x,y)$ がハーモニックであり、$u,v$ がコーシー・リーマンの方程式を満たす場合、$v(x,y)$ は $u(x,y)$ のハーモニック共役であるという。 $$ \\begin{cases} u_{x} (x,y) = v_{y} (x,y) \\\\ u_{y} (x,y) = -v_{x} (x,y) \\end{cases} $$\n説明 狭い意味で 狭い意味では、調和関数または調波とは、サイン関数やコサイン関数を意味する。または、これら二つの組み合わせである複素指数関数を意味する。\n$$ f(x) = A \\sin kx \\quad \\text{or} \\quad f(x) = e^{ix} = \\cos x + i \\sin x $$\n特に、時間-調和関数と言えば、次のように時間に関する変数が加えられた形を意味する。\n$$ f(x,t) = e^{i(kx-\\omega t)} $$\n工学では、定在波とも呼ばれる。\nOsborne (1999). Complex variables and their applications: p58~59.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2078,"permalink":"https://freshrimpsushi.github.io/jp/posts/2078/","tags":null,"title":"調和関数"},{"categories":"다변수벡터해석","contents":"ビルドアップ 多変数関数$f = \\mathbb{R}^{n} \\to \\mathbb{R}$が与えられたとしよう。$f$の導関数を求める場合には、一変数関数の時にはしなかった**「どの方向」**の変化率について考えなければならない。なじみ深い例として偏導関数がある。偏導関数は一つの変数に対してだけ変化率を考えたものである。例えば$f=f(x,y,z)$の$y$変数に対する偏導関数$\\dfrac{\\partial f}{\\partial y}$は$f$の関数値の変化を$(0,1,0)$の方向だけで考えたものである。\n方向導関数は、ここからさらに進み、各変数の方向ではなく任意の方向への変化率を考えるための概念である。\n定義1 多変数関数$f = \\mathbb{R}^{n} \\to \\mathbb{R}$と単位ベクトル$\\mathbf{u} \\in \\mathbb{R}^{n}$が与えられたとしよう。以下の極限が存在すれば、これを$\\mathbf{x}$での$f$の$\\mathbf{u}$方向への方向導関数directional derivativeと呼び、$\\nabla_{\\mathbf{u}}f(\\mathbf{x})$と表記する。\n$$ \\nabla _{\\mathbf{u}} f(\\mathbf{x}) := \\lim \\limits _{t \\to 0} \\dfrac{f (\\mathbf{x} + t \\mathbf{u}) - f(\\mathbf{x})}{t} $$\n説明 偏微分\n$$ \\dfrac{\\partial f}{\\partial x_{i}}(\\mathbf{x}) = \\lim \\limits _{t \\to 0} \\dfrac{f (\\mathbf{x} + t \\mathbf{e}_{i}) - f(\\mathbf{x})}{t} $$\n方向導関数の定義は、偏微分の定義から各変数の方向を意味する$\\mathbf{e}_{i}$が任意の方向$\\mathbf{u}$に変わっただけのものである。このように一般化してみると、偏導関数は方向導関数の特別な場合であるということがわかる。\n以下のような記法が使われる。\n$$ \\nabla _{\\mathbf{u}} f(\\mathbf{x}) = f_{\\mathbf{u}}^{\\prime}(\\mathbf{x}) = D _{\\mathbf{u}} f(\\mathbf{x}) = \\partial_{\\mathbf{u}}f(\\mathbf{x}) = \\dfrac{\\partial f}{\\partial \\mathbf{u}}(\\mathbf{x}) $$\n定められた単位ベクトル$\\mathbf{u}$があるとしよう。すると、$f$が与えられる度に、$\\nabla _{\\mathbf{u}}f$が決定されるので、ベクトル$\\mathbf{u}$自体を一つのオペレーターと見なすことができる。したがって、$\\mathbf{u}f$や$\\mathbf{u}[f]$のような記法も使われる。特に微分幾何学では、接ベクトルをオペレーターのように扱い、「接ベクトル=微分」と考える。参照を参照のこと。\n以下に紹介する定理から、方向導関数は偏導関数で表現することができるとわかる。\nまた、方向導関数の値が最も大きくなるのは、$\\mathbf{u}$が勾配$\\nabla f$と同じ方向の時であり、したがって$\\nabla f$の方向は$f$の変化率が最も大きい方向と同じであると示すことができる。したがって、勾配の記法で$\\nabla$に下添字がない理由は、変化率が最も大きい「その方向」への方向導関数であると考えることができる。\n定理 $f$の方向導関数$\\nabla _{\\mathbf{u}} f$と勾配$\\nabla f$の間に、次の式が成り立つ。\n$$ \\nabla _{\\mathbf{u}} f = \\nabla f \\cdot \\mathbf{u} = \\dfrac{\\partial f}{\\partial x_{1}} u_{1} + \\dfrac{\\partial f}{\\partial x_{2}} u_{2} + \\dots + \\dfrac{\\partial f}{\\partial x_{n}} u_{n} $$\n証明 $g (t) = f (\\mathbf{x} + t \\mathbf{u})$とする。$g$の導関数を求めると、スカラー関数の導関数は勾配であり、連鎖規則により\n$$ g^{\\prime} (t) = f ^{\\prime} (\\mathbf{x} + t \\mathbf{u}) \\cdot \\mathbf{u} = \\nabla f (\\mathbf{x} + t \\mathbf{u}) \\cdot \\mathbf{u} $$\nそして次が得られる。\n$$ g^{\\prime} (0) = \\nabla f (\\mathbf{x}) \\cdot \\mathbf{u} $$\nさらに、方向導関数の定義により、次が成立する。\n$$ \\nabla _{\\mathbf{u}} f = \\lim \\limits _{t \\to 0} \\dfrac{f (\\mathbf{x} + t \\mathbf{u}) - f(\\mathbf{x})}{t} = \\lim \\limits _{t \\to 0} \\dfrac{ g(t) - g(0)}{t} = g^{\\prime}(0) = \\nabla f (\\mathbf{x}) \\cdot \\mathbf{u} $$\n■\n参照 微分幾何学での方向導関数 リーマン幾何学での接ベクトル Walter Rudin, Principles of Mathmatical Analysis (3rd Edition, 1976), p216-218\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3109,"permalink":"https://freshrimpsushi.github.io/jp/posts/3109/","tags":null,"title":"方向微分の定義"},{"categories":"줄리아","contents":"コード using CSV, DataFrames\rA = rand(1:10, 10)\rB = zeros(10)\rAB = DataFrame(hcat(A,B), [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;])\rCSV.write(\u0026#34;AB.csv\u0026#34;, AB) CSVパッケージのwrite関数を通じて簡単に2次元配列を出力できる。A, Bは1次元配列で、hcat関数で束ねてデータフレームに変換した。\n実行結果 julia\u0026gt; using CSV, DataFrames\rjulia\u0026gt; A = rand(1:10, 10)\r10-element Array{Int64,1}:\r8\r5\r4\r3\r6\r4\r10\r6\r2\r9\rjulia\u0026gt; B = zeros(10)\r10-element Array{Float64,1}:\r0.0\r0.0\r0.0\r0.0\r0.0\r0.0\r0.0\r0.0\r0.0\r0.0\rjulia\u0026gt; AB = DataFrame(hcat(A,B), [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;])\r10×2 DataFrame\rRow │ A B │ Float64 Float64\r─────┼──────────────────\r1 │ 8.0 0.0\r2 │ 5.0 0.0\r3 │ 4.0 0.0\r⋮ │ ⋮ ⋮\r9 │ 2.0 0.0\r10 │ 9.0 0.0\r5 rows omitted\rjulia\u0026gt; CSV.write(\u0026#34;AB.csv\u0026#34;, AB)\r\u0026#34;AB.csv\u0026#34; 以下は実際に出力されたcsvファイルだ。\n一緒に見る CSV 出力時に文字化け解決法 CSV.write(bom = true) 環境 OS: Windows julia: v1.6.3 ","id":2073,"permalink":"https://freshrimpsushi.github.io/jp/posts/2073/","tags":null,"title":"ジュリアで2次元配列をCSVファイルに出力する方法"},{"categories":"기하학","contents":"式 1 $\\alpha$が$\\kappa (s) \\ne 0$の単位スピードカーブだとすると $$ \\begin{align*} T^{\\prime}(s) =\u0026amp; \\kappa (s) N(s) \\\\ N^{\\prime}(s) =\u0026amp; - \\kappa (s) T(s) + \\tau (s) B(s) \\\\ B^{\\prime}(s) =\u0026amp; - \\tau (s) N(s) \\end{align*} $$\n説明 行列の形で表すと次のようになる。 $$ \\begin{bmatrix} T \\\\ N \\\\ B \\end{bmatrix} ^{\\prime} = \\begin{bmatrix} 0 \u0026amp; \\kappa \u0026amp; 0 \\\\ - \\kappa \u0026amp; 0 \u0026amp; \\tau \\\\ 0 \u0026amp; - \\tau \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} T \\\\ N \\\\ B \\end{bmatrix} $$\n導出 補助定理: $n$次元の内積空間$V$で、$E = \\left\\{ e_{1} , \\cdots , e_{n} \\right\\}$が直交集合だとすると、$E$は$V$の基底であり、全ての$v \\in V$に対して $$ v = \\sum_{k=1}^{n} \\left\u0026lt; v , e_{k} \\right\u0026gt; e_{k} $$\n内積の微分法: $$\\left\u0026lt; f, g \\right\u0026gt;^{\\prime} = \\left\u0026lt; f^{\\prime}, g \\right\u0026gt; + \\left\u0026lt; f, g^{\\prime} \\right\u0026gt;$$\nフレネ・セレフレーム $\\left\\{ T, N, B \\right\\}$は$\\mathbb{R}^{3}$の直交基底だ。上の補助定理を使って直接導出する。\nパート 1. $T^{\\prime}(s) = \\kappa (s) N(s)$\nノーマルベクトルの定義から、$N(s) = {{ T^{\\prime}(s) } \\over { \\kappa (s) }}$であるため $$ T^{\\prime}(s) = \\kappa (s) N(s) $$\nパート 2. $N^{\\prime}(s) = - \\kappa (s) T(s) + \\tau (s) B(s)$\n補助定理によると $$ N^{\\prime}(s) = \\left\u0026lt; N^{\\prime} , T \\right\u0026gt; T + \\left\u0026lt; N^{\\prime} , N \\right\u0026gt; N + \\left\u0026lt; N^{\\prime} , B \\right\u0026gt; B $$\nパート 2-1. $\\left\u0026lt; N^{\\prime} , T \\right\u0026gt; = -\\kappa$ $\\left\u0026lt; N, T \\right\u0026gt; = 0$であるから、パート1に従って $$ \\begin{align*} \u0026amp; 0^{\\prime} = \\left\u0026lt; N , T \\right\u0026gt;^{\\prime} = \\left\u0026lt; N^{\\prime} , T \\right\u0026gt; + \\left\u0026lt; N , T^{\\prime} \\right\u0026gt; \\\\ \\implies\u0026amp; \\left\u0026lt; N^{\\prime} , T \\right\u0026gt; = - \\left\u0026lt; N , T^{\\prime} \\right\u0026gt; \\\\ \\implies\u0026amp; \\left\u0026lt; N^{\\prime} , T \\right\u0026gt; = - \\left\u0026lt; N , \\kappa N \\right\u0026gt; = - \\kappa \\left| N^{2} \\right| = - \\kappa \\cdot 1 \\end{align*} $$ パート 2-2. $\\left\u0026lt; N^{\\prime} , N \\right\u0026gt; = 0$ $N$は単位ベクトルであるから、$\\left| N^{2} \\right| = 1$であり、両辺を微分すると $$ \\begin{align*} \u0026amp; 0 = 1^{\\prime} = \\left\u0026lt; N , N \\right\u0026gt;^{\\prime} = 2 \\left\u0026lt; N , N^{\\prime} \\right\u0026gt; \\\\ \\implies\u0026amp; \\left\u0026lt; N , N^{\\prime} \\right\u0026gt; = 0 \\end{align*} $$ パート 2-3. $\\left\u0026lt; N^{\\prime} , B \\right\u0026gt; = \\tau$ $\\left\u0026lt; N, B \\right\u0026gt; = 0$であるから、トーションの定義$\\tau (s) := - \\left\u0026lt; B^{\\prime}(s) , N (s) \\right\u0026gt;$により $$ \\begin{align*} \u0026amp; 0^{\\prime} = \\left\u0026lt; N , B \\right\u0026gt;^{\\prime} = \\left\u0026lt; N^{\\prime} , B \\right\u0026gt; + \\left\u0026lt; N , B^{\\prime} \\right\u0026gt; \\\\ \\implies\u0026amp; \\left\u0026lt; N^{\\prime} , B \\right\u0026gt; = - \\left\u0026lt; N , B^{\\prime} \\right\u0026gt; \\\\ \\implies\u0026amp; \\left\u0026lt; N^{\\prime} , T \\right\u0026gt; = \\tau \\end{align*} $$ これにより、次を得る。 $$ N^{\\prime}(s) = - \\kappa (s) T(s) + \\tau (s) B(s) $$\nパート 3. $B^{\\prime}(s) = - \\tau (s) N(s)$\n補助定理によると $$ B^{\\prime}(s) = \\left\u0026lt; B^{\\prime} , T \\right\u0026gt; T + \\left\u0026lt; B^{\\prime} , N \\right\u0026gt; N + \\left\u0026lt; B^{\\prime} , B \\right\u0026gt; B $$\nパート 3-1. $\\left\u0026lt; B^{\\prime} , T \\right\u0026gt; = 0$ $\\left\u0026lt; T, B \\right\u0026gt; = 0 = \\left\u0026lt; N, B \\right\u0026gt;$であるから、パート1に従って $$ 0 = \\left\u0026lt; T^{\\prime}, B \\right\u0026gt; + \\left\u0026lt; T, B^{\\prime} \\right\u0026gt; = \\kappa \\left\u0026lt; N, B \\right\u0026gt; + \\left\u0026lt; T, B^{\\prime} \\right\u0026gt; = \\left\u0026lt; T, B^{\\prime} \\right\u0026gt; $$ パート 3-2. $\\left\u0026lt; B^{\\prime} , N \\right\u0026gt; = -\\tau$ トーションの定義と内積の対称性により $$ \\left\u0026lt; B^{\\prime} , N \\right\u0026gt; = \\left\u0026lt; N , B^{\\prime} \\right\u0026gt; = - \\tau $$ パート 3-3. $\\left\u0026lt; B^{\\prime} , B \\right\u0026gt; = 0$ $\\alpha$は単位スピードカーブと仮定しているから、$B = T \\times N$も単位ベクトルである。パート2-2と同様に $$ 0 = \\left\u0026lt; B^{\\prime} , B \\right\u0026gt; $$ これにより、次を得る。 $$ B^{\\prime}(s) = - \\tau (s) N(s) $$\n■\n結論 ランクレの定理: 単位スピードカーブ$\\alpha$がらせんであることは、ある定数$c \\in \\mathbb{R}$に対して$\\tau = c \\kappa$であることと等価だ。 単位スピードカーブ$\\alpha$の曲率が定数$\\kappa \u0026gt; 0$で、トーションが$\\tau = 0$であることは、$\\alpha$が半径$\\kappa^{-1}$の円の弧であることと等価だ。 $\\alpha$が直線であることは、$\\alpha$の全ての接線がある点$x_{0} \\in \\mathbb{R}^{3}$を通ることと等価だ。 $\\alpha$を$\\kappa \\ne 0$の単位スピードカーブとする。 $\\alpha$が平面に乗っていることは、全ての接平面が平行であることと等価だ。\n単位スピードカーブ$\\alpha$の全ての法平面がある固定点$\\mathbf{x}_{0} \\in \\mathbb{R}^{3}$を向いている場合、$\\alpha$は球面上に乗っている。 Millman. (1977). Elements of Differential Geometry: p30.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2072,"permalink":"https://freshrimpsushi.github.io/jp/posts/2072/","tags":null,"title":"フレネ・セレの公式"},{"categories":"동역학","contents":"定義 1 空間$X$と時点$t \\in T$におけるオペレーター$\\varphi^{t}$をフローと呼ぶ。フローの集合$F := \\left\\{ \\varphi^{t} \\right\\}_{t \\in T}$が関数合成演算$\\circ$に対して$\\left( F , \\circ \\right)$を満たす場合、三つ組$\\left( T, X, \\varphi^{t} \\right)$を動力系と呼ぶ。 $$ \\begin{align*} \\varphi^{0} =\u0026amp; \\text{id} \\\\ \\varphi^{t+s} =\u0026amp; \\varphi^{t} \\circ \\varphi^{s} \\end{align*} $$\n解説 主に$T = \\mathbb{Z}$の場合はマップ、$T = \\mathbb{R}$の場合は微分方程式で表される。これは動力系がマップと微分方程式だけで定義されるわけではないことを意味する。\n動力系はある時点のステートが過去のステートで表されるシステムだとよく説明されるが、これは厳密ではないし、それほど直感的でもない。数学的な表現なしで概念を理解する場合は、マップで表される動力系や微分方程式で表される動力系の例を学ぶ方が良いし、数学的な表現が好きなら上の定義が気に入るはずだ。\n関連項目 マップで表される動力系 微分方程式で表される動力系 動力系の厳密な定義 Kuznetsov. (1998). Elements of Applied Bifurcation Theory(2nd Edition): p27.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2071,"permalink":"https://freshrimpsushi.github.io/jp/posts/2071/","tags":null,"title":"力学系の厳密な定義"},{"categories":"기하학","contents":"定義 1 $\\alpha$を単位速度カーブだとしよう。\n接線 $T(s) = \\alpha^{\\prime} (s)$ のスピード$\\kappa (s) := \\left| T^{\\prime}(s) \\right|$ を$\\alpha (s)$の曲率と言う。 $\\alpha$の接線の速度$T^{\\prime}(s)$を曲率$\\kappa (s)$で割った関数、つまり次のように定義された$N$を法線ベクトル場と言う。 $$ N(s) := {{ T^{\\prime}(s) } \\over { \\left| T^{\\prime}(s) \\right| }} = {{ T^{\\prime}(s) } \\over { \\kappa (s) }},\\qquad \\kappa (s) \\ne 0 $$ 次のように定義された$B$を従法線ベクトル場と言う。 $$ B(s) := T(s) \\times N(s) $$ 次のように定義されたスカラー関数$\\tau$をねじれと言う。 $$ \\tau (s) := - \\left\u0026lt; B^{\\prime}(s) , N (s) \\right\u0026gt; $$ 接線、法線、従法線、曲率、ねじれを集めた集合をフレネ・セレの装置と言う。 $$ \\left\\{ T(s), N(s), B(s), \\kappa (s), \\tau (s) \\right\\} $$ 説明 定義でのスピードの細かい違いに縛られることなく、混乱したらスピードと覚えておくほうがいい。 特に直交集合$\\left\\{ T(s), N(s), B(s) \\right\\}$をフレネ・セレフレームと言う。 定義では$\\alpha$が単位速度カーブだが、簡単に一般化することができる。 曲率 原書を読む人にとっては[曲率]の発音が慣れているが、書くときには曲率とするのが自然だ。\n式で曲率は単に接線(接線、方向)が変わる量を示す。曲線で方向が変わる量が大きいということは、それ自体で曲線がどれだけ曲がっているかを示しているので、有効な定義と言えるだろう。当然だが、$\\kappa (s) = 0$ならば曲線は直線である。\n$\\alpha$が単位速度でない場合の定義は次の通りである。\n$$ \\begin{equation} \\kappa (t) := \\dfrac{\\left| T^{\\prime}(t) \\right|}{\\left| \\alpha^{\\prime}(t) \\right|} \\end{equation} $$\n法線 通常、Normalは「規則正しい」と訳されないが、「垂直」と自然化される。法線もそうだが、従法線と一緒に考えると、これはあまり良い訳ではないと思われる。幾何学の文脈で、Normalを「垂直」と理解するほうが精神衛生上良い。この意味で、「法線」であることは、何かに「垂直」であると受け取ることができる。\n法線ベクトルは定義上、その大きさが常に$1$に固定されており、$\\alpha^{\\prime} = T$であるため、$N$は$\\alpha$の二階導関数に相当する。$B^{\\prime} = T^{\\prime} \\times N + T \\times N^{\\prime}$まで考慮すると、フレネ・セレの装置を扱うためには、$\\alpha$は少なくとも三回微分可能でなければならないことがわかる。\n主単位法線ベクトルとも言う。\n従法線 従法線は「従垂直」と自然化されているが、法線に従うからという名前が付けられたようだ。上で述べた「法線」の感じに従えば、Bi(2、2つ)が付けられたBi-normalは、接線と法線「両方に垂直」の感じになる。\nこれは式で見るとさらに明確になる。接線と法線の外積として定義そのものなので、その名前から接線、法線に垂直であれと決め付けられた感じがする。\nねじれ Torsionは漢字訳なしで「ねじれ」と自然化され、日本語でさえ[ねじれ]捩れ2とひらがなが混ざっている。Torsionのその意味を生かした訳が全部あやふやで、「ねじれ」という言葉自体も学術的な用語としては多少不適切な感じがあるため、英語の発音そのまま[トーション]と呼ぶのが一番楽だろう。\nマイナス$-$が付くか付かないかはそれほど重要ではなく、ただの慣習だ。\n式だけを見た場合、ねじれがなぜこのように定義されたのか全く理解できないが、もっと勉強すれば式で納得できるかもしれない。とりあえずは「このように定義すると式が綺麗になるから」と納得して進むことができるが、これが幾何学を学ぶ上での良い態度であるとは言い難い。無理やりでも直感的に納得しよう。\n$\\alpha (s)$に従えば、$B(s)$に垂直な平面を接平面3と言うが、$B$が定数、つまり同じであれば、曲線$\\alpha$の接平面も変わらず、$\\mathbb{R}^{3}$のような平面に配置されることになる。\n一方、先に述べたように、法線ベクトルのスピードは常に$|N| = \\left| T^{\\prime} / \\left| T^{\\prime} \\right| \\right| = 1$であるため、ねじれ$\\tau$の大きさは$B^{\\prime}$の大きさに比例する。これを上の接平面と結びつけて考えると、ねじれの大きさ$\\left| - \\left\u0026lt; B^{\\prime}(s) , N (s) \\right\u0026gt; \\right|$は「曲線が接平面からどれだけ外れようとしているか」の尺度になるだろう。先に言ったように、$B$が定数であれば、$B^{\\prime} = 0$であると、この曲線は自分が配置されている（接）平面から外れることを全く考えていないことになる。\n物理的な意味4 曲線$\\alpha$は物理学で位置と見なすことができる。$\\alpha^{\\prime}$が速度と呼ばれるのは、$\\alpha$の変数を時間、関数値を位置と考えるためで、実際にはこのような概念が微分幾何学での曲線として数学的に抽象化されたものだ。では、ある物体が曲線$\\alpha$に沿って運動しているとしよう。物理学では、位置は$\\mathbf{r}$と表される。\n$$ \\alpha (t) = \\mathbf{r}(t) $$\n速度は位置の微分であるため、$\\mathbf{v} = \\dfrac{d \\mathbf{r}}{dt} = \\dfrac{d \\alpha}{d t}$が得られる。接線の定義により、$T = \\dfrac{\\alpha^{\\prime}}{\\left| \\alpha^{\\prime} \\right|} = \\dfrac{\\mathbf{v}}{v}$となる。\n$$ \\mathbf{v} = v T $$\n今、両辺を$t$で微分してみよう。速度の微分は加速度であるので、\n$$ \\mathbf{a} = \\dfrac{d \\mathbf{v}}{dt} = v^{\\prime} T + v T^{\\prime} $$\nこのとき、$(1)$により次のように得られる。\n$$ \\kappa = \\dfrac{\\left| T^{\\prime} \\right|}{\\left| \\mathbf{r}^{\\prime} \\right|} = \\dfrac{\\left| T^{\\prime} \\right|}{v} \\implies \\left| T^{\\prime} \\right| = v\\kappa $$\n法線の定義により、次のように得られる。\n$$ N = \\dfrac{T^{\\prime}}{\\left| T^{\\prime} \\right|} = \\dfrac{T^{\\prime}}{v\\kappa} \\implies T^{\\prime} = v\\kappa N $$\nしたがって、加速度は次のように表される。\n$$ \\mathbf{a} = v^{\\prime}T + v^{2}\\kappa N $$\nこれにより、加速度は接平面にあるベクトルであり、$T$と$N$の線形結合として表されることがわかる。\nMillman. (1977). Differential Geometryの要素: p24~26.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://ja.wikipedia.org/wiki/捩れ_(代数学)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMillman. (1977). Differential Geometryの要素: p31。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJames Stewart, Calculus (5th Edition), p874-875\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2070,"permalink":"https://freshrimpsushi.github.io/jp/posts/2070/","tags":null,"title":"フレネ-セレの公式: 曲率, 接線, 法線, 従法線, ねじれ"},{"categories":"기하학","contents":"定義 正則曲線 $\\alpha (t)$ が与えられたとする。\nベクトル場 $\\displaystyle T(t) := {{ d \\alpha / d t } \\over { \\left| d \\alpha / d t \\right| }}$ を接線ベクトル場Tangent Vector Fieldという。 次のように定義された直線 $l$ を$t = t_{0}$ から$\\alpha$ の接線Tangent Lineという。 $$ l := \\left\\{ \\mathbf{w} \\in \\mathbb{R}^{3} : \\mathbf{w} = \\alpha \\left( t_{0} \\right) + \\lambda T \\left( t_{0} \\right) , \\lambda \\in \\mathbb{R} \\right\\} $$ 説明 接線ベクトル場は、微分幾何学で非常に重要なベクトル関数で、正則曲線の接線の方向を考えつつ、その大きさを$1$ に統一している。実際に曲線がどれだけ急に曲がっているかとは無関係に、方向だけを表している。\n","id":2066,"permalink":"https://freshrimpsushi.github.io/jp/posts/2066/","tags":null,"title":"接線とタンジェントベクトル場"},{"categories":"줄리아","contents":"概要 ジュリアでは、変数名にユニコード(UTF-8)を許可している。だから、ギリシャ文字はもちろん、上付き文字、下付き文字、さらには韓国語や絵文字まで使える。わざわざ使う必要はないが、次のような奇妙なコードもちゃんと動く。\njulia\u0026gt; α₁ = 2\r2\rjulia\u0026gt; α₂ = 1\r1\rjulia\u0026gt; println(α₁ \\ast\\ α₂)\r2\rjulia\u0026gt; 사인(t) = sin(t)\r사인 (generic function with 1 method)\rjulia\u0026gt; 😂 = 1:20\r1:20\rjulia\u0026gt; 사인.(😂)\r20-element Array{Float64,1}:\r0.8414709848078965\r0.9092974268256817\r⋮\r0.14987720966295234\r0.9129452507276277 ギリシャ文字 texで使うギリシャ文字を全部、上のような方法で使える。\n上付き文字、下付き文字 上付き文字と下付き文字は\\_、\\^の後に数字を入力して使う。あまりに小さすぎてよく使われないが、ギリシャ文字や英語、括弧も使える。\n絵文字 絵文字は他のエディタと同じようにwindow + .(cmd + コンマ)を押して入力できる。\n","id":2065,"permalink":"https://freshrimpsushi.github.io/jp/posts/2065/","tags":null,"title":"ジュリア変数名にグリーク文字と添え字を書く方法"},{"categories":"함수","contents":"説明 ログは簡単な定義に反して、数学全般で非常に多くの意味を持つ。底Baseは正の数なら何でも良いが、普通は上の定義のようにオイラー定数 $e$ を使うことが多い。教科書や工学では、底が $10$ の常用対数 $\\log_{10}$ と区別するために、自然対数 $\\ln$ と表されるが、自然科学に近いほど単に $\\log$ と区別なく書かれるようになる。\n整数論 解析的数論では、ログはマンゴルト関数の級数そのものであり、算術関数の微分を形式的に定義する際にも登場する。\n情報理論 コンピュータ科学や情報理論では、単位としてビットBitを多用するため、ログの底も主に $2$ であり、自然科学の文献とは異なり、当然のように $\\log = \\log_{2}$ を使用する。特に情報理論では、「情報」という概念が満たすべき複数の条件を満たす機能として指摘される。\n複素関数への拡張 1 ビルドアップ 指数関数の歴史をログ $\\log_{\\mathbb{C}} : \\mathbb{C} \\setminus \\left\\{ 0 \\right\\} \\to \\mathbb{C}$ としてみよう。もし全ての $z \\in \\mathbb{C} \\setminus \\left\\{ 0 \\right\\}$ に対して $z = e^w$ ならば、実数で定義されたログ $\\log_{\\mathbb{C}}$ に対比して以下のように表せる。 $$ \\log_{\\mathbb{C}} z := w(z) $$ ただし、これは $z = r e^{i \\theta}$ の偏角 $\\arg z = \\theta$ に応じて無限に多くの値に対応するため、厳密には関数ではない。極座標表現で見ると $w := u + iv$ に対して $$ z = e^{w} \\implies r \\left( \\cos \\theta + i \\sin \\theta \\right) = e^{u} \\left( \\cos v + i \\sin v \\right) $$ 実部と虚部を別々に考えると $$ r = e^{u} \\\\ \\sin \\theta = \\sin v $$ 従って $k \\in \\mathbb{Z}$ に対して $$ u = \\log_{\\mathbb{R}} r \\\\ v = \\theta + 2 k \\pi $$ まとめると $$ \\log_{\\mathbb{C}} z = \\log_{\\mathbb{R}} |z| + i \\left( \\arg z + 2 k \\pi \\right) $$ 先に述べたように、$\\log_{\\mathbb{C}}$ は $k \\in \\mathbb{Z}$ ほど多くの値を持つため、関数ではない。$k$ により、すなわち一周するごとに生じる $\\mathbb{C} \\setminus \\left\\{ 0 \\right\\}$ の部分集合をログのブランチBranchと呼ぶ。特に $k=0$ の場合を主ブランチPrincipal Branchと呼び、次のように大文字 $L$ を使ってログ関数 $\\text{Log}$ を再定義する。\n拡張 $\\text{Log} : \\mathbb{C} \\setminus \\left\\{ 0 \\right\\} \\to \\mathbb{C}$ を次のように定義する。 $$ \\text{Log} z := \\log_{\\mathbb{R}} |z| + i \\arg z $$\n性質 $(\\log x)^{\\prime \\prime} = -\\dfrac{1}{x^{2}}$であるため、凹関数である。\n次の不等式が成立する。\n$$ 1 - \\dfrac{1}{x} \\le \\ln x \\le x - 1\\qquad \\text{ for } x \\gt 0 $$\nOsborne (1999). Complex variables and their applications: p31.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2063,"permalink":"https://freshrimpsushi.github.io/jp/posts/2063/","tags":null,"title":"対数関数の定義"},{"categories":"기하학","contents":"定義 1 写像 $\\alpha : (a,b) \\to \\mathbb{R}^{3}$ を曲線Curveと呼ぶ。 $\\alpha^{\\prime} = \\dfrac{d \\alpha}{d t} = \\mathbf{0}$ の時の点 $t = t_{0}$ を特異点Singular Pointと言う。 ある $k \\in \\mathbb{N}$ に対して、全ての $t \\in (a,b)$ で $\\displaystyle {{ d \\alpha } \\over { d t }} \\ne \\mathbf{0}$ となる曲線 $\\alpha \\in C^{k}$ を正則曲線Regular Curveと呼ぶ。つまり、正則曲線とは特異点がない曲線のことだ。 曲線 $\\alpha$の $t=t_{0}$ での微分係数 $\\alpha^{\\prime}(t_{0})$を$t = t_{0}$ の時の$\\alpha$ の速度(ベクトル)velocity vectorと呼び、$\\alpha$の導関数 $\\alpha^{\\prime}$を$\\alpha$の速度ベクトル場velocity vector fieldと言う。従って、正則曲線は速度が$\\mathbf{0}$にならない曲線のことを言い、物理的に見た時、進行方向が絶対に変わらないことを意味する。 $t = t_{0}$ の時の $\\alpha$ の速度の大きさ $\\left|\\alpha^{\\prime}(t_{0}) \\right|$を速さspeedと呼ぶ。 $\\left| \\alpha^{\\prime} \\right| = 1$ の曲線を$\\alpha : (a,b) \\to \\mathbb{R}^{3}$ を単位速度曲線Unit Speed Vectorと言う。 $C^{k}$ は $k$ 回微分可能でその導関数が連続関数の集合である。 説明 $$ \\alpha (t) := \\left( \\alpha_{1} (t) , \\alpha_{2} (t) , \\alpha_{3} (t) \\right) $$\n幾何学で扱いたい対象は図形で、定義ではその図形をパラメーター $t$ に対する関数として表していることに注意しろ。これにより、多くの数学的なツールを使って図形を研究することができるようになり、特に、微分幾何学では多くの微積分が使用されるだろう。\n特異点とは、簡単に言えば曲がっているか停止している点のことだ。曲がっている点では、見方によっては2つの方向が出てくることがある。このような点は扱いにくいので、学部レベルの微分幾何学では扱わない。\u0026lsquo;停止する点\u0026rsquo;は例で説明する。\nある $k \\in \\mathbb{N}$ に対して、$\\alpha \\in C^{k}$ ということは、少なくとも一度は微分可能であることを強く意味し、実際に何回微分可能かはあまり重要ではない。通常、$k=1$ 回だけであっても、単にスムースSmoothであると言われる。\n例 直線 $$ l(t) := \\left( t, t, t \\right) $$ 定義によれば、直線 $l : \\mathbb{R} \\to \\mathbb{R}^{3}$ が曲線でない理由は全くない。韓国語では、曲がることを意味する曲曲のため、何かが曲がっているというニュアンスが混乱を招くかもしれないが、そのまま英語の発音でカーブと読んだ方が良いかもしれない。\n螺旋 $$ \\zeta (t) := \\left( \\cos t , \\sin t , t \\right) $$\n$0 \\to t \\to \\infty$ によると、螺旋は以下のように描かれる。\n不規則曲線 $$ \\beta (t) := \\left( t^{2} , t^{3} , t^{4} \\right) $$ 上記の曲線 $\\beta$ を微分すると、 $$ \\beta^{\\prime} (t) := \\left( 2t , 3t^{2} , 4t^{3} \\right) $$ その結果、$t = 0$ で $\\displaystyle {{ d \\beta } \\over { d t }} (0) = \\mathbf{0}$ になる。この特異点は曲がっていないが$t$ を沿って進んでいると $t=0$ で文字通り停止する。従って、$\\beta$ の定義域が $\\mathbb{R}$ なら正則曲線ではない。もちろん、定義域が $0$ を含まない範囲、例えば、$(0,\\infty)$ なら正則曲線だ。定義域によって正則曲線であったり、そうでなかったりすることに注意しよう。\nコード 以下は、螺旋の例で見た動画をJuliaで作成するコードだ。\nusing Plots\rζ(t) = (cos(t), sin(t), t)\ranim = @animate for T ∈ 0.1:0.1:10\rt = 0:0.1:(T*π)\rhelix = plot(ζ.(t), camera = (45,45), legend = :none)\rxlims!(-2,2); ylims!(-2,2); zlims!(0,40)\rend\rgif(anim, \u0026#34;helix.gif\u0026#34;) Millman. (1977). Elements of Differential Geometry: p15.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2062,"permalink":"https://freshrimpsushi.github.io/jp/posts/2062/","tags":["줄리아"],"title":"曲線の定義"},{"categories":"편미분방정식","contents":"定義1 開いた集合 $\\Omega=\\mathbb{R}^{n}$で定義された偏微分方程式が与えられたとする。時間が $t=0$の時の$\\Omega$での未知数 $u$の値、つまり初期値initial valueが与えられたとしよう。こんな偏微分方程式の解を探す問題を コーシー問題Cauchy problem あるいは 初期値問題initial value problemと言う。\n説明 略称のIVPがよく使われる。\n例 例えば熱方程式のコーシー問題を解くというのは、以下の条件が与えられた時の解を探すことである。\n$$ \\left\\{ \\begin{align*} u_{t} -\\Delta u \u0026amp;= 0 \u0026amp;\u0026amp; \\text{in } \\mathbb{R}^{n} \\times (0,\\infty) \\\\ u \u0026amp;= g \u0026amp;\u0026amp; \\text{on } \\mathbb{R}^{n} \\times \\left\\{ t=0 \\right\\} \\end{align*} \\right. $$\n参照 境界値問題(BVP) Lawrence C. Evans, Partial Differential Equations (2nd Edition, 2010), p57\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3093,"permalink":"https://freshrimpsushi.github.io/jp/posts/3093/","tags":null,"title":"コーシー問題、初期値問題"},{"categories":"수리통계학","contents":"定義 数式的な定義 1 パラメータ$\\theta \\in \\Theta$に対するランダムサンプル$X_{1} , \\cdots , X_{n}$の確率質量/密度関数を$f(x;\\theta)$、統計量$Y_{1} := u_{1} \\left( X_{1} , \\cdots , X_{n} \\right)$の確率質量/密度関数を$f_{Y_{1}} \\left( y_{1}; \\theta \\right)$とする。\n$\\theta \\in \\Theta$に依存しない$H \\left( x_{1} , \\cdots , x_{n} \\right)$に対して $$ {{ f \\left( x_{1} ; \\theta \\right) \\cdots f \\left( x_{n} ; \\theta \\right) } \\over { f_{Y_{1}} \\left( u_{1} \\left( x_{1} , \\cdots, x_{n} \\right) ; \\theta \\right) }} = H \\left( x_{1} , \\cdots , x_{n} \\right) $$ であれば、$Y_{1}$を$\\theta$に対する十分統計量という。\n一般的な定義 2 統計量$T(\\mathbf{X})$が、与えられたサンプルの$\\mathbf{X}$条件付き確率分布がパラメータ$\\theta$に依存しない場合、$T(\\mathbf{X})$を$\\theta$に対する十分統計量という。\n説明 定義の数式が意味するのは、直感的に見ると、分子と分母で$\\theta$がキャンセルされること―つまり十分統計量$Y_{1}$が、ランダムサンプル$X_{1} , \\cdots , X_{n}$の情報を正確に保持しているという意味になるだろう。十分統計量の「十分」とは、$\\theta$に関する情報が「十分」に与えられていると受け取れば良く、十分統計量を除いた後は、$\\theta$に関する情報が全く残ってはいけない。\n十分統計量を理解するために、以下の定理を用いよう。\nネイマン分解定理: ランダムサンプル$X_{1} , \\cdots , X_{n}$が、パラメータ$\\theta \\in \\Theta$に対して同じ確率質量/密度関数$f \\left( x ; \\theta \\right)$を持つとする。統計量$Y = u_{1} \\left( X_{1} , \\cdots , X_{n} \\right)$が$\\theta$の十分統計量であるためには、次を満たす非負の二つの関数$k_{1} , k_{2} \\ge 0$が存在することである。 $$ f \\left( x_{1} ; \\theta \\right) \\cdots f \\left( x_{n} ; \\theta \\right) = k_{1} \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) ; \\theta \\right] k_{2} \\left( x_{1} , \\cdots , x_{n} \\right) $$ ただし、$k_{2}$は$\\theta$に依存してはならない。\n響かない例 $$ X_{1} , \\cdots , X_{n} \\sim N \\left( \\mu , \\sigma^{2} \\right) $$\n経験的に、十分統計量は、なぜそんなものを計算するのか、理解することから始める必要がある。典型的に響かない例として、正規分布$N \\left( \\mu , \\sigma^{2} \\right)$の母平均$\\mu$に対する十分統計量を見ることだ。分解定理によれば、$\\mu$の十分統計量は $$ \\begin{align*} \\prod_{k=1}^{n} f \\left( x_{k} ; \\mu \\right) =\u0026amp; \\prod_{k=1}^{n} {{ 1 } \\over { \\sigma \\sqrt{2 \\pi} }} \\exp \\left( - {{ \\left( x_{i} - \\mu \\right)^{2} } \\over { 2 \\sigma^{2} }} \\right) \\\\ =\u0026amp; \\left( {{ 1 } \\over { \\sigma \\sqrt{2 \\pi} }} \\right)^{n} \\exp \\left( - \\sum_{k=1}^{n} {{ \\left( x_{i} - \\mu \\right)^{2} } \\over { 2 \\sigma^{2} }} \\right) \\\\ =\u0026amp; \\left( {{ 1 } \\over { \\sigma \\sqrt{2 \\pi} }} \\right)^{n} \\exp \\left( - \\sum_{k=1}^{n} {{ x_{i}^{2} } \\over { 2 \\sigma^{2} }} \\right) \\exp \\left( - \\sum_{k=1}^{n} {{ \\left( 2 x_{i} - \\mu^{2} \\right) } \\over { 2 \\sigma^{2} }} \\right) \\\\ =\u0026amp; \\left( {{ 1 } \\over { \\sigma \\sqrt{2 \\pi} }} \\right)^{n} \\exp \\left( - \\sum_{k=1}^{n} {{ x_{i}^{2} } \\over { 2 \\sigma^{2} }} \\right) \\cdot \\exp \\left( - {{ 1 } \\over { \\sigma^{2} }} \\sum_{k=1}^{n} x_{i} + {{ n(\\mu/\\sigma)^{2} } \\over { 2 \\ }} \\right) \\\\ =\u0026amp; k_{2} \\left( x_{1} , \\cdots , x_{n} \\right) \\cdot k_{1} \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) ; \\mu \\right] \\end{align*} $$ であり、サンプル和$\\sum_{k=1}^{n} X_{k}$であろうと、分子分母に$n$を掛けてサンプル平均$\\overline{X}$になろうと、関係ない。直感に従って、$\\mu$の十分統計量として、その不偏推定量であり、一致推定量であり、最尤推定量でもあるサンプル平均が出てきたのは良い。数式的には理解できる。しかし、それが一体何を意味するのか、感じることは難しいだろう。\n響く例 $$ X_{1} , \\cdots , X_{n} \\sim U (0,\\theta) \\text{ with } f \\left( x ; \\theta \\right) = \\begin{cases} 1 \u0026amp; , \\text{if } x \\in (0,\\theta) \\\\ 0 \u0026amp; , \\text{otherwise} \\end{cases} = {{ 1 } \\over { \\theta }} I_{(0,\\theta)} (x) $$\n例えば、最大値のパラメータが$\\theta$である一様分布から得られたランダムサンプルを考えてみよう。実際の実現が $$ \\begin{bmatrix}2.3 \\\\ 1.2 \\\\ 1.7 \\\\ 0.1 \\\\ 1.1\\end{bmatrix} $$ であり、これ以上のサンプルを得られない場合、一様分布$U(a,b)$の母平均が${{ b+a } \\over { 2 }}$であるため、次のような推定量を考えることができる。 $$ {{ \\hat{\\theta} + 0 } \\over { 2 }} = {{ \\sum_{k} x_{k} } \\over { n }} \\implies \\hat{\\theta} \\overset{?}{=} {{ 2 \\sum_{k} x_{k} } \\over { n }} $$ 数理統計学的にそんなに悪くない推測のようだ。実際、このデータで計算されたサンプル平均の$2$倍は$2.16$で、かなりもっともらしい。しかし、$2.3$がサンプルにあることを考えると、$\\theta = 2.16$であるはずがない。どう考えても、$\\theta$は$2.3$以上でなければならず、直感的に見て、$\\theta$に対する合理的な推定は、単純に$\\hat{\\theta} = 2.3$になる。今のサンプルを見たとき、$2.3$より大きいと考える理由が全くないからだ。さあ、実際に十分統計量を探してみよう。\n指示関数の積: $$ \\prod_{i=1}^{n} I_{(-\\infty, \\theta]} \\left( x_{i} \\right) = I_{(-\\infty, \\theta]} \\left( \\max_{i \\in [n]} x_{i} \\right) $$\nこの補題と分解定理により考えると、$\\theta$に対する十分統計量は $$ \\begin{align*} \\prod_{k=1}^{n} f \\left( x_{k} ; \\mu \\right) =\u0026amp; \\prod_{k=1}^{n} {{ 1 } \\over { \\theta }} I_{(0,\\theta)} \\left( x_{k} \\right) \\\\ = \u0026amp; {{ 1 } \\over { \\theta^{n} }} I_{(0,\\theta)} \\left( \\max x_{k} \\right) \\cdot 1 \\\\ = \u0026amp; k_{1} \\left[ u_{1} \\left( x_{1} , \\cdots , x_{n} \\right) ; \\theta \\right] k_{2} \\left( x_{1} , \\cdots , x_{n} \\right) \\end{align*} $$ であるため、サンプルの最大値$\\max_{k} X_{k} = X_{(n)}$が十分となる。これが意味するのは、$\\theta$に関する情報を考えるとき、他のサンプルは必要なく、$\\max_{k} X_{k}$だけを考えれば「十分」であるということだ。\nこのアイデアは、データをたくさん引き出してパラメータを推定し、それをどこかに近似する考え方とは全く異なる。これは、直感的な推測に対して、数学と形式でアプローチする統計的推論であり、これを通じて、統計学のさらに深い世界に入れる。\n最小十分統計量 響く例で、$\\max_{k} X_{k}$が$\\theta$に対する十分統計量であることを直感と照らし合わせて確認した。これ以上の十分統計量はないと見えるが、最小十分統計量に関する議論がその答えとなるだろう。\nHogg et al. (2013). Introduction to Mathematical Statistics(7th Edition): p391.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCasella. (2001). Statistical Inference(2nd Edition): p272.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2061,"permalink":"https://freshrimpsushi.github.io/jp/posts/2061/","tags":null,"title":"十分統計量"},{"categories":"편미분방정식","contents":"定義1 次の偏微分方程式を波動方程式wave equationと呼ぶ。\n$$ u_{tt} - \\Delta u =0 $$\nこの式は、波の伝播速度を定数$1$とする場合である。波の伝播速度を$c$とすると、波動方程式は以下のようになる。\n$$ u_{tt} - c^{2}\\Delta u =0 $$\n非同次nonhomogeneousの場合は、以下のようになる。\n$$ u_{tt} - \\Delta u = f $$\n$U \\subset \\mathbb{R}^{n}$は開集合 $u : \\overline{U}\\times [0, \\infty) \\to \\mathbb{R}$ $t\u0026gt;0$ $x = (x_{1}, x_{2}, \\dots, x_{n}) \\in U$ $u=u(x, t)=u(x_{1}, \\dots, x_{n}, t)$ $\\Delta$はラプラシアン $f:U \\times [0, \\infty) \\to \\mathbb{R}$ 説明 時間に対する2次の微分を位置に関する項に置き換えると、ヘルムホルツ方程式になる。\nLawrence C. Evans, Partial Differential Equations (2nd Edition, 2010), p65-66\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3092,"permalink":"https://freshrimpsushi.github.io/jp/posts/3092/","tags":null,"title":"波動方程式"},{"categories":"함수","contents":"概要 指数関数Exponential Functionは、数学全般で一貫して現れる乗算の一般化です。元々の乗算では、底 $a \u0026gt; 0$ が必ずしも $a = e$ である必要はありませんが、底変換式があるため、本質的にどちらの底を使っても関係ありません。便宜上、指数関数とは、その底を $e$ とみなします。\n定義 1 $x, y \\in \\mathbb{R}$ とした場合、複素数 $z \\in \\mathbb{C}$ に対して、$\\exp : \\mathbb{C} \\to \\mathbb{C}$ を次のように定義します。 $$ \\exp z = e^{z} = e^{x + iy} = e^{x} \\left( \\cos y + i \\sin y \\right) $$\n$e = 2.71828182 \\cdots$ はオイラー定数です。 導出 カリキュラムで同じように扱われましたが、もう少し難しい用語で説明します。便宜上、底は $e$ に統一し、高校生のレベルまで詳しく説得するよりは、レビュー感覚で説明します。\n自然数 $\\mathbb{N}$ まず、指数関数の基礎である乗算は、ある自然数 $n \\in \\mathbb{N}$ について次のように自然に表現できます。ここで、$e$ に対する上付き文字としての $n$ は、$e$が何回乗算されたかを直感的に表します。 $$ e^{n} = \\overbrace{e \\cdot e \\cdots e}^{n \\text{ Times}} $$ さらに、二つの自然数 $n, m \\in \\mathbb{N}$ に対して、次が成り立つことを簡単に確認できます。 $$ e^{n+m} = e^{n} e^{m} $$\n整数 $\\mathbb{Z}$ 体の公理により、すべての実数 $a \\ne 0$ に対しては、乗算に関する逆元 $a^{-1}$ が存在しなければなりません。つまり、すべての $a = e^{n}$ に対して、次を満たす $a^{-1}$ が存在する必要があります。 $$ 1 = a \\cdot a^{-1} = e^{n} \\cdot a^{-1} $$ 直感的には、これは $e$ を何回割るかに相当します。この逆元を $a^{-1} = e^{-n}$ として表すことにより、指数関数は整数全体に拡張されます。\n有理数 $\\mathbb{Q}$ 二つの整数 $n,m \\in \\mathbb{Z}$ と $a^{m}$ に対して、次を満たす $e^{n}$ を考えます。 $$ a^{m} = e^{n} $$ これは、$e$ を $n$ 回乗算すると $a^{m}$ となることを意味します。今、$a = e^{ {{ n } \\over { m }} }$ として記述すれば $$ a^{m} = \\left( e^{ {{ n } \\over { m }} } \\right)^{m} = \\overbrace{e^{ {{ n } \\over { m }} } \\cdots e^{ {{ n } \\over { m }} }}^{m \\text{ Times}} $$ のように表せて、指数関数が有理数にもうまく拡張されていることがわかります。\n実数 $\\mathbb{R}$ 実数の密度により、すべての実数 $x \\in \\mathbb{R}$ に収束する有理数の数列 $\\left\\{ r_{k} \\right\\}_{k=1}^{\\infty}$ が存在しなければなりません。したがって、実数 $x \\in \\mathbb{R}$ に対する $e$ の乗算は、次のように定義されます。 $$ \\exp(x) = e^{x} := \\lim_{k \\to \\infty} e^{r_{k}} $$\n複素数 $\\mathbb{C}$ 複素数の極座標表示: 複素数 $z \\ne 0$ は、複素平面上の点 $P(x,y)$ に対応し、線分 $\\overline{OP}$ の長さ $r := |z|$ と $x$ 軸と $\\overline{OP}$ が作る反時計回りの角度 $\\theta$ を通して、以下のように極座標表示Polar Representationできます。 $$ z = r \\left( \\cos \\theta + i \\sin \\theta \\right) $$\n最後に、複素関数への拡張は、上記のように形式的に行われます。上記の引用から、 $$ r = e^{x} \\\\ e^{iy} = \\cos y + i \\sin y $$ が自然に得られ、したがってそれを複素関数の一種である指数関数として定義します。 $$ \\exp z = e^{z} = e^{x + iy} = e^{x} \\left( \\cos y + i \\sin y \\right) $$\nOsborne (1999). Complex variables and their applications: p24.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2060,"permalink":"https://freshrimpsushi.github.io/jp/posts/2060/","tags":null,"title":"指数関数"},{"categories":"함수","contents":"定義 1 $n \\in \\mathbb{N}_{0}$ と $\\left\\{ a_{k} \\right\\}_{k=0}^{n} \\subset \\mathbb{C}$ に対して、次のように定義される $P: \\mathbb{C} \\to \\mathbb{C}$ を**$n$次の多項式関数**Polynomial of degree $n$という。 $$ P(z) := a_{0} + a_{1} z + \\cdots a_{n} z^{n} \\qquad , a_{n} \\ne 0 $$\n説明 多項式関数は、数学全般で最も基本的に考えられる関数であり、代数学の基本定理によって根がちょうど $n$ 個存在することが明らかにされている。\n定義により、定数関数も多項式関数である。 多項式は無限回微分可能である。 連続関数である。 抽象代数 抽象代数の記法で、このような多項式関数の集合は $\\mathbb{C}[x]$ と表される。ここで、係数の集合は必ずしも複素数体 $\\mathbb{C}$ に限定されず、体 $F$ が与えられていれば $F [ x ]$ のように表すことができる。\n多項式の次数は無限大でも構わない。$n = \\infty$ の場合、そのような多項式の集合は $F[[x]]$ のように表される。\nOsborne (1999). Complex variables and their applications: p24.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2058,"permalink":"https://freshrimpsushi.github.io/jp/posts/2058/","tags":null,"title":"多項関数"},{"categories":"함수","contents":"概要 三角関数は直角三角形の底角に三角比を対応させた関数だ。\n定義 三角関数のサイン、コサイン$\\sin, \\cos : \\mathbb{R} \\to \\mathbb{R}$は以下のように定義される。\n$$ \\sin \\theta := {{ y } \\over { \\sqrt{x^{2} + y^{2}} }} \\\\ \\cos \\theta := {{ x } \\over { \\sqrt{x^{2} + y^{2}} }} $$\nこれにより、セカント、コセカント、タンジェント、コタンジェントは次のように定義される。\n$$ \\begin{align*} \\tan \\theta \u0026amp;:= {{ \\sin \\theta } \\over { \\cos \\theta }} \\qquad, \\cos \\theta \\ne 0 \\\\ \\cot \\theta \u0026amp;:= {{ \\cos \\theta } \\over { \\sin \\theta }} \\qquad, \\sin \\theta \\ne 0 \\\\ \\sec \\theta \u0026amp;:= {{ 1 } \\over { \\cos \\theta }} \\qquad, \\cos \\theta \\ne 0 \\\\ \\csc \\theta \u0026amp;:= {{ 1 } \\over { \\sin \\theta }} \\qquad, \\sin \\theta \\ne 0 \\end{align*} $$\n複素関数への拡張 1 三角関数のサイン、コサイン$\\sin, \\cos : \\mathbb{C} \\to \\mathbb{C}$は以下のように定義される。\n$$ \\sin z := {{ 1 } \\over { i2 }} \\left( e^{iz} - e^{-iz} \\right) \\\\ \\cos z := {{ 1 } \\over { 2 }} \\left( e^{iz} + e^{-iz} \\right) $$\n基本性質 [1] 三角関数は実数軸上で$2 \\pi$-周期関数だ。 [2] サイン関数は奇関数で、コサイン関数は偶関数だ。 参照 複素解析における三角関数と双曲線関数の関係 複素解析における三角関数と指数関数の関係 Osborne (1999). 複素変数及びその応用: p28.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2056,"permalink":"https://freshrimpsushi.github.io/jp/posts/2056/","tags":null,"title":"三角関数の定義"},{"categories":"편미분방정식","contents":"定義1 開集合$\\Omega$で定義された偏微分方程式が与えられたとする。以下の境界条件をディリクレ境界条件Dirichlet boundary conditionと呼ぶ。ディリクレ境界条件が与えられた偏微分方程式の解を見つける問題をディリクレ問題Dirichlet problemという。\n$$ u = 0 \\quad \\text{on } \\partial \\Omega $$\n説明 非同次条件 以下のような境界条件を非同次ディリクレ条件nonhomogeneous Dirichlet conditionということがあるが、通常、同次か非同次かを厳密に表記することはない。\n$$ u = g \\quad \\text{on } \\partial \\Omega $$\n例 例えば、ポアソン方程式でディリクレ問題を解くことは、以下を満たす$u$を見つけることである。\n$$ \\left\\{ \\begin{align*} -\\Delta u = f \u0026amp; \\quad \\text{in } \\Omega \\\\ u = 0 \u0026amp; \\quad \\text{on }\\partial \\Omega \\end{align*} \\right. $$\n参照 境界値問題 ノイマン境界条件 ロビン境界条件 Lawrence C. Evans, Partial Differential Equations (2nd Edition, 2010), p311-312\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3087,"permalink":"https://freshrimpsushi.github.io/jp/posts/3087/","tags":null,"title":"ディリクレ境界条件"},{"categories":"함수","contents":"定義 次のように定義される関数 $f$ を絶対値関数absolute value functionと呼び、関数の値を $|x|$ のように表記する。\n$$ |x| := f(x) = \\begin{cases} x \u0026amp;\\text{if } x\u0026gt;0 \\\\ 0 \u0026amp;\\text{if } x=0 \\\\ -x \u0026amp;\\text{if } x\u0026lt;0 \\end{cases},\\quad x\\in \\mathbb{R} $$\n説明 絶対値とは実数の大きさを意味しており、これを一般化したものがノルムである。三角不等式が成り立つ。\n$$ |x + y| \\le |x| + |y|,\\quad x \\in \\mathbb{R} $$\n","id":3083,"permalink":"https://freshrimpsushi.github.io/jp/posts/3083/","tags":null,"title":"絶対値関数"},{"categories":"다변수벡터해석","contents":"ビルドアップ1 一変数関数の導関数の定義を思い出そう。\n$$ \\lim \\limits_{h\\to 0} \\dfrac{f(x+h) - f(x)}{h} = f^{\\prime}(x) $$\n左辺の分子を$h$に対する線形関数で近似すると、次のようになる。\n$$ \\begin{equation} f(x+h) - f(x) = a h + r(h) \\label{1} \\end{equation} $$\nここで、$r(h)$を以下の条件を満たす残余remainder, 残差としよう。\n$$ \\lim \\limits_{h \\to 0} \\dfrac{r(h)}{h}=0 $$\nすると、$\\eqref{1}$の両辺を$h$で割り、$\\lim_{h\\to 0}$である極限を取ると、次のようになる。\n$$ \\lim \\limits_{h\\to 0} \\dfrac{f(x+h) - f(x)}{h} = \\lim \\limits_{h\\to 0} \\dfrac{ah+ r(h)}{h} = a + \\lim \\limits_{h\\to 0} \\dfrac{r(h)}{h} = a $$\nこの時、$a$は$h$に対する線形近似での1次項の係数であった。このセンスで、$a$を$f$の$x$での微分**\u0026lsquo;係数\u0026rsquo;**と呼ぶことにする。式を少し変形すると、$f$の$x$での微分係数は、$a$を満たす式であることがわかる。\n$$ \\lim \\limits_{h\\to 0} \\dfrac{f(x+h) - f(x) - ah}{h} = \\lim \\limits_{h\\to 0} \\dfrac{r(h)}{h} = 0 $$\nこれを基に多変数ベクトル関数の導関数を定義する。\n定義 $E\\subset \\mathbb{R}^{n}$を開集合、$\\mathbf{x}\\in E$とする。$\\mathbf{f} : E \\to \\mathbb{R}^{m}$に対して、次を満たす$\\mathbf{h} \\in \\mathbb{R}^{n}$に対する線形変換 $A\\in L(\\mathbb{R}^{n}, \\mathbb{R}^{m})$が存在する場合、$f$は$\\mathbf{x}$で微分可能とされる。また、$A$を$f$の全導関数total derivativeまたは単に導関数といい、$\\mathbf{f}^{\\prime}(\\mathbf{x})$で表記する。\n$$ \\begin{equation} \\lim \\limits_{|\\mathbf{h}| \\to 0} \\dfrac{| \\mathbf{f} ( \\mathbf{x} + \\mathbf{h}) - \\mathbf{f} (\\mathbf{x}) - A( \\mathbf{h} )|}{|\\mathbf{h}|} = 0 \\label{2} \\end{equation} $$\nもし$\\mathbf{f}$が$E$のすべての点で微分可能であれば、$\\mathbf{f}$は$E$で微分可能であるとされる。\n説明 全全は全体を意味し、偏導関数に対する言葉だ。全$\\check{}$関数ではなく、全$\\check{}$導関数である。\n注意すべき点は、$\\mathbf{f}^{\\prime}(\\mathbf{x})$は関数値ではなく、$\\mathbf{f}^{\\prime}(\\mathbf{x}) : E \\subset \\R^{n} \\to \\R^{m}$を満たす線形変換であることだ。したがって、$\\mathbf{f}^{\\prime}(\\mathbf{x}) = A$は次のように行列で表現できる。\n$$ \\mathbf{f}^{\\prime}(\\mathbf{x}) = A = \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{bmatrix} $$\nすると、$\\mathbf{f}$の全導関数$\\mathbf{f}^{\\prime}$は、$\\mathbf{x} \\in E \\subset \\R^{n}$が与えられるたびにある$m \\times n$行列$A$をマッピングする関数と見なすことができる。この行列は$\\mathbf{f}$の偏導関数から簡単に得ることができ、ヤコビ行列Jacobian matrix, ヤコビ行列とも呼ばれる。\n$$ \\mathbf{f}^{\\prime}(\\mathbf{x}) = \\begin{bmatrix} (D_{1}f_{1}) (\\mathbf{x}) \u0026amp; (D_{2}f_{1}) (\\mathbf{x}) \u0026amp; \\cdots \u0026amp; (D_{n}f_{1}) (\\mathbf{x}) \\\\ (D_{1}f_{2}) (\\mathbf{x}) \u0026amp; (D_{2}f_{2}) (\\mathbf{x}) \u0026amp; \\cdots \u0026amp; (D_{n}f_{2}) (\\mathbf{x}) \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ (D_{1}f_{m}) (\\mathbf{x}) \u0026amp; (D_{2}f_{m}) (\\mathbf{x}) \u0026amp; \\cdots \u0026amp; (D_{n}f_{m}) (\\mathbf{x}) \\end{bmatrix} $$\n全導関数は有限次元上で定義された関数に対する微分の最終形であり、ここで$\\mathbf{f}$の定義域、値域をバナッハ空間に一般化したものをフレシェ導関数という。一変数関数のときに成り立った性質も自然と成り立つ。\n一意性 連鎖律 定理 一意性 $E, \\mathbf{x}, \\mathbf{f}$を定義での通りとする。$A_{1}, A_{2}$が$\\eqref{2}$を満たす場合、その二つの線形変換は等しい。\n$$ A_{1} = A_{2} $$\n証明 $B = A_{1} - A_{2}$とする。すると、三角不等式により、次が成り立つ。\n$$ \\begin{align*} | B( \\mathbf{h} ) | \u0026amp;= \\left| A_{1}(\\mathbf{h}) - A_{2}(\\mathbf{h}) \\right| \\\\ \u0026amp;= | A_{1}(\\mathbf{h}) - \\mathbf{f} (\\mathbf{x} + \\mathbf{h}) - \\mathbf{f} (\\mathbf{x}) + \\mathbf{f} (\\mathbf{x} + \\mathbf{h}) + \\mathbf{f} (\\mathbf{x}) - A_{2}(\\mathbf{h}) | \\\\ \u0026amp;\\le | \\mathbf{f} (\\mathbf{x} + \\mathbf{h}) + \\mathbf{f} (\\mathbf{x}) - A_{1}(\\mathbf{h}) | + | \\mathbf{f} (\\mathbf{x} + \\mathbf{h}) + \\mathbf{f} (\\mathbf{x}) - A_{2}(\\mathbf{h}) | \\end{align*} $$\nすると、固定された$\\mathbf{h} \\ne \\mathbf{0}$に対して、以下の式が成り立つ。\n$$ \\lim _{t \\to 0} \\dfrac{ | B( t\\mathbf{h} ) |}{| t\\mathbf{h} |} \\le \\lim _{t \\to 0}\\dfrac{ | \\mathbf{f} (\\mathbf{x} + t\\mathbf{h}) + \\mathbf{f} (\\mathbf{x}) - A_{1}(t\\mathbf{h}) |}{| t\\mathbf{h} |} + \\lim _{t \\to 0}\\dfrac{| \\mathbf{f} (\\mathbf{x} + t\\mathbf{h}) + \\mathbf{f} (\\mathbf{x}) - A_{2}(t\\mathbf{h}) |}{| t\\mathbf{h} |}=0 $$\nしかし、$B$は線形変換なので、左辺は$t$に無関係であることがわかる。\n$$ \\lim _{t \\to 0} \\dfrac{ | tB( \\mathbf{h} ) |}{| t\\mathbf{h} |} = \\lim _{t \\to 0} \\dfrac{ | B( \\mathbf{h} ) |}{| \\mathbf{h} |} = \\dfrac{ | B( \\mathbf{h} ) |}{| \\mathbf{h} |} \\le 0 $$\n$\\mathbf{h} \\ne \\mathbf{0}$であるため、上記の式が成り立つには、$B=0$でなければならない。したがって、次を得る。\n$$ B=A_{1}-A_{2}=0 \\implies A_{1} = A_{2} $$\n■\n連鎖律 定義の通り、$E \\subset \\R^{n}$を開集合とし、$\\mathbf{f} : E \\to \\R^{m}$を$\\mathbf{x}_{0} \\in E$で微分可能な関数とする。$\\mathbf{g} : \\mathbf{f}(E) \\to \\R^{k}$を$\\mathbf{f}(\\mathbf{x}_{0}) \\in \\mathbf{f}(E)$で微分可能な関数とする。また、$\\mathbf{F} : E \\to \\R^{k}$を$\\mathbf{f}$と$\\mathbf{g}$の合成とする。\n$$ \\mathbf{F} (\\mathbf{x}) = \\mathbf{g} \\left( \\mathbf{f}(\\mathbf{x}) \\right) $$\nすると、$\\mathbf{F}$は$\\mathbf{x}_{0}$で微分可能であり、全導関数は以下の通りである。\n$$ \\mathbf{F}^{\\prime} (\\mathbf{x}_{0}) = \\mathbf{g}^{\\prime} \\left( \\mathbf{f}(\\mathbf{x}_{0}) \\right) \\mathbf{f}^{\\prime} (\\mathbf{x}_{0}) $$\n証明 ノルム空間に対して一般化された証明\n■\nWalter Rudin, Mathematical Analysisの原理 (第3版, 1976), p211-213\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3082,"permalink":"https://freshrimpsushi.github.io/jp/posts/3082/","tags":null,"title":"偏導関数：多変数ベクトル関数の導関数"},{"categories":"함수","contents":"例 サイン$\\sin$とコサイン$\\cos$は代表的な周期関数であり、上の定義に従って$2\\pi$周期関数だ。\n","id":2050,"permalink":"https://freshrimpsushi.github.io/jp/posts/2050/","tags":null,"title":"周期関数"},{"categories":"다변수벡터해석","contents":"定義 スカラー関数の$u : \\mathbb{R}^{n} \\to \\mathbb{R}$のグラジエントのダイバージェンスをラプラシアンLaplacianと呼んで、次のように表記する。\n$$ \\begin{align*} \\Delta u :\u0026amp;= \\mathrm{div}(\\nabla (u)) \\\\ \u0026amp;= \\mathrm{div} \\left( \\left( u_{x_{1}}, u_{x_{2}}, \\dots, u_{x_{n}} \\right) \\right) \\\\ \u0026amp;= u_{x_{1}x_{1}} + u_{x_{2}x_{2}} + \\cdots + u_{x_{n}x_{n}} \\\\ \u0026amp;= \\sum _{i=1}^{n} u_{x_{i}x_{i}} \\end{align*} $$\nここで$u_{x_{i}}=\\dfrac{\\partial u}{\\partial x_{i}}$である。\n説明 数学では、ダイバージェンスを$\\mathrm{div}$と表記することが多いし、ラプラシアンも主に$\\Delta$と表記される。しかし、物理学では、ダイバージェンスを$\\nabla \\cdot$と表記するため、ラプラシアンの表記は主に$\\nabla ^{2}$が使われる。\n$$ \\nabla\\cdot( \\nabla (u))=\\nabla^{2}(u) = \\nabla^{2}u $$\n$D^{2}$をマルチインデックス記法と呼ぶなら、ヘッセ行列のトレースとも同じである。\n$$ \\Delta u = \\sum_{i=1}^{n} u_{x_{i} x_{i}} = \\mathrm{tr} (D^{2}u) $$\n3次元デカルト座標系 $$ \\Delta f = \\nabla ^{2} f = \\frac{ \\partial^{2} f}{ \\partial x^{2} }+\\frac{ \\partial^{2} f}{ \\partial y^{2}}+\\frac{ \\partial^{2} f}{ \\partial z^{2}} $$\n","id":3081,"permalink":"https://freshrimpsushi.github.io/jp/posts/3081/","tags":null,"title":"スカラー場のラプラシアン"},{"categories":"선형대수","contents":"定義1 $V, W$を有限次元のベクトル空間とする。$\\beta = \\left\\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{n} \\right\\}$と$\\gamma = \\left\\{ \\mathbf{w}_{1}, \\dots, \\mathbf{w}_{m} \\right\\}$をそれぞれ$V$と$W$の順序基底とする。$T : V \\to W$を線形変換とする。すると、基底の表現の一意性によって、次を満たすスカラー$a_{ij}$が一意に存在する。\n$$ T(\\mathbf{v}_{j}) = \\sum_{i=1}^{m}a_{ij}\\mathbf{w}_{i} = a_{1j}\\mathbf{w}_{1} + \\cdots + a_{mj}\\mathbf{w}_{m} \\quad \\text{ for } 1 \\le j \\le n $$\nここで、$A_{ij} = a_{ij}$で定義される$m \\times n$行列$A$を順序基底$\\beta$と$\\gamma$に関する$T$の行列表現matrix representation for $T$ relative to the basis $\\beta$ and $\\gamma$と呼び、$[T]_{\\gamma, \\beta}$または$[T]_{\\beta}^{\\gamma}$で表す。 したがって、次の式が成り立つ。\n$$ [T]_{\\gamma, \\beta} [\\mathbf{x}]_{\\beta} = [T(\\mathbf{x})]_{\\gamma} = [T]_{\\beta}^{\\gamma}[\\mathbf{x}]_{\\beta} $$\n直感的には、隣接する（または下付き文字で重複する）2つの$\\beta$を相殺し、$\\mathbf{x}$を$T$に代入したものと見ることができる。\nStephen H. Friedberg, Linear Algebra (第4版, 2002), 80-81ページ\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3078,"permalink":"https://freshrimpsushi.github.io/jp/posts/3078/","tags":null,"title":"線形変換の行列表現"},{"categories":"복소해석","contents":"定義 1 二次方程式$x^{2} +1 = 0$の解$x = \\sqrt{-1}$を、虚数Imaginary Numberというんだ。 ２つの実数$x,y \\in \\mathbb{R}$に対して、$z = x + iy$の形の数を、複素数Complex Numberと言って、$(x,y)$みたいに表示することもある。この時、$\\text{Re} (z) = x$と$\\text{Im} (z) = y$をそれぞれ$z$の実部Real Partと虚部Imaginary Partと言うんだ。 全ての複素数の集合を$\\mathbb{C}$で表すんだ。複素数$z_{1}, z_{2} \\in \\mathbb{C}$が等しいEqualってのは、実部と虚部がそれぞれ等しいってことだ。 $$ \\text{Re} z_{1} = \\text{Re} z_{2} \\\\ \\text{Im} z_{1} = \\text{Im} z_{2} $$ 複素数の大きさをモジュラスModulusって呼び、以下のように定義されるんだ。 $$ | z | := \\sqrt{x^{2} + y^{2}} $$ 説明 虚部$\\text{Im} z = y \\in \\mathbb{R}$には、虚数単位$i$が掛けられていないことに注意しよう。 物理学や工学では、$i$が電流を表すから、虚数単位を$j := \\sqrt{-1}$と表すこともあるんだ。 教科書では、複素数を表示するとき、一般に$1 + 2i$として$i$を数字の後に書くけど、数学に近い文献では、$1 + i2$のように$i$を数字の前に書く傾向が強くなる。これはもう$i$を文字として見なさず、他の数と同じように等価な数として扱いたいって意図があって、$i$を基準にして前が実部、後が虚部と区分されるから、実際に使ってみるとこの表示方法は実用的だと分かるんだ。 歴史 歴史的に見て、虚数は1545年、確率論の先駆者であるカルダノCardanoの著作アルス・マグナArs Magnaで初めて紹介されたんだ。数学界で完全に受け入れられたのは、19世紀頃になってからだった。ガウスGaussは$i$に想像上の数Imaginary Numberって現在の名前を付けて、代数学の基本定理の証明に使ったんだ。記号$i$自体は、オイラーEulerの1777年の回顧録で登場するんだ。\n複素平面 2 $$ \\mathbb{C} \\ni x + iy = (x,y) \\in \\mathbb{R}^{2} $$\n定義から推測できるように、複素数$\\mathbb{C}$の集合は、$2$次元平面$\\mathbb{R}^{2}$のように見ることができ、実際にも、そうと同じ議論を代数的に導出できるんだ。記号そのままで$x$は$x$軸、$y$は$y$軸を表してると見なされ、これからは実数軸、虚数軸を意味することになる。ピタゴラスの定理を考えた時、複素数の大きさモジュラスが$| z | := \\sqrt{x^{2} + y^{2}}$のように定義されることは、非常に合理的だ。\n体の公理 $$ \\begin{align*} z_{1} + z_{2} =\u0026amp; \\left( \\text{Re} z_{1} + \\text{Re} z_{2} , \\text{Im} z_{1} + \\text{Im} z_{2} \\right) \\\\ z_{1} \\cdot z_{2} =\u0026amp; \\left( \\text{Re} z_{1} \\text{Re} z_{2} - \\text{Im} z_{1} \\text{Im} z_{2} , \\text{Re} z_{1} \\text{Im} z_{2} - \\text{Im} z_{1} \\text{Re} z_{2} \\right) \\end{align*} $$\n複素数$z_{1}, z_{2} \\in \\mathbb{C}$に対する二項演算である加算Sum$+: \\mathbb{C}^{2} \\to \\mathbb{C}$と乗算Product$\\cdot: \\mathbb{C}^{2} \\to \\mathbb{C}$を上記のように定義すると、$\\mathbb{C}$は代数的に体になり、$\\mathbb{C}$を複素数体Complex Fieldって呼ぶんだ。解析学序論の実数体と同じように、体の公理が全て成立するんだ。\nOsborne (1999). 『Complex Variables and Their Applications』p1~4よ。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOsborne (1999). 『Complex Variables and Their Applications』p8~9よ。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2046,"permalink":"https://freshrimpsushi.github.io/jp/posts/2046/","tags":null,"title":"複素数の定義"},{"categories":"머신러닝","contents":"この文は逆転派アルゴリズムの原理を数学専攻者が理解しやすいように作成された。\n表記法 上図のような 人工ニューラルネットワーク が与えられたとする。$\\mathbf{x} = (x_{1}, x_{2}, \\dots, x_{n_{0}})$は入力input、 $y_{j}^{l}$は$l$番目の層の$j$ノード、$\\hat{\\mathbf{y}} = (\\hat{y}_{1}, \\hat{y}_{2}, \\dots, \\hat{y}_{\\hat{n}})$ドルは出力outputである。\n$L \\in \\mathbb{N}$は、隠匿層hidden layerの個数であり、$\\mathbf{n}=(n_{0}、n_{1}、\\dots、n_{L}、\\hat{n}) \\in \\mathbb{N}^{N}=(n)、$の成分は順に入力層、$L$個の隠匿層と出力層のノード数を意味する。 また、便宜上、$0$番目の隠匿層は入力層を意味し、$L+1$番目の隠匿層は出力層を意味するとする。\n$w_{ji}^{l}$は、$l$の次の層の$i$のノードとその次の層の$j$のノードを連結する加重値を表す。 すると、各階から次の階への伝播は、以下のGIFのように起こる。\nここで $\\phi$ は任意の活性化関数 である。 $l$ 番目の層から次の層の $j$ 番目のノードに伝達される線形結合を $v_{i}^{l}$で表記しよう。\n$$ \\begin{align*} v_{j}^{l} \u0026amp;= \\sum _{i=1}^{n_{l}} w_{ji}^{l}y_{i}^{l} \\\\ y_{j}^{l+1} \u0026amp;= \\phi ( v_{j}^{l} ) = \\phi \\left( \\sum \\nolimits_{i=1}^{n_{l}} w_{ji}^{l}y_{i}^{l} \\right) \\end{align*} $$\nこれを定理すると次のようになる。\n記号 意味 $\\mathbf{x}=(x_{1}, x_{2}, \\dots, x_{n_{0}})$ 入力 $y^{l}_{j}$ $l$ 番目の層の $j$ 番目のノード $\\hat{\\mathbf{y}} = (\\hat{y}_{1}, \\hat{y}_{2}, \\dots, \\hat{y}_{\\hat{n}} )$ 出力 $n_{l}$ $l$ 番目の層のノード数 $w_{ji}^{l}$ $l$ 番目の層の $i$ 番目のノードと その次の層の $j$ 番目のノードを接続する重み付け $\\phi$ 活性化関数 $v_{j}^{l} = \\sum \\limits _{i=1} ^{n_{l}} w_{ji}^{l}y_{i}^{l}$ 線形結合 $y^{l+1}_{j} = \\phi (v_{j}^{l})$ $l$ 番目の階から次の階への 電波 定理 $E = E(\\hat{\\mathbf{y}})$を微分可能な適切な損失関数とする。 それでは、$E$を最適化する方法は、各層での加重値$w_{ji}^{l}$を次のようにアップデートするものである。\n$$ \\begin{equation} w_{ji}^{l} \\leftarrow w_{ji}^{l} + \\alpha \\delta^{l}_{j} y_{i}^{l} \\label{thm} \\end{equation} $$\nこの時、$\\alpha$は学習率で、$\\delta_{j}^{l}$ は以下の通りである。\n$l=L$の時、\n$$ -\\delta_{j}^{L} = \\phi ^{\\prime} (v_{j}^{L}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{j}} $$\n$l \\in \\left\\{ 0,\\dots, L-1 \\right\\}$の時、\n$$ \\delta_{j}^{l} = \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i=1}^{n_{l}} \\delta_{i}^{l+1} w_{i j}^{l+1} $$\n説明 $(1)$を見てみよう。 $l$番目の層と$l+1$番目の層の間の加重値を更新する時、$l$番目のノードの$y_{j}^{l}$に依存するということですが、各層の出力に応じて最終的に出力$\\hat{\\mathbf{y}}$が決定されるので当然と見ることができる。 また、$y_{j}^{l}$は$l$番目から$l+1$番目の層に伝播される時の入力と見ることができるが、これは線形回帰モデルでLMSLeast Mean Squaresで学習する方法と似ている。\n$$ \\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha (\\mathbf{w}^{T}\\mathbf{x} - \\mathbf{y}) \\mathbf{x} $$\n一方、各層での出力$y_{j}^{l}$は入力層から出力層として計算される反面、最適化のための$\\delta_{j}^{l}$ は次のように出力層から入力層に逆に計算されるため、このような最適化手法を逆伝播アルゴリズムback propagation algorithmという。\n$$ \\begin{align*} \\delta_{j}^{L} \u0026amp;= - \\phi ^{\\prime} (v_{j}^{L}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{j}} \\\\ \\delta_{j}^{L-1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} \\delta_{j}^{L} w_{ij}^{L} \\\\ \\delta_{j}^{L-2} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\delta_{i}^{L-1} w_{ij}^{L-1} \\\\ \\delta_{j}^{L-3} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-3}) \\sum _{i} \\delta_{i}^{L-2} w_{ij}^{L-2} \\\\ \u0026amp;\\vdots \\\\ \\delta_{j}^{1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{1}) \\sum _{i} \\delta_{i}^{2} w_{ij}^{2} \\\\ \\delta_{j}^{0} \u0026amp;= \\phi ^{\\prime} (v_{j}^{0}) \\sum _{i} \\delta_{i}^{1} w_{ij}^{1} \\end{align*} $$\n証明 入力層から出力層への計算が終わったとする。 加重値を損失関数$E$が減る方向に修正する方法は傾斜下降法を使えば次のようになる。\n$$ \\begin{equation} w_{ji}^{l} \\leftarrow w_{ji}^{l} - \\alpha \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial w_{ji}^{l} } \\label{gradesent} \\end{equation} $$\nそれぞれの$y_{i}^{l}$は与えられた値なので、偏微分部分を計算できる形で解くことができる。 右辺の偏微分は連鎖法則によって次のようになる。\n$$ \\begin{equation} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial w_{ji}^{l}} = \\dfrac{\\partial E(\\hat{\\mathbf{y}}) }{\\partial v_{j}^{l}} \\dfrac{\\partial v_{j}^{l}}{\\partial w_{ji}^{l}} = \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial v_{j}^{l}} y_{i}^{l} \\label{chainrule} \\end{equation} $$\n$(3)$の右辺の偏微分を$-\\delta_{j}^{l}$ とすると、$(2)$ から $(1)$ を得る。\n$$ w_{ji}^{l} \\leftarrow w_{ji}^{l} + \\alpha \\delta^{l}_{j} y_{i}^{l} $$\n各層で $\\delta_{j}^{l}$ を次のように求める。\n$l=L$の場合\n$j \\in \\left\\{ 1, \\dots, \\hat{n} \\right\\}$ に対して次が成立する。\n$$ \\begin{equation} -\\delta_{j}^{L} = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial v_{j}^{L}} = \\dfrac{\\partial E ( \\hat{\\mathbf{y}} ) } {\\partial \\hat{y}_{j}} \\dfrac{d \\hat{y}_{j}}{d v_{j}^{L}} \\label{deltamL} \\end{equation} $$\nこの時、$\\hat{y}_{j} =\\phi (v_{j}^{L})$ であるから次を得る。\n$$ -\\delta_{j}^{L} (t) =\\phi ^{\\prime} (v_{j}^{L}(t)) \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{j}} $$\n■\n$l=L-1$の場合\n$j \\in \\left\\{ 1, \\dots, n_{L-1} \\right\\}$については以下の通りである。\n$$ -\\delta_{j}^{L-1} = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial v_{j}^{L-1}} = = \\dfrac{\\partial E ( \\hat{\\mathbf{y}} ) } {\\partial y_{j}^{L}} \\dfrac{d y_{j}^{L}}{d v_{j}^{L-1}} $$\nこの時$y_{j}^{L} =\\phi (v_{j}^{L-1})$ であるので、次を得る。\n$$ -\\delta_{j}^{L-1} = = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial y_{j}^{L}} \\dfrac{\\partial y_{j}^{L}}{\\partial v_{j}^{L-1}} = = \\phi ^{\\prime} (v_{j}^{L-1}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{L}} $$\n右辺の偏微分は連鎖法則によって次のように計算される。\n$$ \\begin{align*} -\\delta_{j}^{L-1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{L}} \\\\ \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{\\partial \\hat{y}_{i}}{\\partial y_{j}^{L}} \\\\ \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}} \\dfrac{\\partial v_{i}^{L}}{\\partial y_{j}^{L}} \\end{align*} $$\nここで $(4)$ と ${\\color{green}v_{i}^{L}=\\sum_{j}w_{ij}^{L}y_{j}^{L}}$ により、次を得る。\n$$ \\begin{align} \u0026amp;\u0026amp; -\\delta_{j}^{L-1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i=1} {\\color{blue}\\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{\\partial \\hat{y}_{i}}{\\partial v_{i}^{L}}} {\\color{green} \\dfrac{d v_{i}^{L}}{d y_{j^{L}}} } \\nonumber \\\\ \u0026amp;\u0026amp; \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} {\\color{blue} -\\delta_{i}^{L}} {\\color{green} w_{ij}^{L} }\\nonumber \\\\ {}\\nonumber \\\\ \\implies \u0026amp;\u0026amp; \\delta_{j}^{L-1} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-1}) \\sum _{i} \\delta_{i}^{L} w_{ij}^{L} \\label{deltajL-1} \\end{align} $$\n■\n$l=L-2$の場合\n$j \\in \\left\\{ 1, \\dots, n_{L-2} \\right\\}$については以下の通りである。\n$$ -\\delta_{j}^{L-2} = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial v_{j}^{L-2}} = \\dfrac{\\partial E ( \\hat{\\mathbf{y}} ) } {\\partial y_{j}^{L-1}} \\dfrac{d y_{j}^{L-1}}{d v_{j}^{L-2}} $$\nこの時$y_{j}^{L-1} =\\phi (v_{j}^{L-2})$ であるから次を得る。\n$$ -\\delta_{j}^{L-2} = \\dfrac{\\partial E (\\hat{\\mathbf{y}})}{\\partial y_{j}^{L-1}} \\dfrac{d y_{j}^{L-1}}{d v_{j}^{L-2}} = \\phi ^{\\prime} (v_{j}^{L-2}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{L-1}} $$\n右辺の偏微分は連鎖法則によって次のように計算される。\n$$ \\begin{align*} -\\delta_{j}^{L-2} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{\\partial \\hat{y}_{i}}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}} \\dfrac{\\partial v_{i}^{L}}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}} \\sum _{k} \\dfrac{\\partial v_{i}^{L}}{\\partial y_{k}^{L}} \\dfrac{\\partial y_{k}^{L}}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}} \\sum _{k} \\dfrac{\\partial v_{i}^{L}}{\\partial y_{k}^{L}} \\dfrac{d y_{k}^{L}}{d v_{k}^{L-1}} \\dfrac{\\partial v_{k}^{L-1}}{\\partial y_{j}^{L-1}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{k} \\sum _{i} {\\color{blue}\\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i}} \\dfrac{d \\hat{y}_{i}}{d v_{i}^{L}}} {\\color{red}\\dfrac{\\partial v_{i}^{L}}{\\partial y_{k}^{L}} } {\\color{green}\\dfrac{d y_{k}^{L}}{d v_{k}^{L-1}}} {\\color{purple}\\dfrac{d v_{k}^{L-1}}{\\partial y_{j}^{L-1}}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{k} \\sum _{i} {\\color{blue} -\\delta_{i}^{L}} {\\color{red} w_{ik}^{L}} {\\color{green} \\phi^{\\prime}(v_{k}^{L-1})} {\\color{purple} w_{kj}^{L-1}} \\end{align*} $$\nしたがって、次を得る。\n$$ \\delta_{j}^{L-2} = -\\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{k} \\sum _{i} \\delta_{i}^{L} w_{ik}^{L} \\phi^{\\prime}(v_{k}^{L-1}) w_{kj}^{L-1} $$\nこのとき、$(5)$ によって次が成立する。\n$$ \\sum _{i} \\delta_{i}^{L} w_{ik}^{L} \\phi^{\\prime}(v_{k}^{L-1}) = \\phi^{\\prime}(v_{k}^{L-1}) \\sum _{i} \\delta_{i}^{L} w_{ik}^{L} = \\delta_{k}^{L-1} $$\nしたがって、次を得る。\n$$ \\begin{align*} \\delta_{j}^{L-2} \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{k} \\delta_{k}^{L-1} w_{kj}^{L-1} \\\\ \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{L-2}) \\sum _{i} \\delta_{i}^{L-1} w_{ij}^{L-1} \\end{align*} $$\n■\n一般化: $l \\in \\left\\{1, \\dots, L-1 \\right\\}$\n上記の結果に基づき、次のように一般化することができる。$j \\in \\left\\{ 1, \\dots, n_{l} \\right\\}$については以下の通りである。\n$$ -\\delta_{j}^{l} = \\phi ^{\\prime} (v_{j}^{l}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{l}} $$\n右辺の偏微分を連鎖法則で解くと次のようになる。\n$$ \\begin{align*} \u0026amp;\\quad \\delta_{j}^{l} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\dfrac{\\partial E(\\hat{\\mathbf{y}})}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{\\partial \\hat{y}_{i_{(1)}}}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{\\partial y_{i_{(2)}}^{L}}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{d y_{i_{(2)}}^{L}}{d v_{i_{(2)}}^{L-1}} \\frac{\\partial v_{i_{(2)}}^{L-1}}{\\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{d y_{i_{(2)}}^{L}}{d v_{i_{(2)}}^{L-1}} \\frac{\\partial v_{i_{(2)}}^{L-1}}{\\partial y_{i_{(3)}}^{L-1} } \\frac{\\partial y_{i_{(3)}}^{L-1} }{ \\partial y_{j}^{l}} \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{d y_{i_{(2)}}^{L}}{d v_{i_{(2)}}^{L-1}} \\frac{\\partial v_{i_{(2)}}^{L-1}}{\\partial y_{i_{(3)}}^{L-1} } \\frac{d y_{i_{(3)}}^{L-1} }{d v_{i_{(3)}}^{L-2} } \\frac{\\partial v_{i_{(3)}}^{L-2} }{ \\partial y_{j}^{l}} \\\\ \u0026amp; \\quad \\vdots \\\\ \u0026amp;= -\\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\cdots \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\sum_{i_{(1)}} \\frac{\\partial E(\\hat{\\mathbf{y}})}{\\partial \\hat{y}_{i_{(1)}}} \\frac{d \\hat{y}_{i_{(1)}}}{d v_{i_{(1)}}^{L}} \\frac{\\partial v_{i_{(1)}}^{L}}{\\partial y_{i_{(2)}}^{L}} \\frac{d y_{i_{(2)}}^{L}}{d v_{i_{(2)}}^{L-1}} \\frac{\\partial v_{i_{(2)}}^{L-1}}{\\partial y_{i_{(3)}}^{L-1} } \\frac{d y_{i_{(3)}}^{L-1} }{d v_{i_{(3)}}^{L-2} } \\frac{\\partial v_{i_{(3)}}^{L-2} }{ \\partial y_{i_{(4)}}^{L-2}} \\cdots \\frac{d y_{i_{(L-l+1)}}^{l+1} }{d v_{i_{(L-l+1)}}^{l} } \\frac{\\partial v_{i_{(L-l+1)}}^{l} }{ \\partial y_{j}^{l}} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\cdots \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\sum_{i_{(1)}} -\\delta_{i_{(1)}}^{L} w_{i_{(1)}i_{(2)}}^{L} \\phi^{\\prime}(v_{i_{(2)}}^{L-1}) w_{i_{(2)} i_{(3)}}^{L-1} \\phi^{\\prime}( v_{i_{(3)}}^{L-2} ) w_{i_{(3)} i_{(4)}}^{L-2} \\cdots \\phi^{\\prime}(v_{L-l+1}^{l})w_{i_{(L-l+1)} j}^{L} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\cdots \\sum_{i_{(3)}} \\sum_{i_{(2)}} \\delta_{i_{(2)}}^{L-1}w_{i_{(2)} i_{(3)}}^{L-1} \\phi^{\\prime}( v_{i_{(3)}}^{L-2} ) w_{i_{(3)} i_{(4)}}^{L-2} \\cdots \\phi^{\\prime}(v_{L-l+1}^{l})w_{i_{(L-l+1)} j}^{L} \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\cdots \\sum_{i_{(3)}} \\delta_{i_{(3)}}^{L-2} w_{i_{(3)} i_{(4)}}^{L-2} \\cdots w_{i_{(L-l)} j}^{L} \\\\ \u0026amp;\\quad \\vdots \\\\ \u0026amp;= \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i_{(L-l)}} \\delta_{i_{(L-l)}}^{l+1} w_{i_{(l-l)} j}^{l} \\end{align*} $$\nしたがって、定理すると次のようになる。\n$$ \\delta_{j}^{l} = \\phi ^{\\prime} (v_{j}^{l}) \\sum_{i} \\delta_{i}^{l+1} w_{ij}^{l+1} $$\n■\n","id":3077,"permalink":"https://freshrimpsushi.github.io/jp/posts/3077/","tags":null,"title":"逆伝播アルゴリズム"},{"categories":"기하학","contents":"定義 1 ベクトル空間 $X$ が与えられているとする。\n次の方程式を満たす点の集まり $L \\subset X$ 又は $\\alpha (t)$ 自体を点 $\\mathbf{x}_{0} \\in X$ を通り、ベクトル $\\mathbf{v} \\ne 0$ と平行な直線と定義する。 $$ \\alpha (t) = \\mathbf{x}_{0} + t \\mathbf{v} \\qquad , t \\in \\mathbb{R} $$ 次の方程式を満たす点の集まり $P \\subset X$ を点 $\\mathbf{x}_{0} \\in X$ を通り、ベクトル $\\mathbf{n} \\ne 0$ に垂直な平面と定義する。 $$ \\left\u0026lt; \\mathbf{x} - \\mathbf{x}_{0} , \\mathbf{n} \\right\u0026gt; = \\mathbf{0} $$ 次の方程式を満たす点の集まり $S \\subset X$ を中心 $\\mathbf{x}_{0} \\in X$ 、半径 $r \u0026gt; 0$ の球体と定義する。 $$ \\left\u0026lt; \\mathbf{x} - \\mathbf{x}_{0} , \\mathbf{x} - \\mathbf{x}_{0} \\right\u0026gt; = r^{2} $$ $\\left\u0026lt; \\cdot , \\cdot \\right\u0026gt;$ は内積だ。 線であり、平面であり、球体であるもの マジで笑 Millman. (1977). Elements of Differential Geometry: p8~10.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2042,"permalink":"https://freshrimpsushi.github.io/jp/posts/2042/","tags":null,"title":"一般的な直線、平面、球の定義"},{"categories":"선형대수","contents":"定義1 線形変換$T_{1} : V \\to W$と$T_{2} : W \\to Z$が与えられたとする。次に定義される変換$T_{2} T_{1}$を$T_{1}$と$T_{2}$の合成composition of $T_{2}$ with $T_{1}$と呼ぶ。\n$$ (T_{2} \\circ T_{1})(\\mathbf{x}) = T_{2}\\left( T_{1}(\\mathbf{x}) \\right) \\quad \\mathbf{x} \\in V $$\n説明 線形変換の合成は、記号を省略して次のように表すことが多い。\n$$ T_{2}T_{1}\\mathbf{x} = (T_{2} \\circ T_{1}) (\\mathbf{x}) $$\n有限次元では、これは本質的に行列の積と同じなので、自然な表記法である。\n性質1 2 線形変換$T_{1} : V \\to W$と$T_{2} : W \\to Z$が与えられたとする。\n(a) $T_{1}$と$T_{2}$の合成$T_{2} T_{1}$も線形変換である。\n(b) $T, U_{1}, U_{2} \\in \\href{../3283}{L(V)}$と$a \\in \\mathbb{R}$に対して、次のことが成り立つ。\n$$ T(U_{1} + U_{2}) = TU_{1} + TU_{2} \\quad \\text{and} \\quad (U_{1} + U_{2})T = U_{1}T + U_{2}T \\\\[0.5em] T(U_{1}U_{2}) = (T_{1})U_{2} \\\\[0.5em] TI = IT = T \\\\[0.5em] a(U_{1}U_{2}) = (aU_{1})U_{2} = U_{1}(aU_{2}) $$\n$T_{1}, T_{2}$が一対一であれば、次が成り立つ。\n(c) $T_{2} T_{1}$が一対一である。\n(d) $(T_{2} T_{1})^{-1} = T_{1}^{-1} T_{2}^{-1}$\n(e) $V, W, Z$を有限次元ベクトル空間、$\\alpha, \\beta, \\gamma$をそれぞれの順序基底とする。そして、$T : V \\to W$、$U : W \\to Z$を線形変換とする。すると、\n$$ [UT]_{\\alpha}^{\\gamma} = [U]_{\\beta}^{\\gamma}[T]_{\\alpha}^{\\beta} $$\n$[T]_{\\alpha}^{\\beta}$は$T$の行列表現である。\n証明 (a) $\\mathbf{x}_{1}, \\mathbf{x}_{2} \\in V$と任意の定数を$k$とする。$T_{1}, T_{2}$が線形であるので、次が成り立つ。\n$$ \\begin{align*} (T_{2} T_{1})(\\mathbf{x}_{1} + k \\mathbf{x}_{2}) \u0026amp;= T_{2} \\left( T_{1} \\left( \\mathbf{x}_{1} + k \\mathbf{x}_{2} \\right) \\right) \\\\ \u0026amp;= T_{2} \\left( T_{1} ( \\mathbf{x}_{1} ) + k T_{1} ( \\mathbf{x}_{2} ) \\right) \\\\ \u0026amp;= T_{2} \\left( T_{1} ( \\mathbf{x}_{1} ) \\right) + k T_{2}\\left( T_{1} ( \\mathbf{x}_{2} ) \\right) \\\\ \u0026amp;= (T_{2} T_{1}) ( \\mathbf{x}_{1} ) + k (T_{2} T_{1})( \\mathbf{x}_{2} ) \\end{align*} $$\n■\n(c) $\\mathbf{x}_{1}$と$\\mathbf{x}_{2}$が$V$の異なるベクトルであるとする。$T_{1}$が一対一なので、$T_{1}(\\mathbf{x}_{1})$と$T_{1}(\\mathbf{x}_{2})$は異なるベクトルである。したがって、$T_{2}$も一対一なので、次の２つのベクトルも異なる。\n$$ (T_{2} T_{1})(\\mathbf{x}_{1}) = T_{2} \\left( T_{1}(\\mathbf{x}_{1}) \\right) \\quad \\text{and} \\quad (T_{2} T_{1})(\\mathbf{x}_{2}) = T_{2} \\left( T_{1}(\\mathbf{x}_{2}) \\right) $$\nしたがって、$T_{2} T_{1}$は一対一である。\n■\n(d) $\\mathbf{z}$を$T_{2} T_{1}$による$\\mathbf{x} \\in V$の像とする。\n$$ \\mathbf{z} = (T_{2} T_{1}) ( \\mathbf{x} ) = T_{2} ( T_{1} (\\mathbf{x})) $$\n両辺に$T_{2}^{-1}$を適用すると、次のようになる。\n$$ T_{2}^{-1}(\\mathbf{z}) = ( T_{2}^{-1} T_{2} T_{1}) ( \\mathbf{x} ) = T_{1} (\\mathbf{x}) $$\nさらに両辺に$T_{1}^{-1}$を適用すると、次のようになる。\n$$ ( T_{1}^{-1} T_{2}^{-1} )(\\mathbf{z}) = ( T_{1}^{-1} T_{1} ) ( \\mathbf{x} ) = \\mathbf{x} $$\nしたがって、次を得る。\n$$ (T_{1}^{-1} T_{2}^{-1}) ( (T_{2} T_{1} )(\\mathbf{x}) ) = \\mathbf{x} $$\n■\nHoward Anton, Elementary Linear Algebra: Aplications Version (12th Edition, 2019), p465-468\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStephen H. Friedberg, Linear Algebra (4th Edition, 2002), p86-89\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3074,"permalink":"https://freshrimpsushi.github.io/jp/posts/3074/","tags":null,"title":"線形変換の合成"},{"categories":"줄리아","contents":"概要 Juliaの便利な機能である補間Interpolationについて説明する。補間をうまく使うと、出力文を簡単できれいに書くことができるので非常に便利だ。数値解析の補間法とは関係ないが、言葉の意味は通じる。数値解析の補間法に関連する機能はInterpolations.jlの使用法を参照してほしい。\nコード 使い方は非常に単純だ。以下のように文字列の中で変数の前にドル記号$를 붙이면 변수가 알아서 문자열처럼 읽힌다. 변수 그대로가 아닌 계산이 필요하면 굳이 밖에서 계산할 필요 없이 $()を書くだけでいい。\njulia\u0026gt; x = 12\r12\rjulia\u0026gt; y = -2\r-2\rjulia\u0026gt; println(\u0026#34;value of x, y: ▷eq2◁y\u0026#34;)\rvalue of x, y: 12, -2\rjulia\u0026gt; println(\u0026#34;value of x+y: $(x+y)\u0026#34;)\rvalue of x+y: 10 環境 OS: Windows julia: v1.5.0 ","id":2041,"permalink":"https://freshrimpsushi.github.io/jp/posts/2041/","tags":null,"title":"ジュリアで変数の値を便利に出力する方法、補間"},{"categories":"선형대수","contents":"定義1 $T : V \\to W$を線形変換とする。\n$T$の値域 $R(T)$が有限次元ならば、$R(T)$の次元を**$T$のランク**rankといい、次のように示す。\n$$ \\mathrm{rank}(T) := \\dim (R(T)) $$\n$T$の零空間 $N(T)$が有限次元ならば、$N(T)$の次元を**$T$の零次元数**nullityといい、次のように示す。\n$$ \\mathrm{nullity}(T) := \\dim\\left( N(T) \\right) $$\n説明 これは行列のランク, 零次元数の概念を一般化したものだ。実際に$V, W$が有限次元ならば$T$は実質的に行列であり、$N(T)$は$T$を表す行列$M_{T}$の零空間である。行列の零次元数は零空間の次元だから、次が成立する。\n$$ \\mathrm{nullity}(T) = \\dim\\left( N(T) \\right) = \\dim (\\mathcal{N}(M_{T})) $$\n行列の次元定理を線形変換に対して一般化すると、次の定理が得られる。\n定理 $T : V \\to W$が線形変換であり、$V$が有限次元ならば、次が成立する。\n$$ \\mathrm{rank}(T) + \\mathrm{nullity}(T) = \\dim (V) $$\nHoward Anton, Elementary Linear Algebra: Aplications Version (12th Edition, 2019), p455-456\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3072,"permalink":"https://freshrimpsushi.github.io/jp/posts/3072/","tags":null,"title":"線形変換の階数、零空間の次元、次元定理"},{"categories":"선형대수","contents":"定義1 $T : V \\to W$を線形変換とする。$T$が$\\mathbf{0}$にマッピングする$V$の要素の集合をカーネルまたは零空間と言い、以下のように表記する。\n$$ \\text{ker}(T) = N(T) := \\left\\{ \\mathbf{v} \\in V : T( \\mathbf{v} ) = \\mathbf{0} \\right\\} $$\nすべての$\\mathbf{v} \\in V$の$T$による像の集合を$T$の値域またはイメージと言い、以下のように表記する。\n$$ R(T) := \\left\\{ T(\\mathbf{v}) : \\forall \\mathbf{v} \\in V \\right\\} $$\n説明 $T : V \\to W$が線形変換で、$V, W$が有限次元であれば、$T$は実質的に行列と同じで、$N(T)$は$T$を表す行列の零空間である。\n定理 $T : V \\to W$を線形変換とする。その場合、\n(a) $T$のカーネルは$V$の部分空間である。 (b) $T$の値域は$W$の部分空間である。 証明 部分空間であることを示すためには、空集合ではなく、加法とスカラー倍で閉じていることを示せばよい。\n(a) $T$が線形変換であれば、$T(\\mathbf{0})=\\mathbf{0}$によって$N(T)$は空集合ではない。今、$\\mathbf{v}_{1}, \\mathbf{v}_{2} \\in N(T)$であり、$k$を任意のスカラーとする。すると、以下が成立する。\n$$ \\begin{align*} T( \\mathbf{v}_{1} + \\mathbf{v}_{2} ) \u0026amp;= T(\\mathbf{v}_{1}) + T(\\mathbf{v}_{2}) = \\mathbf{0} + \\mathbf{0} = \\mathbf{0} \\\\ T( k\\mathbf{v}_{1}) \u0026amp;= kT(\\mathbf{v}_{1}) = k\\mathbf{0} = \\mathbf{0} \\end{align*} $$\nしたがって、$N(T)$は$V$の部分空間である。\n■\n(b) $T$が線形変換であれば、$T(\\mathbf{0})=\\mathbf{0}$によって$R(T)$は空集合ではない。今、$\\mathbf{w}_{1}, \\mathbf{w}_{2} \\in R(T)$であり、$k$を任意のスカラーとする。すると、以下を満たす$\\mathbf{a}, \\mathbf{b} \\in V$が存在することを示せば十分である。\n$$ T(\\mathbf{a}) = \\mathbf{w}_{1} + \\mathbf{w}_{2} \\quad \\text{and} \\quad T(\\mathbf{b}) = k\\mathbf{w}_{1} $$\nしかし、$\\mathbf{w}_{1}, \\mathbf{w}_{2} \\in R(T)$ということは、以下を満たす$\\mathbf{v}_{1}, \\mathbf{v}_{2} \\in V$が存在するという意味である。\n$$ T(\\mathbf{v}_{1}) = \\mathbf{w}_{1} \\quad \\text{and} \\quad T(\\mathbf{v}_{2}) = \\mathbf{w}_{2} $$\nしたがって、以下の式が成立する。\n$$ \\begin{align*} \\mathbf{w}_{1} + \\mathbf{w}_{2} \u0026amp;= T(\\mathbf{v}_{1}) + T(\\mathbf{v}_{2}) = T(\\mathbf{v}_{1} + \\mathbf{v}_{2}) = T(\\mathbf{a}) \\\\ k\\mathbf{w}_{1} \u0026amp;= kT(\\mathbf{v}_{1}) = T(k\\mathbf{v}_{1})= T(\\mathbf{b}) \\end{align*} $$\nしたがって、$R(T)$は$W$の部分空間である。\n■\nHoward Anton, Elementary Linear Algebra: アプリケーションバージョン (12版, 2019), p455-456\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3071,"permalink":"https://freshrimpsushi.github.io/jp/posts/3071/","tags":null,"title":"線形変換：カーネルと値域"},{"categories":"기하학","contents":"定義 1 $V$ をベクトル空間とする。二つのベクトル $\\mathbb{u}, \\mathbb{v} \\in V$ に対して、以下を満たす $\\theta$ を二つのベクトルの間の角度Angleと定義する。 $$ \\cos \\theta = {{ \\left\u0026lt; \\mathbb{u}, \\mathbb{v} \\right\u0026gt; } \\over { \\left| \\mathbb{u} \\right| \\left| \\mathbb{v} \\right| }} $$ もし二つのベクトル $\\mathbb{u}, \\mathbb{v}$ が $\\left\u0026lt; \\mathbb{u}, \\mathbb{v} \\right\u0026gt; = 0$ を満たせば、$\\mathbb{u}$ は $\\mathbb{v}$ に直交Orthogonalまたは垂直Perpendicularと言い、$\\mathbb{u} \\perp \\mathbb{v}$ のように示される。\n$\\left\u0026lt; \\cdot, \\cdot \\right\u0026gt;$ は内積で、$| \\cdot |$ はベクトルの長さであり、$\\left| \\mathbb{u} \\right| := \\sqrt{ \\left\u0026lt; \\mathbb{u}, \\mathbb{u} \\right\u0026gt; }$ のように計算される。 説明 誰もが同意するわけではないが、直交は抽象的なニュアンスを持ち、垂直は幾何学的な感じがする。基本的にはどちらの言葉を使っても構わないが、texで記号 $\\perp$ が \\perp を使用するほどに\u0026rsquo;垂直\u0026rsquo;が大幅にマイナーな表現ではないことだけは覚えておくこと。\n教育課程で内積をベクトルの大きさと内角と考えていたのとは違い、多次元ユークリッド空間 $V = \\mathbb{R}^{n}$ などでは、むしろ内積によって角度を考える。このように一般化された定義によると、特に\u0026rsquo;座標\u0026rsquo;という言葉なしに二つのベクトルの\u0026rsquo;方向性の違い\u0026rsquo;を考えることができる。\n応用 機械学習をはじめとする応用数学では、このようにしてコサイン類似度Cosine Similarityのような測定法を使用することもある。例えば、二つの文書A、Bで特定の単語a、b、cの頻度をベクトルとして表した場合、二つの文書がどれだけ類似しているかを判断する尺度になり得る。\nもし文書Bの長さが文書Aよりも圧倒的に長い場合、例えば100ページと1000ページであれば、単語の頻度も自然とそれに比例するため、単なる頻度の比較は意味がなくなる。この時、コサイン類似度を使用すれば、単なる回数ではなく、二つの文書の方向性自体を比較することになり、より合理的な結果を得ることができるだろう。\nMillman. (1977). Elements of Differential Geometry: p3.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2038,"permalink":"https://freshrimpsushi.github.io/jp/posts/2038/","tags":null,"title":"一般的な角度と垂直の定義"},{"categories":"줄리아","contents":"ガイド ステップ0. julia 1.6 以上をインストール\nバージョン1.6からは、インストール過程で環境変数に追加できる。示されたオプションをチェックしてインストールすればいい。古いバージョンを使っている場合は、1.6以上にアップデートするか、以下の指示に従えばいい。\nステップ1. Juliaのインストールパスを確認\nJuliaのインストールパスを確認する。特にいじっていなければ、次のパスに保存されているはずだ。\nC:\\Users\\사용자명\\AppData\\Local\\Programs\\Julia x.x.x\\bin 普通、C:\\Users\\ユーザー名\\AppDataは隠しフォルダだから見えなくても焦らないで。\n該当するパスで、上のようにjulia.exeファイルがあるか確認する必要がある。ステップ3で使うためにパスをコピーしておこう。\nステップ2. 環境変数を編集\nウィンドウ+sを押すか、コントロールパネルで「システム環境変数の編集」を検索する。\n「環境変数(N)」をクリックする。\nユーザー変数ウィンドウでPathを探し、「編集(E)」をクリックする。\nステップ3. Juliaのパスを追加\n「新規作成(N)」か、最下行を押してステップ1でコピーしたパスを上のように入力し、OKを押して環境変数の編集を終了する。\nステップ4. 再起動\n再起動後、powershellなどでjuliaコマンドを実行するとJuliaが起動することを確認できる。\n環境 OS: Windows julia: v1.5.2 ","id":2036,"permalink":"https://freshrimpsushi.github.io/jp/posts/2036/","tags":null,"title":"WindowsのCMDとPowerShellでJuliaを使用する方法"},{"categories":"확률론","contents":"概要 シャノンエントロピーShannon Entropyまたは情報エントロピーは、確率変数によって定義される無秩序の尺度であり、確率分布上でどれほど不確かであるかの量化と見なすことができる。\n簡単かつ複雑な定義 離散エントロピー 1 離散確率変数$X$の確率質量関数が$p(x)$である場合、$X$のエントロピーは次のように表される。 $$ H(X) := - \\sum p(x) \\log_{2} p(x) $$\n連続エントロピー 2 連続確率変数$X$の値が確率密度関数$f(x)$である場合、$X$のエントロピーは次のように表される。 $$ H(X) := - \\int_{-\\infty}^{\\infty} f(x) \\log_{2} f(x) dx $$\n難しくも簡単な定義 確率変数$X$に対するシャノン情報$I(X)$の期待値$H(X)$をエントロピーという。 $$ H(X) := E(I(X)) $$\n説明 確率変数$X, Y$の確率質量関数が$p, q$である場合、エントロピーも次のように表されることがある。 $$ H(X) = H(p) \\\\ H(Y) = H(q) $$\nエントロピーは科学全般で広く使用される概念であり、どのように定義されていても、その抽象的な意味は一般に「無秩序の度合い」である。熱力学でのエントロピーとは一見関係ないように見えるかもしれないが、ギブスのエントロピー表現 $$ S = - k_{B} \\sum_{i} P_{i} \\ln P_{i} $$ に従えば、その形も驚くほど似ており、歴史的にも深い関係がある。エピソードによると、クロード・シャノンClaude Shannonが$H(X)$を発見し、その重要性を最初に認識したとき、どんな名前をつけるべきかジョン・フォン・ノイマンVon Neumannに相談したところ、フォン・ノイマンは次のように答えたという：\n2つの理由で、$H$はエントロピーと呼ぶべきだ。第一に、その関数はすでに熱力学でエントロピーと呼ばれている。第二に、ほとんどの人がエントロピーが何であるか本当にわからないため、任意の議論で「エントロピー」という言葉を使えば勝つことができるだろう。\n無秩序度 情報量の期待値であるエントロピーがどのように自然に無秩序度を表すか見てみよう。\n確率$p$が与えられたベルヌーイ分布を考えてみよう。例えば、表が出る確率が$p \\in (0,1)$に操作されたコインを想像してみよう。この時、コインの表裏を表示する確率変数$X$のエントロピーは正確に次のように計算されるだろう。 $$ H(X) = - p \\log_{2} p - (1-p) \\log_{2} (1-p) $$ $p$が$0$や$1$に近ければ近いほど、不確実性は減少し、無秩序度は上がると考えられる。表が出る確率が$90\\% $のコインを投げて表裏を当てるゲームがある場合、わざわざ裏を選ぶ必要はなく、少しでも有利な表を選ぶだろう。実際に計算して、この直観と一致するか確認してみよう。もし$p = 1/4$であれば、 $$ \\begin{align*} H(X) =\u0026amp; - {{ 1 } \\over { 4 }} \\log_{2} {{ 1 } \\over { 4 }} - {{ 3 } \\over { 4 }} \\log_{2} {{ 3 } \\over { 4 }} \\\\ =\u0026amp; {{ 1 } \\over { 4 }} \\log_{2} 4 - {{ 3 } \\over { 4 }} \\left( \\log_{2} 3 - \\log_{2} 4 \\right) \\\\ =\u0026amp; {{ 1 } \\over { 2 }} - {{ 3 } \\over { 4 }} \\log_{2} 3 + {{ 3 } \\over { 2 }} \\\\ =\u0026amp; 2 - {{ 3 } \\over { 4 }} \\log_{2} 3 \\end{align*} $$ この値を実数で計算すると、約$0.81$程度になる。今、$p = 1/2$の場合を計算してみると、 $$ \\begin{align*} H(X) =\u0026amp; - {{ 1 } \\over { 2 }} \\log_{2} {{ 1 } \\over { 2 }} - {{ 1 } \\over { 2 }} \\log_{2} {{ 1 } \\over { 2 }} \\\\ =\u0026amp; {{ 1 } \\over { 2 }} + {{ 1 } \\over { 2 }} \\\\ =\u0026amp; 1 \\end{align*} $$ $p=1/4$の場合よりもエントロピーが大きくなったことがわかる。実際に、これは前もって表か裏か全く分からない、最も混乱し、無秩序な状態を表している。\n別の例として、一様分布$\\text{Uni}(a,b)$に従う確率変数$X$を考えてみると、そのエントロピーは $$ \\begin{align*} H(X) =\u0026amp; - \\int_{a}^{b} {{ 1 } \\over { b-a }} \\log_{2} {{ 1 } \\over { b-a }} dx \\\\ =\u0026amp; \\log_{2} \\left( b-a \\right) \\end{align*} $$ として簡単に計算できる。エントロピーは無秩序度の尺度と言われているが、$b$と$a$の間隔が広がることは、$X$の範囲が広がり、どのような値になるかを密接に予想することがより困難になると同時に、$\\log_{2} (b-a)$も大きくなることを意味する。これにより、エントロピーが自然に無秩序の尺度であることが確認できた。\n簡単な定義の限界 ある程度学んだ人なら、簡単な定義と難しい定義には違いがないように見えるべきだ。後者は単により一般的であり、前者が述べているすべてをカバーしている。参考文献での離散エントロピーの定義は事象が有限の場合にのみ定義されており、連続エントロピーを定義することは良いが、限界の概念でアプローチしたときに問題があると指摘されている。\n抽象的に見れば、シャノン情報は元の確率分布にイベント毎にそれに対応する情報量を割り当てる確率変数であるため、わざわざ離散か連続かを定義する必要はなく、有限、無限、可算、不可算、積分範囲などを考慮する必要もない。情報量の定義に問題がなければ、エントロピーは「情報量の期待値」として簡単に定義できる。\nApplebaum. (2008). Probability and Information(2nd Edition): p108。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nApplebaum. (2008). Probability and Information(2nd Edition): p180。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2035,"permalink":"https://freshrimpsushi.github.io/jp/posts/2035/","tags":null,"title":"シャノンエントロピー：確率変数によって定義されるエントロピー"},{"categories":"수리통계학","contents":"概要 数学を使用する科目では、正則性Regularity Conditionsとは、一般的に応用の範囲が広く、理論的な展開を容易にする条件を指します。数理統計学では、以下のようになります。\n前提 1 パラメーター$\\theta \\in \\Theta$に対する確率密度関数が$f \\left( x ; \\theta \\right)$である確率変数$X$を考える。$X$と同じ分布からiidで抽出されたランダムサンプル$X_{1} , \\cdots , X_{n}$は、同じ確率密度関数$f(x ; \\theta)$と実現$\\mathbf{x} := \\left( x_{1} , \\cdots , x_{n} \\right)$を持つ。以下の関数$L$は尤度関数Likelihood Functionと呼ばれる。 $$ L ( \\theta ; \\mathbf{x} ) := \\prod_{k=1}^{n} f \\left( x_{k} ; \\theta \\right) $$ 最後に、$\\theta_{0}$を$\\theta$の真の値としましょう。\n(R0): 確率密度関数$f$は$\\theta$に対して単射です。式では、以下を満たします。 $$ \\theta \\ne \\theta\u0026rsquo; \\implies f \\left( x_{k} ; \\theta \\right) \\ne f \\left( x_{k} ; \\theta\u0026rsquo; \\right) $$ (R1): 確率密度関数$f$は、すべての$\\theta$に対して同じサポートを持つ。 (R2): 真の値$\\theta_{0}$は$\\Omega$の内点です。 (R3): 確率密度関数$f$は$\\theta$に対して二回微分可能です。 (R4): 積分$\\int f (x; \\theta) dx$は$\\theta$に対して二回微分可能であり、微分は積分記号との交換が可能です。 (R5): 確率密度関数$f$は$\\theta$に対して三回微分可能です。さらに、すべての$\\theta \\in \\Theta$に対して、$E_{\\theta_{0}} \\left[ M ( X ) \\right] \u0026lt; \\infty$を満たしつつ、以下を満たす定数$c\u0026gt; 0$と関数$M(x)$が存在します。 $$ \\left| {{ \\partial^{3} } \\over { \\partial \\theta ^{3} }} \\log f (x ; \\theta) \\right| \\le M (x) \\qquad , \\forall x \\in \\mathcal{S}_{X} , \\forall \\theta \\in \\left( \\theta_{0} - c , \\theta_{0} + c \\right) $$ Hogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p328, 334.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2029,"permalink":"https://freshrimpsushi.github.io/jp/posts/2029/","tags":null,"title":"数理統計学における正則性条件"},{"categories":"최적화이론","contents":"簡単な定義 最大値Maximumと最小値Minimumを合わせて最適値Optimumと言う。\n集合$X$で最も大きい要素を最大値$\\max X$、最も小さい要素を最小値$\\min X$と示す。 関数$f : X \\to \\mathbb{R}$の最も大きい関数値を$\\max_{X} f$、最も小さい関数値を$\\min_{X} f$と示す。 $\\mathbb{R}$は実数全体の集合を示す。 最大、最小は漢字で、値は純粋な韓国語だから、実は2021年基準で最大値、最小値が正しいが、検索エンジンにあまりフレンドリーではないためにㅅを省略した。同様に値を使って最大値、最小値とも書けるが、現在の言語習慣で最も馴染みがある言葉を選んだ。 説明 数学者の視点から、大きいものを探すか、小さいものを探すかは実際にはあまり意味がない。特に最適化問題では、アルゴリズム全般を説明する際に最大化をするか最小化をするかは区別せずに、ただ最適化すると言う。\nMaximumの複数形はMaxima、Minimumの複数形はMinima、Optimumの複数形はOptimaだ。最適値が複数あるのはおかしいから、これらの表現が使われた場合は、文脈上、最適値Valueではなく最適点Pointと見なさなければならない。\n最適値と極値を対比すると、最適値はこのポストの文脈で言えば、全域Global最適値、極値は局所Local最適値と見ることもできる。普通、この文脈で全域と言う言葉はあまり意味がない。\n文脈上、値と集合自体が重要ではない場合は、集合、要素、関数の表現が適当な場合が多いから、注意が必要だ。\nこのポストの意義は、実は子供の頃から親しみやすく使ってきた最大値と最小値を関数の形で明確に定義することにある。上の簡単な定義に直感的に同意できれば、次の難しい定義も見てみよう。\n難しい定義 集合の最適値 全順序集合$\\left( Y, \\le \\right)$が与えられているとする。\n$\\max, \\min : 2^{Y} \\to Y$は$Y$の各部分集合$A \\in 2^{Y}$に対して$B$の最も小さい要素か最も大きい要素である$y_{\\ast} \\in B$に対応させる関数だ。$$ \\max : B \\mapsto y_{\\ast} \\ge b \\qquad , \\forall b \\in B \\\\ \\min : B \\mapsto y_{\\ast} \\le b \\qquad , \\forall b \\in B $$\n関数の最適値 集合$X$が定義域で、$Y$が値域である関数の集合$Y^{X}$が与えられているとする。\n$A \\subset X$に対して、$\\max_{A}, \\min_{A} : Y^{X} \\to Y$は関数$f : X \\to Y$、すなわち$f \\in Y^{X}$について次のように定義される。$$ \\max_{A} f = \\max f(A) \\\\ \\min_{A} f = \\min f(A) $$\n$f(A)$は$A$に対する$f$のイメージとして、次のように定義される。 $$ f(A) := \\left\\{ f(a) : \\forall a \\in A \\right\\} $$ 例 集合$[0,1)$について、最大値は存在せず、最小値は$\\min [0, 1) = 0$だ。\n二次関数$f(x) := 2x^{2} + 1$の最小値は$\\min_{\\mathbb{R}} f = f(0) = 1$だ。別に$A = [2,3] \\subset \\mathbb{R}$内で最大値を考えると、$\\max_{a \\in A} f(a) = f(2) = 9$だ。\n上記の例でも表記が乱れがちだが、説明したように、普通は大体見逃しても問題ない。\n","id":2027,"permalink":"https://freshrimpsushi.github.io/jp/posts/2027/","tags":null,"title":"最適値：最大値と最小値"},{"categories":"수리통계학","contents":"ビルドアップ パラメータ$\\theta \\in \\Theta$に対して、確率密度関数が$f \\left( x ; \\theta \\right)$である確率変数$X$について考えよう。同じ確率密度関数$f(x ; \\theta)$と実現$\\mathbf{x} := \\left( x_{1} , \\cdots , x_{n} \\right)$を持っている、$X$と同じ分布からiidに抽出されたランダムサンプル$X_{1} , \\cdots , X_{n}$がある。これに対して定義された関数$L$を、尤度関数Likelihood Functionと言う。 $$ L ( \\theta ; \\mathbf{x} ) := \\prod_{k=1}^{n} f \\left( x_{k} ; \\theta \\right) $$ 以下で説明する通り、我々はこの関数の最大値に関心があるので、掛け算$\\prod$を足し算$\\sum$に変えて、ログを取った$l$として表す方が便利である。 $$ l ( \\theta ; \\mathbf{x} ) := \\sum_{k=1}^{n} \\log f \\left( x_{k} ; \\theta \\right) $$\n定義 1 以下を満たす推定量$\\hat{\\theta} := \\hat{\\theta} \\left( \\mathbf{X} \\right)$を、最尤推定量Maximum Likelihood Estimator、略してmleと呼ぶ。 $$ \\hat{\\theta} = \\argmax L \\left( \\theta ; \\mathbf{X} \\right) $$\n$\\mathbf{X}$はランダムベクター$\\mathbf{X} := \\left( X_{1} , \\cdots , X_{n} \\right) $である。 $\\argmax g$は関数$g$の最大引数で、$g$が最大になるような値である。 説明 直感 実際、尤度は英語表現で見る方がもっと直感的で、「ありそうな」を意味する。\n例えば、通りで偶然見かけたどんな男性3人の身長を計測したところ、169cm、171cm、182cmだったとしよう。そして、韓国男性の身長は正規分布$N \\left( \\mu , \\sigma^{2} \\right)$に従っていると仮定しよう。正規分布の確率密度関数$f (x; \\mu)$は平均$x = \\mu$で最大値を取るので、その関数値の積で定義される$L \\left( \\theta ; \\mathbf{x} \\right)$は$\\theta = \\mu$の時に最も大きな値を持つ可能性が高い。\nここで、関数$L$の主な引数はデータ$\\mathbf{x}$ではなく$\\theta$に注目しよう。つまり、$L$は、確率密度関数$f(x)$に入れる$x$が動きながら値が変わらないが、$f_{\\theta}$自体が$\\theta$によって左右に動きながら変わる関数だと想像するといい。\nまだ$L$の性質についてよくわかっていないから、$L$が最も大きくなる場所が$\\theta = 171$だと確信を持って言えないが、確実に$\\theta = 182$ではない。尤度という言葉や$\\argmax$が見慣れないかもしれないが、実際には最尤推定量とは「最もありそうな値」を指しているのだ。\n数式 もし$L$が微分可能であれば、最尤推定量は次の推定方程式Estimating Equation、すなわち偏微分方程式を満たす。 $$ {{ \\partial l ( \\theta ) } \\over { \\partial \\theta }} = 0 $$ これはカリキュラムで関数の最大値を求める際に微分を使った解法の延長に過ぎない。ただし、教科書でこの部分を見ると、特に統計学の学生は大学1年生以降に微分方程式を扱うことがほとんどないので、馴染みがなく怖く感じられるかもしれない。しかし実際には微分方程式を解く必要はなく、よく知らなくても大丈夫なので、あまり心配しないでほしい。\nHogg et al. (2013). Introduction to Mathematical Statistics(7th Edition): p209, 329.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2026,"permalink":"https://freshrimpsushi.github.io/jp/posts/2026/","tags":null,"title":"最尤推定量"},{"categories":"동역학","contents":"概要 SIRモデルは、疾病や情報の拡散をシンプルかつ直感的によく説明する、最も単純で多くの変形を持つ力学的区画モデルのひとつだ。\nモデル 1 $$ \\begin{align*} {{d S} \\over {d t}} =\u0026amp; - {{ \\beta } \\over { N }} I S \\\\ {{d I} \\over {d t}} =\u0026amp; {{ \\beta } \\over { N }} S I - \\mu I \\\\ {{d R} \\over {d t}} =\u0026amp; \\mu I \\end{align*} $$\n変数 $S(t)$: $t$の時点で病気にかかりうるSusceptible集団の個体数を示す。 $I(t)$: $t$の時点で病気を伝えうるInfectious集団の個体数を示す。情報の拡散の文脈では、Informedの頭文字をとることもある。 $R(t)$: $t$の時点で回復したRecovered集団の個体数を示す。情報の拡散の文脈では、RefractoryやRemovedの頭文字をとり、シミュレーション上で反応しなくなり扱わない意味をもつこともある。 $N(t) = S(t) + I(t) + R(t)$: 総個体数を示す。バイタルダイナミクスVital Dynamicsが考慮されていない場合、通常は保存量(定数)として扱われ、変数を全人口の比率として扱う場合には、しばしば$N(t) = 1$と表される。 パラメータ $\\beta\u0026gt;0$: 伝染率Infection Rateである。 $\\mu\u0026gt;0$: 回復率Recovery Rateである。 説明 変数で言及されたバイタルダイナミクスは、文字通り各個体の生涯を考慮し、生まれて年を取り、死んでいくことで総個体数自体が変化することを意味する。長い時間にわたる分析や、風土病に関してではない限り、特に重視されることはない。\n導出 ロトカ-ヴォルテラ 捕食者-被食者モデル: $$ \\begin{align*} \\dot{x} =\u0026amp; a x - b y \\cdot x \\\\ \\dot{y} =\u0026amp; c x \\cdot y - d y \\end{align*} $$\nロトカ-ヴォルテラ競争モデルの特別な場合と見なせば、導出はほとんど終わったと同じだ。$S$は病気に関する被食者$S = x$、$I$は自然に捕食者$I = y$となる。被食者集団が病気に対する対抗手段がないと仮定すると$a = 0$で、$b = c := \\beta / N$、$d = \\mu$とすれば\n$$ \\begin{align*} {{d S} \\over {d t}} =\u0026amp; - {{ \\beta } \\over { N }} I S \\\\ {{d I} \\over {d t}} =\u0026amp; {{ \\beta } \\over { N }} S I - \\mu I \\end{align*} $$\nここに、単に$R$の変化率を$\\displaystyle {{d R} \\over {d t}} = \\mu I$のように加えるだけで、SIRモデルのシステムを得る。\n■\n基本感染再生産数 $$\\mathcal{R}_{0} = {{ \\beta } \\over { \\mu }}$$\n正確には、ヤコビ行列で固有値を正確に求めることができるが、計算が多いので省略し、手軽な方法を考えよう。最初の伝染病が広がり始める時期、つまり$S \\approx N$の時、この伝染病が最終的に大発生へとつながるためには$\\displaystyle {{ d I } \\over { d t }} \u0026gt; 0$でなければならない。言い換えれば$I(0) \u0026gt; 0$に対して $$ {{ \\beta } \\over { N }} N I - \\mu I \\approx ( \\beta - \\mu ) I \u0026gt; 0 $$ 数式で$\\displaystyle {{ \\beta } \\over { \\mu }} \u0026gt; 1$であれば、$I$は続けて増加し、大発生が起こる。この観点から、$\\displaystyle \\mathcal{R}_{0} := {{ \\beta } \\over { \\mu }}$は流行の臨界値Epidemic Threshold2とも呼べる。\n変形 SIRSモデル：一時的免疫 34 基本的にRRecovered状態は、病気から回復した状態、つまり病気に対する永久的な免疫を得たものと仮定している。しかし、以下のように項$\\nu R$を入れることで免疫喪失を反映させることができる。SIRとは異なり、直感的に風土病Endemicを扱うことができる。\n$$ \\begin{align*} {{d S} \\over {d t}} =\u0026amp; - {{ \\beta } \\over { N }} I S + \\nu R \\\\ {{d I} \\over {d t}} =\u0026amp; {{ \\beta } \\over { N }} S I - \\mu I \\\\ {{d R} \\over {d t}} =\u0026amp; \\mu I - \\nu R \\end{align*} $$\n保菌者 3 保菌者Carrierは病気を広げるが、臨床的な症状はない個体を指す。これらの数$C$が一定であれば、SIRシステムは次のように修正できる。\n$$ \\begin{align*} {{d S} \\over {d t}} =\u0026amp; - {{ \\beta } \\over { N }} (I + C) S \\\\ {{d I} \\over {d t}} =\u0026amp; {{ \\beta } \\over { N }} S (I + C) - \\mu I \\\\ {{d R} \\over {d t}} =\u0026amp; \\mu I \\end{align*} $$\nバイタルダイナミクス ロジスティック成長モデルと同様に、出生率$r\u0026gt;0$と死亡率$\\gamma\u0026gt;0$を与えてバイタルダイナミクスを考慮できる。ここでは死亡率は感染の有無にかかわらず同様に適用され、成長率も感染状態を問わず現在の総個体数$N(t) = S(t) + I(t) + R(t)$に比例する。\n$$ \\begin{align*} {{d S} \\over {d t}} =\u0026amp; - \\gamma S - {{ \\beta } \\over { N }} I S + r N \\\\ {{d I} \\over {d t}} =\u0026amp; - \\gamma I + {{ \\beta } \\over { N }} S I - \\mu I \\\\ {{d R} \\over {d t}} =\u0026amp; - \\gamma R + \\mu I \\end{align*} $$\n垂直感染 垂直感染または母子感染は母体から新生児に直接伝わる感染5を指し、B型肝炎ウイルスがその例の一つだ。これを反映するために、上記のバイタルダイナミクスから得られたシステムを、垂直感染確率$q \\in (0,1)$を与え、次のように修正することができる。\n$$ \\begin{align*} {{d S} \\over {d t}} =\u0026amp; - \\gamma S - {{ \\beta } \\over { N }} I S + r ( 1- q) N \\\\ {{d I} \\over {d t}} =\u0026amp; - \\gamma I + {{ \\beta } \\over { N }} S I - \\mu I + r q N \\\\ {{d R} \\over {d t}} =\u0026amp; - \\gamma R + \\mu I \\end{align*} $$\n$r q N$は病気を持って生まれてくる新生児に対する項である。\nAllen. (2006). Mathematical Biologyの入門: p273.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCapasso. (1993). Mathematical Structures of Epidemic Systems: p41.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCapasso. (1993). Mathematical Structures of Epidemic Systems: p9.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAllen. (2006). Mathematical Biologyの入門: p275.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://terms.naver.com/entry.nhn?docId=1115841\u0026amp;cid=40942\u0026amp;categoryId=32316\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2025,"permalink":"https://freshrimpsushi.github.io/jp/posts/2025/","tags":null,"title":"SIRモデル：最も基本的な拡散モデル"},{"categories":"줄리아","contents":"## コード [^1] [^1]: https://docs.julialang.org/en/v1/manual/metaprogramming/ Juliaでは[メタプログラミング](../1457)を言語レベルでサポートしている。これは文字列をそのままのコードとして読み込んで実行した結果だ。 julia\u0026gt; text = \u0026ldquo;f(x) = 2x + 1; f(2)\u0026rdquo; \u0026ldquo;f(x) = 2x + 1; f(2)\u0026rdquo;\njulia\u0026gt; code = Meta.parse(text) :($(Expr(:toplevel, :(f(x) = begin #= none:1 =# 2x + 1 end), :(f(2)))))\njulia\u0026gt; eval(code) 5\n- `Meta.Parse()`: 이 함수를 통해 입력된 문자열을 **표현식**\u0026lt;sup\u0026gt;Expression\u0026lt;/sup\u0026gt;으로 바꿔 반환한다.\r- `eval()`: 표현식을 **평가**\u0026lt;sup\u0026gt;Evaluate\u0026lt;/sup\u0026gt;한다. 위 예제코드에서는 $f(2)$ 가 실제로 평가되어 함숫값인 $5) ## 環境 - OS: Windows - julia: v1.5.0 ","id":2024,"permalink":"https://freshrimpsushi.github.io/jp/posts/2024/","tags":null,"title":"ジュリアのメタプログラミング"},{"categories":"줄리아","contents":"コード vec() 関数を使えばいい。\njulia\u0026gt; A = rand(0:9, 3,4)\r3×4 Array{Int64,2}:\r6 8 7 3\r2 9 3 2\r5 0 6 7\rjulia\u0026gt; vec(A)\r12-element Array{Int64,1}:\r6\r2\r5\r8\r9\r0\r7\r3\r6\r3\r2\r7 人間の目には、1次元配列と同じように見えるが、タイプ上では2次元配列でエラーが出るケースも、この方法で解決できる。次の2つの命令は完全に同じ配列に見えるが、$\\mathbb{N}^{10 \\times 1}$ 行列か $\\mathbb{N}^{10 }$ ベクトルかの違いがある。\njulia\u0026gt; b = rand(0:9, 10,1)\r10×1 Array{Int64,2}:\r4\r8\r0\r4\r7\r4\r4\r2\r4\r7\rjulia\u0026gt; vec(b)\r10-element Array{Int64,1}:\r4\r8\r0\r4\r7\r4\r4\r2\r4\r7 実際の flatten() 関数 実は Base.Iterators に、本当の名前がフラッテンの flatten() が実装されている。実行結果は以下の通りで、正直、使いたくないかもしれない。正確に言うと、配列を変えるというよりは、ループなどに入る時にイテレータIteratorsとして使う時に必要になるかもしれない。正直、必要ない。\njulia\u0026gt; c = rand(0:9, 3,3)\r3×3 Matrix{Int64}:\r7 7 4\r9 3 8\r4 4 5\rjulia\u0026gt; Iterators.flatten(c)\rBase.Iterators.Flatten{Matrix{Int64}}([7 7 4; 9 3 8; 4 4 5])\rjulia\u0026gt; vec(c)\r9-element Vector{Int64}:\r7\r9\r4\r7\r3\r4\r4\r8\r5 環境 OS: Windows julia: v1.5.0 ","id":2022,"permalink":"https://freshrimpsushi.github.io/jp/posts/2022/","tags":null,"title":"ジュリアで配列をフラット化する方法"},{"categories":"줄리아","contents":"結論 $n$ 個の座標間の距離を計算しようとする。\n全ての座標間を計算する必要がなければ、グループに分けて長方形の距離行列を作ればいい。 長方形の距離行列は pairwise() 関数で簡単かつ速く計算できる。 速度比較 例えば、SIRモデルに対して移動するエージェントベースのシミュレーションを行うと考えてみよう。元の時間計算量は $O \\left( n^{2} \\right)$ だが、$S$ と $I$ のグループに分けて計算すると、時間計算量は $O \\left( n_{S} n_{I} \\right)$ に大きく下がる。通常、病気の伝播は $S$ と $I$ 間の距離行列を計算して一定の半径 $\\varepsilon$ 内に入るかを判断し、どれだけ接触したかによって実装される。この作業で速度を比較してみよう。\nusing Distances\rusing StatsBase\rusing Random\rRandom.seed!(0);\rN = 10^4\rlocation = rand(2, N);\rstate = sample([\u0026#39;S\u0026#39;, \u0026#39;I\u0026#39;], Weights([0.1, 0.9]), N);\rS = location[:, state .== \u0026#39;S\u0026#39;]\rI = location[:, state .== \u0026#39;I\u0026#39;]\rfunction foo(S, I)\rcontact = Int64[]\rfor s in 1:996\rpush!(contact, sum(sum((I .- S[:,s]).^2, dims = 1) .\u0026lt; 0.1))\rend\rreturn contact\rend\r@time foo(S, I) もちろん、グループに分けて計算するという発想は、ジュリアだけでなくどんな手法を使っても性能向上をもたらす。ポイントは、無理にループを回す必要がなく、Distance パッケージの pairwise() 関数を上手く使えばいいということだ。\njulia\u0026gt; @time foo(S, I);\r0.170835 seconds (7.98 k allocations: 210.854 MiB, 12.56% gc Time)\rjulia\u0026gt; @time sum(pairwise(Euclidean(),S,I) .\u0026lt; 0.1, dims = 1);\r0.087875 seconds (14 allocations: 69.639 MiB, 4.15% gc Time) これら二つの命令は正確に同じ機能を持つが、速度面では約2倍の差があり、allocationに関しては計算したくもないほど大きな差が出るし、コーディングの難易度もループに比べてずっと簡単だ。\n追加研究 1 ユークリッド距離が距離関数である場合、Euclidean() の代わりに SqEuclidean() を使うと、ルートを取る計算を省略できるため、速度がさらに上がる。次は正確に同じ結果を出すが、速度面では約1.5倍の差が出る。\njulia\u0026gt; @time sum(pairwise(Euclidean(),S,I) .\u0026lt; 0.1, dims = 1);\r0.091917 seconds (14 allocations: 69.639 MiB, 7.60% gc Time)\rjulia\u0026gt; @time sum(pairwise(SqEuclidean(),S,I) .\u0026lt; 0.01, dims = 1);\r0.061776 seconds (14 allocations: 69.639 MiB, 11.37% gc Time) さらに、もっと速くなることができる。ここでは、単純なコード最適化だけでは速度を上げるのが難しく、多次元検索に有利なデータ構造であるk-d木2を使用しなければならない。NearstNeighbors.jlで速く距離を計算する方法を参照。\n環境 OS: Windows julia: v1.5.0 https://github.com/JuliaStats/Distances.jl#pairwise-benchmark\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/K-d_tree\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2020,"permalink":"https://freshrimpsushi.github.io/jp/posts/2020/","tags":null,"title":"ジュリアで距離行列計算を最適化する方法"},{"categories":"동역학","contents":"概要 1 伝染病の区分モデルは伝染病の流行に関するモデルであり、人口動態に伝染病を加え、「人口」をいくつかの区分に分ける。\n疫学は伝染病を扱う学問であり、海老寿司屋で扱われる他の力学とは関係がない。 説明 カーマックとマッケンドリックによっていわゆるSIRモデルが考案されてから、多くの変化と発展があり、このアイデアの源流にある全てのモデルは基本的に伝染病の区分モデルと考えられる。最も代表的なモデルは、前述した最初のSIRモデルで、全人口 $N$ を次の三つの区分に分けた:\n$S$ 感受性: 健康で、病気になりうる状態 $I$ 感染: 病気で、病気を広げる状態 $R$ 回復: 回復し、免疫を持つ状態 これらは伝染率 $\\beta \u0026gt; 0$ と回復率 $\\mu \u0026gt; 0$ に関して、次のように単純な自立システムとして表される。\n$$ \\begin{align*} {{d S} \\over {d t}} =\u0026amp; - \\beta I S \\\\ {{d I} \\over {d t}} =\u0026amp; \\beta S I - \\mu I \\\\ {{d R} \\over {d t}} =\u0026amp; \\mu I \\end{align*} $$\n双線型システムの一般的構造 2 ベレッタとカパッソは、多くの派生モデルの一般的な形を次のように定式化した。 $$ {{dz} \\over {dt}} = \\text{diag} (z) (e + A z) + c $$\n$n$ は区分の数。 $z(t) \\in \\mathbb{R}^{n}$ は時間 $t$ に依存する区分のベクトル。 $e, c \\in \\mathbb{R}^{n}$ は定数ベクトル。 $A \\in \\mathbb{R}^{n \\times n}$ は競合係数として表される定数行列。 なぜこの形になるのか? 通常、システムを導出するときは質量作用の法則に基づき、$A$ と $B$ が出会い反応するからである。これらは理想的な空間の中で一定の比率で出会い反応し、その程度はそれぞれの量に比例するので、反応する量も $[A][B]$ に比例する。したがって、少なくとも各区分は２回は乗算されなければならないが、実際には三つの区分が集まり共に反応する現象は珍しい。したがって、ほとんどの線形モデルは双線型で表されることができる。\n例 単純なSIRモデルを例にとると、 $$ {{ d } \\over { dt }} \\begin{bmatrix} S(t) \\\\ I(t) \\\\ R(t) \\end{bmatrix} = \\begin{bmatrix} S(t) \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; I(t) \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; R(t) \\end{bmatrix} \\left( \\begin{bmatrix} 0 \\\\ -\\mu \\\\ \\mu \\end{bmatrix} + \\begin{bmatrix} 0 \u0026amp; -\\beta \u0026amp; 0 \\\\ \\beta \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} S(t) \\\\ I(t) \\\\ R(t) \\end{bmatrix} \\right) + \\mathbf{0} $$ このように展開することができる。 $$ e = \\begin{bmatrix} 0 \\\\ -\\mu \\\\ \\mu \\end{bmatrix} \\\\ A = \\begin{bmatrix} 0 \u0026amp; -\\beta \u0026amp; 0 \\\\ \\beta \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} \\\\ c = \\mathbf{0} $$\nCapasso. (1993). Mathematical Structures of Epidemic Systems: p7.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCapasso. (1993). Mathematical Structures of Epidemic Systems: p10.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2019,"permalink":"https://freshrimpsushi.github.io/jp/posts/2019/","tags":null,"title":"力学区画モデル"},{"categories":"줄리아","contents":"概要 Juliaで、Rのsample()やPythonパッケージnumpyのrandom.choice()と同じ役割をするsample()関数とWeights関数の使用方法です。\nコード 1 ■コード１■\n実行結果 ■コード２■\n環境 OS: Windows julia: v1.5.0 https://stackoverflow.com/a/27560273/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2018,"permalink":"https://freshrimpsushi.github.io/jp/posts/2018/","tags":null,"title":"ジュリアで重み付けとランダムサンプリングをする方法"},{"categories":"줄리아","contents":"結論 配列の各要素をEqualオペレータ==で比較すると、整数よりもCharの方が早い。\n速度比較 julia\u0026gt; integer = rand(1:5, N); print(typeof(integer))\rArray{Int64,1}\rjulia\u0026gt; character = rand([\u0026#39;S\u0026#39;,\u0026#39;E\u0026#39;,\u0026#39;I\u0026#39;,\u0026#39;R\u0026#39;,\u0026#39;D\u0026#39;], N); print(typeof(character))\rArray{Char,1}\rjulia\u0026gt; @time integer .== 1;\r0.009222 seconds (6 allocations: 1.196 MiB)\rjulia\u0026gt; @time character .== \u0026#39;S\u0026#39;;\r0.005266 seconds (7 allocations: 1.196 MiB) 上のコードは、整数と文字で構成された配列で1とSがどこにあるかを特定するプログラムだ。整数か文字列かの違いを除いて、全て正確に同じだが、時間の消費はほぼ二倍の大きな差がある。したがって、コード最適化の過程で一般的に使用される方法なので、可能な限り文字を使用することが推奨される。\n追加研究 julia\u0026gt; string = rand([\u0026#34;S\u0026#34;,\u0026#34;E\u0026#34;,\u0026#34;I\u0026#34;,\u0026#34;R\u0026#34;,\u0026#34;D\u0026#34;], N); print(typeof(string))\rArray{String,1}\rjulia\u0026gt; @time string .== \u0026#34;S\u0026#34;;\r0.072692 seconds (7 allocations: 1.196 MiB) 当然ながら、文字Characterではなく文字列Stringを使用すると、10倍以上の速度の低下が発生する。\n環境 OS: Windows julia: v1.5.0 ","id":2016,"permalink":"https://freshrimpsushi.github.io/jp/posts/2016/","tags":null,"title":"ジュリアでの文字と整数の等価オペレータ==の速度比較"},{"categories":"르벡공간","contents":"定義1 以下のように定義される関数空間を重み付き$L^{p}$空間あるいは具体的に**$w$-重み付き$L^{p}$空間**という。\n$$ L_{w}^{p}(a,b):= \\left\\{ f : \\mathbb{R}\\to \\mathbb{C}\\ \\big|\\ \\int_{a}^{b} \\left| f(x) \\right|^{p}w(x)dx \u0026lt;\\infty \\right\\} $$\nこの時、$w:\\mathbb{R}\\to[0,\\infty)$を重み関数という。\n説明 $L^{p}$空間を一般化した空間の一つである。$w(x)=1$の場合に$L_{w}^{p}=L^{p}$が成立する。重み付き$L^{p}$空間のノルムは、$1\\le p \u0026lt;\\infty$に対して以下のように定義される。\n$$ \\left\\| f\\right\\|_{p,w}=\\left\\| f\\right\\|_{L_{w}^{p}(a,b)}=\\left( \\int_{a}^{b}\\left| f(x) \\right|^{p}w(x)dx \\right)^{\\frac{1}{p}} $$\n$L_{w}^{p}$空間の定義により、上記の値が有限であることは自明だ。特に$p=2$の場合には、下記のように内積を定義することができる。\n$$ \\langle f,g \\rangle_{L_{w}^{2}(a,b)}=\\int_{a}^{b}f(x)\\overline{g(x)}w(x)dx,\\quad f,g \\in L_{w}^{2}(\\mathbb{R}) $$\n$L^{2}$空間と同様に、ヒルベルト空間になる。\nGerald B. Folland, Fourier Analysis and Its Applications (1992), p81\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1856,"permalink":"https://freshrimpsushi.github.io/jp/posts/1856/","tags":null,"title":"重み付きLp空間"},{"categories":"수리통계학","contents":"要約 1 確率変数 $X_{1} , \\cdots , X_{n}$ がiidで正規分布 $N\\left( \\mu,\\sigma^{2} \\right)$ に従うとすると\n(a): $$ \\overline{X} \\sim N\\left( \\mu , { {\\sigma^2} \\over {n} } \\right) $$ (b): $$ \\overline{X} \\perp S^2 $$ (c): $$ (n-1) { {S^2} \\over {\\sigma^2} } \\sim \\chi^2 (n-1) $$ (d): $$ T = { {\\overline{X} - \\mu } \\over {S / \\sqrt{n}} } \\sim t(n-1) $$ 標本平均 $\\overline{X}$ と 標本分散 $S^{2}$ は、次のように定義される確率変数だ。 $$ \\overline{X} := {{ 1 } \\over { n }} \\sum_{k=1}^{n} X_{k} \\\\ S^{2} := {{ 1 } \\over { n-1 }} \\sum_{k=1}^{n} \\left( X_{k} - \\overline{X} \\right)^{2} $$\n説明 統計学をする人たちは当たり前のように使っているけど、事実これにも名前がある。四つのパートに分けられていて、具体的に引用するのも難しい。\n(b)は、標本平均でも標本分散でも、同じデータから出ているにもかかわらず、独立であるというのがそう。\n小標本に対する母平均の推定 スチューデントの定理の証明は小標本での母平均に対する仮説検定の導出そのものだ。\n証明 (a) $\\displaystyle \\overline{X} = { { (X_1 + X_2 + \\cdots + X_n )} \\over {n}}$ なので、正規分布に従う確率変数の和を考えた場合 $$ \\overline{X} \\sim N \\left( \\mu, {{1} \\over {n}} \\sigma^2 \\right) $$\n■\n(b) $\\mathbf{0}$ はゼロベクトルを示す。 $\\mathbf{1} = (1, \\cdots , 1) = \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}$ はすべての成分が$1$ であるベクトルを示す。 $I$ は単位行列を示す。 $A^{T}$ は行列$A$の転置行列を示す。 $\\displaystyle \\mathbf{v} := {{ 1 } \\over { n }} \\mathbf{1}$ とする。\nランダムベクトル $X := \\left( X_{1} , \\cdots , X_{n} \\right)$ は多変量正規分布に従い $$ \\begin{align*} \\overline{X} =\u0026amp; {{ 1 } \\over { n }} \\left( X_{1} + \\cdots + X_{n} \\right) \\\\ \u0026amp;= {{ 1 } \\over { n }} \\begin{bmatrix} 1 \u0026amp; \\cdots \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} X_{1} \\\\ \\vdots \\\\ X_{n} \\end{bmatrix} \\\\ =\u0026amp; {{ 1 } \\over { n }} \\mathbf{1}^{T} \\mathbf{X} \\\\ =\u0026amp; \\mathbf{v}^{T} \\mathbf{X} \\end{align*} $$\n今、ランダムベクトル $\\mathbf{Y} := \\left( X_{1} - \\overline{X} , \\cdots , X_{n} - \\overline{X} \\right)$ を定義すれば、あるランダムベクトル $\\mathbf{W}$ は次のように表せる。 $$ \\mathbf{W} = \\begin{bmatrix} \\overline{X} \\\\ \\mathbf{Y} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{v}^{T} \\\\ I - \\mathbf{1} \\mathbf{v}^{T} \\end{bmatrix} \\mathbf{X} $$\n$\\mathbf{W}$ は多変量正規分布に従うランダムベクトルが線形変換されたものなので、やはり多変量正規分布に従い、その母平均ベクトルは上の式に期待値を取った $$ E \\mathbf{W} = \\begin{bmatrix} \\mathbf{v}^{T} \\\\ I - \\mathbf{1} \\mathbf{v}^{T} \\end{bmatrix} \\mu \\mathbf{1} = \\begin{bmatrix} \\mu \\\\ \\mathbf{0}_{n} \\end{bmatrix} $$ であり、共分散行列 $\\Sigma$ は $$ \\begin{align*} \\Sigma =\u0026amp; \\begin{bmatrix} \\mathbf{v}^{T} \\\\ I - \\mathbf{1} \\mathbf{v}^{T} \\end{bmatrix} \\sigma^{2} I \\begin{bmatrix} \\mathbf{v}^{T} \\\\ I - \\mathbf{1} \\mathbf{v}^{T} \\end{bmatrix}^{T} \\\\ =\u0026amp; \\sigma^{2} \\begin{bmatrix} 1/n \u0026amp; \\mathbf{0}_{n}^{T} \\\\ \\mathbf{0}_{n} \u0026amp; I - \\mathbf{1} \\mathbf{v}^{T} \\end{bmatrix} \\end{align*} $$ である。ここで、$\\overline{X}$ は$\\mathbf{Y}$ と独立であることがわかり $$ S^{2} = {{ 1 } \\over { n-1 }} \\sum_{k=1}^{n} \\left( X_{k} - \\overline{X} \\right)^{2} = {{ 1 } \\over { n-1 }} \\mathbf{Y}^{T} \\mathbf{Y} $$ したがって、$\\overline{X} \\perp S^{2}$ である。\n■\n(c) $\\displaystyle V = \\sum_{i=1}^{n} \\left( { {X_{i} - \\mu } \\over {\\sigma} } \\right) ^2 $とした場合 $\\displaystyle { {X_{i} - \\mu } \\over {\\sigma} } \\sim N(0,1)$なので$V \\sim \\chi^2 (n)$であるとし\n$$ \\begin{align*} V =\u0026amp; \\sum_{i=1}^{n} \\left( { {X_{i} - \\mu } \\over {\\sigma} } \\right) ^2 \\\\ =\u0026amp; \\sum_{i=1}^{n} \\left( { { ( X_{i} -\\overline{X} ) + ( \\overline{X} - \\mu ) } \\over {\\sigma} } \\right) ^2 \\\\ =\u0026amp; \\sum_{i=1}^{n} \\left( { { X_{i} -\\overline{X} } \\over {\\sigma} } \\right) ^2 + \\left( { { \\overline{X} - \\mu } \\over {\\sigma / \\sqrt{n} } } \\right) ^2 \\end{align*} $$\nここで $$ \\sum_{i=1}^{n} \\left( { { X_{i} -\\overline{X} } \\over {\\sigma} } \\right) ^2 = { {n-1} \\over {\\sigma^2} } \\sum_{i=1}^{n} { { ( X_{i} -\\overline{X} ) ^ 2 } \\over {n-1} } = (n-1) { {S^2} \\over {\\sigma^2} } $$\n整理すると $$ V = (n-1) { {S^2} \\over {\\sigma^2} } + \\left( { { \\overline{X} - \\mu } \\over {\\sigma / \\sqrt{n} } } \\right) ^2 $$\n$V \\sim \\chi^2 (n)$ であり、スチューデントの定理の(a)により $$ \\left( { { \\overline{X} - \\mu } \\over {\\sigma / \\sqrt{n} } } \\right) \\sim N(0,1) $$ であり、標準正規分布の二乗はカイ二乗分布に従うので $$ \\left( { { \\overline{X} - \\mu } \\over {\\sigma / \\sqrt{n} } } \\right)^2 \\sim \\chi^2 (1) $$\nスチューデントの定理の(b)で、$\\overline{X}$ と $S^2$ が独立であることが示されたので、両辺をモーメント生成関数として $$ (1-2t)^{-n/2} = E \\left\\{ \\exp \\left( (n-1) { {S^2} \\over {\\sigma^2} } t \\right) \\right\\} (1-2t)^{-1/2} $$\nしたがって、$\\displaystyle (n-1) { {S^2} \\over {\\sigma^2} }$のモーメント生成関数は$(1-2t)^{-(n-1)/2}$である。\n■\n(d) 正規分布とカイ二乗分布からスチューデントのt分布の導出: $W \\sim N(0,1)$が真であり、$V \\sim \\chi^2 (r)$ならば $$ T = { {W} \\over {\\sqrt{V/r} } } \\sim t(r) $$\n$$ T = { {\\overline{X} - \\mu } \\over {S / \\sqrt{n}} } = { {( \\overline{X} - \\mu ) / (\\sigma / \\sqrt{n}) } \\over { \\sqrt{ (n-1) S^2 / ( \\sigma^2 ( n-1 ) ) } } } $$ スチューデントの定理の(a)で$\\displaystyle \\overline{X} \\sim N\\left( \\mu , { {\\sigma^2} \\over {n} } \\right) $が、(c)で$\\displaystyle (n-1) { {S^2} \\over {\\sigma^2} } \\sim \\chi^2 (n-1) $であることが示されたので、 $$ T \\sim t(n-1) $$\n■\nHogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p195.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":203,"permalink":"https://freshrimpsushi.github.io/jp/posts/203/","tags":null,"title":"スチューデントのt検定の証明"},{"categories":"프로그래밍","contents":"概要 よく使われるRGB色の商標だ。\nコード ","id":2013,"permalink":"https://freshrimpsushi.github.io/jp/posts/2013/","tags":null,"title":"RGBカラーチートシート"},{"categories":"함수","contents":"定義 関数 $f: X \\to Y$, $g: f(X) \\to Z$について次のように定義される$h: X \\to Z$を**$f$と$g$の合成**composition of $g$ with $f$と呼び、$h=g \\circ f$と表記する。\n$$ h(x) = (g\\circ f) (x) := g\\left( f(x) \\right) $$\n","id":3048,"permalink":"https://freshrimpsushi.github.io/jp/posts/3048/","tags":null,"title":"関数の合成"},{"categories":"선형대수","contents":"定義1 関数 $T : V \\to W$がベクトル空間からベクトル空間への写像である場合、つまり $V$、$W$がベクトル空間である場合、$T$を変換transformationと呼ぶ。\n変換 $T$が線形関数である場合、すなわち全ての$\\mathbf{v},\\mathbf{u} \\in V$とスカラー$k$について次の二つの条件を満たす場合、線形変換linear transformationと呼ぶ。\n$T(k \\mathbf{u}) = k T(\\mathbf{u})$ $T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})$ 特に$W=\\mathbb{C}$であれば、$T$を線形汎関数と呼ぶ。\n説明 関数、写像、変換は事実上同じ言葉として考えてもいい。ただし、線形代数、関数解析学などベクトル空間からベクトル空間への写像を扱う場合は、主に変換と呼び、transformationの頭文字を取って$T$と表記する。\n有限次元から有限次元への線形変換の場合は、行列の積と同様に扱うので、以下のように表記する。\n$$ T(\\mathbf{x}) = T\\mathbf{x} $$\n$T : V \\to V$を満たす線形変換を$V$上の線形作用素linear operator on $V$と呼ぶこともある。しかし、定義域と値域が同じでなければ作用素と呼ばれるわけではない。実用的な理由から、多くの教科書では$T : V \\to V$を線形作用素として定義している。\n$$ \\text{linear transformation form } V \\text{ to } V \\to \\text{linear operator on } V $$\nベクトル空間$X$から$Y$への全ての線形変換の集合は$L(X, Y)$のように表記する。2\n$$ L(X,Y) = \\mathcal{L}(X, Y) := \\left\\{ T : X \\to Y\\enspace |\\enspace T \\text{ is linear } \\right\\} $$\n行列変換は線形変換の一種である。\n恒等変換 線形変換$I : V \\to V$が全ての$\\mathbf{v} \\in V$に対して\n$$ I(\\mathbf{v}) = \\mathbf{v} $$\nを満たす場合、恒等変換identity transformationと呼ぶ。具体的には$I_{V}$のように表記することもある。\n零変換 線形変換$T_{0} : V \\to W$が全ての$\\mathbf{v} \\in V$に対して\n$$ T_{0}(\\mathbf{v}) = \\mathbf{0}_{W} $$\nを満たす場合、零変換zero transformationと呼ぶ。この時$\\mathbf{0}_{W}$は$W$の零ベクトルである。$O$、$0$などで表記することもある。簡単に言うと、零関数である。\n性質 $T : V \\to W$が線形変換であれば、以下が成立する。\n(a) $T(\\mathbf{0}) = \\mathbf{0}$\n(b) 全ての$\\mathbf{u}, \\mathbf{v} \\in V$に対して、$T(\\mathbf{u} - \\mathbf{v}) = T(\\mathbf{u}) - T(\\mathbf{v})$\n証明 (a) ベクトル空間の性質により、$0\\mathbf{v} = \\mathbf{0}$であるので、\n$$ T(\\mathbf{0}) = T( 0\\mathbf{u}) = 0T(\\mathbf{u}) = \\mathbf{0} $$\n■\n(b) 同様に、ベクトル空間の性質により$-\\mathbf{v} = (-1)\\mathbf{v}$であるので、\n$$ \\begin{align*} T(\\mathbf{u} - \\mathbf{v}) \u0026amp;= T \\big( \\mathbf{u} + (-1)\\mathbf{v} \\big) \\\\ \u0026amp;= T(\\mathbf{u}) + T\\big( (-1)\\mathbf{v} \\big) \\\\ \u0026amp;= T(\\mathbf{u}) + (-1)T(\\mathbf{v}) \\\\ \u0026amp;= T(\\mathbf{u}) - T(\\mathbf{v}) \\end{align*} $$\n■\nHoward Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), p446-447\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWalter Rudin, Principles of Mathematical Analysis (3rd Edition, 1976), p207\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3026,"permalink":"https://freshrimpsushi.github.io/jp/posts/3026/","tags":null,"title":"線形変換"},{"categories":"행렬대수","contents":"定義1 線形システムで、定数項が全部$0$だったら、同次homogeneousという。\n$$ \\begin{align*} a_{11}x_{1} + a_{12}x_{2} + \\cdots + a_{1n}x_{n} \u0026amp;= 0 \\\\ a_{21}x_{1} + a_{22}x_{2} + \\cdots + a_{2n}x_{n} \u0026amp;= 0 \\\\ \u0026amp;\\vdots \\\\ a_{m1}x_{1} + a_{m2}x_{2} + \\cdots + a_{mn}x_{n} \u0026amp;= 0 \\end{align*} $$\n一般的な線形システムと違い、全ての同次線形システムは常に解を持っている。その理由は、定数項が$0$ならば、当然$x_{1}=0, x_{2}=0, \\dots, x_{n}=0$を解として持っているからだ。これを自明解trivial solutionという。自明解でない解を非自明解nontrivial solutionという。同次線形システムは常に自明解を持っているので、解についてはただ以下の二つの場合だけが存在する。\n自明解だけがある。\n自明解と無数に多くの非自明解がある。\n同次システムの自由変数に関する定理 未知数が$n$個あるある同次線形システムが与えられたとする。拡大行列の階段行列の$0$ではない行の数が$r$であるとする。すると、同次システムは簡単に以下のように表される。\n$$ \\begin{align*} \u0026amp; x_{1} \u0026amp; \u0026amp; \u0026amp; \u0026amp;+ (\\quad) \u0026amp;= 0 \u0026amp; \\\\ \u0026amp; \u0026amp; x_{2} \u0026amp; \u0026amp; \u0026amp;+ (\\quad) \u0026amp;= 0 \u0026amp; \\\\ \u0026amp; \u0026amp; \u0026amp; \\ddots \u0026amp; \u0026amp; \u0026amp; \\vdots \u0026amp; \\\\ \u0026amp; \u0026amp; \u0026amp; \u0026amp; x_{r} \u0026amp;+ (\\quad) \u0026amp;= 0 \u0026amp; \\end{align*} $$\n上の式は$n-r$個の自由変数を持っている。従って、$r \u0026lt; n$の場合、自由変数が1つ以上存在するので、無数に多くの解を持つ。したがって、方程式の数よりも未知数の数が多い同次線形システムは、無数に多くの解を持つ。\nHoward Anton, Elementary Linear Algebra: アプリケーションバージョン (12版, 2019), p17-19\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3020,"permalink":"https://freshrimpsushi.github.io/jp/posts/3020/","tags":null,"title":"同時同次一次方程式"},{"categories":"행렬대수","contents":"定義1 拡大行列が次の条件を満たす場合、行段階形echelon formと言います。\n$0$じゃない要素がある行で、最初に現れる$0$じゃない数が$1$である。これを先導1leading 1と呼びます。\n全ての要素が$0$の行は、最も下に置きます。\n$0$じゃない要素がある行が連続している場合、上の行の先導1が下の行の先導1より左にある必要があります。\n行段階形の行列がさらに下記の条件を満たす場合、簡約行段階形reduced echelon formと言います。\n先導1がある列の他の要素が全て$0$である。 次の行列は簡約行段階形です。\n$$ \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 3 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 7 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; -1 \\end{bmatrix},\\quad \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix},\\quad \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; -2 \u0026amp; 0 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 3 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix},\\quad \\begin{bmatrix} 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\end{bmatrix} $$\n次の行列は行段階形ですが、簡約行段階形ではありません。\n$$ \\begin{bmatrix} 1 \u0026amp; 4 \u0026amp; -3 \u0026amp; 7 \\\\ 0 \u0026amp; 1 \u0026amp; 6 \u0026amp; 2 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 5 \\end{bmatrix},\\quad \\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix},\\quad \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; 2 \u0026amp; 6 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\n与えられた線形システムの拡大行列に基本行操作を行い、簡約行段階形を作るプロセスをガウス・ジョルダン消去法Gauss-Jordan eliminationと呼びます。簡約行段階形を作る過程で、先導1の下部分を全て$0$にすることを前進forward、上部を全て$0$にすることを後退backwordと言います。\n性質 すべての行列は唯一の簡約行段階形を持ちます。つまり、どんな順番で基本行操作を行っても、同じ簡約行段階形の行列が得られます。\n行段階形は一意ではありません。つまり、基本行操作の順序によって異なる行段階形が得られます。\n行段階形の全ての要素が$0$の行の数は互いに同じであり、先導1の位置も互いに同じです。これらの位置をピボットpivot位置と呼びます。\n一般解2 線形システムが無数に多くの解を持つ場合、パラメーターを代入して解を得られるパラメーター方程式の集合を線形システムの一般解general solutionと言います。\n例えば、ある線形システムの拡大行列を基本行操作で次のような簡約行段階形に変形したとします。\n$$ \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 3 \u0026amp; -1 \\\\ 0 \u0026amp; 1 \u0026amp; -4 \u0026amp; 2 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} $$\nその場合、パラメーター方程式は次のようになります。\n$$ x = -1 -3t,\\quad y = 2 + 4t, \\quad z = t $$\nこの時、先導1に対応する変数 $x,y$を先導変数leading variable、その他の変数 $z$を自由変数free variableと呼びます。\nHoward Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), p11\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHoward Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), p115\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3019,"permalink":"https://freshrimpsushi.github.io/jp/posts/3019/","tags":null,"title":"ガウス-ジョルダン消去法"},{"categories":"줄리아","contents":"エラー ERROR: SystemError: opening file \u0026quot;C:\\\\Users\\\\rmsms\\\\.julia\\\\registries\\\\General\\\\Registry.toml\u0026quot;: No such file or directory\r原因 人を本当にイライラさせるエラーだけど、言葉通りこのパスにRegistry.tomlファイルがなくて発生するエラーだ。\n解決法 C:\\Users\\사용자이름\\.julia\\registries\\General フォルダを削除してもう一度試してみる。\nその後、上のようにRegistry.tomlファイルも生まれて、インストールも正常に進行することを確認できる。\n","id":2069,"permalink":"https://freshrimpsushi.github.io/jp/posts/2069/","tags":null,"title":"Juliaパッケージのインストール時に\\General\\Registry.toml: No such file or directoryというエラーを解決"},{"categories":"선형대수","contents":"定義1 $S = \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r} \\right\\}$をベクトル空間$V$の部分集合としよう。$S$が下記の二条件を満たす時、$S$を$V$の基底basisという。\n$S$が$V$を生成する。\n$$ V = \\text{span}(S) $$\n$S$が線形独立である。\n説明 基底の名前から推測できるように、「ベクトル空間を作り出すことができる最も小さいもの」の概念に当たる。生成という条件が「ベクトル空間を作る」という意味を持ち、線形独立という条件が「最も小さい」という意味を持つ。ベクトル空間を作ることは分かるが、最も小さなものでなければならない理由については直ちに理解できないかもしれない。しかし、簡単な例を一つ見ればすぐに理解できるだろう。例えば、私たちは$(2,3)$というベクトルを\n$$ (2,3)=1(1,0) + 2(0,1) + 1(1,1) $$\nのように表さない。$(1,1)$を$(1,0), (0,1)$の線形結合で表すことができるからだ。つまり、上の式は不必要に長く記載した表現にすぎないということだ。従って、線形独立という条件はそのベクトルを基底の線形結合で表す時、最もすっきりと、必要なものだけをまとめた形で表されるようにしてくれる。\nここで注意すべきことは、一つのベクトル空間に対して基底が特に一意に存在する必要はないということだ。例を挙げると、$\\left\\{ (1,0) , (0,1) \\right\\}$は$\\mathbb{R}^{2}$を生成する基底だ。しかし、定義によれば$\\left\\{ (2,0) , (0,2) \\right\\}$も$\\mathbb{R}^2$の基底になることができる。それだけか？実は$\\left\\{ (1,1) , (-1,1) \\right\\}$も$\\mathbb{R}^2$を生成する上で全く問題がない。ただ、一般的に$\\mathbb{R}^{n}$では、下記のベクトルから成る基底を扱う。\n$$ \\mathbf{e}_{1} = (1,0,0,\\dots,0), \\quad \\mathbf{e}_{2}=(0,1,0,\\dots,0),\\quad \\mathbf{e}_{n}=(0,0,0,\\dots,1) $$\nこのような基底を$\\left\\{ \\mathbf{e}_{1}, \\mathbf{e}_{2}, \\dots, \\mathbf{e}_{n} \\right\\}$を$\\mathbb{R}^{n}$上の標準基底standard basis for $\\mathbb{R}^{n}$という。各$\\mathbf{e}_{i}$は標準単位ベクトルstandard unit vectorと呼ばれる。特に$n=3$の場合は、一般的に以下のように表記される。\n$$ \\begin{align*} \\hat{\\mathbf{x}} =\u0026amp;\\ \\mathbf{e}_{1} = \\hat{\\mathbf{x}}_{1} = \\mathbf{i}=(1,0,0) \\\\ \\hat{\\mathbf{y}} =\u0026amp;\\ \\mathbf{e}_{2} = \\hat{\\mathbf{x}}_{2} = \\mathbf{j}=(0,1,0) \\\\ \\hat{\\mathbf{z}} =\u0026amp;\\ \\mathbf{e}_{3} = \\hat{\\mathbf{x}}_{3} = \\mathbf{k}=(0,0,1) \\end{align*} $$\n以下の定理から、座標の概念を抽象化されたベクトル空間でも話すことができる。$\\mathbf{v} \\in V$が$(1)$のように表される時、$[\\mathbf{v}]_{S}$を基底$S$に対する$\\mathbf{v}$の座標ベクトルcoordinate vector $\\mathbf{x}$ of relative of $S$という。\n$$ [\\mathbf{v}]_{S} = \\begin{bmatrix} c_{1} \\\\ c_{2} \\\\ \\vdots \\\\ c_{n} \\end{bmatrix} $$\n定理: 基底表現の一意性 $S = \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{n} \\right\\}$をベクトル空間$V$の基底としよう。すると、全てのベクトル$\\mathbf{v} \\in V$に対して\n$$ \\begin{equation} \\mathbf{v} = c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + \\cdots + c_{n}\\mathbf{v}_{n} \\end{equation} $$\nと表現する方法は一意である。つまり、上記の式を満たす係数の組$(c_{1},c_{2},\\dots,c_{n})$が一意に存在する。\n証明 $S$が$V$を生成するため、生成の定義に従い、$V$の全てのベクトルは$S$の線形結合で表せる。あるベクトル$\\mathbf{v}$が下記の二つの線形結合で表せるとしよう。\n$$ \\begin{align*} \\mathbf{v} \u0026amp;= c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + \\cdots + c_{n}\\mathbf{v}_{n} \\\\ \\mathbf{v} \u0026amp;= k_{1}\\mathbf{v}_{1} + k_{2}\\mathbf{v}_{2} + \\cdots + k_{n}\\mathbf{v}_{n} \\end{align*} $$\n上記の式から下の式を引くと、次のようになる。\n$$ \\mathbf{0} = (c_{1} - k_{1}) \\mathbf{v}_{1} + (c_{2} - k_{2}) \\mathbf{v}_{2} + \\cdots + (c_{n} - k_{n}) \\mathbf{v}_{n} $$\nしかし$\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{n}$は線形独立であるため、上記の式を満たす解はオロジ\n$$ c_{1} - k_{1} = 0,\\quad c_{2} - k_{2} = 0,\\quad \\dots,\\quad c_{n} - k_{n} = 0 $$\nのみである。従って、次が成り立つ。\n$$ c_{1} = k_{1},\\quad c_{2} = k_{2},\\quad \\dots,\\quad c_{n} = k_{n} $$\nよって、二つの線形結合表現は同一である。\n■\nHoward Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), p240\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3017,"permalink":"https://freshrimpsushi.github.io/jp/posts/3017/","tags":null,"title":"ベクトル空間の基底"},{"categories":"줄리아","contents":"ガイド ステップ1. ジュリアのインストール\nジュリアのダウンロードページからインストールファイルをダウンロードして、実行する。\nステップ2. VSコードのインストール\nビジュアルスタジオコードのダウンロードページからインストールファイルをダウンロードして、実行する。\nステップ3. ジュリア拡張のインストール\n左側から5番目のアイコンをクリックするか、Ctrl + Shift + XでExtensionsを開く。\u0026lsquo;julia\u0026rsquo;と検索すると、最上段にJulia Language Supportが表示される。\nInstallをクリックしてインストールする。\nエディターで拡張子が.jlのファイルを作り、ジュリアコードを書き、Shift + Enterで全体を実行してみる。上のスクリーンショットでは、\u0026ldquo;helloworld\u0026quot;を出力するためにprintln(helloworld)という1行だけ書かれている。\n環境 OS: Windows julia: v1.5.4 ","id":2067,"permalink":"https://freshrimpsushi.github.io/jp/posts/2067/","tags":null,"title":"WindowsでJuliaの最新バージョンをインストールする方法"},{"categories":"힐베르트공간","contents":"定義 関数の集合 $X$ がベクトル空間である場合、$X$を関数空間function spaceと呼ぶ。\n説明 関数空間 $X$ では、内積は以下のように積分で定義される。\n$$ \\langle f, g \\rangle = \\int f(x) g(x) dx,\\quad f,g\\in X $$\n主に扱う関数空間には、次のものがある。\n連続関数空間 $C^{m}$\n$$ C^{m}(\\mathbb{R}) : =\\left\\{ f \\in C(\\mathbb{R}) : f^{(n)} \\text{ is continuous } \\forall n \\le m \\right\\} $$\nテスト関数空間 $C_{c}^{\\infty} = \\mathcal{D}$ シュワルツ空間 $\\mathcal{S}$ ヘルダー連続関数空間 ルベーグ空間 $L^{p}$\n$$ L^{p} (E) : = \\left\\{ f : \\int_{E} | f |^{p} dm \u0026lt; \\infty \\right\\} $$\n重み付きルベーグ空間 収束数列空間 $\\ell^{p}$\nソボレフ空間 $W^{m,\\ p}$\n$$ W^{m,\\ p}(\\Omega):=\\left\\{ u \\in L^p(\\Omega)\\ :\\ D^\\alpha u \\in L^p(\\Omega),\\ 0\\le |\\alpha | \\le m \\right\\} $$\n併せて見る 位相数学での関数空間 ","id":3032,"permalink":"https://freshrimpsushi.github.io/jp/posts/3032/","tags":null,"title":"様々な関数空間"},{"categories":"행렬대수","contents":"定義1 $n\\times n$ 行列 $A$が与えられたとしよう。$\\mathbf{0}$でない$n\\times 1$列ベクトル$\\mathbf{x}$、そして定数$\\lambda$に対して、次の式を固有値方程式または固有値問題という。\n$$ \\begin{equation} A \\mathbf{x} = \\lambda \\mathbf{x} \\end{equation} $$\n与えられた$A$に対して、上のように固有値方程式を満たす$\\lambda$を$A$の固有値と言い、$\\mathbf{x}$を$\\lambda$に対応する$A$の固有ベクトルという。\n説明 上の定義は$\\lambda \\in \\mathbb{R}$、$\\mathbf{x} \\in \\mathbb{R}^{n}$の時だけでなく、$\\lambda \\in \\mathbb{C}$、$\\mathbf{x} \\in \\mathbb{C}^{n}$の時にもそのまま適用される。「$\\mathbf{0}$でない」という条件がついているのは、下の式から分かるように、$\\mathbf{x} = \\mathbf{0}$ならば常に成り立つからだ。\n$$ A \\mathbf{0} = \\mathbf{0} = \\lambda \\mathbf{0} $$\n幾何学的な動機 ベクトル$\\mathbf{x}$を行列$A$で変換した$A \\mathbf{x}$と$\\mathbf{x}$の方向が同じだとすると、何か実数$\\lambda$に対して\n$$ A \\mathbf{x} = \\lambda \\mathbf{x} $$\nが成り立つことになる。行列$A$は本来、どんな方向の概念も持たないが、$A$の固有ベクトルが存在するならば、$A$が何か特有の方向を指していると言えるだろう。だから、このようなベクトル$\\mathbf{x}$を固有ベクトルと呼ぶのだ。例えば、以下のような$2\\times 2$行列を考えてみよう。\n$$ A = \\begin{bmatrix} 6 \u0026amp; 2 \\\\ 2 \u0026amp; 3 \\end{bmatrix} $$\nすると、ベクトル$\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$は$\\begin{bmatrix} 6 \u0026amp; 2 \\\\ 2 \u0026amp; 3 \\end{bmatrix}$に変換された時、$\\begin{bmatrix} 14 \\\\ 7 \\end{bmatrix}$となって方向が同じである。ここでベクトル$\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$に$\\lambda = 7$を掛けると、ベクトルの長さも同じになり、固有値方程式\n$$ \\begin{align*} A \\mathbf{x} \u0026amp;= \\lambda \\mathbf{x} \\\\ \\begin{bmatrix} 6 \u0026amp; 2 \\\\ 2 \u0026amp; 3 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} \u0026amp;= 7 \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} \\end{align*} $$\nの形の等式を満たす。このような理由で$\\lambda=7$を固有値と呼ぶのだ。よく見ると、固有ベクトルは$\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$を伸ばしたり縮めたりして無数に見つけることができるが、固有値は変わらないことが分かる。だから、$\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$を固有値$7$に対応する$A$の固有ベクトルと表現するのだ。\nこのように幾何学的に説明した議論を一般的に拡張すると、固有値は代数的に方程式$A \\mathbf{x} = \\lambda \\mathbf{x}$を満たす$\\lambda$であり、固有ベクトルは与えられた$\\lambda$に対する方程式の非自明な解である。\n固有値方程式の解法 固有値を求めることは、固有値方程式から始まる。$(1)$の式を整理すると、次のようになる。\n$$ \\begin{align*} \u0026amp;\u0026amp; A \\mathbf{x} \u0026amp;= \\lambda \\mathbf{x} \\\\ \\implies \u0026amp;\u0026amp; A \\mathbf{x} - \\lambda \\mathbf{x} \u0026amp;= \\mathbf{0} \\\\ \\implies \u0026amp;\u0026amp; A \\mathbf{x} - \\lambda I \\mathbf{x} \u0026amp;= \\mathbf{0} \\\\ \\implies \u0026amp;\u0026amp; \\left( A - \\lambda I \\right) \\mathbf{x} \u0026amp;= \\mathbf{0} \\end{align*} $$\nこの時、固有ベクトルは条件$\\mathbf{x} \\ne \\mathbf{0}$を満たさなければならない。上記線形システムが$\\mathbf{0}$でない解を持つ同値条件は、$\\left( A - \\lambda I \\right)$の逆行列が存在しないことであり、これは次の式が成り立つことと同値である。\n$$ \\det (A -\\lambda I) = 0 $$\nしたがって、上の式を満たす$\\lambda$が$A$の固有値になる。上記の式を$A$の特性方程式と言い、$\\det (A -\\lambda I)$は$A$が$n\\times n$行列の時、$n$次の多項式になり、これを特性多項式と呼ぶ。\nちなみに、$A+B$の固有値は$A$、$B$の固有値の和と異なる場合があり、$AB$の固有値も$A$、$B$の固有値の積と異なる場合がある。また、方程式の解として固有値を求めることから分かるように、必ずしも実数であるという保証は全くない。\n例 固有値を求める 解の例として、再び$A = \\begin{bmatrix} 6 \u0026amp; 2 \\\\ 2 \u0026amp; 3 \\end{bmatrix}$を考えてみよう。$A-\\lambda I = \\begin{bmatrix} 6 - \\lambda \u0026amp; 2 \\\\ 2 \u0026amp; 3 - \\lambda \\end{bmatrix}$なので、$A$の特性方程式を解くと次のようになる。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\det (A - \\lambda I) \u0026amp;= 0 \\\\ \\implies \u0026amp;\u0026amp; (6 - \\lambda)(3 - \\lambda) - 4 \u0026amp;= 0 \\\\ \\implies \u0026amp;\u0026amp; \\lambda^2 - 9 \\lambda + 18 - 4 \u0026amp;= 0 \\\\ \\implies \u0026amp;\u0026amp; (\\lambda - 2)(\\lambda - 7) \u0026amp;= 0 \\end{align*} $$\nしたがって、$A$の固有値は$\\lambda = 2$と$\\lambda = 7$である。$2$と$7$を$\\lambda$に代入してみると、それぞれの固有値に対応する固有ベクトルを求めることができる。ここでは、$\\lambda = 7$の場合のみ紹介する。\n$\\lambda = 7$に対応する固有ベクトルを求める $\\lambda = 7$を$(1)$に代入して整理すると、次のようになる。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\begin{bmatrix} 6 \u0026amp; 2 \\\\ 2 \u0026amp; 3 \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} \u0026amp;= 7\\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} \\\\ \\implies \u0026amp;\u0026amp; \\begin{bmatrix} 6x_{1} + 2x_{2} \\\\ 2x_{1} + 3x_{2} \\end{bmatrix} \u0026amp;= \\begin{bmatrix} 7x_{1} \\\\ 7x_{2} \\end{bmatrix} \\\\ \\implies \u0026amp;\u0026amp; \\begin{bmatrix} -x_{1} + 2x_{2} \\\\ 2x_{1} - 4x_{2} \\end{bmatrix} \u0026amp;= \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\end{align*} $$\nこれを解くと、次のようになる。\n$$ \\left\\{ \\begin{align*} -x_{1} + 2x_{2} \u0026amp;= 0 \\\\ 2x_{1} - 4x_{2} \u0026amp;= 0 \\end{align*} \\right. $$\n$$ \\implies x_{1} = 2x_{2} $$\nしたがって、$0$でないすべての$x_{2}$に対して、ベクトル$\\begin{bmatrix} 2x_{2} \\\\ x_{2} \\end{bmatrix}$が$\\lambda = 7$に対応する固有ベクトルになる。通常、最も単純な形または大きさが$1$になる単位ベクトルを選ぶ。$x_{2} = 1$を代入すると、以下の固有ベクトルを得る。\n$$ A = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} $$\n性質 正の整数$k$に対して、$\\lambda$が行列$A$の固有値であり、$\\mathbf{x}$が$\\lambda$に対応する固有ベクトルであれば、$\\lambda ^{k}$は$A^{k}$の固有値であり、$\\mathbf{x}$は$\\lambda ^{k}$に対応する固有ベクトルである。 Howard Anton, Elementary Linear Algebra: Aplications Version (12th Edition, 2019), p291-292\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":319,"permalink":"https://freshrimpsushi.github.io/jp/posts/319/","tags":null,"title":"固有値と固有ベクトル"},{"categories":"다변수벡터해석","contents":"定義1 $E\\subset \\mathbb{R}^{n}$を開集合とし、$\\mathbf{x}\\in E$、そして$\\mathbf{f} : E \\to \\mathbb{R}^{m}$と定義しよう。$\\left\\{ \\mathbf{e}_{1}, \\mathbf{e}_{2}, \\dots, \\mathbf{e}_{n} \\right\\}$と$\\left\\{ \\mathbf{u}_{1}, \\mathbf{u}_{2}, \\dots, \\mathbf{u}_{m} \\right\\}$をそれぞれ$\\mathbb{R}^{n}$と$\\mathbb{R}^{m}$の標準基底としよう。\nそれでは、$\\mathbf{f}$の成分 $f_{i} : \\mathbb{R}^{n} \\to \\mathbb{R}$は次のように定義される。\n$$ \\mathbf{f} (\\mathbf{x}) = \\sum_{i=1}^{m} f_{i}(\\mathbf{x})\\mathbf{u}_{i}, \\quad \\mathbf{x} \\in E $$\nまたは\n$$ f_{i} (\\mathbf{x}) := \\mathbf{f} (\\mathbf{x}) \\cdot \\mathbf{u}_{i},\\quad i \\in \\left\\{ 1,\\dots, m \\right\\} $$\n次の極限が存在するならば、$f_{i}$に対する$x_{j}$の偏微分とし、$D_{j}f_{i}$または$\\dfrac{\\partial f_{i}}{\\partial x_{j}}$と記される。\n$$ \\dfrac{\\partial f_{i}}{\\partial x_{j}} = D_{j}f_{i} := \\lim _{t \\to 0} \\dfrac{f_{i}(\\mathbf{x}+ t \\mathbf{e}_{j}) -f_{i}(\\mathbf{x})}{t} $$\n説明 偏とは偏っていることを意味し、微分を全ての変数ではなく、一つの変数に対してだけ考えようという意味である。これは全微分と対比する言葉である。\n偏$\\check{}$関数ではなく、偏$\\check{}$の微分関数である。\n$\\mathbf{f}$の全微分と偏微分の間には、以下の定理と同様の関係が成立する。\n定理 $E, \\mathbf{x}, \\mathbf{f}$を定義で述べた通りとする。$\\mathbf{f}$が$\\mathbf{x}$で微分可能であるとする。それならば、各偏微分$D_{j}f_{i}(\\mathbf{x})$が存在し、次の式が成立する。\n$$ \\mathbf{f}^{\\prime}(\\mathbf{x})\\mathbf{e}_{j} = \\sum_{i=1}^{m} D_{j}f_{i}(\\mathbf{x})\\mathbf{u}_{i},\\quad j \\in \\left\\{ 1,\\dots, n \\right\\} $$\n系 $\\mathbf{f}^{\\prime}(\\mathbf{x})$は、次のような行列で表される線形変換である。\n$$ \\mathbf{f}^{\\prime}(\\mathbf{x}) = \\begin{bmatrix} (D_{1}f_{1}) (\\mathbf{x}) \u0026amp; (D_{2}f_{1}) (\\mathbf{x}) \u0026amp; \\cdots \u0026amp; (D_{n}f_{1}) (\\mathbf{x}) \\\\ (D_{1}f_{2}) (\\mathbf{x}) \u0026amp; (D_{2}f_{2}) (\\mathbf{x}) \u0026amp; \\cdots \u0026amp; (D_{n}f_{2}) (\\mathbf{x}) \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ (D_{1}f_{m}) (\\mathbf{x}) \u0026amp; (D_{2}f_{m}) (\\mathbf{x}) \u0026amp; \\cdots \u0026amp; (D_{n}f_{m}) (\\mathbf{x}) \\end{bmatrix} $$\nこれを$\\mathbf{f}$のヤコビ行列とも言う。\n証明 $j$を固定しよう。$\\mathbf{f}$が$\\mathbf{x}$で微分可能であると仮定すると、次の式が成立する。\n$$ \\mathbf{f}( \\mathbf{x} + t \\mathbf{e}_{j}) - \\mathbf{f}(\\mathbf{x}) = \\mathbf{f}^{\\prime}(\\mathbf{x})(t\\mathbf{e}_{j}) + \\mathbf{r}(t\\mathbf{e}_{j}) $$\nここで、$\\mathbf{r}(t\\mathbf{e}_{j})$は次を満たす剰余である。\n$$ \\lim _{t \\to 0} \\dfrac{|\\mathbf{r}(t\\mathbf{e}_{j}) |}{t}=0 $$\n$\\mathbf{f}^{\\prime}(\\mathbf{x})$は線形変換であるから、次が成立する。\n$$ \\dfrac{\\mathbf{f}( \\mathbf{x} + t \\mathbf{e}_{j}) - \\mathbf{f}(\\mathbf{x})}{t} = \\dfrac{\\mathbf{f}^{\\prime}(\\mathbf{x})(t\\mathbf{e}_{j})}{t} + \\dfrac{\\mathbf{r}(t\\mathbf{e}_{j})}{t} = \\mathbf{f}^{\\prime}(\\mathbf{x})(\\mathbf{e}_{j}) + \\dfrac{\\mathbf{r}(t\\mathbf{e}_{j})}{t} $$\n両辺に$\\lim _{t \\to 0}$の極限を取ると、次のようになる。\n$$ \\lim _{t \\to 0} \\dfrac{\\mathbf{f}( \\mathbf{x} + t \\mathbf{e}_{j}) - \\mathbf{f}(\\mathbf{x})}{t} = \\mathbf{f}^{\\prime}(\\mathbf{x})\\mathbf{e}_{j} $$\n$\\mathbf{f}$を成分で表示すると、次を得る。\n$$ \\begin{align*} \\mathbf{f}^{\\prime}(\\mathbf{x})\\mathbf{e}_{j} \u0026amp;= \\lim _{t \\to 0} \\dfrac{\\mathbf{f}( \\mathbf{x} + t \\mathbf{e}_{j}) - \\mathbf{f}(\\mathbf{x})}{t} \\\\ \u0026amp;= \\lim _{t \\to 0} \\dfrac{\\sum_{i=1}^{m} f_{i}( \\mathbf{x} + t \\mathbf{e}_{j})\\mathbf{u}_{i} - \\sum_{i=1}^{m} f_{i}(\\mathbf{x})\\mathbf{u}_{i}}{t} \\\\ \u0026amp;= \\sum_{i=1}^{m} \\lim _{t \\to 0} \\dfrac{f_{i}( \\mathbf{x} + t \\mathbf{e}_{j}) - f_{i}(\\mathbf{x})}{t} \\mathbf{u}_{i} \\end{align*} $$\nそれならば、偏微分の定義により、次を得る。\n$$ \\mathbf{f}^{\\prime}(\\mathbf{x})\\mathbf{e}_{j} = \\sum_{i=1}^{m} D_{j}f_{i}(\\mathbf{x}) \\mathbf{u}_{i} $$\n■\n例 $f : \\R^{3} \\to \\R, \\gamma : \\R \\to \\R^{3}$が微分可能な関数だとしよう。また、\n$$ \\gamma (t) = \\left( x(t), y(t), z(t) \\right) $$\nそして$f$と$\\gamma$の合成を$g = f \\circ \\gamma$としよう。\n$$ g(t) = f \\circ \\gamma (t) = f \\left( \\gamma (t) \\right) $$\nすると$g^{\\prime}$は、連鎖律、偏微分の定義、上述の定理により、次のようになる。\n$$ \\begin{align*} \\dfrac{d g}{d t}(t_{0}) = g^{\\prime}(t_{0}) =\u0026amp;\\ f^{\\prime}(\\gamma (t_{0})) \\gamma^{\\prime}(t_{0}) \\\\ =\u0026amp;\\ \\begin{bmatrix} D_{1}f(\\gamma (t_{0})) \u0026amp; D_{2}f(\\gamma (t_{0})) \u0026amp; D_{3}f(\\gamma (t_{0})) \\end{bmatrix} \\begin{bmatrix} D\\gamma_{1} (t_{0}) \\\\ D\\gamma_{2} (t_{0}) \\\\ D\\gamma_{3} (t_{0}) \\end{bmatrix} \\\\ =\u0026amp;\\ \\begin{bmatrix} \\dfrac{\\partial f}{\\partial x}(\\gamma (t_{0})) \u0026amp; \\dfrac{\\partial f}{\\partial y}(\\gamma (t_{0})) \u0026amp; \\dfrac{\\partial f}{\\partial z}(\\gamma (t_{0})) \\end{bmatrix} \\begin{bmatrix} \\dfrac{d x}{d t}(t_{0}) \\\\ \\dfrac{d y}{d t}(t_{0}) \\\\ \\dfrac{d z}{d t}(t_{0}) \\end{bmatrix} \\\\ =\u0026amp;\\ \\dfrac{\\partial f}{\\partial x}(\\gamma (t_{0}))\\dfrac{d x}{d t}(t_{0}) + \\dfrac{\\partial f}{\\partial y}(\\gamma (t_{0}))\\dfrac{d y}{d t}(t_{0}) + \\dfrac{\\partial f}{\\partial z}(\\gamma (t_{0}))\\dfrac{d z}{d t}(t_{0}) \\end{align*} $$\nしたがって、\n$$ \\implies \\dfrac{d g}{d t} = \\dfrac{\\partial f}{\\partial x}\\dfrac{d x}{d t} + \\dfrac{\\partial f}{\\partial y}\\dfrac{d y}{d t} + \\dfrac{\\partial f}{\\partial z}\\dfrac{d z}{d t} $$\nWalter Rudin, Principles of Mathematical Analysis (3rd Edition, 1976), p215\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3036,"permalink":"https://freshrimpsushi.github.io/jp/posts/3036/","tags":null,"title":"偏微分_functions"},{"categories":"행렬대수","contents":"定義1 正定値行列 二次形式$\\mathbf{x}^{\\ast} A \\mathbf{x}$が\nすべての$\\mathbf{x} \\ne \\mathbf{0}$に対して$\\mathbf{x}^{\\ast} A \\mathbf{x} \u0026gt; 0$を満たすならば、二次形式や行列$A$を正定positive definiteという。\nすべての$\\mathbf{x} \\ne \\mathbf{0}$に対して$\\mathbf{x}^{\\ast} A \\mathbf{x} \u0026lt; 0$を満たすならば、二次形式や行列$A$を負定negative definiteという。\n$\\mathbf{x}$に従って、正の数も負の数も成り立つ場合には、二次形式や行列$A$を不定indefiniteという。\n実行列の場合は、定義の$\\mathbf{x}^{\\ast} A \\mathbf{x}$部分を$\\mathbf{x}^{T} A \\mathbf{x}$に置き換えて考えればよい。\n準正定値行列 二次形式$\\mathbf{x}^{\\ast} A \\mathbf{x}$が\nすべての$\\mathbf{x} \\ne \\mathbf{0}$に対して$\\mathbf{x}^{\\ast} A \\mathbf{x} \\ge 0$を満たすならば、二次形式や行列$A$を正の準定positive semidefiniteという。\nすべての$\\mathbf{x} \\ne \\mathbf{0}$に対して$\\mathbf{x}^{\\ast} A \\mathbf{x} \\le 0$を満たすならば、二次形式や行列$A$を負の準定negative semidefiniteという。\n説明 これらの定義はクリアだが、省略されたものが多く、頭で追うのが難しい。式と説明をじっくり見ながら、概念自体を理解しよう。二次形式の定数が複素数の場合、すなわち$A$がエルミート行列である場合を考えてみよう。$A \\mathbf{x} = \\lambda \\mathbf{x}$を見ると、$\\lambda$は$A$の固有値になる。左辺に共役転置$\\mathbf{x}^{\\ast}$をかけると次のようになる。\n$$ \\mathbf{x}^{\\ast} A \\mathbf{x} = \\lambda \\mathbf{x}^{\\ast} \\mathbf{x} = \\lambda \\mathbf{x} \\cdot \\mathbf{x} = \\lambda | \\mathbf{x} |^{2} $$\nここで$\\mathbf{x} \\ne \\mathbf{0}$であるため、$|\\mathbf{x}| ^2 \u0026gt; 0$であり、エルミート行列の固有値は実数なので、$\\lambda |\\mathbf{x}| ^2$も実数だ。したがって、$\\mathbf{x}^{\\ast} A \\mathbf{x}$は実数であり、正か負かを確認できるということだ。行列とベクトルの乗算で表記すると理解しにくかったものが、$\\lambda |\\mathbf{x}| ^2$として表すとずっと分かりやすくなる。\n$\\lambda |\\mathbf{x}|^{2}$の符号を考えると、常に$|\\mathbf{x}|^{2} \u0026gt;0$なので、$\\lambda$の符号だけを考えればよい。結局、ゼロベクターでない任意のベクターに対して$\\mathbf{x}^{\\ast} A \\mathbf{x} \u0026gt; 0$という言葉は$A$のすべての固有値が正であるという意味になる。反対に考えれば、負定値行列はすべての固有値が負である行列であるという意味だ。こうして、定義性はもともと正負の概念がない行列に正負(positive/negative)といった概念を定義(definite)することと考えることができるだろう。これを含むのが定理1である。\nさらに、可逆行列であるための同値条件により、正定値行列と負定値行列は$0$の固有値を持たないため、可逆行列である。(定理2)\n応用 数値線形代数学では、特に正定値に多くの関心が持たれる。条件として正定値を考えると、エルミート行列が基本でありながら、すべての固有値が正であるという、非常に強い条件だとわかる。 動力学では、負定値行列の性質を利用して、システムの平衡点の安定性を研究することもある。 統計学では、基本的に共分散行列が正の半定行列であるため、非常に重要だ。 定理1 二次形式 $\\mathbf{x}^{\\ast} A\\mathbf{x}$について、\n$\\mathbf{x}^{\\ast} A\\mathbf{x}$が正定であるための必要十分条件は$A$のすべての固有値が正であることである。\n$\\mathbf{x}^{\\ast} A\\mathbf{x}$が負定であるための必要十分条件は$A$のすべての固有値が負であることである。\n$\\mathbf{x}^{\\ast} A\\mathbf{x}$が不定であるための必要十分条件は$A$が少なくとも1つの負の固有値と少なくとも1つの正の固有値を持つことである。\n定理2 正の定値行列と負の定値行列は常に可逆行列である。\n定理3 対称行列$A$について、\n$A$が正定値であれば、$\\mathbf{x}^{T}A\\mathbf{x}=1$は楕円の方程式である。\n$A$が負定値であれば、$\\mathbf{x}^{T}A\\mathbf{x}=1$はグラフを持たない。\n$A$が不定であれば、$\\mathbf{x}^{T}A\\mathbf{x}=1$は双曲線の方程式である。\nHoward Anton, Chris Rorres, Anton Kaul, 「Elementary Linear Algebra: Applications Version」(第12版). 2019年, p423\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":336,"permalink":"https://freshrimpsushi.github.io/jp/posts/336/","tags":null,"title":"政府号行列"},{"categories":"행렬대수","contents":"性質 $A,B$を$n\\times n$行列とし、$k$を定数とする。行列式は次の性質を満たす。\n(a) $\\det(kA) = k^{n}\\det(A)$\n(b) $\\det(AB) = \\det(A)\\det(B)$\n(c) $\\det(AB)=\\det(BA)$\n(d) $A$が可逆行列ならば、$\\det(A^{-1}) = \\dfrac{1}{\\det(A)}$\n(e) $\\det(A^{T}) = \\det(A)$。ここで、$A^{T}$は$A$の転置である。\n","id":3015,"permalink":"https://freshrimpsushi.github.io/jp/posts/3015/","tags":null,"title":"行列式の性質"},{"categories":"기하학","contents":"説明 $r\u0026gt; 0$を単位速度曲線としよう。そして、$\\boldsymbol{\\alpha}$から得られる回転面を$M$とする。\n$$ M = \\left\\{ \\left( r(s)\\cos\\theta, r(s)\\sin\\theta, z(s) \\right) : 0 \\le \\theta \\le 2\\pi, s \\in (s_{0}, s_{1}) \\right\\} $$\n$M$の座標片写像$\\mathbf{x}$は次のようである。\n$$ \\mathbf{x}(s, \\theta) = \\left( r(s)\\cos\\theta, r(s)\\sin\\theta, z(s) \\right) $$\n$\\boldsymbol{\\alpha}$が単位速度曲線であるので、$\\boldsymbol{\\alpha}^{\\prime} = (r^{\\prime})^{2} + (z^{\\prime})^{2} = 1$となる。したがって、$z^{\\prime} = \\pm\\sqrt{1 - (r^{\\prime})^{2}}$である。他の基本的な性質には以下のようなものがある。\n第一基本形式： $$ \\begin{bmatrix} g_{ij} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; r^{2} \\end{bmatrix} $$\n第二基本形式： $$ \\begin{bmatrix} L_{ij} \\end{bmatrix} = \\begin{bmatrix} r^{\\prime}z^{\\prime \\prime} - z^{\\prime}r^{\\prime \\prime} \u0026amp; 0 \\\\ 0 \u0026amp; rz^{\\prime} \\end{bmatrix} $$\nガウス曲率： $\\boldsymbol{\\alpha}$が単位速度曲線であるので、以下の条件を得る。\n$$ (r^{\\prime})^{2} + (z^{\\prime})^{2} = 1 \\implies 2r^{\\prime}r^{\\prime \\prime} + 2z^{\\prime}z^{\\prime \\prime} = 0 \\implies z^{\\prime}z^{\\prime \\prime} = - r^{\\prime}r^{\\prime \\prime} $$\nしたがって、ガウス曲率は、\n$$ \\begin{align*} K = \\dfrac{(r^{\\prime}z^{\\prime \\prime} - z^{\\prime}r^{\\prime \\prime})}{((r^{\\prime})^{2} + (z^{\\prime})^{2})^{2}} \\dfrac{z^{\\prime}}{r} \u0026amp;= \\dfrac{r^{\\prime}z^{\\prime}z^{\\prime \\prime} - r^{\\prime \\prime}(z^{\\prime})^{2}}{r} \\\\ \u0026amp;= \\dfrac{-(r^{\\prime})^{2}r^{\\prime \\prime} - r^{\\prime \\prime}(1-(r^{\\prime})^{2})}{r} \\\\ \u0026amp;= \\dfrac{-r^{\\prime \\prime}}{r} \\\\ \\end{align*} $$\n回転面はガウス曲率の符号によって分類される。$a \u0026gt; 0$において、\n$K= a^{2}$の場合 $K= 0$の場合 $K= -a^{2}$の場合 ","id":3034,"permalink":"https://freshrimpsushi.github.io/jp/posts/3034/","tags":null,"title":"ガウス曲率による回転面の分類"},{"categories":"행렬대수","contents":"定義 $A$を次のような$2 \\times 2$ 行列としよう。\n$$ A = \\begin{bmatrix} a \u0026amp; b \\\\ c \u0026amp; d \\end{bmatrix} $$\n$A$の行列式determinantを以下のように定義し、$\\det(A)$で表す。\n$$ \\det(A) := ad - bc $$\n説明 行列式を話す上で、線形代数の目的自体を話さずにはいられない。ほとんどの数学で言う問題は基本的に「方程式を解けるか」と要約できると言っても過言ではない。例えば、簡単な方程式\n$$ ax = b $$\nを考えてみれば、$ a = 0$が0でない限り、この方程式には解があることが簡単にわかる。二次方程式\n$$ a x^2 + b x + c = 0 $$\nも解の公式を通じて簡単に解ける。こうして、数学者たちは$x$の次数を上げながら、より一層難しい問題に挑戦した。しかし、不運な天才アーベルによって「5次以上の代数方程式は一般解を持たない」と証明されてしまう。\n一方、代数の次元を上げる代わりに未知数や方程式の数そのものを増やして研究する道が残されていた。ここで行列式が登場する。韓国語で見れば、行列が出来た後行列式が出来たように見えるかもしれないが、実はそうではない。歴史的に行列式は行列が登場する前に先に登場したし1、実際に英語を見るとdeterminantとmatrixは特に関連がない。determinantという名前は次のような未知数が2つある連立一次方程式の解が存在するか、存在しないかを判別してくれる公式だから付けられた名前だ。\n$$ \\left\\{ \\begin{align*} ax + by \u0026amp;= 0 \\\\ cx + dy \u0026amp;= 0 \\end{align*} \\right. $$\n上記のような連立方程式が与えられた時$ad-bc = 0$ならば唯一の自明解しか存在せず、$ad-bc \\ne 0$ならば非自明な唯一の解を持つことになる。したがって、$ad-bc$が与えられた連立方程式の解があるかないかを判別してくれる公式となるため、判別式という名前が付けられたのだ。\nしかし、ご存知のように連立方程式は行列の形で表現できる。「簡単な」連立方程式は次のように表現できる。\n$$ A \\mathbf{x} = \\mathbf{b} $$\n$ax = b$の解が$x = \\dfrac{b}{a}$だったことをよく考えてみよう。$\\dfrac{1}{a}$は$a$の逆元なので、両辺にかけるだけで$x$だけを残すことができた。解が存在する条件と結びつけて言えば、$a= 0$は逆元が存在しないので$ax = b$の解も存在しないことになる。同様に、$A \\mathbf{x} = \\mathbf{b}$も$A$の逆行列を求めることができるかの問題に帰着される。$A$によって表される線形システムの解の存在自体が$A$の逆行列の存在であり、この逆元を求めることが解を求めることになる。この時、$A$の逆行列が存在する条件と$A$によって表される線形システムが唯一の解を持つ条件が同じであることがわかる。\n$A = \\begin{bmatrix} a \u0026amp; b \\\\ c \u0026amp; d\\end{bmatrix}$の逆行列は次のようだ。\n$$ A^{-1} = \\dfrac{1}{ad-bc} \\begin{bmatrix} d \u0026amp; -b \\\\ -c \u0026amp; a \\end{bmatrix} $$\nこれは単純に$A$と$A^{-1}$を直接掛け合わせて証明される。もし$\\det (A) = ad - bc = 0$ならば、行列の形状に関係なく、$A^{-1}$の前の定数が$\\dfrac{1}{0}$となるので、逆行列が存在できない。可逆性invertibilityをたまに非特異性nonsingularityと呼ぶのもこのためだ。singularという言葉は「特異的な」と訳されるが、数学的に言えば「0で割る」くらいの感じになる。\n一方、$n\\times n$個の実数を$1$個の実数にマッピングする関数の観点から行列式を見れば、次のように定義することができる。\n定義 関数$ \\det : \\mathbb{R}^{n \\times n } \\to \\mathbb{R} $が次の条件を満たすならば、行列式と定義する。\n単位行列$I_{n}$に対して、$\\det(I_{n}) = 1$ $1 \\le i,j \\le n$に対して、$\\det \\begin{bmatrix} \\mathbb{r_{1}} \\\\ \\vdots \\\\ \\mathbb{r_{i}} \\\\ \\vdots \\\\ \\mathbb{r_{j}} \\\\ \\vdots \\\\ \\mathbb{r_{n}} \\end{bmatrix} = - \\det \\begin{bmatrix} \\mathbb{r_{1}} \\\\ \\vdots \\\\ \\mathbb{r_{j}} \\\\ \\vdots \\\\ \\mathbb{r_{i}} \\\\ \\vdots \\\\ \\mathbb{r_{n}} \\end{bmatrix}$ $\\det \\begin{bmatrix} k \\mathbb{r_{1}} + l \\mathbb{r_{1}}^{\\prime} \\\\ \\vdots \\\\ \\mathbb{r_{n}} \\end{bmatrix} = k \\det \\begin{bmatrix} \\mathbb{r_{1}} \\\\ \\vdots \\\\ \\mathbb{r_{n}} \\end{bmatrix} + l \\det \\begin{bmatrix} \\mathbb{r_{1}}^{\\prime} \\\\ \\vdots \\\\ \\mathbb{r_{n}} \\end{bmatrix}$ 説明 このように行列式を一般化すると、連立方程式の解が存在するか存在しないかを話すことがずっと簡単になるだろう。そして、このような議論が一行で完結したものがちょうど下の定理だ。\n$$ \\forall A \\in \\mathbb{C}^{n \\times n},\\quad \\exists A^{-1} \\iff \\det{A} \\ne 0 $$\n定理と言うよりほぼ定義のレベルで受け入れられるほど当たり前の事実だ。しかし、なぜこの定理が残るのか、本当に当たり前なのかきちんと説明できないなら、行列式を理解していないのと同じだ。特に行列式の場合は、定義よりも概念が先行するため、理解できなければ、時間をかけても知っておくべきだ。\nhttps://en.wikipedia.org/wiki/Determinant#History\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":252,"permalink":"https://freshrimpsushi.github.io/jp/posts/252/","tags":null,"title":"行列式"},{"categories":"행렬대수","contents":"定義1 次のような線形システムが与えられたとしよう。\n$$ \\begin{equation} \\begin{aligned} a_{11}x_{1} + a_{12}x_{2} + \\cdots + a_{1n}x_{n} \u0026amp;= b_{1}\\\\ a_{21}x_{1} + a_{22}x_{2} + \\cdots + a_{2n}x_{n} \u0026amp;= b_{2}\\\\ \u0026amp;\\vdots\\\\ a_{m1}x_{1} + a_{m2}x_{2} + \\cdots + a_{mn}x_{n} \u0026amp;= b_{m} \\end{aligned} \\label{linsys2} \\end{equation} $$\n線形システムの定数を行列で表現したものを増大行列と呼ぶ。\n$$ \\begin{equation} \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \u0026amp; b_{1} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \u0026amp; b_{2} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \u0026amp; b_{m} \\end{bmatrix} \\label{augmented} \\end{equation} $$\n説明 行列は連立一次方程式を簡単に解くために考案された。例えば $\\eqref{linsys2}$ の定数だけを取り出して $\\eqref{augmented}$ のように表現することも、次のように表現することもできる。\n$$ \\begin{align*} \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n} \\end{bmatrix} \u0026amp;= \\begin{bmatrix} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{m} \\end{bmatrix} \\\\ A\\mathbf{x} \u0026amp;= \\mathbf{b} \\end{align*} $$\n特に増大行列で表された場合、基本行操作を通じて線形システムを解くことができるが、実際これは中学校で習った加減法と本質的に同じだ。\n定義 以下の三つの操作を基本行操作という。\n$0$ でない定数を一行に掛ける。\n二行の位置を交換する。\n一行の定数倍を他の行に加える。\n例 加減法で連立方程式を解くことは、増大行列を基本行操作で各行の最後の列を引いて一つの要素だけ残すことと同じだ。線形代数学の言葉で言えば、「増大行列を基本行操作を通じて階段行列にすること」だ。\n$$ \\begin{array}{rcrcrcr} x \u0026amp;+\u0026amp; y \u0026amp;+\u0026amp; 2z \u0026amp;=\u0026amp;\\ 9 \\\\ 2x \u0026amp;+\u0026amp; 4y \u0026amp;-\u0026amp; 3z \u0026amp;=\u0026amp;\\ 1 \\\\ 3x \u0026amp;+\u0026amp; 6y \u0026amp;-\u0026amp; 5z \u0026amp;=\u0026amp;\\ 0 \\end{array} \\quad \\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 2 \u0026amp; 9 \\\\ 2 \u0026amp; 4 \u0026amp; -3 \u0026amp; 1 \\\\ 3 \u0026amp; 6 \u0026amp; -5 \u0026amp; 0 \\end{bmatrix} $$\n一行目に $-2$ を掛けたものを二行目に加える。\n$$ \\begin{array}{rcrcrcr} x \u0026amp;+\u0026amp; y \u0026amp;+\u0026amp; 2z \u0026amp;=\u0026amp;\\ 9 \\\\ \u0026amp;\u0026amp; 2y \u0026amp;-\u0026amp; 7z \u0026amp;=\u0026amp;\\ -17 \\\\ 3x \u0026amp;+\u0026amp; 6y \u0026amp;-\u0026amp; 5z \u0026amp;=\u0026amp;\\ 0 \\end{array} \\quad \\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 2 \u0026amp; 9 \\\\ 0 \u0026amp; 2 \u0026amp; -7 \u0026amp; -17 \\\\ 3 \u0026amp; 6 \u0026amp; -5 \u0026amp; 0 \\end{bmatrix} $$\n一行目に $-3$ を掛けたものを三行目に加える。\n$$ \\begin{array}{rcrcrcr} x \u0026amp;+\u0026amp; y \u0026amp;+\u0026amp; 2z \u0026amp;=\u0026amp;\\ 9 \\\\ \u0026amp;\u0026amp; 2y \u0026amp;-\u0026amp; 7z \u0026amp;=\u0026amp;\\ -17 \\\\ \u0026amp;\u0026amp; 3y \u0026amp;-\u0026amp;11z \u0026amp;=\u0026amp;\\ -27 \\end{array} \\quad \\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 2 \u0026amp; 9 \\\\ 0 \u0026amp; 2 \u0026amp; -7 \u0026amp; -17 \\\\ 0 \u0026amp; 3 \u0026amp; -11 \u0026amp; -27 \\end{bmatrix} $$\n二行目に $\\dfrac{1}{2}$ を掛ける。\n$$ \\begin{array}{rcrcrcr} x \u0026amp;+\u0026amp; y \u0026amp;+\u0026amp; 2z \u0026amp;=\u0026amp;\\ 9 \\\\ \u0026amp;\u0026amp; y \u0026amp;-\u0026amp; \\dfrac{7}{2} z \u0026amp;=\u0026amp;\\ -\\dfrac{17}{2} \\\\ \u0026amp;\u0026amp; 3y \u0026amp;-\u0026amp;11z \u0026amp;=\u0026amp;\\ -27 \\end{array} \\quad \\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 2 \u0026amp; 9 \\\\ 0 \u0026amp; 1 \u0026amp; -\\dfrac{7}{2} \u0026amp; -\\dfrac{17}{2} \\\\ 0 \u0026amp; 3 \u0026amp; -11 \u0026amp; -27 \\end{bmatrix} $$\n二行目に $-3$ を掛けたものを三行目に加える。\n$$ \\begin{array}{rcrcrcr} x \u0026amp;+\u0026amp; y \u0026amp;+\u0026amp; 2z \u0026amp;=\u0026amp;\\ 9 \\\\ \u0026amp;\u0026amp; y \u0026amp;-\u0026amp; \\dfrac{7}{2} z \u0026amp;=\u0026amp;\\ -\\dfrac{17}{2} \\\\ \u0026amp;\u0026amp; \u0026amp;-\u0026amp;\\dfrac{1}{2}z \u0026amp;=\u0026amp;\\ -\\dfrac{3}{2} \\end{array} \\quad \\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 2 \u0026amp; 9 \\\\ 0 \u0026amp; 1 \u0026amp; -\\dfrac{7}{2} \u0026amp; -\\dfrac{17}{2} \\\\ 0 \u0026amp; 0 \u0026amp; -\\dfrac{1}{2} \u0026amp; -\\dfrac{3}{2} \\end{bmatrix} $$\n三行目に $-2$ を掛ける。\n$$ \\begin{array}{rcrcrcr} x \u0026amp;+\u0026amp; y \u0026amp;+\u0026amp; 2z \u0026amp;=\u0026amp;\\ 9 \\\\ \u0026amp;\u0026amp; y \u0026amp;-\u0026amp; \\dfrac{7}{2} z \u0026amp;=\u0026amp;\\ -\\dfrac{17}{2} \\\\ \u0026amp;\u0026amp; \u0026amp;\u0026amp;z \u0026amp;=\u0026amp;\\ 3 \\end{array} \\quad \\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 2 \u0026amp; 9 \\\\ 0 \u0026amp; 1 \u0026amp; -\\dfrac{7}{2} \u0026amp; -\\dfrac{17}{2} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 3 \\end{bmatrix} $$\n二行目に $-1$ を掛けたものを一行目に加える。\n$$ \\begin{array}{rcrcrcr} x \u0026amp;\u0026amp; \u0026amp;+\u0026amp; \\dfrac{11}{2}z \u0026amp;=\u0026amp;\\ \\dfrac{35}{2} \\\\ \u0026amp;\u0026amp; y \u0026amp;-\u0026amp; \\dfrac{7}{2} z \u0026amp;=\u0026amp;\\ -\\dfrac{17}{2} \\\\ \u0026amp;\u0026amp; \u0026amp;\u0026amp;z \u0026amp;=\u0026amp;\\ 3 \\end{array} \\quad \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; \\dfrac{11}{2} \u0026amp; \\dfrac{35}{2} \\\\ 0 \u0026amp; 1 \u0026amp; -\\dfrac{7}{2} \u0026amp; -\\dfrac{17}{2} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 3 \\end{bmatrix} $$\n三行目に $-\\dfrac{11}{2}$ を掛けたものを一行目に加えて、三行目に $\\dfrac{7}{2}$ を掛けたものを一行目に加える。\n$$ \\begin{array}{rcrcrcr} x \u0026amp;\u0026amp; \u0026amp;\u0026amp; \u0026amp;=\u0026amp;\\ 1 \\\\ \u0026amp;\u0026amp; y \u0026amp;\u0026amp; \u0026amp;=\u0026amp;\\ 2 \\\\ \u0026amp;\u0026amp; \u0026amp;\u0026amp;z \u0026amp;=\u0026amp;\\ 3 \\end{array} \\quad \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 2 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 3 \\end{bmatrix} $$\n従って、与えられた線形システムの解は $x=1$、$y=2$、$z=3$ だ。\nHoward Anton, Elementary Linear Algebra: Applications Version (12版, 2019), p6-8\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3014,"permalink":"https://freshrimpsushi.github.io/jp/posts/3014/","tags":null,"title":"拡張行列と基本行操作"},{"categories":"행렬대수","contents":"定義1 定数$a_{1}$、$a_{2}$、$\\dots$、$a_{n}$、$b$に対して、変数$x_{1}$、$x_{2}$、$\\dots$、$x_{n}$の一次方程式linear equationを次のように定義する。\n$$ \\begin{equation} a_{1}x_{1} + a_{2}x_{2} + \\cdots + a_{n}x_{n} = b \\label{lineq} \\end{equation} $$\nこのとき、少なくとも一つの$a$は$0$ではない。つまり「全ての$a$が$0$」ではない。一次方程式の有限集合を連立一次方程式system of linear equationsまたは単に線形系linear systemと呼び、変数を未知数unknownsと呼ぶ。韓国語で線形と一次は同じ意味である。一般に$n$個の変数$x_{1}$、$x_{2}$、$\\dots$、$x_{n}$に対する$m$個の一次方程式で構成される線形系は次のように表される。\n$$ \\begin{equation} \\begin{aligned} a_{11}x_{1} + a_{12}x_{2} + \\cdots + a_{1n}x_{n} \u0026amp;= b_{1} \\\\ a_{21}x_{1} + a_{22}x_{2} + \\cdots + a_{2n}x_{n} \u0026amp;= b_{2} \\\\ \u0026amp;\\vdots \\\\ a_{m1}x_{1} + a_{m2}x_{2} + \\cdots + a_{mn}x_{n} \u0026amp;= b_{m} \\end{aligned} \\label{linsys} \\end{equation} $$\nこれを行列で表すと、次のようになる。\n$$ \\begin{align*} \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n} \\end{bmatrix} \u0026amp;= \\begin{bmatrix} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{m} \\end{bmatrix} \\\\ A\\mathbf{x} \u0026amp;= \\mathbf{b} \\end{align*} $$\n説明 線形系を真にする$x_{1}$、$x_{2}$、$\\dots$、$x_{n}$の値を解solutionと呼ぶ。線形系が与えられると、以下の三つのうちの一つを満たさなければならない。それ以外の場合は存在しない。証明は記事の下部で紹介する。\n解が一意に存在する。 解が無数に存在する。 解が存在しない。 少なくとも一つ以上の解が存在する場合、線形系は一致するconsistentと言われる。解が存在しない場合、線形系は不一致inconsistentと言われる。\n具体的に変数が2つの場合、一次方程式は直線の方程式を意味する。変数が2つの線形系で解が一意に存在する場合、直線が一点で交わる場合を意味する。解が無数に存在する場合、直線が無数の点で交わる場合、つまり重なっている場合を意味する。解が存在しない場合、直線が交わる点が存在しない場合を意味する。\n変数が3つの一次方程式は平面の方程式を意味するので、線形系の解によって、平面がどのように重なっているかを意味することになる。\n例 次の線形系を解いてみよう。\n$$ \\begin{align*} 4x -2y \u0026amp;= 1 \\\\ 16x -8y \u0026amp;= 4 \\end{align*} $$\n上の式に$-4$を掛けて下の式に加えると、次のようになる。\n$$ \\begin{align*} 4x -2y \u0026amp;= 1 \\\\ 0 \u0026amp;= 0 \\end{align*} $$\nすると、下の式は何の情報も表さないので、上の式だけで表そう。\n$$ 4x -2y = 1 $$\nこの場合、幾何学的に二つの直線が一致することを意味する。このような場合、$x$を$y$に対して整理して$x = \\dfrac{1}{2}y + \\dfrac{1}{4}$と表記した後、$y$に任意の数$t$を代入して解を表す。\n$$ x = \\dfrac{1}{4} + \\dfrac{1}{2}t, \\quad y = t $$\nこのような$t$をパラメーターparameterと呼び、上の方程式をパラメーター方程式parametric equationsと呼ぶ。\n証明2 連立一次方程式は、解が存在しないか、一つだけか、無数に存在するかのどれかである。他の場合は存在しない。\n異なる二つの解があるとき、無数に多くの解が存在することを示せば、証明が完了する。$\\mathbf{x}_{1}$、$\\mathbf{x}_{2}$を連立一次方程式$A\\mathbf{x} =\\mathbf{b}$の異なる二つの解としよう。そして、$\\mathbf{x}_{0} = \\mathbf{x}_{1} - \\mathbf{x}_{2}$とする。$\\mathbf{x}_{1}$と$\\mathbf{x}_{2}$が異なる二つの解であるので、$\\mathbf{x}_{0} \\ne \\mathbf{0}$である。さらに、以下の式が成り立つ。\n$$ A \\mathbf{x}_{0} = A (\\mathbf{x}_{1} - \\mathbf{x}_{2}) = \\mathbf{b} - \\mathbf{b} = \\mathbf{0} $$\nこのとき、$k$を任意の定数としよう。すると、上記の結果により、以下の式も成り立つ。\n$$ \\begin{align*} A (\\mathbf{x}_{1} + k\\mathbf{x}_{0}) \u0026amp;= A\\mathbf{x}_{1} + A(k\\mathbf{x}_{0}) \\\\ \u0026amp;= A\\mathbf{x}_{1} + kA\\mathbf{x}_{0} \\\\ \u0026amp;= \\mathbf{b} + \\mathbf{0} \\\\ \u0026amp;= \\mathbf{b} \\end{align*} $$\nしたがって、$\\mathbf{x}_{1} + k\\mathbf{x}_{0}$も連立一次方程式$A\\mathbf{x} = \\mathbf{b}$の解である。これは任意の定数$k$に対して成り立つので、解が無数に存在する。\n■\nHoward Anton, Elementary Linear Algebra: Aplications Version (12th Edition, 2019), p2-6\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHoward Anton, Elementary Linear Algebra: Aplications Version (12th Edition, 2019), p62\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3013,"permalink":"https://freshrimpsushi.github.io/jp/posts/3013/","tags":null,"title":"連立一次方程式"},{"categories":"함수","contents":"定義 関数 $f : X \\to Y$が以下の二つの条件を満たす場合、線形linearという。$x,x_{1},x_{2}\\in X$、$a \\in \\mathbb{R}$に対して、\n$f(ax) = af(x)$ $f(x_{1} + x_{2}) = f(x_{1}) + f(x_{2})$ 説明 線形でない場合、非線形nonlinearという。二つの条件をまとめて次のように表すこともある\n$$ f(ax_{1} + x_{2}) = af(x_{1}) + f(x_{2}) $$\n2.で等式ではなく、以下である$\\le$が成り立つ場合、準線形という。\n双線形 二変数関数$f = f(x,y)$が各変数に対して線形である場合、双線形bilinearという。\n多重線形 多変数関数$f= f(x_{1}, \\dots, x_{n})$が各変数に対して線形である場合、多重線形multilinearという。\n","id":3037,"permalink":"https://freshrimpsushi.github.io/jp/posts/3037/","tags":null,"title":"線形関数"},{"categories":"행렬대수","contents":"定義 ユニタリ行列 $A$を正方形の複素数行列とする。$A$が以下の式を満たす時、ユニタリ行列unitaryと呼ぶ。\n$$ A^{-1}=A^{\\ast} $$\nこの時、$A^{-1}$は$A$の逆行列、$A^{\\ast}$は$A$の共役転置である。\nユニタリ対角化1 サイズが$n \\times n$の正方行列$A$が与えられたとする。$A$が対角行列$D$とユニタリ行列$P$に対して次の式を満たす場合、ユニタリ対角化可能unitarily diagonalizableと言う。\n$$ P^{\\ast} A P = D $$\nこのような条件を満たす$P$は行列$A$をユニタリ対角化するunitarily diagonalizeと言う。\n説明 ユニタリ行列は簡単に言うと、複素数行列に対して拡張された直交行列である。従って、直交行列の性質をそのまま持つ。以下のユニタリ行列の同値条件に関する証明は、直交行列の証明に置き換える。\n定理2 ユニタリ行列の同値条件: $n \\times n$サイズの複素数行列$A$に対して、以下の命題は全て同値である。\n$A$はユニタリ行列である。\n$A$の行ベクトルの集合は$\\mathbb{C}^n$の正規直交集合である。\n$A$の列ベクトルの集合は$\\mathbb{C}^n$の正規直交集合である。\n$A$は内積を保持する。つまり、全ての$\\mathbf{x},\\mathbf{y}\\in \\mathbb{C}^{n}$に対して、以下が成立する。\n$$ (A \\mathbf{x}) \\cdot (A\\mathbf{y}) = \\mathbf{x} \\cdot \\mathbf{y} $$\n$A$は長さを保持する。つまり、全ての$\\mathbf{x}\\in \\mathbb{C}^{n}$に対して、以下が成立する。 $$ \\left\\| A \\mathbf{x} \\right\\| = \\left\\| \\mathbf{x} \\right\\| $$\nHoward Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), 440ページ\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHoward Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), 439ページ\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3008,"permalink":"https://freshrimpsushi.github.io/jp/posts/3008/","tags":null,"title":"ユニタリ行列"},{"categories":"행렬대수","contents":"要約 $A$を$n \\times n$のサイズを持つエルミート行列としよう。そうすると、$A$の固有値は全て実数である。\n説明 一般の行列において、固有値が実数である保証はなく、エルミート行列については、証明を通して実数であることが確認できる。\n直感的には思いつきにくいが、証明自体は比較的簡単な方であり、事実としても非常に有用である。正定値性などの概念と組み合わせることにより、様々な良い結果を生むので、是非知っておくべきだ。\n証明 $A$の固有値を$\\lambda$、それに対応する固有ベクトルを$\\mathbf{x}$とする。すると、固有値方程式は以下のようになる。\n$$ A \\mathbf{x} = \\lambda \\mathbf{x} $$\n両辺の左側に$\\mathbf{x}^{\\ast}$を乗じると、以下のようになる。\n$$ \\begin{equation} \\mathbf{x}^{\\ast} A \\mathbf{x} = \\lambda \\mathbf{x}^{\\ast} \\mathbf{x} \\end{equation} $$\n両辺に共役転置$^{ \\ast }$を取ると、共役転置の性質により、以下のようになる。\n$$ \\begin{align*} \\left( \\mathbf{x}^{\\ast} A \\mathbf{x} \\right) ^{\\ast} = \u0026amp; \\left( \\lambda \\mathbf{x}^{\\ast} \\mathbf{x} \\right) ^{\\ast} \\\\ \\implies \\mathbf{x}^{\\ast} A^{ \\ast } \\mathbf{x} = \u0026amp; \\overline{\\lambda} \\mathbf{x}^{ \\ast } \\mathbf{x} \\end{align*} $$\n$A$はエルミート行列なので、$A=A^{\\ast}$が成り立ち、上の式は以下のようになる。\n$$ \\begin{equation} \\mathbf{x}^{\\ast} A \\mathbf{x} = \\overline{\\lambda} \\mathbf{x}^{ \\ast } \\mathbf{x} \\end{equation} $$\n$(1)$と$(2)$により、次の式が成立する。\n$$ \\lambda \\mathbf{x}^{ \\ast } \\mathbf{x} = \\mathbf{x}^{ \\ast } A \\mathbf{x} = \\mathbf{x}^{ \\ast } A^{ \\ast } \\mathbf{x} = \\overline{\\lambda} \\mathbf{x}^{ \\ast } \\mathbf{x} $$\nしたがって、 $$ ( \\lambda - \\overline{\\lambda} ) \\mathbf{x}^{ \\ast } \\mathbf{x} = 0 $$\nしかし、$\\mathbf{x}$は固有ベクトルなので、$\\mathbf{0}$ではない。したがって、$\\mathbf{x}^{\\ast} \\mathbf{x} \\ne \\mathbf{0}$である。よって、\n$$ \\lambda = \\overline{\\lambda} $$\nこれは$\\lambda$が実数であることを意味する。\n■\n参考 数理物理学における証明 ","id":310,"permalink":"https://freshrimpsushi.github.io/jp/posts/310/","tags":null,"title":"エルミート行列の固有値は常に実数である"},{"categories":"행렬대수","contents":"定義 $A$を正方の複素数行列とする。$A$が下の式を満たせば、エルミート行列Hermitianまたは自己共役行列self-adjoint matrixという。\n$$ A^{\\ast}=A $$\nここで、$A^{\\ast}$は$A$の共役転置である。$A$が下の式を満たせば、反エルミート行列skew-Hermitian, anti-Hermitianという。\n$$ A^{\\ast}=-A $$\n説明 実数行列なら、$A^{\\ast}=A^{T}$なので対称行列ならエルミート行列だ。次の性質から分かるように、エルミート行列の対角成分は必ず実数だ。だから、行列のサイズが小さいなら、目で見てエルミート行列かどうか簡単に判断できる。\nエルミート行列の対角成分が必ず実数であるのと同じ理由で、反エルミート行列の対角成分は全部$0$だ。\n性質 $A$をエルミート行列とする。\n(a) $A$の対角成分は必ず実数だ。\n(b) $A$の固有値は全部実数だ。\n(c) $A$の異なる固有値を持つ固有ベクトルは互いに直交する。\n(b) 量子力学の観点から言えば、「エルミート演算子の期待値は常に実数だ」となる。\n証明 (a) 行列$A$の転置$A^{T}$は、$A$の成分を主対角線を基準にして対称移動させたものだ。だから、二つの行列の対角成分は常に同じだ。これはすなわち$a_{ij}=\\overline{a_{ij}}$を意味し、対角成分は実数だ。\n■\n(b) (c) ","id":3007,"permalink":"https://freshrimpsushi.github.io/jp/posts/3007/","tags":null,"title":"エルミート行列"},{"categories":"행렬대수","contents":"定義 $n\\times n$ 行列が以下のように与えられたとする。\n$$ A= \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{bmatrix} $$\n$A$の対角要素の合計を$A$の対角合計traceと定義し、以下のように表記する。\n$$ \\text{tr}(A)=\\text{Tr}(A)=a_{11}+a_{22}+\\cdots + a_{nn}=\\sum \\limits_{i=1}^{n} a_{ii} $$\n説明 次のように対角合計を関数として考えることもできる。$M_{n\\times n}(\\mathbb{R})$を実数を成分とする$n\\times n$行列の集合とする。すると$\\text{Tr}$は次のように定義される関数である。\n$$ \\text{Tr} : M_{n\\times n} (\\mathbb{R}) \\to \\mathbb{R},\\quad \\text{Tr}(A)=\\sum \\limits_{i=1}^{n} a_{ii} $$\n性質 $A,B,C$が$n \\times n$行列で、$k$が定数とする。\n(a) スカラー倍のトレースとトレースのスカラー倍が同じである。\n$$ \\text{Tr}(kA)= k\\text{Tr}(A) $$\n(b) 合計のトレースとトレースの合計が同じである。\n$$ \\text{Tr}(A+B)=\\text{Tr}(A)+\\text{Tr}(B) $$\n(a)+(b) トレースは線形である。\n$$ \\text{Tr}(kA+B)=k\\text{Tr}(A)+\\text{Tr}(B) $$\n(c) $AB$と$BA$のトレースが同じである。\n$$ \\text{Tr}(AB) = \\text{Tr}(BA) $$\n(c\u0026rsquo;) 循環性Cyclic Property: 上記の事実から、次の式が成り立つことがわかる。\n$$ \\text{Tr}(ABC) = \\text{Tr}(BCA) = \\text{Tr}(CAB) $$\n(d) $A$と$A^{T}$のトレースが同じである。\n$$ \\text{Tr}(A) = \\text{Tr}(A^{T}) $$\n","id":1924,"permalink":"https://freshrimpsushi.github.io/jp/posts/1924/","tags":null,"title":"トレース"},{"categories":"확률분포론","contents":"定義 平均ベクトル $\\mathbf{\\mu} \\in \\mathbb{R}^{p}$ および共分散行列 $\\Sigma \\in \\mathbb{R}^{p \\times p}$ に基づく以下のような確率密度関数を持つ多変量分布 $N_{p} \\left( \\mu , \\Sigma \\right)$ を多変量正規分布と呼ぶ。\n$$ f (\\textbf{x}) = \\left( (2\\pi)^{p} \\det \\Sigma \\right)^{-1/2} \\exp \\left[ - {{ 1 } \\over { 2 }} \\left( \\textbf{x} - \\mathbf{\\mu} \\right)^{T} \\Sigma^{-1} \\left( \\textbf{x} - \\mathbf{\\mu} \\right) \\right] \\qquad , \\textbf{x} \\in \\mathbb{R}^{p} $$\n$\\mathbf{x}^{T}$ は $\\mathbf{x}$ の転置を意味する。 定理 $$ \\begin{align*} \\mathbf{X} =\u0026amp; \\begin{bmatrix} \\mathbf{X}_{1} \\\\ \\mathbf{X}_{2} \\end{bmatrix} \u0026amp; : \\Omega \\to \\mathbb{R}^{n} \\\\ \\mu =\u0026amp; \\begin{bmatrix} \\mu_{1} \\\\ \\mu_{2} \\end{bmatrix} \u0026amp; \\in \\mathbb{R}^{n} \\\\ \\Sigma =\u0026amp; \\begin{bmatrix} \\Sigma_{11} \u0026amp; \\Sigma_{12} \\\\ \\Sigma_{21} \u0026amp; \\Sigma_{22} \\end{bmatrix} \u0026amp; \\in \\mathbb{R}^{n \\times n} \\end{align*} $$ 以下の定理の記述で、特に説明がない限り、$\\mathbf{X}$、$\\mu$、$\\Sigma$ は同じブロック行列を指す。\n多変量正規分布の線形変換 行列 $A \\in \\mathbb{R}^{m \\times n}$ とベクトル $\\mathbf{b} \\in \\mathbb{R}^{m}$ に対して多変量正規分布に従うランダムベクトル $\\mathbf{X} \\sim N_{n} \\left( \\mu , \\Sigma \\right)$ の線形変換 $\\mathbf{Y} = A \\mathbf{X} + \\mathbf{b}$ もやはり多変量正規分布 $N_{m} \\left( A \\mu + \\mathbf{b} , A \\Sigma A^{T} \\right)$ に従う。\n多変量正規分布における独立性とゼロ相関性は等価である 多変量正規分布に従うランダムベクトル $\\mathbf{X} \\sim N_{n} \\left( \\mu , \\Sigma \\right)$ があるとする。すると、以下が成立する。 $$ \\mathbf{X}_{1} \\perp \\mathbf{X}_{2} \\iff \\Sigma_{12} = \\Sigma_{21} = O $$\n多変量正規分布の条件付き平均と分散 多変量正規分布に従うランダムベクトル $\\mathbf{X} \\sim N_{n} \\left( \\mu , \\Sigma \\right)$ があるとする。その場合、条件付き確率ベクトル $\\mathbf{X}_{1} | \\mathbf{X}_{2} : \\Omega \\to \\mathbb{R}^{m}$ もやはり多変量正規分布に従い、具体的には以下のような平均ベクトルと共分散行列を持つ。 $$ \\mathbf{X}_{1} | \\mathbf{X}_{2} \\sim N_{m} \\left( \\mu_{1} + \\Sigma_{12} \\Sigma_{22}^{-1} \\left( \\mathbf{X}_{2} - \\mu_{2} \\right) , \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21} \\right) $$\n回帰係数ベクトルの多変量正規性 回帰係数の推定量 $\\hat{\\beta}$ は以下のような多変量正規分布に従う。 $$ \\hat{\\beta} \\sim N_{1+p} \\left( \\beta , \\sigma^{2} \\left( X^{T} X \\right)^{-1} \\right) $$\n積率生成関数 $X \\sim N_{p} \\left( \\mu , \\Sigma \\right)$ の積率生成関数は以下のとおりである。 $$ M_{X} \\left( \\mathbf{t} \\right) = \\exp \\left( \\mathbf{t}^{T} \\mu + {{ 1 } \\over { 2 }} \\mathbf{t}^{T} \\Sigma \\mathbf{t} \\right) \\qquad , \\mathbf{t} \\in \\mathbb{R}^{p} $$\nエントロピー 多変量正規分布 $N_{p}(\\mu, \\Sigma)$ のエントロピーは以下のとおりである。\n$$ H = \\dfrac{1}{2}\\ln \\left[ (2 \\pi e)^{p} \\left| \\Sigma \\right| \\right] = \\dfrac{1}{2}\\ln (\\det (2\\pi e \\Sigma)) $$\n$\\left| \\Sigma \\right|$ は共分散行列の行列式である。\n参照 一変量正規分布: $p = 1$ に続いて $\\mu \\in \\mathbb{R}^{1}$ があり、そして $\\Sigma \\in \\mathbb{R}^{1 \\times 1}$ の時、上記の確率密度関数は正確に一変量正規分布の確率密度関数となる。 ","id":1954,"permalink":"https://freshrimpsushi.github.io/jp/posts/1954/","tags":null,"title":"多変量正規分布"},{"categories":"수리통계학","contents":"定義1 $p$次元のランダムベクター$\\mathbf{X} = \\left( X_{1}, \\cdots , X_{p} \\right)$に対して以下のように定義される$\\text{Cov} (\\mathbf{X})$を共分散行列Covariance Matrixという。\n$$ \\left( \\text{Cov} \\left( \\mathbf{X} \\right) \\right)_{ij} := \\text{Cov} \\left( X_{i} , X_{j} \\right) $$\n$\\text{Cov}$は共分散である。 説明 定義をもっと簡単に書いてみると以下の通り。\n$$ \\text{Cov} \\left( \\mathbf{X} \\right) := \\begin{pmatrix} \\text{Var} \\left( X_{1} \\right) \u0026amp; \\text{Cov} \\left( X_{1} , X_{2} \\right) \u0026amp; \\cdots \u0026amp; \\text{Cov} \\left( X_{1} , X_{p} \\right) \\\\ \\text{Cov} \\left( X_{2} , X_{1} \\right) \u0026amp; \\text{Var} \\left( X_{2} \\right) \u0026amp; \\cdots \u0026amp; \\text{Cov} \\left( X_{2} , X_{p} \\right) \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\text{Cov} \\left( X_{p} , X_{1} \\right) \u0026amp; \\text{Cov} \\left( X_{p} , X_{2} \\right) \u0026amp; \\cdots \u0026amp; \\text{Var} \\left( X_{p} \\right) \\end{pmatrix} $$\nすべての共分散行列は正の半定値行列である。つまり、すべてのベクター$\\mathbf{x} \\in \\mathbb{R}^{p}$に対して以下が成り立つ。\n$$ 0 \\le \\textbf{x}^{T} \\text{Cov} \\left( \\mathbf{X} \\right) \\textbf{x} $$\n定理 [1]: $\\mathbf{\\mu} \\in \\mathbb{R}^{p}$が$\\mathbf{\\mu} := \\left( EX_{1} , \\cdots , EX_{p} \\right)$として与えられた場合、 $$ \\text{Cov} (\\mathbf{X}) = E \\left[ \\mathbf{X} \\mathbf{X}^{T} \\right] - \\mathbf{\\mu} \\mathbf{\\mu}^{T} $$ [2]: 定数の行列$A \\in \\mathbb{R}^{k \\times p}$が$(A)_{ij} := a_{ij}$として与えられた場合、 $$ \\text{Cov} ( A \\mathbf{X}) = A \\text{Cov} \\left( \\mathbf{X} \\right) A^{T} $$ $A^{T}$は$A$の転置行列である。 証明 [1] $$ \\begin{align*} \\text{Cov} \\left( \\mathbf{X} \\right) =\u0026amp; E \\left[ \\left( \\mathbf{X} - \\mathbf{\\mu} \\right) \\left( \\mathbf{X} - \\mathbf{\\mu} \\right)^{T} \\right] \\\\ =\u0026amp; E \\left[ \\mathbf{X} \\mathbf{X}^{T} - \\mathbf{\\mu} \\mathbf{X}^{T} - \\mathbf{X} \\mathbf{\\mu}^{T} + \\mathbf{\\mu} \\mathbf{\\mu}^{T} \\right] \\\\ =\u0026amp; E \\left[ \\mathbf{X} \\mathbf{X}^{T} \\right] - \\mathbf{\\mu} E \\left[ \\mathbf{X}^{T} \\right] - E \\left[ \\mathbf{X} \\right] \\mathbf{\\mu}^{T} + E \\left[ \\mathbf{\\mu} \\mathbf{\\mu}^{T} \\right] \\\\ =\u0026amp; E \\left[ \\mathbf{X} \\mathbf{X}^{T} \\right] - \\mathbf{\\mu} \\mathbf{\\mu}^{T} \\end{align*} $$\n■\n[2] 2 $$ \\begin{align*} \\text{Cov} \\left( A \\mathbf{X} \\right) =\u0026amp; E \\left[ \\left( A\\mathbf{X} - A\\mathbf{\\mu} \\right) \\left( A\\mathbf{X} - A\\mathbf{\\mu} \\right)^{T} \\right] \\\\ =\u0026amp; E \\left[ A\\left(\\mathbf{X} -\\mathbf{\\mu} \\right) \\left( \\mathbf{X} - \\mathbf{\\mu} \\right)^{T} A^{T} \\right] \\\\ =\u0026amp; A E \\left[ \\left(\\mathbf{X} -\\mathbf{\\mu} \\right) \\left( \\mathbf{X} - \\mathbf{\\mu} \\right)^{T}\\right] A^{T} \\\\ =\u0026amp; A \\text{Cov}\\left( \\mathbf{X} \\right) A^{T} \\end{align*} $$\n■\nHogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p126.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://stats.stackexchange.com/a/106207/172321\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1950,"permalink":"https://freshrimpsushi.github.io/jp/posts/1950/","tags":null,"title":"共分散行列"},{"categories":"머신러닝","contents":"定義 強化学習とは、エージェントが環境と相互作用して累積報酬を最大化するポリシーを見つけることができるようにすることである。\n説明1 強化学習を構成する要素は次のとおりである。\nエージェントagent: 与えられた状態において、ポリシーに従って行動を決定する。 ステートstate, 状態: エージェントが置かれている状況を指す。 アクションaction, 行動: エージェントが与えられた状態で選ぶことができる選択肢を指す。 ポリシーpolicy, 方針: エージェントが与えられた状態で行動を決定する戦略を指す。 リワードreward, 報酬: エージェントが与えられた状態で選んだ行動によって得られる点数を指す。エージェントが達成すべき目標と見なすことができる。 環境environment: エージェントが与えられた状態でどのような行動を決定すれば、MDPに従って次の状態とそれに伴う報酬を決定するか。 エピソードepisode: エージェントと環境の相互作用が始まった時から終わるまでを指す。 これをさまざまな状況に例えると次のようになる。\n強化学習 試験勉強 囲碁 エージェント 学生 囲碁の棋士 ステート 試験まで残り日数 碁盤 アクション 勉強、飲酒、ゲームなど 着手 ポリシー 日付別勉強計画 戦略 リワード 試験点数 勝敗 エピソード 試験期間 一局 強化学習の問題：グリッドモデル 強化学習を説明するための代表的な例としてグリッドワールドgrid worldがある。これから次のグリッドモデルを例に各要素を具体的に説明する。一度に上下左右の4方向のうち一つに一マスずつ動けるロボットが下記のような$4 \\times 4$のグリッドで動く場合を考えてみよう。スタート地点は$\\boxed{\\ 2\\ }$から$\\boxed{15}$まで任意に決められ、ロボットが$\\fcolorbox{black}{darkgray}{\\ 1\\ }$または$\\fcolorbox{black}{darkgray}{16}$まで最短距離で行くことが目標とする。\nエージェント 強化学習におけるエージェントは学習する主体として説明されるが、実際には存在しない。後述する他の概念が確率変数などで定義されるのに対し、エージェントには明確な数学的定義がない。したがって、強化学習に関する理論的な勉強はエージェントという対象がなくても可能であり、実際にそうである。強化学習理論において本質的にエージェントを意味するのはポリシーである。しかし直感的には、学習する対象があると考える方が便利なため、「エージェントが行動する」「エージェントの状態が変わった」といった表現を用いる。エージェントは単にコンピュータシミュレーション（特にゲーム）においてキャラクターのように学習しているように見えるものに過ぎない。たとえば、グリッドモデルではエージェントが下の右側の図のように移動するのは、単純に状態の列挙で表すこともできる。 $$ \\boxed{\\ 3\\ } \\to \\boxed{\\ 2\\ } \\to \\fcolorbox{black}{darkgray}{\\ 1\\ } $$ $3, 2, 1$を順番にprintするだけでよい。強化学習の最終的に私たちが得たいのは本質的にポリシーであるため、エージェントというものを定義しなくても学習することができる。一言で言えば、エージェントはポリシーの視覚化（実現化）であると言える。\nもちろん、上記の話は理論やコンピュータシミュレーションでの話であり、自動運転のような実際の応用では、ポリシーに従って実際に動くドローンや自動車が必要である。この場合、ドローンや自動車などのロボットや機械がエージェントとなり、それがなければポリシーの学習は不可能である。\n状態 状態stateは確率変数であり、stateの頭文字をとって$S$と表記する。エピソードは時間に沿って順次進行するため、インデックスとして$t$を使用する。したがって、タイムステップが$t$のときのステート関数を$S_{t}$と表記する。初期ステートは通常$t=0$で表される。まとめると、$S_{t}$は時間が$t$のとき、各グリッドに対して次のような関数値を与える関数である。\n$$ S_{t} \\left( \\boxed{ N } \\right) = n,\\quad 1\\le n \\le 16 $$\nこのとき、可能なすべての状態値（状態関数の関数値）の集合を$\\mathcal{S}\\subset \\mathbb{R}$と表記し、その要素を$s$と表記する。\n$$ \\mathcal{S} = \\left\\{ s_{1}, s_{2},\\dots \\right\\} $$\nそれでは上記の格子モデルに対する状態関数は次のようになります。\n$$ S_{t} : \\left\\{ \\fcolorbox{black}{darkgray}{\\ 1\\ } , \\boxed{\\ 2\\ }, \\dots, \\boxed{15}, \\fcolorbox{black}{darkgray}{16} \\right\\} \\to \\mathcal{S} \\\\ S_{t} \\left( \\boxed{\\ n\\ } \\right) = s_{n} = n,\\quad 1\\le n \\le 16 $$\nそれでは時間が$t$のときの状態値が$s_{6}$から次のタイムステップで状態値が$s_{10}$に変わる確率は次のようになります。\n$$ P \\left( S_{t+1} = s_{10} | S_{t} = s_{6} \\right) $$\n到達した瞬間にエピソードが終了する状態をターミナルステートterminal stateと呼びます。上記の格子モデルではターミナルステートは$\\fcolorbox{black}{darkgray}{1}, \\fcolorbox{black}{darkgray}{16}$です。\n行動 行動actionとはエージェントが現在の状態で取ることができる選択肢のことであり、これもまた確率変数です。actionの頭文字を取って$A_{t}$と表記します。上記の格子モデルの例では、$\\boxed{2}$ ~ $\\boxed{15}$の各々で上下左右を選択することができます。可能な全ての行動値（行動関数の関数値）の集合を$\\mathcal{A}\\subset \\mathbb{R}$と表記し、その要素を$a$と表記します。\n$$ \\mathcal{A} = \\left\\{ a_{1}, a_{2}, \\dots \\right\\} $$\nそれではタイムステップ$t$での行動関数は次のようになります。\n$$ A_{t} : \\left\\{ \\uparrow, \\rightarrow, \\downarrow, \\leftarrow \\right\\} \\to \\mathcal{A} \\\\ \\begin{cases} A_{t}(\\uparrow) = a_{1} \\\\ A_{t}(\\rightarrow) = a_{2} \\\\ A_{t}(\\downarrow) = a_{3} \\\\ A_{t}(\\leftarrow) = a_{4} \\end{cases} $$\nエージェントは与えられた状態で確率に従って行動を決定します。例えばタイムステップが$t$のときの状態値が$s_{6}$で行動$a_{1}$を選択した確率は次のようになります。\n$$ P(A_{t} = a_{1} | S_{t} = s_{6}) $$\n方針 方針policyとは状態$s$で行動$a$を決定する確率を全ての$s$と$a$に対して明記したものを言い、$\\pi$で表記します。ゲームや戦争に例えると戦略です。格子モデルの例で行動を決定する確率が$\\dfrac{1}{4}$で全て同じだとすると、方針$\\pi$は次のようになります。\n$$ \\pi \\begin{cases} P(a_{1} | s_{2}) = \\dfrac{1}{4} \\\\ P(a_{2} | s_{2}) = \\dfrac{1}{4} \\\\ P(a_{3} | s_{2}) = \\dfrac{1}{4} \\\\ \\vdots \\\\ P(a_{2} | s_{15}) = \\dfrac{1}{4} \\\\ P(a_{3} | s_{15}) = \\dfrac{1}{4} \\\\ P(a_{4} | s_{15}) = \\dfrac{1}{4} \\end{cases} \\quad \\text{or} \\quad \\pi : \\mathcal{S} \\times \\mathcal{A} \\to [0,1] $$\nもちろんこれは最適化された方針ではありません。簡単に$\\boxed{2}$の場合だけ考えても、上に行くと格子の外に出てしまうため、上に行く確率自体が全く無い方がより良い方針です。したがって、下の図で$\\pi_{1}$よりも$\\pi_{2}$がより良い方針だと言えます。\n強化学習アルゴリズムの目標は最適な方針を見つけることです。では、最適な方針をどのように見つけるかというと、方針の良さを評価する価値関数value functionを通じて見つけることができます。\n報酬 報酬rewardとは、与えられた状態でエージェントが選択した行動に対して実数をマッピングする関数であり、rewardの頭文字を取って$R_{t}$と表記します。全ての報酬値（報酬関数の関数値）の集合を$\\mathcal{R} \\subset \\mathbb{R}$と表記し、その要素を$r$と表記します。\n$$ \\mathcal{R} = \\left\\{ r_{1}, r_{2}, \\dots \\right\\} \\\\ R_{t} = \\mathcal{S} \\times \\mathcal{A} \\to \\mathcal{R} $$\n報酬は一回のタイムステップごとに一回ずつ受け取り、一回のエピソードで受け取った総報酬、つまり蓄積された報酬が最も大きくなるような方針を見つけることが強化学習の究極的な目標です。\nでは、なぜ各タイムステップの報酬よりも蓄積された報酬が大きくなるようにするのか疑問に思うかもしれません。これは試験勉強に例えると簡単に理解できます。試験期間中に毎晩勉強する代わりにお酒を飲んだり遊んだりゲームをした場合、当面は勉強するよりも楽しいでしょう。しかし、蓄積された報酬、つまり試験の成績は散々なものになります。したがって、今は勉強することが疲れて大変だとしても、将来の大きな報酬のために勉強する方が良いと判断し、試験勉強をするわけです。\n報酬は人が設定するハイパーパラメータです。したがって、エージェントが行うべき仕事に応じて適切に設定する必要があります。例えば、格子モデルの例で格子が迷路であり、エージェントが迷路を脱出するロボットである場合、一マス移動するごとに$-1$の報酬、ターミナルステートに到達した場合は$+10$の報酬を与えるなどの設定ができます。格子が公園であり、エージェントがペットの散歩をするロボットである場合、一マス移動するごとに$0$の報酬、ターミナルステートに到達した場合は$+10$の報酬を与えるなどの設定ができます。\n環境 環境environmentとはエージェントが与えられた状態で選択した行動に応じて次の状態と報酬を決定する関数、すなわち$f : (s,a) \\mapsto (s^{\\prime},r)$です。したがって、常に現実にぴったりと当てはまる比喩を見つけるのは難しいです。\nタイムステップが$t$のときの状態を$s_{t}$、$s_{t}$で選択した行動を$a_{t}$とします。これにより、環境が決定した次の状態を$s_{t+1}$、報酬を$r_{t+1}$とすると次のように表されます。\n$$ f(s_{t}, a_{t}) = (s_{t+1}, r_{t+1}) $$\n格子モデルの例について具体的に説明すると、エージェントが$\\boxed{7}$で$\\uparrow$を選択し、環境が次の状態$\\boxed{3}$と報酬$-1$を決定した場合は、次のような数式で\n表されます。\n$$ f(s_{7}, a_{1}) = (s_{3}, -1) $$\nエージェントが行動を決定する戦略を方針と呼ぶならば、環境が次の状態と報酬を決定することをMDPmarkov decision process, マルコフ決定プロセスと言います。エージェントと環境の相互作用を図で表すと次のようになります。\nエピソード エージェントと環境が相互作用しながら決定された状態、行動、報酬の数列を経路trajectory, 軌跡または履歴historyと言います。経路が有限の場合をepisode taskと言います。上で例に挙げた試験期間、囲碁、格子モデルもこれに該当します。\n$$ s_{0}, a_{0}, s_{1}, r_{1}, a_{1}, s_{2}, r_{2}, \\dots, a_{T-1}, s_{T}, r_{T} \\\\ \\text{or} \\\\ (s_{0},) \\overset{a_{0}}{\\to} (s_{1}, r_{1}) \\overset{a_{1}}{\\to} (s_{2}, r_{2}) \\overset{a_{2}}{\\to} \\cdots \\overset{a_{T-1}}{\\to} (s_{T}, r_{T}) $$\n経路が無限の場合をcontinuing taskと言います。ただし、非常に長い時間にわたって続くエピソードは無限の場合とみなされることもあります。\n$$ s_{0}, a_{0}, s_{1}, r_{1}, a_{1}, s_{2}, r_{2}, \\dots, a_{t-1}, s_{t}, r_{t}, a_{t}, s_{t+1}, r_{t+1},\\dots \\\\ \\text{or} \\\\ (s_{0},) \\overset{a_{0}}{\\to} (s_{1}, r_{1}) \\overset{a_{1}}{\\to} (s_{2}, r_{2}) \\overset{a_{2}}{\\to} \\cdots \\overset{a_{t-1}}{\\to} (s_{t}, r_{t}) \\overset{a_{t}}{\\to} (s_{t+1}, r_{t+1}) \\overset{a_{t+1}}{\\to} \\cdots $$\nオ・イルソク, 機械学習(MACHINE LEARNING). 2017, p466-480\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3029,"permalink":"https://freshrimpsushi.github.io/jp/posts/3029/","tags":null,"title":"機械学習における強化学習とは？"},{"categories":"수리통계학","contents":"要約 1 $\\left\\{ X_{k} \\right\\}_{k=1}^{n}$がiid確率変数で、確率分布$\\left( \\mu, \\sigma^2 \\right) $に従うとき、$n \\to \\infty$で $$ \\sqrt{n} {{ \\overline{X}_n - \\mu } \\over {\\sigma}} \\overset{D}{\\to} N (0,1) $$\n$\\overset{D}{\\to}$は分布収束を意味する。 解説 統計学では、大数の法則と共に非常に有名な定理として挙げられる。頻繁に聞き、使われる定理だが、実際に証明するのは数理統計学を学ぶときぐらいだ。しかし、実際には利用度を超えて、証明自体が楽しいため、より価値のある定理だと言えるだろう。\n証明 戦略：モーメント生成関数とテイラーの定理を使ったトリックを使用する。\nまず、$\\displaystyle Y := \\sqrt{n} {{ \\overline{X}_{n} - \\mu } \\over { \\sigma }}$のモーメント生成関数$M(t) = E(e^{t Y}), -h\u0026lt;t\u0026lt;h$が存在すると仮定する。新しい関数$m(t) := E[e^{t(X-\\mu)}] = e^{-\\mu t} M(t)$を定義すると $$ \\begin{align*} M(t) =\u0026amp; E \\left( e^{ t \\sqrt{n} {{ \\overline{X}_n - \\mu } \\over {\\sigma}} } \\right) \\\\ =\u0026amp; E \\left( e^{ t {{ \\sum_{i=1}^{n} X_i - n \\mu } \\over {\\sigma \\sqrt{n} }} } \\right) \\\\ =\u0026amp; E \\left( e^{ t {{ X_1 - \\mu } \\over {\\sigma \\sqrt{n} }} } \\right) E \\left( e^{ t {{ X_2 - \\mu } \\over {\\sigma \\sqrt{n} }} } \\right) \\cdots E \\left( e^{ t {{ X_n - \\mu } \\over {\\sigma \\sqrt{n} }} } \\right) \\\\ =\u0026amp; E \\left( e^{ t {{ X - \\mu } \\over {\\sigma \\sqrt{n} }} } \\right) E \\left( e^{ t {{ X - \\mu } \\over {\\sigma \\sqrt{n} }} } \\right) \\cdots E \\left( e^{ t {{ X - \\mu } \\over {\\sigma \\sqrt{n} }} } \\right) \\\\ =\u0026amp; { \\left\\{ E \\left( e^{ t {{ X - \\mu } \\over {\\sigma \\sqrt{n} }} } \\right) \\right\\} }^n \\\\ =\u0026amp; { \\left\\{ m \\left( { {t} \\over {\\sigma \\sqrt{n} } } \\right) \\right\\} } ^{n} \\qquad , -h \u0026lt; { {t} \\over {\\sigma \\sqrt{n} } } \u0026lt; h \\end{align*} $$\nテイラーの定理：関数$f(x)$が$[a,b]$で連続であり、$(a,b)$で$n$回微分可能ならば、$x_{0} \\in (a,b)$に対して$\\displaystyle f(x) = \\sum_{k=0}^{n-1} {{( x - x_{0} )^{k}\\over{ k! }}{f^{(k)}( x_{0} )}} + {(x - x_{0} )^{n}\\over{ n! }}{f^{(n)}(\\xi)}$を満たす$\\xi \\in (a,b)$が存在する。\n$n=2$にテイラーの定理を適用すると、$\\xi$が$(-t,0)$または$(0,t)$の少なくとも一方を満たすことがわかる。 したがって、$m(t)$は $$ m(t) = m(0) + m ' (0)t + { {m '' (\\xi) t^2} \\over {2} } $$ と表せる。一方、 $$ \\begin{cases} m(0)=1 \\\\ m ' (0) = E(X-\\mu) = 0 \\\\ m '' (0) = E[(X-\\mu)^2] = {\\sigma}^2 \\end{cases} $$ であるため、$\\displaystyle m(t) = 1 + { {m '' (\\xi) t^2} \\over {2} }$である。ここでトリックが登場するが、右辺に$\\displaystyle {{\\sigma^2 t^2} \\over {2}}$を加えてから引くと $$ m(t) = 1 + { { \\sigma^2 t^2} \\over {2} } + { { [ m '' (\\xi) - \\sigma^2 ] t^2} \\over {2} } $$ つまり、 $$ M(t) = { \\left\\{ m \\left( { {t} \\over {\\sigma \\sqrt{n} } } \\right) \\right\\} } ^{n} = { \\left\\{ 1 + { { t^2} \\over {2n} } + { { [ m '' (\\xi) - \\sigma^2 ] t^2} \\over {2n \\sigma^2 } } \\right\\} } ^{n} $$\nテイラーの定理により、$\\xi$は$\\displaystyle \\left( -{ {t} \\over {\\sigma \\sqrt{n} } },0 \\right)$または$\\displaystyle \\left( 0,{ {t} \\over {\\sigma \\sqrt{n} } } \\right) $の間にあるため、$n \\to \\infty$のとき$\\xi \\to 0$で、したがって$ m '' (\\xi) \\to m '' (0) = \\sigma^2$である。そのようにして収束する項を除去すると\n$$ \\lim _{n \\to \\infty} M(t) = \\lim _{n \\to \\infty} \\left( 1 + { { t^2} \\over {2n} } \\right)^{n} = e^{t^2 / 2} $$\nここで、$e^{t^2 / 2}$は標準正規分布のモーメント生成関数であるため、\n$$ \\sqrt{n} {{ \\overline{X}_n - \\mu } \\over {\\sigma}} \\overset{D}{\\to} N (0,1) $$\n■\nHogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): 313~315.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":43,"permalink":"https://freshrimpsushi.github.io/jp/posts/43/","tags":null,"title":"中心極限定理の証明"},{"categories":"행렬대수","contents":"定義 $A$を正方実数行列としよう。$A$が下の式を満たす場合、直交行列orthogonal matrixという。\n$$ A^{-1} = A^{T} $$\nこの条件を別の形で表すと次のようになる。\n$$ AA^{T} = A^{T}A =I $$\n説明 定義を言葉で解くと、直交行列とは、それぞれの行ベクトルまたは列ベクトルが互いに直交する単位ベクトルである行列だ。複素数行列に拡張した場合は、ユニタリ行列と呼ばれる。直交行列の具体的な例には、回転行列がある。2次元平面上のベクトルを反時計回りに$\\theta$だけ回転させる変換は以下のようになる。\n$$ A = \\begin{bmatrix} \\cos \\theta \u0026amp; -\\sin \\theta \\\\ \\sin \\theta \u0026amp; \\cos \\theta \\end{bmatrix} $$\n下の式により、回転変換は任意の$\\theta$に対して直交行列であることがわかる。\n$$ A^{T} A = \\begin{bmatrix} \\cos \\theta \u0026amp; -\\sin \\theta \\\\ \\sin \\theta \u0026amp; \\cos \\theta \\end{bmatrix} \\begin{bmatrix} \\cos \\theta \u0026amp; \\sin \\theta \\\\ -\\sin \\theta \u0026amp; \\cos \\theta \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{bmatrix} = I $$\n性質 直交行列の転置も直交行列である。\n直交行列の逆行列は直交行列である。\n二つの直交行列の積は直交行列である。\n直交行列の行列式は$1$または$-1$である。\n$$ \\det(A)=\\pm 1 $$\n直交行列の同値条件 $n \\times n$実数行列$A$について、以下の命題はすべて同値である。\n$A$は直交行列である。\n$A$の行ベクトルの集合は$\\mathbb{R}^n$の正規直交集合である。\n$A$の列ベクトルの集合は$\\mathbb{R}^n$の正規直交集合である。\n$A$は内積を保存する。つまり、すべての$\\mathbf{x},\\mathbf{y}\\in \\mathbb{R}^{n}$に対して以下が成り立つ。\n$$ (A \\mathbf{x}) \\cdot (A\\mathbf{y}) = \\mathbf{x} \\cdot \\mathbf{y} $$\n$A$は長さを保存する。つまり、すべての$\\mathbf{x}\\in \\mathbb{R}^{n}$に対して以下が成り立つ。 $$ \\left\\| A \\mathbf{x} \\right\\| = \\left\\| \\mathbf{x} \\right\\| $$\n","id":3009,"permalink":"https://freshrimpsushi.github.io/jp/posts/3009/","tags":null,"title":"直交行列"},{"categories":"행렬대수","contents":"定義：二つの列ベクトルの内積1 大きさが$n \\times 1$の二つの列ベクトル$\\mathbf{u}$、$\\mathbf{v}$ $\\in \\mathbb{R}^{n}$の内積inner productを以下のように定義する。\n$$ \\begin{equation} \\mathbf{u} \\cdot \\mathbf{v} := \\mathbf{u}^{T}\\mathbf{v}=u_{1}v_{1} + u_{2}v_{2} + \\cdots + u_{n}v_{n} \\label{EuclideanIP} \\end{equation} $$\n$\\mathbf{u}$、$\\mathbf{v}$ $\\in \\mathbb{C}^{n}$の場合は、以下のようになる。\n$$ \\mathbf{u} \\cdot \\mathbf{v} := \\mathbf{u}^{\\ast}\\mathbf{v}=u^{\\ast}_{1}v_{1}^{\\ } + u_{2}^{\\ast}v_{2}^{\\ } + \\cdots + u_{n}^{\\ast}v_{n}^{\\ } $$\nここで、$\\mathbf{u}$は$\\mathbf{u}$の共役転置である。二つのベクトル$\\mathbf{u}$、$\\mathbf{v}$が次の方程式を満たす時、$\\mathbf{u}$と$\\mathbf{v}$が直交orthogonalすると言い、$\\mathbf{u} \\perp \\mathbf{v}$と表される。\n$$ \\mathbf{u} \\cdot \\mathbf{v} = 0 $$\n列ベクトル$\\mathbf{v}$のノルムnormまたは長さlengthを以下のように定義する。\n$$ \\left\\| \\mathbf{v} \\right\\| := \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}} $$\nノルムが$1$のベクトルを単位ベクトルunit vecterと呼ぶ。二つのベクトル$\\mathbf{u}$、$\\mathbf{v}$の距離を$d(\\mathbf{u}. \\mathbf{v})$として表し、以下のように定義する。\n$$ d(\\mathbf{u}, \\mathbf{v}) := \\left\\| \\mathbf{u} - \\mathbf{v} \\right\\| = \\sqrt{(\\mathbf{u}-\\mathbf{v}) \\cdot (\\mathbf{u}-\\mathbf{v})} = \\sqrt{(\\mathbf{u}-\\mathbf{v})^{\\ast} (\\mathbf{u}-\\mathbf{v})} $$\n説明 座標空間で二つのベクトルの内積は行列の積として再表現されたものと、複素数まで拡張したものに過ぎない。したがって、$\\eqref{EuclideanIP}$をユークリッド内積Euclidean inner productまたは標準内積standard inner productと呼ぶ。だから、内積の記法には$\\cdot$を使うこともあるが、一般的な内積の記法は以下のようである。\n$$ \\left\\langle \\mathbf{u}, \\mathbf{v} \\right\\rangle $$\n定義により、実数行列の場合は$\\mathbf{u} \\cdot \\mathbf{v} = \\mathbf{v} \\cdot \\mathbf{u}$が成立し、複素数行列の場合は$\\mathbf{u} \\cdot \\mathbf{v} = \\overline{\\mathbf{v} \\cdot \\mathbf{u}}$が成立する。\n内積の核心的な概念は「同じ成分同士を掛け合わせて全てを足す」であり、これを$n\\times n$行列に対して一般化すると以下のようになる。\n性質 $A$を$n\\times n$実数行列、$\\mathbf{u},\\mathbf{v}$を$n\\times 1$実数行列とする。すると、次の式が成立する。\n$$ \\begin{align*} A \\mathbf{u} \\cdot \\mathbf{v} \u0026amp;= \\mathbf{u} \\cdot A^{T} \\mathbf{v} \\\\ \\mathbf{u} \\cdot A \\mathbf{v} \u0026amp;= A^{T} \\mathbf{u} \\cdot \\mathbf{v} \\end{align*} $$\n複素数行列の場合、次の式が成立する。\n$$ \\begin{align*} A \\mathbf{u} \\cdot \\mathbf{v} \u0026amp;= \\mathbf{u} \\cdot A^{\\ast} \\mathbf{v} \\\\ \\mathbf{u} \\cdot A \\mathbf{v} \u0026amp;= A^{\\ast} \\mathbf{u} \\cdot \\mathbf{v} \\end{align*} $$\n証明 四つの式の証明方法は同じだから、最初の式の証明だけ紹介する。\n転置行列の性質により、以下が成立する。\n$$ \\begin{align*} A \\mathbf{u} \\cdot \\mathbf{v} \u0026amp;= \\left( A \\mathbf{u} \\right)^{T} \\mathbf{v} \\\\ \u0026amp;= \\left( \\mathbf{u}^{T} A^{T} \\right) \\mathbf{v} \\\\ \u0026amp;= \\mathbf{u}^{T} \\left( A^{T} \\mathbf{v} \\right) \\\\ \u0026amp;= \\mathbf{u} \\cdot A^{T} \\mathbf{v} \\end{align*} $$\n■\n参照 内積の一般的な定義 ノルムの一般的な定義 距離の一般的な定義 内積とノルムと距離の関係 Howard Anton, Elementary Linear Algebra: Aplications Version (12th Edition, 2019), p342\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3011,"permalink":"https://freshrimpsushi.github.io/jp/posts/3011/","tags":null,"title":"行列の内積"},{"categories":"행렬대수","contents":"定義 $A$をサイズが$m \\times n $の複素数行列とする。$\\overline{A}$を次のように定義して、$A$の共役行列conjugate matrixと呼ぶ。\n$$ \\overline{A} :=\\begin{bmatrix} \\overline{a_{11}} \u0026amp; \\overline{a_{12}} \u0026amp; \\cdots \u0026amp; \\overline{a_{1n}} \\\\ \\overline{a_{21}} \u0026amp; \\overline{a_{22}} \u0026amp; \\cdots \u0026amp; \\overline{a_{2n}} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\overline{a_{m1}} \u0026amp; \\overline{a_{m2}} \u0026amp; \\cdots \u0026amp; \\overline{a_{mn}} \\end{bmatrix} = \\left[ \\overline{a_{ij}} \\right] $$\nここで、$\\overline{a}$は$a$の共役複素数だ。つまり、各要素がある行列の要素の共役複素数である行列を共役行列という。$A$をサイズが$m\\times n$の複素数行列としよう。$A^{\\ast}$を次のように定義して、$A$の共役転置conjugate transposeと呼ぶ。\n$$ A^{\\ast} := \\overline{A^{T}} = \\left( \\overline{A} \\right) ^{T} $$\n説明 $A^{\\ast}$以外に使われる表記法としては、$A^{\\dagger}$、$A^{H}$がある。$A^{\\dagger}$は[エイダガー]と読み、$A^{H}$の$H$はエルミート行列から取られている。物理学、特に量子力学では、$A^{\\ast}$を共役行列の意味でのみ使うこともある。それゆえ、$A^{\\dagger}=(A^{\\ast})^{T}$として表記する。一方、数値線形代数などでは、逆行列ではないが逆行列のように振る舞う\u0026rsquo;擬似逆行列\u0026rsquo;の表記として$A^{\\dagger}$を使う。線形代数が非常に広く使われているため、このような記法問題は、その時に勉強している科目にしっかりと従って、注意深く対処するしかない。\n性質1 $A,B$を任意の複素数行列、$k\\in \\mathbb{C}$とする。\n(a) $\\overline{\\overline{A}}=A$\n(b) $\\overline{(AB)} = \\overline{A}\\ \\overline{B}$\n(c) $(A^{\\ast})^{\\ast}=A$\n(d) $\\left( A \\pm B\\right)^{\\ast} = A^{\\ast} \\pm B^{\\ast}$\n(e) $(kA)^{\\ast}=\\overline{k}A^{\\ast}$\n(f) $\\left( AB \\right)^{\\ast} = B^{\\ast} A^{\\ast}$\n証明 (a) (b) 共役複素数と行列乗算の定義により自明。\n■\n(c) (d) (e) (a)、転置行列の性質 $ \\left( A^{T} \\right) ^{T} = A $ 、行列の足し算の定義により成り立つ。\n■\n(f) (b)、転置行列の性質$\\left( AB \\right) ^{T} = B^{T} A^{T}$により成り立つ。\n■\nHoward Anton, Elementary Linear Algebra: Aplications Version (12th Edition, 2019), p437\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3006,"permalink":"https://freshrimpsushi.github.io/jp/posts/3006/","tags":null,"title":"共役転置行列"},{"categories":"복소해석","contents":"定義 開集合$A \\subset \\mathbb{C}$と$f: A \\to \\mathbb{C}$が定義されていて、$\\alpha \\in A$としよう。\n$\\displaystyle \\lim_{z \\to \\alpha } f(z) = f (\\alpha)$ならば、$f$は$\\alpha$で連続だといい、複素領域 $\\mathscr{R}$の全ての点で連続ならば、$f$は$\\mathscr{R}$上で連続だという。特に$f$が定義域上で連続ならば、連続関数と呼ばれる1。\n$\\alpha$での$f$の微分係数を以下のように定義し、$\\alpha$で微分係数が存在すれば、$f$は$\\alpha$で微分可能であるという。 $$ f ' (\\alpha) := \\lim_{h \\to 0} {{ f ( \\alpha + h ) - f ( \\alpha ) } \\over { h }} $$ ここで$h \\in \\mathbb{C}$とし、複素平面上のどの方向でも関係なくなければならない。\n$f$が複素領域 $\\mathscr{R}$の全ての点で微分可能ならば、$f$は$\\mathscr{R}$で解析的であるという。特に$f:\\mathbb{C} \\to \\mathbb{C}$が$\\mathbb{C}$で解析的ならば、全解析Entire関数という2。\n説明 実数集合$\\mathbb{R}$を定義域とする関数とは異なり、一般的に$\\mathbb{C}$を定義域とする関数も同じ幾何学的意味を持つわけではないが、形式的な定義上、複素解析における微分が微分と呼ばれる理由が全くないわけではない。もちろん、複素平面として$\\mathbb{C} \\simeq \\mathbb{R}^{2}$を考えるならば、やはり傾きと似た意味で見ることができる。 解析的関数は、正則関数Regular Function、ホロモーフィック関数Holomorphic Functionとも呼ばれる。しかし、解析的連続の条件として使われる点で、解析的関数という表現が最もメジャーだ。\u0026ldquo;なぜこれを微分可能な関数ではなく、わざわざ解析的関数という言葉を作って呼ぶのか？\u0026ldquo;については、複素解析が発展した当時の視点が入っていると見ることができるだろう。言及したように、複素平面での微分というのは、形式的な定義であって、我々が実数空間$\\mathbb{R}$で扱ったように考えるべきではない、という意味だったのではないかと思われる。 Osborne (1999). Complex variables and their applications: p39.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOsborne (1999). Complex variables and their applications: p50.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1929,"permalink":"https://freshrimpsushi.github.io/jp/posts/1929/","tags":null,"title":"解析関数"},{"categories":"행렬대수","contents":"定義1 任意の正方行列$A$が次の式を満たすとき、$A$を対称行列symmetric matrixと呼ぶ。\n$$ A=A^{T} $$\nこのとき、$A^{T}$は$A$の転置行列である。$A$が次の式を満たすとき、$A$を反対称行列anti-symmetric matrixと呼ぶ。\n$$ A =-A^{T} $$\n説明 転置行列の定義により、正方行列ではない行列は対称行列、反対称行列にはなれない。$A$が反対称行列なら、定義により$a_{ii}=-a_{ii}$となるので、対角成分は必ず$0$である。\n性質 $A$、$B$が同じ大きさの対称行列で、$k$を任意の定数とする。\n(a) $A^{T}$は対称行列である。\n(b) $A \\pm B$は対称行列である。\n(c) $kA$は対称行列である。\n(d) $A$が可逆であれば、$A^{-1}$も対称行列である。\n(e) $A$を$m \\times n$行列とする。すると、$AA^{T}$は$m \\times m$対称行列であり、$A^{T}A$は$n \\times n$対称行列である。\n(f) $A$が可逆であれば、$A^{T}A$と$AA^{T}$も可逆である。\n証明 (d) $A$が可逆行列だとする。すると$(A^{T})^{-1} = (A^{-1})^{T}$が適用されるので、$A^{-1}$も対称行列である。\n■\n(e) $A$を$m \\times n$行列とする。すると$AA^{T}$のサイズは$(m \\times \\cancel{n} ) \\times (\\cancel{n} \\times m) = m \\times m$であり、転置行列の性質により以下が成立する。\n$$ (AA^{T})^{T}=AA^{T} $$\nしたがって、$AA^{T}$は対称行列である。$A^{T}A$についても証明は同じである。\n■\n(f) 可逆行列の性質により、$A$が可逆であれば、$A^{T}$も可逆であり、可逆行列の積は可逆なので、$AA^{T}$、$A^{T}A$も可逆である。\n■\n定理 二つの行列の積が対称行列であるための必要十分条件は、二つの行列の積が交換可能であることである。\n二つの行列の積二つの行列の積は一般に交換可能ではないことを心に留めておく。\nHoward Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), p72-74\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3005,"permalink":"https://freshrimpsushi.github.io/jp/posts/3005/","tags":null,"title":"対称行列、歪対称行列"},{"categories":"줄리아","contents":"コード Juliaで色を取り扱うために提供されるパッケージはColors.jlだ。可視化パッケージのPlots.jlを読み込めば、Colors.jl内の機能も一緒に使える。RGB空間を表す色コードには、RGB、BGR、RGB24、RGBX、XRGBがサポートされており、これらはAbstractRGBのサブタイプだ。RGBAはRGBに透明度が加わったものだ。\njulia\u0026gt; using Plots\rjulia\u0026gt; subtypes(AbstractRGB)\r5-element Vector{Any}:\rBGR\rRGB\rRGB24\rRGBX\rXRGB\rjulia\u0026gt; subtypes(AbstractRGBA)\r2-element Vector{Any}:\rBGRA\rRGBA 文字列 plot()関数の色を指定するキーワードに文字列として\u0026quot;#FF0000\u0026quot;のように入力すると、16進RGBコードであるHEXコードを使える。下を見ればわかるが、文字列を入力しても良い理由は、plot()が文字列を自動でパースしてくれるからと見られる。\nusing Plots\rr = \u0026#34;#FF0000\u0026#34; # R빨간색 RGB(255, 0, 0)의 6자리 HEX 코드\rg = \u0026#34;#00FF0033\u0026#34; # 투명도가 0.2인 초록색 RGBA(0, 255, 0, 0.2)의 8자리 HEX 코드\rp = \u0026#34;#80F\u0026#34; # 보라색 RGB(255, 0, 136)의 3자리 HEX 코드\rplot([1 2 3; 2 3 4], ones(2, 3), fillrange = 2,\rfillcolor = [r g p],\rlabel = [r g p]) パーシング colorant\u0026quot;#FF0000\u0026quot;のような形でHEXコードをパースできる。\njulia\u0026gt; r = colorant\u0026#34;#FF0000\u0026#34; # R빨간색 RGB(255, 0, 0)의 6자리 HEX 코드\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; g = colorant\u0026#34;#00FF0033\u0026#34; # 투명도가 0.2인 초록색 RGBA(0, 255, 0, 0.2)의 8자리 HEX 코드\rRGBA{N0f8}(0.0,1.0,0.0,0.2)\rjulia\u0026gt; p = colorant\u0026#34;#80F\u0026#34; # 보라색 RGB(255, 0, 136)의 3자리 HEX 코드\rRGB{N0f8}(0.533,0.0,1.0)\rjulia\u0026gt; plot([1 2 3; 2 3 4], ones(2, 3), fillrange = 2,\rfillcolor = [r g p]) parse(RGB, \u0026quot;#FF0000\u0026quot;)のようにパースすることもできる。\njulia\u0026gt; parse(RGB, \u0026#34;#FF0000\u0026#34;)\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; parse(RGBA, \u0026#34;#FF000080\u0026#34;)\rRGBA{N0f8}(1.0,0.0,0.0,0.502) 色名を取得 hex()関数は色のHEXコードを文字列で返す。\njulia\u0026gt; hex(colorant\u0026#34;red\u0026#34;)\r\u0026#34;FF0000\u0026#34;\rjulia\u0026gt; hex(colorant\u0026#34;rgb(0, 255, 128)\u0026#34;)\r\u0026#34;00FF80\u0026#34;\rjulia\u0026gt; hex(RGBA(1, 0.5, 0, 0.5), :RRGGBB)\r\u0026#34;FF8000\u0026#34;\rjulia\u0026gt; hex(RGBA(1, 0.5, 0, 0.5), :RRGGBBAA)\r\u0026#34;FF800080\u0026#34;\rjulia\u0026gt; hex(HSV(30,1.0,1.0), :AARRGGBB)\r\u0026#34;FFFF8000\u0026#34; 併せて見る Plotsで色を使う方法 RGB色コードを使う方法 HEX色コードを使う方法 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0, Colors v0.12.10, FixedPointNumbers v0.8.4 併せて見る 色を使う方法 パレットを使う方法 カラーグラデーションを使う方法 色処理のためのパッケージ Colors.jl RGBコードを使う方法 RGB(1, 0, 0) HEXコードを使う方法 \u0026quot;#000000\u0026quot; グラフ要素の色を指定する方法 サブプロットごとにグラフの色を指定する方法 軸、軸名、目盛り、目盛り値の色を指定する方法 背景色を指定する方法 ","id":1921,"permalink":"https://freshrimpsushi.github.io/jp/posts/1921/","tags":null,"title":"ジュリアで16進数RGBコード（HEX）を使用する方法"},{"categories":"확률분포론","contents":"要約 $T_n \\sim t(n)$ ならば $$ T_n \\ \\overset{D}{\\to} N(0,1) $$\n$N \\left( \\mu , \\sigma^{2} \\right)$ は平均が $\\mu$ で分散が $\\sigma^{2}$ の正規分布だ。 $t(r)$ は自由度 $r$ のt-分布だ。 $\\overset{D}{\\to}$ はそれぞれ分布収束を意味する。 もともと学生のt-分布は、サンプルサイズが小さい時に統計的分析をするために生まれた。サンプルサイズが大きくなると、標準正規分布と似てくるが、統計学的な用語では分布収束すると言う。だから、特別な過程がなくても、ただサンプルが多ければ標準正規分布が導かれる。\n導出 t-分布の定義: 自由度 $\\nu \u0026gt; 0$ に対する次のような確率密度関数をもつ連続確率分布 $t \\left( \\nu \\right)$ をt-分布と言う。 $$ f(x) = {{ \\Gamma \\left( {{ \\nu + 1 } \\over { 2 }} \\right) } \\over { \\sqrt{\\nu \\pi} \\Gamma \\left( {{ \\nu } \\over { 2 }} \\right) }} \\left( 1 + {{ x^{2} } \\over { \\nu }} \\right)^{- {{ \\nu + 1 } \\over { 2 }}} \\qquad ,x \\in \\mathbb{R} $$\n標準正規分布の定義: 次のような確率密度関数をもつ正規分布 $N \\left( 0,1^{2} \\right)$ を標準正規分布と言う。 $$ f(z) = {{ 1 } \\over { \\sqrt{2 \\pi} }} \\exp \\left[ - {{ z^{2} } \\over { 2 }} \\right] $$\n$$ F_n(t) = \\int_{-\\infty}^{t} {{\\Gamma ( (n+1)/2 ) } \\over { \\sqrt{\\pi n} \\Gamma (n/2) }} { {1} \\over {(1 + y^{2} / n)^{(n+1)/2} } } dy $$ $T_n$ の累積分布関数は上記の通りである。$F_{n}$ の連続性により $$ \\lim_{n \\to \\infty} F_n (t) = \\lim_{n \\to \\infty} \\int_{-\\infty}^{t} f_n (y) dy = \\int_{-\\infty}^{t} \\lim_{n \\to \\infty} f_n (y) dy $$ $\\Gamma (1/2) = \\sqrt{\\pi} $ なので $\\displaystyle |f_n (y)| \\le 2 f_1 (y) = { {1} \\over {\\pi} } { {2} \\over {1 + y^2 } }$ であり、アークタンジェント関数の微分法によると $$ \\displaystyle\\lim_{n \\to \\infty} \\int_{-\\infty}^{t} f_n (y) dy\u0026lt; \\int_{-\\infty}^{t} 2 f_1 (y) dy = { {2} \\over {\\pi} } \\tan ^{-1} t \u0026lt; \\infty $$ 今、$\\displaystyle \\lim_{n \\to \\infty} f_n (y)$ が具体的にどこに収束するかを示せばいい。まず、$f_n$ を次のように分けてみよう。 $$ \\begin{align*} f_{n} (y) =\u0026amp; {{\\Gamma ( (n+1)/2 ) } \\over { \\sqrt{\\pi n} \\Gamma (n/2) }} { {1} \\over {(1 + y^{2} / n)^{(n+1)/2} } } \\\\ =\u0026amp; {{\\Gamma ( (n+1)/2 ) } \\over { \\sqrt{ n/2} \\Gamma (n/2) }} \\cdot { {1} \\over { \\sqrt{2 \\pi} (1 + y^{2} / n)^{(n+1)/2} } } \\\\ =\u0026amp; {{\\Gamma ( (n+1)/2 ) } \\over { \\sqrt{ n/2} \\Gamma (n/2) }} \\cdot { {1} \\over {(1 + y^{2} / n)^{1/2} } } \\cdot { {1} \\over {\\sqrt{2 \\pi }} } \\left( 1 + { {y^{2}} \\over {n} } \\right) ^{-n/2} \\end{align*} $$\nスターリング近似: $$ \\lim_{n \\to \\infty} {{n!} \\over {e^{n \\ln n - n} \\sqrt{ 2 \\pi n} }} = 1 $$\n最初の因子の極限から求めよう。\n1 スターリング近似により、十分に大きな $n \\in \\mathbb{N}$ に対して $$ \\Gamma (n) \\approx e^{n \\ln n - n } \\sqrt{ 2 \\pi n} = \\left( {{ n } \\over { e }} \\right)^{n} \\sqrt { 2 \\pi n } $$ とすると、十分に大きな $n$ に対して $$ \\begin{align*} {{ {\\Gamma ( (n+1)/2 ) } } \\over { { \\sqrt{ n / 2 } \\Gamma (n/2) } }} \\approx\u0026amp; \\sqrt{ {{ 2 } \\over { n }} } {{ \\left( {{ n+1 } \\over { 2e }} \\right)^{{{ n+1 } \\over { 2 }}} \\sqrt{ 2 \\pi (n+1)} } \\over { \\cdot \\left( {{ n } \\over { 2e }} \\right)^{{{ n } \\over { 2 }}} \\sqrt{ 2 \\pi n} }} \\\\ \\approx\u0026amp; \\sqrt{ {{ 2 } \\over { n }} } \\sqrt{ {{ n+1 } \\over { 2e }} } \\left( {{ n+1 } \\over { n }} \\right)^{n/2} \\sqrt{ {{ n+1 } \\over { n }} } \\\\ \\approx\u0026amp; \\sqrt{ {{ 1 } \\over { e }}} \\left( 1 + {{ 1 } \\over { n }} \\right)^{n/2} \\end{align*} $$ なので $$ \\lim_{n \\to \\infty} {{ {\\Gamma ( (n+1)/2 ) } } \\over { { \\sqrt{ n / 2 } \\Gamma (n/2) } }} = 1 $$ で、二番目の因子は $$ \\lim_{n \\to \\infty} { {1} \\over {(1 + y^{2} / n)^{1/2} } } =1 $$ で、三番目の因子は $$ \\lim_{n \\to \\infty} { {1} \\over {\\sqrt{2 \\pi }} } \\left( 1 + { {y^{2}} \\over {n} } \\right) ^{-n/2} = { {1} \\over {\\sqrt{2 \\pi }} } e ^{- y^{2} / 2} $$ だ。すなわち、 $$ F_n(t) = \\int_{-\\infty}^{t} { {1} \\over {\\sqrt{2 \\pi }} } e ^{- y^{2} / 2} dy $$ したがって、$T_n$ は標準正規分布に従う確率変数に分布収束する。\n■\nhttps://math.stackexchange.com/a/3240565/459895\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":195,"permalink":"https://freshrimpsushi.github.io/jp/posts/195/","tags":null,"title":"スチューデントのt分布の極限分布としての標準正規分布の導出"},{"categories":"줄리아","contents":"環境 OS: Windows11 バージョン: Julia 1.9.0, DataFrames v1.5.0 ","id":1930,"permalink":"https://freshrimpsushi.github.io/jp/posts/1930/","tags":null,"title":"ジュリアでのデータフレームと2次元配列間の変換方法"},{"categories":"확률분포론","contents":"まとめ $X_{n} \\sim \\text{Poi} \\left( n \\right)$ かつ $\\displaystyle Y_{n} := {{ X_{n} - n } \\over { \\sqrt{n} }}$ ならば $$ Y_{n} \\overset{D}{\\to} N(0,1) $$\n$N \\left( \\mu , \\sigma^{2} \\right)$ は平均が $\\mu$ で分散が $\\sigma^{2}$ の正規分布だ。 $\\text{Poi} (\\lambda)$ は平均と分散が $\\lambda$ のポアソン分布だ。 説明 二項分布のポアソン分布近似を考えると当然のように、ポアソン分布から標準正規分布も導出できる。\n導出1 $Y_{n}$ のモーメント生成関数 $M_{Y_{n}} (t)$ を通して分布収束を示す。\nポアソン分布のモーメント生成関数: $$ m(t) = \\exp \\left[ \\lambda \\left( e^{t} - 1 \\right) \\right] \\qquad , t \\in \\mathbb{R} $$\n$X_{n} \\sim \\text{Poi} (n)$ なので、 $$ \\begin{align*} M_Y (t) =\u0026amp; E \\left[ \\text{exp} \\left( Y_{n} t \\right) \\right] \\\\ =\u0026amp; E \\left[ \\text{exp} \\left( {{ X_{n} - n } \\over { \\sqrt{n} }} t \\right) \\right] \\\\ =\u0026amp; E \\left[ \\text{exp} \\left( {{ X_{n} } \\over { \\sqrt{n} }} t \\right) \\text{exp} ( -t \\sqrt{n} ) \\right] \\\\ =\u0026amp; \\text{exp} ( -t \\sqrt{n} ) E \\left[ \\text{exp} \\left( X_{n} {{ t } \\over { \\sqrt{n} }} \\right) \\right] \\\\ =\u0026amp; \\text{exp} ( -t \\sqrt{n} ) \\exp \\left( n \\left( e^{t/\\sqrt{n}} - 1 \\right) \\right) \\end{align*} $$ 二番目の引数のテイラー展開を通じて、 $$ \\begin{align*} \u0026amp; \\text{exp} \\left( -t \\sqrt{n} + n \\left( 1 + {{t} \\over {\\sqrt{n}}} + {{1} \\over {2!}} {{t^2} \\over {n}} + {{1} \\over {3!}} {{t^3} \\over {n \\sqrt{n} }} + \\cdots - 1 \\right) \\right) \\\\ =\u0026amp; \\text{exp} \\left( -t \\sqrt{n} + n \\left( {{t} \\over {\\sqrt{n}}} + {{1} \\over {2!}} {{t^2} \\over {n}} + {{1} \\over {3!}} {{t^3} \\over {n \\sqrt{n} }} + \\cdots \\right) \\right) \\\\ =\u0026amp; \\text{exp} \\left( -t \\sqrt{n} + t \\sqrt{n} + {{t^2} \\over {2!}} + {{1} \\over {3!}} {{t^3} \\over { \\sqrt{n} }} + \\cdots \\right) \\\\ =\u0026amp; \\text{exp} \\left( {{t^2} \\over {2!}} + {{1} \\over {3!}} {{t^3} \\over { \\sqrt{n} }} + \\cdots \\right) \\end{align*} $$ 従って、 $$ \\lim_{n \\to \\infty} M_{Y_{n}} = e^{ t^2 \\over 2 } $$\n■\nhttp://www.math.wm.edu/~leemis/chart/UDR/PDFs/PoissonNormal.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":197,"permalink":"https://freshrimpsushi.github.io/jp/posts/197/","tags":null,"title":"ポアソン分布の極限分布としての標準正規分布の導出"},{"categories":"행렬대수","contents":"まとめ1 $A$を大きさ$n\\times n$の正方行列とする。すると、下記の命題は全て同値である。\n(a) $A$は可逆行列である。\n(b) 同次線形システム $A\\mathbf{x}=\\mathbf{0}$は唯一の自明な解を持つ。\n(c) $A$の簡約行階段形が$I_{n}$である。\n(d) $A$は基本行列の積で表せる。\n(e) $A\\mathbf{x}=\\mathbf{b}$は全ての $n\\times 1$行列$\\mathbf{b}$に対して解を持つ。\n(f) $A\\mathbf{x}=\\mathbf{b}$は全ての $n\\times 1$行列$\\mathbf{b}$に対してただ１つの解を持つ。すなわち$\\mathbf{x}=A^{-1}\\mathbf{b}$が成立する。\n(g) $\\det (A) \\ne 0$\n(h) $A$の列ベクトルは線形独立である。\n(i) $A$の行ベクトルは線形独立である。\n(j) $A$の列ベクトルは$\\mathbb{R}^{n}$を生成する。\n(k) $A$の行ベクトルは$\\mathbb{R}^{n}$を生成する。\n(l) $A$の列ベクトルは$\\mathbb{R}^{n}$の基底である。\n(m) $A$の行ベクトルは$\\mathbb{R}^{n}$の基底である。\n(n) $A$のランクは$n$である。\n(o) $A$の無効次元は$0$である。\n(p) $A$の零空間の直交補空間は$\\mathbb{R}^{n}$である。\n(q) $A$の行空間の直交補空間は$\\mathbb{R}^{n}$である。\n(r) $A$の固有値に$0$は存在しない。\n(s) $A^{T}A$は可逆である。\n(t) $T_{A}$の核は$\\left\\{ \\mathbf{0} \\right\\}$である。\n(u) $T_{A}$の値域は$\\mathbb{R}^{n}$である。\n(v) $T_{A}$は単射である。\n証明 (a) $\\iff$ (b) $\\iff$ (c) $\\iff$ (d) (a) $\\iff$ (e) $\\iff$ (f) Howard Anton, 初等線形代数 アプリケーションバージョン (12版, 2019), p463\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3004,"permalink":"https://freshrimpsushi.github.io/jp/posts/3004/","tags":null,"title":"正則行列であるための同値条件"},{"categories":"줄리아","contents":"ガイド 旧バージョン julia v1.5.0では、*.csvファイルを以下のように読み込んだ。 実際、Juliaはまだデータ入力に特別便利な言語ではない。しかし、速さを求めるならば、PythonやR、MatlabよりもJuliaを選ばなければならない時が来るかもしれない。例えば、Eドライブ直下にある*.csvファイルを読み込みたい場合、次のように入力すればいい。\nusing CSV\rdata = CSV.read(\u0026#34;E:/example.csv\u0026#34;) 実行結果を見ると、*.csvファイルがデータフレームとしてうまく読み込まれたことが確認できる。\n新バージョン 正確な時期はわからないが、julia v1.7.0以降では、データフレームを別途読み込む必要がある。 using CSV, DataFrames\rdata = CSV.read(\u0026#34;E:/example.csv\u0026#34;, DataFrame) 環境 OS: Windows ","id":1923,"permalink":"https://freshrimpsushi.github.io/jp/posts/1923/","tags":null,"title":"ジュリアで *.csvファイルを読み込む方法"},{"categories":"줄리아","contents":"ガイド Juliaでは並列計算が日常的に使用されるため、場合によってはコンピュータの全リソースを計算に集中させる必要がある。スレッド数を変更する方法はいくつかあるが、最もスタティックで便利な方法は環境変数を編集することだ。\nステップ1. システム環境変数の編集\nWindowsキーまたはWindows+Sを押して「システム環境変数の編集」を探す。\nシステムプロパティというウィンドウが表示されたら、「環境変数」をクリックする。\nステップ2. JULIA_NUM_THREADS を探す\nユーザー変数で上記のような変数を探す。この値がスレッドの数だ。存在する場合は「編集」を、存在しない場合は「新規作成」を選択して ステップ3. に進む。\nステップ3. 変数の値を変更\n上のスクリーンショットで示された部分に、希望するスレッド数を記入する。適切なスレッド数はコンピュータのスペックによって異なるが、私たちは例を扱っているので、$5$個に変更してみよう。\nステップ4. 確認\nusing Base.Threads\rnthreads() 上のコードをJuliaコンソールで実行して確認しよう。\n反映されない場合はまず再起動を試みて、それでもだめならシステム変数で変更を試みよう。\n環境 OS: Windows julia: v1.5.0 ","id":1933,"permalink":"https://freshrimpsushi.github.io/jp/posts/1933/","tags":null,"title":"WindowsでJuliaの並列計算に使用するスレッド数を変更する方法"},{"categories":"동역학","contents":"モデル $$ \\dot{N} = {{ r } \\over { K }} N ( K - N) $$\n変数 $N(t)$：$t$時点での集団の個体数を表す。 パラメータ $r \\in \\mathbb{R}$：固有増加率Intrinsic Rate of Increaseとして、$0$より大きければ成長し、$0$より小さければ衰退する。出生率Birth Rate $b$ と死亡率Death Rate $d$ の差 $r:=b-d$ としても定義される。 $K$：環境容量Carrying Capacityとして、集団を収容できる環境の大きさを描写する。 説明 この微分方程式は、ロジスティック方程式またはベルフルスト方程式Verhulst\u0026rsquo;s Equationとも呼ばれる。\nロジスティック成長モデルは、マルサスの成長モデルで個体数が現実にはありえないほど増加するだけのことを補うために考案された。個体数が多いほど人口成長にペナルティを与えることで、いわば方程式にマルサスの罠Malthusian Trapを添加するようなものだ。個体数 $N$ が環境容量 $K$ を超えると $(K-N) \u0026lt; 0$ になるため、個体数の変化量 $n '$ がマイナスになり、人口がむしろ減少することになる。もちろん、このアイデアが単に思いつきで導入されたわけではなく、どのようにペナルティを与えるかの導出過程を直接見てみよう。\n導出 マルサスの成長モデル: $$ \\dot{N} = r N $$\nこのような単純なモデルでは、個々の個体の平均的な成長率は、両辺を $N$ で割ることで得られる。指数成長モデルでは、集団の大きさに関係なく成長率が一定なので、次のように一定となる。\n$$ {{ \\dot{N} } \\over { N }} = r $$\nここで右辺を修正しよう。個体数が多いほど成長率が減少するようにしたいが、どのように減少するかはすぐには明確な答えがないので、最もシンプルに直線的に減少すると仮定する。つまり、個々の成長率が定数関数ではなく直線を描くことになる。\n図に示されているように、右辺を修正すると、$N$ 軸の切片が $K$ であり、$N\u0026rsquo;/N$ 軸の切片が $r$ の直線の方程式になるようにすると\n$$ {{ \\dot{N} } \\over { N }} = {{ r } \\over { K }} ( K - N) $$\n両辺に $N$ を掛けると次の結果が得られる。\n$$ \\dot{N} = {{ r } \\over { K }} N ( K - N) $$\n■\n固定点 ロジスティック成長モデルは、二つの固定点 $N=0, N=K$ を持ち、特に $N=K$ はリャプノフ安定である1。\n視覚的理解 ロジスティック成長モデルを格子ベースのシミュレーションで実装し、視覚的に理解してみよう。\nアクション （全てのセルで、毎ターン毎に）\n計算：各セルを基準に上下左右にどれだけの成分があるか数える。$i$行 $j$列で計算された数を$n_{ij}$とする。 拡散：各セルは、$1-(1-\\beta)^{n_{ij}}$の確率で成分を含むようになる。 格子空間での拡散という単純なアクションのみを使用しても、格子空間自体が有限であるため、自然に成長に制限がかかることがわかる。\n次に、理論的な成長推移と比較したものである。\n見ての通り、シミュレーションと理論が正確に一致はしない。曲線を描くというよりは、直線的に成長し、初期の成長があまりに急である。しかし、いずれにしても環境容量を満たしながら折れ曲がる部分に注意することが重要だ。これを実生活に当てはめれば、ある培養皿で細菌が増殖していくことに例えられる。どんなに成長を続けたいと思っても、培養皿から溢れるほど無限に成長することはできない。\n理論とシミュレーションが正確に一致しない理由は、このシミュレーションが十分に洗練されておらず、モデルもあまりにも単純だからである。あるいは、単にシミュレーションで運が悪かっただけかもしれないが、格子が与える影響を絶対に無視できない。ただし、この場合は単に運のせいではない。\n実際に理論的な推移を比較してみると、このポストで紹介された視覚化は、むしろゴンペルツ成長モデルに近いと思われる。実際に格子空間で腫瘍が成長すると考えると、体積に対する表面積の比が小さく見え、細胞自体は増えているが、実際に腫瘍の大きさを増やせる細胞が徐々に減っていく現象が起こっている。\nコード 以下は、GIFを作成するためのJuliaのコードである。\ncd(@__DIR__) # 파일 저장 경로\r@time using Plots\r@time using Random\r@time using DifferentialEquations\rrow_size = 20\rcolumn_size = 20\rβ = 0.1 # 번식률\rγ = 0.1 # 사망률\r#---\rK = row_size*column_size\rfunction logistic_growth!(du,u,p,t)\rN = u[1]\rr = p\rdu[1] = dN = r/K*N*(K-N)\rend\ru0 = [1.0]\rp = 0.8\rtspan = (0.,18)\rprob = ODEProblem(logistic_growth!,u0,tspan,p)\rsol = solve(prob; saveat=(0.0:0.1:18))\rcompare = plot(sol,vars=(0,1),\rlinestyle = :dash, color = :black,\rsize = (400,300), label = \u0026#34;Theoretical\u0026#34;, legend=:topleft)\r#---\rmax_iteration = 180\rRandom.seed!(0);\rtime_evolution = Int64[]\rstage_lattice = zeros(Int64, row_size, column_size)\rstage_lattice[rand(1:row_size), rand(1:column_size)] = 1\rlet colormap_SI = cgrad([colorant\u0026#34;#EEEEEE\u0026#34;, colorant\u0026#34;#111111\u0026#34;])\ranim1 = @animate for t = (0:max_iteration)/10\rheatmap(reverse(stage_lattice,dims=1), color=colormap_SI,\rxaxis=false,yaxis=false,axis=nothing, size = [400,400], legend = false)\rI_lattice = (stage_lattice .== 1)\rcount_lattice =\rvcat(I_lattice[2:end, :], zeros(Int64, 1, column_size)) +\rvcat(zeros(Int64, 1, column_size), I_lattice[1:end-1, :]) +\rhcat(I_lattice[:, 2:end], zeros(Int64, row_size, 1)) +\rhcat(zeros(Int64, column_size, 1), I_lattice[:, 1:end-1])\rprobability_lattice = 1 .- (1-β).^count_lattice\rhit_lattice = probability_lattice .\u0026gt; rand(row_size, column_size)\rstage_lattice[hit_lattice] .= 1\rif sum(stage_lattice) ≥ row_size*column_size\rcolormap_SI = cgrad([colorant\u0026#34;#111111\u0026#34;, colorant\u0026#34;#EEEEEE\u0026#34;])\rend\rpush!(time_evolution, sum(stage_lattice))\rend; gif(anim1, \u0026#34;logistic_growth_lattice.gif\u0026#34;, fps = 12)\rend\ranim2 = @animate for t = 1:max_iteration\rcompare = plot(sol,vars=(0,1),\rlinestyle = :dash, color = :black,\rlabel = \u0026#34;Theoretical\u0026#34;, legend=:bottomright)\rplot!(compare, 0.1:0.1:(t/10), time_evolution[1:t],\rcolor = colorant\u0026#34;#111111\u0026#34;, linewidth = 2, label = \u0026#34;Simulation\u0026#34;,\rsize = [400, 300])\rend; gif(anim2, \u0026#34;logistic_growth_time_evolution.gif\u0026#34;, fps = 12) 導出で使用された図を見ると、固定点の左側では個体数が増加するので右に移動し、右側では個体数が減少するので左に移動することがわかる。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1915,"permalink":"https://freshrimpsushi.github.io/jp/posts/1915/","tags":null,"title":"ロジスティック成長モデル：集団成長の限界"},{"categories":"행렬대수","contents":"定義 $A$をサイズ$n\\times n$の任意の正方行列としよう。$A$と行列の積が可能で、以下の式を満たす行列$L$を$A$の左逆行列という。\n$$ LA=I_{n} $$\nここで$I_{n}$はサイズ$n\\times n$の単位行列である。$A$と行列の積が可能で、以下の式を満たす行列$R$を$A$の右逆行列という。\n$$ AR=I_{n} $$\n$A$が左/右逆行列を両方持っていれば、これらは互いに等しく$A^{-1}$と表記され、$A$の逆行列という。\n$$ A^{-1}A=I_{n}=AA^{-1} $$\n$A$が逆行列を持つ場合、$A$を可逆行列または非特異行列という。$A$が逆行列を持たない場合、$A$を特異行列という。\n説明 定義により、$LA$のサイズが$n\\times n$でなければならないので、$L$は必ず$n \\times n$行列でなければならず、$R$も同様である。$A$を正方行列に限定した理由は、$A^{-1}$が$A$の両側から掛けられる必要があるためである。同様に行列の積は交換可能ではないため、左/右逆行列が両方存在する必要がある。実際に、任意の行列が左/右逆行列を持つ場合、これらは常に同じである。\n性質 $A$と$B$を任意の$n \\times n$正方行列としよう。すると、以下が成り立つ。\n(a) $A$が左逆行列$L$と右逆行列$R$を持つ場合、これらは同じである。\n$$ L=A^{-1}=R $$\n(b) $A$の逆行列が存在する場合、それは一意である。\n(c) $AB = I \\iff BA = I$\n(d) $A$と$B$を可逆行列としよう。すると、2つの行列の積$AB$も可逆であり、その逆行列は次の通りである。\n$$ (AB)^{-1}=B^{-1}A^{-1} $$\n(d\u0026rsquo;) 同じサイズの可逆行列の積も可逆であり、その逆行列はそれぞれの逆行列を逆順に掛けたものと同じである。つまり、$A_{1},A_{2},\\dots,A_{n}$が可逆行列ならば、次が成り立つ。\n$$ \\left( A_{1}A_{2}\\cdots A_{n} \\right)^{-1} = A_{n}^{-1}\\cdots A_{2}^{-1} A_{1}^{-1} $$\n(e) $AB$が可逆ならば、$A$と$B$も可逆である。\n(f) $A$が可逆ならば、転置も可逆であり、その逆行列は次の通りである。\n$$ \\left( A^{T} \\right)^{-1} = \\left( A^{-1} \\right)^{T} $$\n従って、(c) $\\iff$ (d) であることがわかる。\n証明 (a) $n\\times n$行列$A$が与えられたとしよう。$L$が$A$の左逆行列だとしよう。すると、下記の式が成り立つ。\n$$ LA=I_{n} $$\n$R$を$A$の右逆行列としよう。$R$を上記式の右辺に掛けると、次のようになる。\n$$ LAR = I_{n}R =R $$\nしかし、$R$は$A$の右逆行列であるため、$LAR=LI_{n}=L$が成り立つ。従って、上記の式は次のようになる。\n$$ L=R $$\n■\n(b) 任意の正方行列$A$が異なる2つの逆行列$B$と$C$を持つと仮定しよう。それから下記の計算ができる。\n$$ B=BI=B(AC)=(BA)C=IC=C $$\nしかし、この結果は$B$と$C$が異なるという仮定に矛盾する。従って、仮定は誤りであり、逆行列が存在する場合、それは一意である。\n■\n(c) 一般性を失うことなく、$BA = I \\implies AB = I$だけを証明しよう。$BA = I$と仮定しよう。これから式$A \\mathbf{x} = \\mathbf{0}$を考える。\n$$ \\begin{align*} A\\mathbf{x} = \\mathbf{0} \u0026amp;\\implies B(A\\mathbf{x}) = B \\mathbf{0} \\\\ \u0026amp;\\implies (BA)\\mathbf{x} = \\mathbf{0} \\end{align*} $$\nここで、$BA = I$と仮定したので、$\\mathbf{x} = \\mathbf{0}$である。従って、$A \\mathbf{x} = \\mathbf{0}$は自明の解だけを持つ。\n可逆行列の同値条件\n$A$をサイズ$n\\times n$の正方行列としよう。すると、以下の命題は全て同値である。\n$A$は可逆行列である。 同次線形システム$A\\mathbf{x}=\\mathbf{0}$は自明の解だけを持つ。 可逆行列の同値条件により、$A$は可逆である。従って、$A^{-1}$が存在し、\n$$ BA = I \\implies A(BA)A^{-1} = AIA^{-1} \\implies AB = I $$\n■\n(d) $A$と$B$をサイズ$n\\times n$の可逆行列としよう。その時、$A^{-1}$と$B^{-1}$が存在する。まず、$B^{-1}A^{-1}$を$AB$の右に掛けてみよう。それは次のようになる。\n$$ \\begin{align*} (AB)(B^{-1}A^{-1}) \u0026amp;= ABB^{-1}A^{-1} \\\\ \u0026amp;= AI_{n}A^{-1} = AA^{-1} \\\\ \u0026amp;= I_{n}\\end{align*} $$\n左に掛けると、次のようになる。\n$$ \\begin{align*} (B^{-1}A^{-1})(AB) \u0026amp;= B^{-1}A^{-1}AB \\\\ \u0026amp;= B^{-1}I_{n}B = B^{-1}B \\\\ \u0026amp;= I_{n}\\end{align*} $$\n従って、$AB$は可逆行列であり、その逆行列は$B^{-1}A^{-1}$である。\n■\n(d') これは**(d)**の帰結として成立する。\n■\n(e) $AB$の逆行列を$C$としよう。すると、$ABC=I_{n}$が成り立つ。従って、(c)により、$A$は可逆であり、$A^{-1}=BC$が成り立つ。また、$CAB=I_{n}$であるため、$B$も可逆であり、$B^{-1}=CA$が成り立つ。\n(f) 2つの行列を掛け合わせて単位行列が出るか確認すればよい。転置行列の性質により、次のようになる。\n$$ A^{T} \\left( A^{-1} \\right)^{T} = \\left( A^{-1} A \\right) ^{T} = I^{T} = I $$\n$$ \\left( A^{-1} \\right)^{T} A^{T} = \\left( A A^{-1} \\right)^{T} = I^{T} = I $$\n従って\n$$ \\left( A^{T} \\right)^{-1} = \\left( A^{-1} \\right)^{T} $$\n■\n","id":3003,"permalink":"https://freshrimpsushi.github.io/jp/posts/3003/","tags":null,"title":"逆行列、可逆行列"},{"categories":"줄리아","contents":"ガイド ジュリアを使っている人なら、サーバーを含む複数のオペレーティングシステムやコンピューターを使うことに慣れている可能性が高い。ファイル入出力がある場合、開発環境が変わるたびにそのパスを設定するのはとても面倒くさいだろう。これを解決してくれるのが@__DIR__マクロだ。例えば、次のようなジュリアのコードファイルがあるとしよう。\n基本的に、ターミナルから実行するとき、pwd()と@__DIR__は以下のように区別されないように見える。\nこれらの違いは、アトムなどのIDE(統合開発環境)を使う時に出る。pwd()は単に現在の作業ディレクトリを返すのに対し、@__DIR__は実際のコードファイルがある場所を教えてくれるのがその違いだ。複雑で繰り返しの多い作業をしていると、作業ディレクトリをこっちに変えたり、あっちに変えたりすることは多いが、実行されているコードファイルの場所が変わることはあまりないから、便利に使える。\n","id":1935,"permalink":"https://freshrimpsushi.github.io/jp/posts/1935/","tags":null,"title":"ジュリアで実行されるコードファイルの位置を確認する方法"},{"categories":"확률분포론","contents":"要約 ド・モアブル-ラプラス定理 $X_i \\sim B(1,p)$ であり、かつ $Y_n = X_1 + X_2 + \\cdots + X_n$ であれば、$Y_n \\sim B(n,p)$ となり $$ { { Y_n - np } \\over {\\sqrt{ np(1-p) } } }\\overset{D}{\\to} N(0,1) $$\n$N \\left( \\mu , \\sigma^{2} \\right)$ は平均が $\\mu$ で分散が $\\sigma^{2}$ の正規分布だ。 $B(n,p)$ は試行 $n$ 回で確率 $p$ の二項分布だ。 $\\overset{D}{\\to}$ は分布収束を意味する。 説明 この定理は ド・モアブル-ラプラス定理De Moivre–Laplace Theoremとしても知られ、中心極限定理の特別な例として広く知られている。\n統計を初めて学ぶときから、二項分布の標本が大きくなると正規分布に近づくことが学ばれてきた。経験的にも明らかであり、証明の過程はそれほど大きな意味を持たないが、公式だけでは直感的に理解しづらい分布収束を具体的に把握するのに適している例である。\n導出 $$ { { Y_n - np } \\over {\\sqrt{ np(1-p) } } } = \\sqrt{n} { { \\overline{X_n} - p } \\over { \\sqrt{p(1-p)} } } $$ $X_i \\sim B(1,p)$ であるから、$E(X_i ) = p$ そして $\\text{Var}(X_i ) = p(1-p)$ である。そして、中心極限定理により、 $$ \\sqrt{n} { { \\overline{X_n} - p } \\over { \\sqrt{p(1-p)} } } \\overset{D}{\\to} N(0,1) $$\n■\n","id":196,"permalink":"https://freshrimpsushi.github.io/jp/posts/196/","tags":null,"title":"二項分布の極限分布としての標準正規分布の導出"},{"categories":"줄리아","contents":"ガイド Juliaでは、並列計算を日常的に使用するため、場合によってはコンピューターの全てのリソースを計算に集中させる必要がある。スレッド数を変更する方法はいくつかあるが、最もスタティックで便利な方法は、環境変数を編集することだ。\nステップ1. システム環境変数の編集\nCtrl + Alt + T を押してターミナルを開き、gedit ~/.bashrcと入力する。そうすると、以下のように環境変数を編集できるウィンドウが表示される。\nステップ2. 修正\n一番下にexport JULIA_NUM_THREADS=5を追加する。スクリーンショットで指示されている場所に希望のスレッド数を記入すると修正される。適切なスレッド数はコンピュータのスペックによって異なるが、我々は例を扱っているので、偶然に決まることがないように$5$個に修正してみよう。\nステップ3. 確認\nusing Base.Threads\rnthreads() 上記のコードをJuliaコンソールで実行して確認してみよう。\n","id":1937,"permalink":"https://freshrimpsushi.github.io/jp/posts/1937/","tags":null,"title":"Linux上のJuliaでの並列計算に使用するスレッド数の変更方法"},{"categories":"확률분포론","contents":"要約 $X_{n} \\sim B(n,p)$としよう。\nもし$\\mu \\approx np$ならば $$ X_{n} \\overset{D}{\\to} \\text{Poi} (\\mu) $$\n$B(n,p)$は試行回数$n$、確率$p$の二項分布だ。 $\\text{Poi} (\\lambda)$は平均と分散が$\\lambda$のポアソン分布だ。 $\\overset{D}{\\to}$は分布収束を意味する。 説明 ここで、$\\mu \\approx np$という条件が必要であることに注意しよう。$ np \\approx npq$であるから、これは$q = (1-p) \\approx 1$、すなわち$p \\approx 0$を意味する。これは$p$が非常に小さいことを意味する。\n一方で、$\\displaystyle p \\approx { {\\mu} \\over {n} }$であるから、$n$は非常に大きくなければならない。この条件が出てくる理由は、ポアソン分布の平均と分散が同じであることから簡単に納得できるだろう。\n証明 モーメント生成関数$M_{X} (t)$を考えよう。 $$ M_{X} (t) = \\left\\{ (1-p) + p e^{t} \\right\\} ^{n} = \\left\\{ 1 + p (e^{t} - 1 ) \\right\\} ^{n} $$ $\\displaystyle p \\approx { {\\mu} \\over {n} } $であるから、 $$ M_{X} (t) = \\left\\{ 1 + { {\\mu (e^{t} - 1 )} \\over {n} } \\right\\} ^{n} $$ したがって、 $$ \\lim_{n \\to \\infty} M_{X} (t) = e^{ \\mu (e^{t} - 1 ) } $$ $ e^{ \\mu (e^{t} - 1 ) }$は$\\text{Poi}(\\mu)$のモーメント生成関数であるから、$X_{n}$は$ \\text{Poi} (\\mu)$に分布収束する。\n■\n","id":198,"permalink":"https://freshrimpsushi.github.io/jp/posts/198/","tags":null,"title":"二項分布の極限分布としてのポアソン分布の導出"},{"categories":"행렬대수","contents":"定義1 サイズが$m\\times n$の行列を$A$としよう。$A$の行と列を入れ替えた行列を$A$の転置行列transpose, 転置と言い、$A^{T}$または$A^{t}$と表記する。\n説明 定義に従えば、$A$が$m \\times n$行列ならば、$A^{T}$は$n \\times m$行列になる。また、$A$の$i$番目の行は$A^{T}$の$i$番目の列と同じで、その逆もまた然りだ。\n$$ A=\\begin{bmatrix} 10 \u0026amp; 0 \u0026amp; 3 \\\\ 0 \u0026amp; 8 \u0026amp; 22 \\end{bmatrix} ,\\quad A^{T} = \\begin{bmatrix} 10 \u0026amp; 0 \\\\ 0 \u0026amp; 8 \\\\ 3 \u0026amp; 22 \\end{bmatrix} $$\n主対角線を基準に左右対称にしたと考えることもできる。\n性質 $r,s\\in \\mathbb{R}$且つ$A,B$がそれぞれの場合において行列の演算がうまく定義されるサイズであるとしよう。すると、以下が成り立つ。\n(a) 線形性: $$\\left( rA + sB\\right)^{T}=r A^{T} + s B^{T}$$\n(b) 積の転置は、転置を逆順に掛けたものと同じである。\n$$ (AB)^{T}=B^{T}A^{T} $$\n(b\u0026rsquo;) 複数の行列の積の転置は、それぞれの転置を逆順に掛けたものと同じである。\n$$ \\left( A_{1} A_{2}\\cdots A_{n} \\right)^{T} = A_{n}^{T} \\cdots A_{2}^{T} A_{1}^{T} $$\n証明 (b) $m\\times n$行列$A$と$n\\times k$行列$C$について\n$$ \\begin{align*} \\left[ { \\left( AC \\right) }^{ T } \\right] _{ km } \u0026amp;= \\sum _{ i=1 }^{ n }{ [A] _{ m i } { [C] } _{ i k } } \\\\ \u0026amp;= \\sum _{ i=1 }^{ n }{ { \\left[ { A }^{ T } \\right] } _{ i m } { \\left[ { C }^{ T } \\right] } _{ k i } } \\\\ \u0026amp;= \\sum _{ i=1 }^{ n }{ { \\left[ { C }^{ T } \\right] } _{ k i }{ \\left[ { A }^{ T } \\right] } _{ i m } } \\\\ \u0026amp;= { \\left[ { C }^{ T } { A }^{ T } \\right] } _{ km } \\end{align*} $$\n従って、各成分が互いに等しい場合、行列は等しく次の式が成り立つ。\n$$ \\left( AC \\right) ^{ T } = { C }^{ T } { A }^{ T } $$\n■\n(b') これは**(b)**の帰結として成り立つ。\n■\nJim Hefferon, Linear Algebra(4th Edition). 2020, p138\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3002,"permalink":"https://freshrimpsushi.github.io/jp/posts/3002/","tags":null,"title":"転置行列"},{"categories":"줄리아","contents":"コード julia\u0026gt; f(x) = 2x + 1\rf (generic function with 1 method)\rjulia\u0026gt; g(x) = x^2\rg (generic function with 1 method)\rjulia\u0026gt; (g ∘ f)(3)\r49 説明 Juliaでは、関数の合成はプログラミングでのパイプオペレーターに似ている。この合成の最大の利点は、数学者が式をコードとして表現しやすくなることだ。上の例は、以下の数式をコードに翻訳したものに過ぎない。\n$$ f(x) := 2x + 1 \\\\ g(x) := x^2 \\\\ (g \\circ f) (3) $$\n最近の多くの言語がそうであるように、関数をファーストクラスオブジェクトとして扱うことは同じだが、純粋数学で関数空間を扱うように、もう少し極端な哲学を経て文法にまで発展したと見ることができる。ちなみに、関数合成演算子はtexの文法そのままに\\circを入力することで使用できる。\n","id":1942,"permalink":"https://freshrimpsushi.github.io/jp/posts/1942/","tags":null,"title":"ジュリアで合成関数を使用する方法"},{"categories":"행렬대수","contents":"定義 大きさが$n\\times n$で、対角成分がすべて$1$の対角行列を 単位行列identity matrixあるいは 単位行列unit matrixと言い、$I_{n}$または$I_{n\\times n}$と表記する。\n$$ I_{n\\times n}= \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 1 \\end{bmatrix} $$\n説明 単位行列は行列の積における単位元である。つまり任意の$n\\times n$行列$A$に対して以下の式が成り立つ。\n$$ I_{n}A=A=AI_{n} $$\n性質 行列式 単位行列は対角行列なので、行列式は$1$である。\n$$ \\det I = 1 $$\n","id":3001,"permalink":"https://freshrimpsushi.github.io/jp/posts/3001/","tags":null,"title":"同一行列、単位行列"},{"categories":"수리통계학","contents":"定義 1 確率変数 $X$ と確率変数のシーケンス $\\left\\{ X_{n} \\right\\}$ が次を満たす場合、$n \\to \\infty$ の時、$X_{n}$ へ分布収束Convergence in Distributionすると言い、$X_{n} \\overset{D}{\\to} X$ と表示される。 $$ \\lim_{n \\to \\infty} F_{X_{n}} (x) = F_{X} (x) \\qquad, \\forall x \\in C_{F_{X}} $$\n$F_{X}$ は確率変数$X$の累積分布関数である。 $C_{F_{X}}$ は関数$F_{X}$が連続である点の集合を示している。 説明 分布収束は、分布のセンスで収束を定義した概念であり、確率収束と同様である。それぞれの$x \\in C_{F_{X}}$に対する収束は、実際には解析学で言う関数の点収束と似ており、この類似点は、確率収束するならば分布収束するという事実にもつながる。\n注意すべきは、分布収束と言っても、$X_{n} \\overset{D}{\\to} X$ で正確に表されるように、「分布収束」もまた「確率変数の収束」について議論したいということだ。分布関数が連続部分で点収束するということは、正確には確率変数が収束するわけではなく、それが持つ性質の一つである分布が収束するということである。当然ながら、これは確率変数自体の収束よりもはるかにゆるい前提となる。分布の観点から違いがないとしても、確率変数が本質的に収束するわけではない。\n実際には、$X_{n} \\overset{D}{\\to} X$ としても $Y_{n} \\overset{D}{\\to} Y$ と $X_{n} + Y_{n}$ が$X + Y$ に分布収束することが保証されるわけではない。確率収束とは異なり、分布収束は累積分布関数の点収束という軽い条件だけで充分であり、そのためにこれら常識的な性質すら持たない。\n理論 $X_{n} \\overset{D}{\\to} X$ とする。\n[1] 連続写像の定理：連続関数$g$について $$ g\\left( X_{n} \\right) \\overset{D}{\\to} g (X) $$ $$ X_{n} \\overset{P}{\\to} X \\implies X_{n} \\overset{D}{\\to} X $$ [4] スルツキーの定理2: 定数$a,b$と確率変数$A_{n}, B_{n} ,X_{n} , X$に対して$a_{n} \\overset{P}{\\to} a $、$ B_{n} \\overset{P}{\\to} b $、$ X_{n} \\overset{D}{\\to} X $であれば $$ A_{n} + B_{n} X_{n} \\overset{D}{\\to} a + b X $$ 極限分布 一方、$X_{n} \\overset{D}{\\to} X$ であれば、$X$ の分布を$\\left\\{ X_{n} \\right\\}$の漸近Asyptoticまたは極限Limiting分布とも言う。便宜上、$X$の分布をそのまま使うこともあるが、例えば$X \\sim N(0,1)$ であれば次のように表される3。 $$ X_{n} \\overset{D}{\\to} N(0,1) $$\n例 [a] 二項分布の極限分布としてのポアソン分布の導出: $X_{n} \\sim B(n,p)$とする。\n$\\mu \\approx np$であれば $$ X_{n} \\overset{D}{\\to} \\text{Poi} (\\mu) $$ [b] 二項分布の極限分布としての標準正規分布の導出: $X_i \\sim B(1,p)$であり、$Y_n = X_1 + X_2 + \\cdots + X_n$であれば、$Y_n \\sim B(n,p)$である $$ { { Y_n - np } \\over {\\sqrt{ np(1-p) } } }\\overset{D}{\\to} N(0,1) $$ [c] ポアソン分布の極限分布としての標準正規分布の導出: $X_{n} \\sim \\text{Poi} \\left( n \\right)$であり、$\\displaystyle Y_{n} := {{ X_{n} - n } \\over { \\sqrt{n} }}$であれば $$ Y_{n} \\overset{D}{\\to} N(0,1) $$ [d] スチューデントのt分布の極限分布としての標準正規分布の導出: $T_n \\sim t(n)$であれば $$ T_n \\ \\overset{D}{\\to} N(0,1) $$\n極限分布が必要な理由 これらの漸近分布から、分布収束が確率変数自体の収束と呼ぶには不十分であることがわかる。例えば、十分に大きな分布$n$が与えられ、正規分布に近似することができたとしても、その確率変数自体の本質が正規分布を模倣することはできない。$n$がどれだけ大きくても、二項分布は二項分布であり、正規分布は正規分布である。しかし、分布が似ているため、一見して区別がつかないだけである。\nそれでも分布収束を考える理由は、その区別がつかない程度で十分であり、条件でこれ以上妥協する余地がない場合があるからである。前述のように、どれだけ変わっても離散確率分布は連続確率分布になることはない。しかし、弱収束の概念を導入してすぐに離散確率分布を連続確率分布のように使えるならば、考慮しない理由はない。\n証明 [1][4] ■\n[2](../175) ■\n[3](../176) ■\n[a] ■\n[b] ■\n[c] ■\n[d] ■\n厳密な定義 測度論で定義される分布収束 Hogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p208.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p306.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p300.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1888,"permalink":"https://freshrimpsushi.github.io/jp/posts/1888/","tags":null,"title":"数理統計学における分布収束"},{"categories":"행렬대수","contents":"対角行列1 $A$をサイズが$n\\times m$の行列としよう。行と列の番号が同じ要素、つまり$a_{ii} (1 \\le i \\le \\min(n,m))$を主対角成分main diagonal elementsという。主対角成分を結ぶ仮想の線を主対角線main diagonal, principal diagonalと言う。\n主対角成分以外の全ての成分が$0$である行列$A$を対角行列diagonal matrixという。\n$$ A = [a_{ij}] = \\delta_{ij} = \\begin{cases} 1 \u0026amp; i=j \\\\ 0 \u0026amp; i \\ne j \\end{cases} $$\nここで、$\\delta$はクロネッカーのデルタだ。\n説明 $$ A=\\begin{bmatrix} \\color{red}{a_{11}} \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\color{red}{a_{22}} \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\color{red}{a_{33}} \\end{bmatrix} \\quad A=\\begin{bmatrix} \\color{red}{a_{11}} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\color{red}{a_{22}} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\color{red}{a_{33}} \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\color{red}{a_{44}} \u0026amp; 0 \\end{bmatrix} $$\n上の例示の通り、正方行列でなくても主対角成分、対角行列を定義できる。\n定義により、対角行列は下三角行列であり、同時に上三角行列でもある。\n性質 べき乗 $A = \\begin{bmatrix} a_{ij}\\end{bmatrix}$をサイズが$n\\times n$の対角行列としよう。すると、$A$のべき乗は次のようになる。\n$$ A^{k}=\\begin{bmatrix} (a_{11})^{k} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; (a_{22})^{k} \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; (a_{nn})^{k} \\end{bmatrix} $$\n逆行列 $A$の逆行列は次の通りだ。言い換えれば、べき乗に関する性質は$k$が負の時も自然に拡張される。\n$$ A^{-1} = \\begin{bmatrix} \\dfrac{1}{a_{11}} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\dfrac{1}{a_{22}} \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; \\dfrac{1}{a_{nn}} \\end{bmatrix} $$\n行列式 余因子展開を考えれば、対角行列の行列式は全ての対角成分の積であることが分かる。対角行列$n \\times n$の行列式は、\n$$ \\det [a_{ij}] = a_{11} \\times \\cdots \\times a_{nn} $$\nHoward Anton, Elementary Linear Algebra: Applications Version (12版, 2019), p69-71\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1958,"permalink":"https://freshrimpsushi.github.io/jp/posts/1958/","tags":null,"title":"対角行列"},{"categories":"행렬대수","contents":"定義 任意の行列 $A$の行と列の数が同じならば、行列 $A$を正方行列という。\n説明 正方行列は取り扱いやすく、多くの良い性質がある。\n例 単位行列\n可逆行列\n基本行列\n対称行列\n直交行列\nエルミート行列\nユニタリ行列\n","id":1956,"permalink":"https://freshrimpsushi.github.io/jp/posts/1956/","tags":null,"title":"正方行列"},{"categories":"행렬대수","contents":"スカラー乗算 サイズが$m \\times n$の任意の行列 $A$とスカラー $k$の積は、$A$の各成分に$k$を掛けることで定義され、以下のように表記される。\n$$ kA = k\\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{bmatrix} := \\begin{bmatrix} ka_{11} \u0026amp; ka_{12} \u0026amp; \\cdots \u0026amp; ka_{1n} \\\\ ka_{21} \u0026amp; ka_{22} \u0026amp; \\cdots \u0026amp; ka_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; ka_{m2} \u0026amp; \\cdots \u0026amp; ka_{mn} \\end{bmatrix} $$\n定義により、スカラーと行列の積は交換関係が成り立つ。ただし、通常はスカラーを前に書く。\n$$ kA = Ak $$\n加算 サイズが$m \\times n$の二つの行列$A$、$B$の加算は、同じ行、列にある成分同士を足すことで定義され、以下のように表記される。\n$$ \\begin{align*} A+B \u0026amp;= \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{bmatrix} + \\begin{bmatrix} b_{11} \u0026amp; b_{12} \u0026amp; \\cdots \u0026amp; b_{1n} \\\\ b_{21} \u0026amp; b_{22} \u0026amp; \\cdots \u0026amp; b_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ b_{m1} \u0026amp; b_{m2} \u0026amp; \\cdots \u0026amp; b_{mn} \\end{bmatrix} \\\\ \u0026amp;:=\\begin{bmatrix} a_{11} + b_{11} \u0026amp; a_{12} + b_{12} \u0026amp; \\cdots \u0026amp; a_{1n} + b_{1n} \\\\ a_{21} + b_{21} \u0026amp; a_{22} + b_{22} \u0026amp; \\cdots \u0026amp; a_{2n} + b_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} + b_{m1} \u0026amp; a_{m2} + b_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} + b_{mn} \\end{bmatrix} \\end{align*} $$\n定義によれば、二つの行列の加算は同じサイズの行列間でのみ定義され、交換関係が成り立つ。\n$$ A+B=BA $$\n乗算 行列にスカラーを掛けることや二つの行列を足すことは直感的に受け入れやすいかもしれないが、乗算の場合は少し異なる。まずは行ベクトルと列ベクトルの積から見ていこう。\nサイズが$1\\times n$の行ベクトル$A=\\begin{bmatrix} a_{1} \u0026amp; a_{2} \u0026amp; \\cdots \u0026amp; a_{n} \\end{bmatrix}$とサイズが$n \\times 1$の列ベクトル$B= \\begin{bmatrix} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{n} \\end{bmatrix}$の積を以下のように定義する。\n$$ \\begin{align*} AB =\\begin{bmatrix} a_{1} \u0026amp; a_{2} \u0026amp; \\cdots \u0026amp; a_{n} \\end{bmatrix}\\begin{bmatrix} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{n} \\end{bmatrix} \u0026amp;:= a_{1}b_{1}+a_{2}b_{2} + \\cdots +a_{n}b_{n} \\\\ \u0026amp;= \\sum \\limits_{i=1}^{n}a_{i}b_{i} \\end{align*} $$\nこの定義を言葉で表すと「同じ順序にある成分同士を掛けたものの合計」となるが、これは高校で学んだ二つのベクトルの内積と概念的に同じである。\n$$ \\begin{align*} \\vec{a} \u0026amp;=(a_{1},a_{2},a_{3}) \\\\ \\vec{b} \u0026amp;= (b_{1},b_{2},b_{3}) \\end{align*},\\quad \\vec{a} \\cdot \\vec{b} = a_{1}b_{1} + a_{2}b_{2} + a_{3}b_{3} $$\n二つの行列の積は、この概念の拡張と考えることができる。 $m\\times n$行列$A$と$m\\times k$行列$B$の積を以下のように定義する。\n$$ \\begin{align*} AB \u0026amp;= \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{bmatrix} \\begin{bmatrix} b_{11} \u0026amp; b_{12} \u0026amp; \\cdots \u0026amp; b_{1k} \\\\ b_{21} \u0026amp; b_{22} \u0026amp; \\cdots \u0026amp; b_{2k} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ b_{n1} \u0026amp; b_{n2} \u0026amp; \\cdots \u0026amp; b_{nk} \\end{bmatrix} \\\\ \u0026amp;:= \\begin{bmatrix} \\sum_{i=1}^{n} a_{1i}b_{i1} \u0026amp; \\sum_{i=1}^{n} a_{1i}b_{i2} \u0026amp; \\cdots \u0026amp; \\sum_{i=1}^{n} a_{1i}b_{ik} \\\\ \\sum_{i=1}^{n} a_{2i}b_{i1} \u0026amp; \\sum_{i=1}^{n} a_{2i}b_{i2} \u0026amp; \\cdots \u0026amp; \\sum_{i=1}^{n} a_{2i}b_{ik} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\sum_{i=1}^{n} a_{mi}b_{i1} \u0026amp; \\sum_{i=1}^{n} a_{mi}b_{i2} \u0026amp; \\cdots \u0026amp; \\sum_{i=1}^{n} a_{mi}b_{ik} \\end{bmatrix} \\end{align*} $$\n数式が長く見えて難しそうだが、行ベクトルと列ベクトルの積を複数回行っただけである。$A$と$B$の積から得られる行列$AB$の$n$行、$k$列の成分は、$A$の$n$行と$B$の$k$列の内積と同じである。従って、$A$の列の数と$B$の行の数が同じでなければ、両者の乗算が定義されない。また、二つの行列の乗算は一般に交換法則が成り立たない。\n$$ AB \\ne BA $$\nこれは簡単な例でも確認できる。$A=\\begin{bmatrix} 1 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \\end{bmatrix}$、$\\begin{bmatrix} 2 \u0026amp; -1 \\\\ 1 \u0026amp; 1 \\end{bmatrix}$とすると、\n$$ \\begin{align*} AB \u0026amp;=\\begin{bmatrix} 2+1 \u0026amp; -1+1 \\\\ 0+1 \u0026amp; 0+1 \\end{bmatrix}=\\begin{bmatrix} 3 \u0026amp; 0 \\\\ 1 \u0026amp; 1 \\end{bmatrix} \\\\ BA \u0026amp;=\\begin{bmatrix} 2+0 \u0026amp; 2-1 \\\\ 1+0 \u0026amp; 1+1 \\end{bmatrix} = \\begin{bmatrix} 2 \u0026amp; 1 \\\\ 1 \u0026amp; 2 \\end{bmatrix} \\end{align*} $$\n従って、\n$$ AB\\ne BA $$\n行列の乗算過程を視覚的に表現すると、次のようになる。\n性質1 $A$、$B$、$C$を任意の$m \\times n$サイズの行列とする。$r$、$s$を任意のスカラーとする。行列演算について、次の性質が成り立つ。\n(a) 加算に対する交換法則: $A + B = B + A$\n(b) 加算に対する結合法則: $A + (B + C) = (A + B) + C$\n(c) $(r + s)A = rA + sA$\n(d) $r(A + B) = rA + rB $\n(e) $(rs)A = r(sA)$\n$A$、$B$、$C$を任意の$n\\times n$サイズの行列とする。\n(f) 乗算に対する結合法則: $A(BC) = (AB)C$\n(g) 乗算に対する分配法則 $A(B+C) = AB + AC \\quad \\\u0026amp; \\quad (A+B)C=AC + BC$\n行列の乗算については、交換法則が成り立たないことを再度注意しよう。\nJim Hefferon, Linear Algebra(4th Edition). 2020, p235\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1957,"permalink":"https://freshrimpsushi.github.io/jp/posts/1957/","tags":null,"title":"行列の演算: スカラー乗法、加法、乗法"},{"categories":"매트랩","contents":"Imagesc imagesc 함수를 사용하면 2차원 배열을 히트맵으로 출력할 수 있다. colorbar는 스케일을 보여주는 컬러바를 같이 출력하는 설정이다.\nN=2^8;\rp=phantom(\u0026#39;Modified Shepp-Logan\u0026#39;,N);\rfigure()\rimagesc(p)\rcolorbar 保存 方法1 saveas 함수를 사용하여 위에서 표시한 figure를 저장할 수 있다. 여기서 gcf 설정은 현재 figure를 의미한다. 그러면 아래 그림이 저장된다.\nN=2^8;\rp=phantom(\u0026#39;Modified Shepp-Logan\u0026#39;,N);\rfigure()\rimagesc(p)\rcolorbar\rsaveas(gcf,\u0026#39;phantom.png\u0026#39;) 方法2 下記の写真のように、figureウィンドウから直接保存することもできる。\n他の言語で ジュリアで ","id":1948,"permalink":"https://freshrimpsushi.github.io/jp/posts/1948/","tags":null,"title":"MATLABで2次元配列をヒートマップ画像として出力および保存する方法"},{"categories":"행렬대수","contents":"定義1 数を次のように長方形の形に並べたものを行列matrixという。\n$$ A=\\begin{bmatrix} 10 \u0026amp; 0 \u0026amp; 3 \\\\ 0 \u0026amp; 8 \u0026amp; 22 \\end{bmatrix} $$\n並べた各々の数をエントリーentryまたは要素elementと呼ぶ。横の列を行row、縦の列を列columnという。また、ある行列が$m$行と$n$列を持つ場合、その行列のサイズを$m \\times n$と表す。\n上の例では、行列$A$は2行と3列を持ち、サイズは$2\\times 3$である。ここで注意すべき点は、$\\times$が乗算を意味するわけではないということである。サイズは必ず総行数と列数が明らかになるように$2\\times 3$のように表記しなければならず、絶対に$6$と書いてはいけない。ちなみに'$2 \\times 3$行列'は [ツーバイスリー行列]と読む。\n表記法 行列は主に下のように角括弧[]または丸括弧()で表記されるが、どちらの表現も一般的に見られる。ただし、手で書くときは丸括弧を使うとキレイに書きにくい。また、2次元、3次元空間の座標を表記するときとは違い、成分と成分の間にカンマ(,)を書かないのが基本である。\n$$ A=\\begin{bmatrix} 10 \u0026amp; 0 \u0026amp; 3 \\\\ 0 \u0026amp; 8 \u0026amp; 22 \\end{bmatrix} \\quad A=\\begin{pmatrix} 10 \u0026amp; 0 \u0026amp; 3 \\\\ 0 \u0026amp; 8 \u0026amp; 22 \\end{pmatrix} $$\n通常、行列は大文字で、成分は小文字で表記する。例えば、行列$A$の1行目3列目の成分は$3$であり、次のように表記される。\n$$ a_{13}=3 $$\n最初の下付き添え字は行の位置を、2番目の下付き添え字は列の位置を示す。同様に、$i$行目、$j$列目の成分が$a_{ij}$である行列を$\\begin{bmatrix} a_{ij} \\end{bmatrix}$のように表記する。$A$の$(i,j)$成分は$[A]_{ij}$と表記される。\n$$ A = \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\end{bmatrix} = \\begin{bmatrix} a_{ij} \\end{bmatrix},\\qquad [A]_{ij} = a_{ij} $$\nすべての$m\\times n$行列の集合を次のように表記する。\n$$ M_{m \\times n} $$\nサイズが$m\\times n$で成分が実数$\\mathbb{R}$、複素数$\\mathbb{C}$の行列の集合は、それぞれ次のように表記される。\n$$ M_{m\\times n}(\\mathbb{R}),\\quad M_{m \\times n}(\\mathbb{C}) $$\nもう少し抽象的に、成分が体$F$の$n \\times n$行列の集合を$M_{m \\times n}(F)$と表記する。\n列ベクトルと行ベクトル ベクトルとは、数を横または縦に並べたものをいう。この点を考えると、ある行列は列ベクトルまたは行ベクトルを並べたものと見ることができる。例として続けて使用されてきた行列$A$を見てみよう。\n$$ A= \\begin{bmatrix} 10 \u0026amp; 0 \u0026amp; 3 \\\\ 0 \u0026amp; 8 \u0026amp; 22 \\end{bmatrix} $$\n$A$の各列は列ベクトル$\\begin{bmatrix} 10 \\\\ 0 \\end{bmatrix}$、$\\begin{bmatrix} 0 \\\\ 8 \\end{bmatrix}$、$\\begin{bmatrix} 3 \\\\ 22 \\end{bmatrix}$で構成されていると考えられる。または、各行が行ベクトル$\\begin{bmatrix} 10 \u0026amp; 0 \u0026amp; 3 \\end{bmatrix}$、$\\begin{bmatrix} 0 \u0026amp; 8 \u0026amp; 22 \\end{bmatrix}$で構成されていると見ることができる。\nJim Hefferon, Linear Algebra(4th Edition). 2020, p15\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1955,"permalink":"https://freshrimpsushi.github.io/jp/posts/1955/","tags":null,"title":"行列の定義"},{"categories":"행렬대수","contents":"定義 数の並びをベクトルと言う。\n説明 通常の教科書では、ベクトルは「大きさと方向を持つ幾何学的なオブジェクト」として学習される。物理学で最初に接する概念だから、$3$次元以下のベクトルに慣れることが避けられない。\n$$ (3,4) = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} $$ $$ (x,y,z) = \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} $$\nでも実際には、ベクトルはそれより多くの座標に対して一般化が可能だ。単に数を下に更に並べるだけでいいので、例えば時間$t$を考慮した$4$次元のベクトルは、以下のように示すことができる。\n$$ (t,x,y,z) = \\begin{bmatrix} t \\\\ x \\\\ y \\\\ z \\end{bmatrix} $$\n$4$次元以上のベクトルにはどんな意味があるだろうか？例えば、各酸素分子の時間$t$での位置$(x,y,z)$と熱エネルギー$E$を表したいなら、次のように$5$次元に拡張すればいい。\n$$ (t,x,y,z,E) = \\begin{bmatrix} t \\\\ x \\\\ y \\\\ z \\\\ E \\end{bmatrix} $$\n要するに、ベクトルの長さ、つまり次元が高くなることを特に恐れる必要はないということだ。与えられた形式の下での数学の自由な世界では、このような次元の拡張は自然であり、当然のことだ。同じ方法で、$n$次元まで一般化したベクトルを考えることができ、通常ボールド体 $\\mathbf{x}$ を使って示される。\n$$ \\mathbf{x} = \\left( x_{1}, \\cdots , x_{n} \\right) = \\begin{bmatrix} x_{1} \\\\ \\vdots \\\\ x_{n} \\end{bmatrix} $$\nこの簡単な定義から、$n$次元のベクトルは**$n$-組**$n$-tupleと区別されない。物理から離れ、数学に近づくほど、$\\vec{x}$のような矢印を使った表現は少なくなり、抽象的で一般的な数学に入ると、「座標」や「並び」などの表現なしに、厳密で正確な定義が行われる。\n併せて見る ベクトルの難しい定義 ","id":1947,"permalink":"https://freshrimpsushi.github.io/jp/posts/1947/","tags":null,"title":"ベクトルの定義"},{"categories":"수리물리","contents":"公式 $f=f(x,y,z)$をスカラー関数とする。$\\mathbf{A} = A_{x}\\hat{\\mathbf{x}} + A_{y}\\hat{\\mathbf{y}} + A_{z}\\hat{\\mathbf{z}}, \\mathbf{B} = B_{x}\\hat{\\mathbf{x}} + B_{y}\\hat{\\mathbf{y}} + B_{z}\\hat{\\mathbf{z}}$をベクター関数とする。すると、次の式が成り立つ。\nグラディエント勾配\n(a) $\\nabla{(fg)}=f\\nabla{g}+g\\nabla{f}$\n(b) $\\nabla(\\mathbf{A} \\cdot \\mathbf{B}) = \\mathbf{A} \\times (\\nabla \\times \\mathbf{B}) + \\mathbf{B} \\times (\\nabla \\times \\mathbf{A})+(\\mathbf{A} \\cdot \\nabla)\\mathbf{B}+(\\mathbf{B} \\cdot \\nabla) \\mathbf{A}$\nダイバージェンス発散\n(c) $\\nabla \\cdot (f\\mathbf{A}) = f(\\nabla \\cdot \\mathbf{A}) + \\mathbf{A} \\cdot (\\nabla f)$\n(d) $\\nabla \\cdot (\\mathbf{A} \\times \\mathbf{B}) = \\mathbf{B} \\cdot (\\nabla \\times \\mathbf{A}) - \\mathbf{A} \\cdot (\\nabla \\times \\mathbf{B})$\nカール回転\n(e) $\\nabla \\times (f\\mathbf{A}) = (\\nabla f) \\times \\mathbf{A} + f(\\nabla \\times \\mathbf{A})$\n(f) $\\nabla \\times (\\mathbf{A} \\times \\mathbf{B}) = (\\mathbf{B} \\cdot \\nabla)\\mathbf{A} - (\\mathbf{A} \\cdot \\nabla)\\mathbf{B} + \\mathbf{A} (\\nabla \\cdot \\mathbf{B}) - \\mathbf{B} (\\nabla \\cdot \\mathbf{A})$\n説明 証明全体でアインシュタインの表記法を使用しているので、混乱しないよう注意。つまり、一つの式に同じインデックスが二回出る場合は次のような意味である。\n$$ x_{i}y_{i}=\\sum \\limits_{i=1}^{3} x_{i}y_{i}=x_{1}y_{1}+x_{2}y_{2}+x_{3}y_{3} $$\nまた、クロネッカーのデルタとレビ・チビタ記号を使用することに慣れ、その二つの関係を知っている必要がある。\n証明 (a) グラディエントの定義と微分の性質で簡単に示すことができる。\n$$ \\begin{align*} \\nabla(fg) =\u0026amp;\\ \\dfrac{\\partial (fg)}{\\partial x} \\hat{\\mathbf{x}}+\\dfrac{\\partial (fg)}{\\partial y} \\hat{\\mathbf{y}} +\\dfrac{\\partial (fg)}{\\partial z} \\hat{\\mathbf{z}} \\\\ =\u0026amp;\\ \\left( g\\dfrac{\\partial f}{\\partial x} + f\\dfrac{\\partial g}{\\partial x} \\right) \\hat{\\mathbf{x}} +\\left( g\\dfrac{\\partial f}{\\partial y} + f\\dfrac{\\partial g}{\\partial y} \\right) \\hat{\\mathbf{y}} + \\left( g\\dfrac{\\partial f}{\\partial z} + f\\dfrac{\\partial g}{\\partial z} \\right) \\hat{\\mathbf{z}} \\\\ =\u0026amp;\\ g\\left( \\dfrac{\\partial f}{\\partial x}\\hat{\\mathbf{x}} +\\dfrac{\\partial f}{\\partial y}\\hat{\\mathbf{y}} + \\dfrac{\\partial f}{\\partial z}\\hat{\\mathbf{z}} \\right) + f\\left( \\dfrac{\\partial g}{\\partial x}\\hat{\\mathbf{x}} +\\dfrac{\\partial g}{\\partial y}\\hat{\\mathbf{y}} + \\dfrac{\\partial g}{\\partial z}\\hat{\\mathbf{z}} \\right) \\\\ =\u0026amp;\\ g\\nabla f+ f\\nabla g \\end{align*} $$\n■\n(b) 左辺を直接計算してみると以下のようになる。\n$$ \\begin{align*} \\nabla \\left( \\mathbf{A}\\cdot \\mathbf{B} \\right) =\u0026amp;\\ \\frac{ \\partial \\left( \\mathbf{A}\\cdot \\mathbf{B} \\right)}{ \\partial x_{1}}\\mathbf{e}_{1}+\\frac{ \\partial \\left( \\mathbf{A}\\cdot \\mathbf{B} \\right)}{ \\partial x_{2}}\\mathbf{e}_{2}+\\frac{ \\partial \\left( \\mathbf{A}\\cdot \\mathbf{B} \\right)}{ \\partial x_{3}}\\mathbf{e}_{3} \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{3} \\frac{ \\partial \\left( \\mathbf{A}\\cdot \\mathbf{B} \\right)}{ \\partial x_{i}}\\mathbf{e}_{i} \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{3} \\frac{ \\partial \\left( \\sum _{j=1}^{3}A_{j}B_{j} \\right)}{ \\partial x_{i}}\\mathbf{e}_{i} \\\\ =\u0026amp;\\ \\sum \\limits_{i=1}^{3}\\sum \\limits_{j=1}^{3} \\frac{ \\partial \\left( A_{j}B_{j} \\right)}{ \\partial x_{i}}\\mathbf{e}_{i} \\end{align*} $$\nこれをアインシュタイン表記法で簡潔に表すと以下のようになる。\n$$ \\nabla (\\mathbf{A} \\cdot \\mathbf{B}) = \\frac{\\partial(A_{j}B_{j})}{\\partial x_{i}}\\mathbf{e}_{i}=\\frac{\\partial A_{j}}{\\partial x_{i}} B_{j}\\mathbf{e}_{i}+A_{j} \\frac{\\partial B_{j}}{\\partial x_{i}} \\mathbf{e}_{i} $$\nさらにクロネッカーのデルタを使って上記式を$X_{i}Y_{i}=X_{i}Y_{j}\\delta_{ij}$のように表せば以下のようになる。\n$$ \\begin{align*} \u0026amp;\u0026amp;\\frac{\\partial A_{j}}{\\partial x_{i}} B_{j}\\mathbf{e}_{i}+A_{j} \\frac{\\partial B_{j}}{\\partial x_{i}} \\mathbf{e}_{i} =\u0026amp;\\ {\\color{blue}\\delta_{jm}}\\frac{ \\partial {\\color{blue}A_{j}}}{ \\partial x_{i}} {\\color{blue}B_{m}} \\mathbf{e}_{i} + {\\color{blue}\\delta_{jm} A_{m}}\\frac{ \\partial {\\color{blue}B_{j}} }{ \\partial x_{i} }\\mathbf{e}_{i} \\\\ \u0026amp;\u0026amp; =\u0026amp;{\\color{red}\\delta_{il}}{\\color{blue}\\delta_{jm}}\\frac{ {\\color{red}\\partial} {\\color{blue}A_{j}}}{ {\\color{red}\\partial x_{i}} } {\\color{blue}B_{m}} {\\color{red}\\mathbf{e}_{l}} + {\\color{red}\\delta_{il}}{\\color{blue}\\delta_{jm} A_{m}} {\\color{red}\\frac{ \\partial {\\color{blue}B_{j}} }{ \\partial x_{i} }} {\\color{red}\\mathbf{e}_{l}} \\\\ \u0026amp;\u0026amp; =\u0026amp;{\\color{red}\\delta_{jl}}{\\color{blue}\\delta_{jm}} \\left( {\\color{red}\\frac{ \\partial {\\color{blue}A_{j}}}{ \\partial x_{i} }} {\\color{blue}B_{m}}\\color{red} {\\mathbf{e}_{l}} + {\\color{blue}A_{m}}{\\color{red}\\frac{ \\partial {\\color{blue}B_{j}} }{ \\partial x_{i} } \\mathbf{e}_{l} }\\right) \\\\ \\implies \u0026amp;\u0026amp; \\nabla \\left( \\mathbf{A} \\cdot \\mathbf{B} \\right) =\u0026amp;\\ \\delta_{jl}\\delta_{jm} \\left( \\frac{ \\partial A_{j} }{ \\partial x_{i} } B_{m} \\mathbf{e}_{l} + A_{m}\\frac{ \\partial B_{j} }{ \\partial x_{i} } \\mathbf{e}_{l} \\right) \\end{align*} $$\nまた、$\\epsilon_{ijk} \\epsilon_{klm} = \\delta_{il} \\delta_{jm} - \\delta_{im} \\delta_{jl}$であるため、上記式を以下のように展開できる。\n$$ \\begin{align*} \\nabla(\\mathbf{A} \\cdot \\mathbf{B}) =\u0026amp;\\ (\\epsilon_{ijk} \\epsilon_{klm} + \\delta_{im} \\delta_{jl}) \\left(\\frac {\\partial A_{j}}{\\partial x_{i}}B_{m} \\mathbf{e}_{l} + A_{m} \\frac{\\partial B_{j}}{\\partial x_{i}}\\mathbf{e}_{l}\\right) \\\\ =\u0026amp;\\ \\epsilon_{ijk} \\epsilon_{klm } \\frac{\\partial A_{j}}{\\partial x_{i}}B_{m} \\mathbf{e}_{l} + \\epsilon_{ijk} \\epsilon_{klm} A_{m} \\frac{\\partial B_{j}}{\\partial x_{i}} \\mathbf{e}_{l} + \\delta_{im} \\delta_{jl} \\frac{\\partial A_{j}}{\\partial x_{i}} B_{m} \\mathbf{e}_{l} + \\delta_{im} \\delta_{jl}A_{m} \\frac{\\partial B_{j}}{\\partial x_{i}} \\mathbf{e}_{l} \\end{align*} $$\nここでレビ・チビタ記号の定義により$\\epsilon_{ijk} \\dfrac{\\partial A_{j}}{\\partial x_{i}}=(\\nabla \\times \\mathbf{A})_{k}$、$\\epsilon_{ijk} \\dfrac {\\partial B_{j}}{\\partial x_{i}}=(\\nabla \\times \\mathbf{B})_{k}$であるため、次の結果を得る。\n$$ \\begin{align*} \\nabla (\\mathbf{A} \\cdot \\mathbf{B}) =\u0026amp;\\ \\epsilon _{ klm }(\\nabla \\times \\mathbf{A})_{ k }B_{ m }\\hat { \\mathbf{e}_{ l } }+\\epsilon _{ klm }A_{ m }(\\nabla \\times \\mathbf{B})_{ k }\\hat { \\mathbf{e}_{ l } }+\\frac { \\partial A_{ j } }{ \\partial x_{ i } }B_{ i }\\hat { e_{ j} }+A_{ i }\\frac { \\partial B_{ j } }{ \\partial x_{ i } }\\hat { \\mathbf{e}_{ j } } \\\\ =\u0026amp;\\ \\mathbf{B}\\times (\\nabla \\times \\mathbf{A})+\\mathbf{A} \\times (\\nabla \\times \\mathbf{B})+(\\mathbf{B} \\cdot \\nabla )\\mathbf{A}+(\\mathbf{A} \\cdot \\nabla )\\mathbf{B} \\end{align*} $$\n■\n(c)​ $$ \\begin{align*} \\nabla \\cdot (f \\mathbf{A}) =\u0026amp;\\ \\delta _{ ij }\\nabla _{ i }(fA_{ j }) \\\\ =\u0026amp;\\ \\delta _{ ij }(\\nabla _{ i }f)A_{ j }+\\delta _{ ij }f(\\nabla _{ i }A_{ j }) \\\\ =\u0026amp;\\ (\\nabla _{ i }f)A_{ i }+f(\\nabla _{ i }A_{ i }) \\\\ =\u0026amp;\\ (\\nabla f)\\cdot \\mathbf{A}+f(\\nabla \\cdot \\mathbf{A}) \\end{align*} $$\n■\n(d) $$ \\begin{align*} \\nabla \\cdot (\\mathbf{A} \\times \\mathbf{B}) \u0026amp;= \\delta _{ ij }\\nabla _{ i }(\\mathbf{A} \\times \\mathbf{B})_{ j } \\\\ \u0026amp;= \\delta _{ ij }\\nabla _{ i }(\\epsilon _{ jkl }A_{ k }B_{ l }) \\\\ \u0026amp;= \\delta _{ ij }\\epsilon _{ jkl }\\nabla _{ i }(A_{ k }B_{ l }) \\\\ \u0026amp;= \\delta _{ ij }\\epsilon _{ jkl }(\\nabla _{ i }A_{ k })B_{ l }+\\delta _{ ij }\\epsilon _{ jkl }A_{ k }(\\nabla _{ i }B_{ l }) \\\\ \u0026amp;= (\\epsilon _{ jkl }\\nabla _{ j }A_{ k })B_{ l }+A_{ k }(\\epsilon _{ jkl }\\nabla _{ j }B_{ l }) \\\\ \u0026amp;= (\\nabla \\times A)_{ l }B_{ l }-A_{ k }(\\nabla \\times B)_{ k } \\\\ \u0026amp;= (\\nabla \\times \\mathbf{A})\\cdot \\mathbf{B}-\\mathbf{A}\\cdot (\\nabla \\times \\mathbf{B}) \\end{align*} $$\n■\n(e)​ $$ \\begin{align*} \\nabla \\times (f\\mathbf{A}) \u0026amp;= \\epsilon _{ ijk }\\nabla _{ i }(fA_{ j })\\mathbf{e}_{k} \\\\ \u0026amp;= \\epsilon _{ ijk }(\\nabla _{ i }f)A_{ j }\\mathbf{e}_{k}+\\epsilon _{ ijk }f(\\nabla _{ i }A_{ j })\\mathbf{e}_{k} \\\\ \u0026amp;= (\\nabla f)\\times \\mathbf{A}+f\\epsilon _{ ijk }(\\nabla _{ i }A_{ j })\\mathbf{e}_{k} \\\\ \u0026amp;= (\\nabla f)\\times \\mathbf{A}+f(\\nabla \\times \\mathbf{A}) \\\\ \u0026amp;= f(\\nabla \\times \\mathbf{A})-\\mathbf{A} \\times (\\nabla f) \\end{align*} $$\n■\n(f) アインシュタイン表記法に慣れていなければ、証明についていくのが難しいかもしれない。\n$$ \\begin{align*} \u0026amp; \\nabla \\times (\\mathbf{A}\\times \\mathbf{B}) \\\\ =\u0026amp;\\ \\epsilon_{ijk} \\nabla_{i} \\left(\\mathbf{A}\\times \\mathbf{B}\\right)_{j} \\mathbf{e}_{k} \\\\ =\u0026amp;\\ \\epsilon_{ijk} \\nabla_{i} (\\epsilon_{jlm} A_{l} B_{m})\\mathbf{e}_{k} \\\\ =\u0026amp;\\ \\epsilon_{ijk} \\epsilon_{jlm} \\nabla_{i} (A_{l} B_{m}) \\mathbf{e}_{k} \\\\ =\u0026amp;\\ \\epsilon_{jki} \\epsilon_{jlm} \\left[ B_{m}(\\nabla_{i} A_{l}) \\mathbf{e}_{k} + A_{l} (\\nabla_{i} B_{m}) \\mathbf{e}_{k} \\right] \\\\ =\u0026amp;\\ (\\delta_{kl} \\delta_{im} - \\delta_{km} \\delta_{il} ) [ B_{m} (\\nabla_{i} A_{l} ) \\mathbf{e}_{k} + A_{l} ( \\nabla_{i} B_{m} ) \\mathbf{e}_{k} ] \\\\ =\u0026amp;\\ \\delta_{kl} \\delta_{im} B_{m} ( \\nabla_{i} A_{l}) \\mathbf{e}_{k} - \\delta_{km} \\delta_{il} B_{m} (\\nabla_{ i} A_{l} ) \\mathbf{e}_{k} + \\delta_{kl} \\delta_{im} A_{l} ( \\nabla_{i} B_{m} ) \\mathbf{e}_{k} - \\delta_{km} \\delta_{il} A_{l} ( \\nabla_{i} B_{m} ) \\mathbf{e}_{k} \\\\ =\u0026amp;\\ B_{i} ( \\nabla_{i} A_{k}) \\mathbf{e}_{k} - B_{k} (\\nabla_{i} A_{i} ) \\mathbf{e}_{k} + A_{k} ( \\nabla_{i} B_{i} ) \\mathbf{e}_{k} - A_{i} ( \\nabla_{i} B_{k} ) \\mathbf{e}_{k} \\\\ =\u0026amp;\\ (\\mathbf{B}\\cdot \\nabla )\\mathbf{A}-(\\nabla \\cdot \\mathbf{A})\\mathbf{B}+\\mathbf{A}(\\nabla \\cdot \\mathbf{B})-(\\mathbf{A}\\cdot \\nabla )\\mathbf{B} \\\\ =\u0026amp;\\ (\\mathbf{B}\\cdot \\nabla )\\mathbf{A}-(\\mathbf{A}\\cdot \\nabla )\\mathbf{B}+\\mathbf{A}(\\nabla \\cdot \\mathbf{B})-\\mathbf{B}(\\nabla \\cdot \\mathbf{A}) \\end{align*} $$\n四行目は$\\epsilon_{jki} \\epsilon_{jlm} = \\delta_{kl} \\delta_{im} - \\delta_{km} \\delta_{il}$により成立する。七行目はアインシュタイン表記法により成立する。\n■\n","id":93,"permalink":"https://freshrimpsushi.github.io/jp/posts/93/","tags":null,"title":"デル演算子を含む乗法則"},{"categories":"단층촬영","contents":"定義 ある2次元ドメイン$D\\subset \\mathbb{R}^{2}$で定義された関数$f :D \\to \\mathbb{R}$が与えられたとする。$f$のラドン変換$\\mathcal{R}f$を、$s \\in \\mathbb{R}$、$\\boldsymbol{\\theta} = (\\cos \\theta, \\sin \\theta) \\in S^{1}$に対して、以下のように定義する。\n$$ \\begin{align*} \\mathcal{R} f(s, \\boldsymbol{\\theta}):=\u0026amp;\\ \\int \\limits_{t=-\\infty}^{\\infty} f ( s \\boldsymbol{\\theta} + t \\boldsymbol{\\theta}^{\\perp} )dt \\\\ =\u0026amp;\\ \\int \\limits_{t=-\\infty} ^{\\infty} f \\left( s\\cos\\theta-t\\sin\\theta, s\\sin\\theta + t\\cos\\theta \\right)dt \\end{align*} $$\n説明 ラドン変換は積分変換の一種で、オーストリアの数学者ヨハン・ラドンJohann Radon, 1887-1956にちなんで名づけられた。\n放射性元素のラドンは、数学者のラドンにちなんで名づけられたわけではなく、\u0026lsquo;radioactive\u0026rsquo;という単語に不活性ガスの接尾辞\u0026rsquo;-on\u0026rsquo;を組み合わせて名づけられた。\n$\\mathcal{R} f (s, \\boldsymbol{\\theta})$の幾何学的な意味は、原点から$s$離れていて、$\\boldsymbol{\\theta}$と垂直な全ての点で$f$を積分することである。\n$f$はデカルト座標$(x, y)$に対する関数である一方、ラドン変換$\\mathcal{R}f$は極座標$(s, \\theta)$に対する関数である。\nラドン変換はCTの核心原理の一つであり、ベール-ランベールの法則という物理法則に基づいている。これは、X線の強度が通過する媒質の種類によって異なるように減少するという内容を含んでいる。X線の強度が減少したということは、つまり媒質がX線を吸収したということと同じである。媒質が光を吸収する度合いを減衰係数attenuated coefficient、吸収係数absorption coefficient、あるいは吸光度absorbanceと呼ぶ。媒質によって減衰係数が異なることを利用して、X線を使った非破壊検査をCTで実施する。X線撮影で骨が白く見えるのは、骨が他の物質よりもX線を多く吸収するからである。\n定義の別の表現 $l_{s, \\theta}$を極座標$(s,\\theta)$によって決まる直線とするとき、\n$$ \\mathcal{R} f(s, \\boldsymbol{\\theta}) = \\int _{l_{s, \\theta}} f $$\n幾何学的な意味を考えると、\n$$ \\mathcal{R} f(s, \\boldsymbol{\\theta}) = \\int \\limits_{ \\mathbf{x} \\cdot \\boldsymbol{\\theta} = s} f (\\mathbf{x}) d \\mathbf{x} $$\n$\\boldsymbol{\\theta}^{\\perp} := \\left\\{ \\mathbf{u} : \\mathbf{u} \\cdot \\boldsymbol{\\theta} = 0 \\right\\}$として定義すると、\n$$ \\mathcal{R} f(s, \\boldsymbol{\\theta}) = \\int \\limits_{ \\boldsymbol{\\theta}^{\\perp}} f (s \\boldsymbol{\\theta} + \\mathbf{u}) d \\mathbf{u} $$\nディラックのデルタ関数$\\delta$に対して、\n$$ \\mathcal{R} f (s, \\boldsymbol{\\theta}) = \\int\\limits_{\\mathbb{R}^{2}} f( \\mathbf{x} ) \\delta ( \\mathbf{x} \\cdot \\boldsymbol{\\theta} - s) d \\mathbf{x} $$\n一般化 $s \\in \\mathbb{R}^{1}$、$\\boldsymbol{\\theta} \\in S^{n-1}$に対して、ラドン変換$\\mathcal{R} : L^{2}(\\mathbb{R}^{n}) \\to L^{2}(Z_{n})$を以下のように定義する。\n$$ \\mathcal{R} f (s, \\boldsymbol{\\theta}) = \\int\\limits_{\\mathbf{x} \\cdot \\boldsymbol{\\theta} = s} f(\\mathbf{x}) d \\mathbf{x} $$\nここで、$Z_{n} := \\mathbb{R}^{1} \\times S^{n-1}$は$n+1$次元のユニットシリンダーである。\n導出1 $x$を位置、$I(x)$をX線の強度、$A(x)$を媒質の減衰係数とする。\nビール-ランベール法則\nX線の強度の変化率は、以下の通りである。\n$$ \\begin{equation} \\frac{ dI }{ dx } = -A(x)I(x) \\end{equation} $$\n$x_{0}$、$x_{1}$をそれぞれX線が開始した位置、終了した位置とし、$I_{0}$、$I_{1}$を各点でのX線の強度とする。$(1)$で変数分離を行い、両辺を積分すると、以下のようになる。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\int_{x_{0}}^{x_{1}} \\frac{1}{I(x)}dI \u0026amp;= - \\int_{x_{0}}^{x_{1}}A(x)dx \\\\ \\implies \u0026amp;\u0026amp; \\ln \\left( I_{1} \\right) - \\ln \\left( I_{0} \\right)\u0026amp;= -\\int_{x_{0}}^{x_{1}}A(x)dx \\\\ \\implies \u0026amp;\u0026amp; \\ln \\left( \\frac{I_{1}}{I_{0}}\\right) \u0026amp;= -\\int_{x_{0}}^{x_{1}}A(x)dx \\\\ \\implies \u0026amp;\u0026amp; \\ln \\left( \\frac{I_{0}}{I_{1}}\\right) \u0026amp;= \\int_{x_{0}}^{x_{1}}A(x)dx \\end{align*} $$\nこの式を見ると、$I_{0}$はX線を撮影した時の強度であり、われわれが知っている値である。$I_{1}$は物体を通過した後の強度で、$x_{0}$に位置する検出器がこの値を測定する。従って、左辺はわれわれが知っている値である。\n右辺の積分範囲は、われわれが撮影したX線の進行路であるため、知っている。よって、X線の進行路$L$とその両端での強度$I_{0}$、$I_{1}$が与えられると、$A(x)$を路$L$に沿って積分した値が得られる。これを$A(x)$に対するラドン変換と呼ぶ。\n$$ \\mathcal{R}f (L) := \\int_{L} f(x) dx = \\ln \\left( \\frac{I_{0}}{I_{1}}\\right) $$\n性質 ラドン変換の基本的な性質は、以下の通りである。\n線形性\n$$ \\mathcal{R} \\left( \\alpha f + \\beta g \\right) = \\alpha \\mathcal{R}f + \\beta \\mathcal{R}g $$\n平行移動不変性shift invariance\n$$ \\mathcal{R}T_{\\mathbf{a}}f (s, \\boldsymbol{\\theta}) = T_{\\mathbf{a} \\cdot \\boldsymbol{\\theta}}\\mathcal{R}f(s,\\boldsymbol{\\theta}) $$\n回転不変性rotation invariance\n$$ RAf = ARf $$\n拡大縮小不変性dilation invariance\n$$ RD_{r}f = \\dfrac{1}D_{r}Rf $$\nTimothy G. Feeman, The Mathematics of Medical Imaging: A Beginner\u0026rsquo;s Guide. Springer, 2010, p4\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1945,"permalink":"https://freshrimpsushi.github.io/jp/posts/1945/","tags":null,"title":"ラドン変換"},{"categories":"상미분방정식","contents":"説明 係数が定数の微分方程式は、変数分離法を使ったり、積分因子法を使ったりするなどして比較的簡単に解くことができる。しかし、以下のように係数に独立変数が含まれている微分方程式は簡単に解くことができない。\n$$ \\begin{equation} P(x)\\dfrac{d^2 y}{dx^2} + Q(x)\\dfrac{dy}{dx}+R(x)y=0 \\label{1}\\end{equation} $$\nこの時、$P$、$Q$、$R$は多項式で共通の因数がないと仮定する。上の形を持つ方程式としては\nベッセル方程式Bessel equation\n$$ x^2 y^{\\prime \\prime} +xy^{\\prime}+(x^2-\\nu ^2)y=0,\\quad \\nu \\text{ is constant} $$\nルジャンドル方程式Legendre equation\n$$ (1-x^2)y^{\\prime \\prime}-2xy^{\\prime}+l(l+1)y=0,\\quad l \\text{ is constant} $$\nなどがある。このような微分方程式を解くときは、べき級数形式の解を見つけることを目標とする。\n定義1 $\\eqref{1}$の中で、$P(x_{0}) \\ne 0$である$x_{0}$を普通点ordinary pointという。$P$が連続であるために、$P(x) \\ne 0$で示されるような$x_{0}$を含む開いた区間が存在する。この時、我々の目標は普通点$x_{0}$の周辺で$\\eqref{1}$の解となるべき級数解を見つけることである。つまり、$\\eqref{1}$の解は\n$$ y=a_{0}+a_{1}(x-x_{0})+a_2(x-x_{0})^2+\\cdots = \\sum \\limits _{n=0}^\\infty a_{n}(x-x_{0})^n $$\nの形のべき級数であり、収束半径$|x-x_{0}| \u0026lt; \\rho$内で収束すると仮定して問題を解く。この方法を使って、係数に独立変数$x$が含まれる難しい微分方程式を解くことができる。\n一方、$P(x_{0})=0$である$x_{0}$は特異点singular pointという。特異点の中で、\n$$ \\lim \\limits_{x\\rightarrow x_{0}}(x-x_{0})\\frac{Q(x)}{P(x)} \u0026lt; \\infty\\quad \\mathrm{and}\\quad \\lim \\limits_{x\\rightarrow x_{0}} (x-x_{0})^{2}\\frac{R(x) }{P(x)}\u0026lt;\\infty $$\nを満たす特異点を正則特異点regular singular pointという。正則特異点でない場合は、非正則特異点irregular singular pointという。$x_{0}$が正則特異点の場合、解を次のように仮定して解法を始める。\n$$ y=\\sum \\limits_{n=0}^{\\infty}a_{n}x^{n+s} $$\nこの解法をフロベニウス法Frobenius methodという。\n例 $y^{\\prime \\prime}+y=0$, $-\\infty \u0026lt; x \u0026lt; \\infty$の級数解を求めよ。\n内容は長いが難しくはないので、ゆっくりと読もう。例として与えられた方程式は、わざわざ級数解を使わなくても十分に簡単に解ける方程式だが、級数解を使う解法を練習することに意味を見出そう。まず$y^{\\prime \\prime}+y=0$は$P(x)=1$、$Q(x)=0$、$R(x)=1$の場合だ。したがって、すべての点が普通点であるが、式を簡単にするために$x_{0}=0$を選ぼう。与えられた微分方程式の解が$|x| \u0026lt; \\rho$で収束する下記のべき級数と仮定しよう。\n$$ y=a_{0}+a_{1}x+a_2x^2+\\cdots = \\sum \\limits_{n=0}^\\infty a_{n}x^n $$\n微分方程式に代入するために、$y^{\\prime \\prime}$を計算すると\n$$ y^{\\prime \\prime}=2a_2+3\\cdot 2 a_{3} x + \\cdots +n(n-1)a_{n}x^{n-2}+\\cdots = \\sum \\limits_{n=2} ^\\infty n(n-1)a_{n}x^{n-2} $$\n与えられた微分方程式に$y$と$y^{\\prime \\prime}$を代入すると\n$$ \\sum \\limits_{n=2} ^\\infty n(n-1)a_{n}x^{n-2} + \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\n級数解法で重要な点は、$x$の次数を揃えることだ。一番最初の項の級数に$n+2$の代わりに$n$を代入すると次のようになる。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\sum \\limits_{n=0} ^\\infty (n+2)(n+1)a_{n+2}x^{n} + \\sum \\limits_{n=0}^\\infty a_{n}x^n\u0026amp;=0 \\\\ \\implies \u0026amp;\u0026amp; \\sum \\limits_{n=0} ^\\infty \\left[ (n+2)(n+1)a_{n+2}+ a_{n} \\right] x^{n}\u0026amp;=0 \\end{align*} $$\nべき級数の性質から、上の式が成り立つためには、すべての係数が$0$でなければならない。したがって、次の式を得る。\n$$ \\begin{align*} \u0026amp;\u0026amp; (n+2)(n+1)a_{n+2}+ a_{n}\u0026amp;=0 \\\\ \\implies \u0026amp;\u0026amp; (n+2)(n+1)a_{n+2}\u0026amp;=-a_{n} \\\\ \\implies\u0026amp;\u0026amp; a_{n+2}\u0026amp;=\\dfrac{-1}{(n+2)(n+1)} a_{n} \\end{align*} $$\nこのようにして、先行する係数と後続する係数の関係を説明する式を漸化式recurrence relationという。漸化式から各項の係数を求めることができる。$n+2$番目の係数は$n$番目の係数から求めることができるので、最初の二つの係数$a_{0}, a_{1}$を知れば、すべての係数を知ることができる。したがって、級数を大きく$a_{0}$と$a_{1}$の二つの部分に分けることができる。偶数の$n$に対して一般的に示すと、$n=2k(k=1,2,\\dots)$に対して次のようになる。\n$$ \\begin{align*} a_2 \u0026amp;=\\dfrac{-1}{2\\cdot 1}a_{0}=\\dfrac{-1}{2!}a_{0} \\\\ a_{4} \u0026amp;=\\dfrac{-1}{4\\cdot 3}{a_2}=\\dfrac{-1}{4\\cdot 3}\\dfrac{-1}{2!}a_{0}=\\dfrac{1}{4!}a_{0} \\\\ a_{6}\u0026amp;=\\dfrac{-1}{6!}a_{0} \\\\ \u0026amp;\\vdots \\\\ a_{n} \u0026amp;=a_{2k}=\\dfrac{(-1)^k}{(2k)!}a_{0} \\end{align*} $$\n奇数の$n$に対して一般的に示すと、$n=2k+1(k=1,2,\\dots)$に対して次のようになる。\n$$ \\begin{align*} a_{3} \u0026amp;=\\dfrac{-1}{3\\cdot 2}a_{1}=\\dfrac{-1}{3!}a_{1} \\\\ a_{5}\u0026amp;=\\dfrac{-1}{5 \\cdot 4}a_{3}=\\dfrac{-1}{5\\cdot 4}\\dfrac{-1}{3!}a_{0}=\\dfrac{1}{5!}a_{1} \\\\ a_{7}\u0026amp;=\\dfrac{-1}{7!}a_{1} \\\\ \u0026amp;\\vdots \\\\ a_{n} \u0026amp;=a_{2k+1}=\\dfrac{(-1)^k}{(2k+1)!}a_{1} \\end{align*} $$\n上記の結果を$y$に代入して整理すると次のようになる。\n$$ \\begin{align*} y\u0026amp;= a_{0}+a_{1}x-\\dfrac{a_{0}}{2!}x^2-\\dfrac{a_{1}}{3!}x^3+\\cdots +\\dfrac{(-1)^na_{0}}{(2n)!}x^{2n}+\\dfrac{ (-1)^{n} a_{1}}{(2n+1)!} x^{2n+1} +\\cdots \\\\ \u0026amp;=a_{0}\\left[ 1-\\dfrac{1}{2!}x^2+\\dfrac{1}{4!}x^4+\\cdots + \\dfrac{(-1)^n}{(2n)!}x^{2n} + \\cdots \\right] + a_{1} \\left[ x-\\dfrac{1}{3!}x^3 +\\dfrac{1}{5!}x^5+\\cdots +\\dfrac{ (-1)^n}{(2n+1)!}x^{2n+1} +\\cdots \\right] \\\\ \u0026amp;= a_{0} \\sum \\limits_{n=0}^\\infty \\dfrac{(-1)^n } {(2n)!} x^{2n}+a_{1}\\sum \\limits_{n=0}^\\infty \\dfrac{(-1)^n}{(2n+1)!}x^{2n+1} \\end{align*} $$\n与えられた2次微分方程式の二つの解と一般解を求めた。一般解は、下記の二つの独立した解の線形結合で表される。 $$ y_{1}(x)=\\sum \\limits_{n=0}^\\infty \\dfrac{(-1)^n } {(2n)!} x^{2n},\\quad y_{2}(x)=\\sum \\limits_{n=0}^\\infty \\dfrac{(-1)^n}{(2n+1)!}x^{2n+1} $$\n比率判定法を使うと、二つの級数$y_{1}, y_{2}$がすべての$x$に対して収束することが分かる。また、上記の二つの級数は正確に$\\cos$と$\\sin$のテイラー級数と一致する。つまり$y=a_{0}\\cos x+a_{1}\\sin x$であり、これは係数が定数の2次微分方程式の解法を通じて得られた解と同じである。\n■\nWilliam E. Boyce, Boyce\u0026rsquo;s Elementary Differential Equations and Boundary Value Problems (11th Edition, 2017), p195-219\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":888,"permalink":"https://freshrimpsushi.github.io/jp/posts/888/","tags":null,"title":"級数解を用いた微分方程式の解法"},{"categories":"동역학","contents":"モデル $$ \\dot{N} = rN $$\n変数 $N(t)$: $t$ 時点での集団の個体数を示している。 パラメータ $r \\in \\mathbb{R}$ : 固有増加率Intrinsic Rate of Increaseで、$0$ より大きければ成長し、$0$ より小さければ衰退する。出生率Birth Rate $b$ と 死亡率Death Rate $d$ の差 $r:=b-d$ で定義されることもある。 説明 人口動態は、数学生物学へと続く動力学の最初の通路であり、集団の個体数や種の共存などのテーマに対する数学的アプローチである。マルサス成長モデルはそのような成長モデルの中でも最もシンプルなもので、理想的に（何の障害もなく）繁殖できる種の個体数は、単純な常微分方程式で表すことができる。与えられた方程式は分離可能な1次微分方程式なので、その解は初期人口 $N_{0}$ に対して\n$$ N(t) = N_{0} e^{rt} $$\nのように求められ、式に指数関数が登場するため、指数成長モデルと呼ばれることがある。マルサス成長モデルという名前は、人口論の著者であるトーマス・ロバート・マルサスの名前に由来する。\n一見すると、個体数モデルがどこで役立つのか疑問に思うかもしれないが、より進化したモデルは実際にバイオ分野で使われており、菌やウイルス、特定の塩基配列の数に関連し、個体数という意味を超えて「動く量的変数」そのものとしてアプローチし、多くの応用モデルの基礎となっている。たとえば、インテルの共同創設者であるゴードン・ムーアは、「半導体集積回路の性能は2年ごとに倍増する」と述べたが、これはまさに指数的成長を意味し、現在はムーアの法則と呼ばれている。ムーアがマルサス成長モデルを適用したわけではないが、成長という現象が一般的にこのように説明されるため重要であるということである。バイオ分野以外にも、この成長は経済・経営においてサービスを使用する顧客や無リスク資産の元本合計に適用されることもあり、逆に原子核の放射性崩壊を原子核数の逆成長と見ることもできる。\n導出 集団の成長をモデリングするには、二分裂で繁殖する細菌を想像するとよい。個体数の増加速度は、各自の成長速度に加えて、全体の個体数自体にも比例する。たとえば、細菌が$10$匹いるペトリ皿$A$と、$20$匹いるペトリ皿$B$を考えると、ある時点で個体数が2倍に増加した場合、$A$には$20$匹、$B$には$40$匹がいることになる。つまり、\n$$ (N = 10 \\text{일 때의 증가량}) = 10 \\\\ (N = 20 \\text{일 때의 증가량}) = 20 $$\n変数$N$を使って表すと、\n$$ (N \\text{의 증가량}) = N $$\n成長速度を考慮できるように式を修正すると、ある実数$r \\in \\mathbb{R}$に対して\n$$ (N \\text{의 변화량}) = rN $$\nのように表すことができる。$r\u0026gt;0$ならば人口の変化量が正で、個体数が増加し、$r\u0026lt;0$ならば変化量が負で、時間と共に個体数が減少することになる。この変化量を言葉ではなく数式で表現するには、微分が必要である。$t$時点で$N$が変化する度合いは$dN / dt$であるので、左辺を修正すると次のようになる。\n$$ {{dN} \\over {dt}} = r N $$\n■\n限界 微分方程式という言葉が、方程式が怖いかもしれないが、一つ一つを紐解いてみると、すべてが常識的な前提から始まる導出であることに共感できるだろう。しかしながら、常識的な前提という説明に反して、このモデルの最大の問題点は、現実を全く反映できないことである。簡単でシンプルなため、教科書の最初の例としては適切かもしれないが、これだけでは実質的な応用は全くできない。\n実際の集団の成長は、いわゆるマルサスの罠に直面する。ペトリ皿に入れた細菌の例を続けて考えると、ペトリ皿という限られた環境では、彼らが永遠に成長し、繁殖するのに十分な栄養が提供されることはなく、その空間自体も有限である。結局、ある時点を迎えると成長が鈍化し、理想的な成長は停止する。\nこのモデルは最も単純であるため、実際の個体数に直接適用されることはほとんどなく、それ自体で1次項としての意味を持つか、非現実性を克服するために様々なモデルが用いられる。たとえば、死を反映させたり、複数の種が互いに競争するような方法が考えられるだろう。\n視覚的理解 マルサス成長モデルをエージェントベース シミュレーションで実装して、視覚적に理解しよう。\nアクション （すべてのエージェントが、毎ターン）\n繁殖：bの確率で自分の位置に新しいエージェントを作る。 死亡：dの確率でシミュレーションから除外される。 b # 번식률\rd # 사망률\rreplicated = (rand(N) .\u0026lt; b) # 번식 판정\rnew\\_coordinate = coordinate[replicated,:]\rcoordinate = coordinate[rand(N) .\u0026gt; d,:] # 사망 판정\rcoordinate = cat(coordinate, new\\_coordinate, dims = 1); エージェントの繁殖と死亡という単純なアクションだけを使用しており、システム独自の固有増加率パラメータ$r$が変わるとシステムがどのように変化するかをチェックするだけで十分である。\nシミュレーションで使用される繁殖確率 b とシステムの出生率 $b$、死亡確率 d とシステムの死亡率 $d$ は、一般的に同じではないことに注意する必要がある。自律システムによって記述される微分方程式に正確に一致するように、直感的にまあまあ適切なパラメータを見つける形でシミュレーションのフィットが行われた。\n$r\u0026gt;0$ の場合 N0 = 50 # 초기 인구수\rb = 0.05 # 번식률\rd = 0.02 # 사망률 シミュレーションなので最初は少し遠慮があるかもしれないが、出生率が死亡率を上回るため、最終的には爆発的な成長が起こる。\n$r\u0026lt;0$ の場合 N0 = 50 # 초기 인구수\rb = 0.04 # 번식률\rd = 0.05 # 사망률 ![malthusian_growth_integration2.gif](malthus\n","id":1871,"permalink":"https://freshrimpsushi.github.io/jp/posts/1871/","tags":["줄리아"],"title":"マルサス成長モデル：理想的な集団成長"},{"categories":"줄리아","contents":"コード もともとさくらすし店では、もっと詳しい説明を加えることが多いが、ジュリアでアニメGIFを作るのがどれほど簡単かを強調するために、できるだけ説明を短くする。\nランダムウォークをシミュレーションすることはさておき、上のようなアニメGIFを作ることは、言語によってはとても難しく、大変なことがある。しかし、ジュリアでは@animateマクロとgif()関数を使用することで、信じられないほど簡単にアニメGIFを作ることができる。原理は単純だ。ループの前にマクロを付け、ループを回してその都度フレームを直接描くだけだ。そうして集めたフレームを変数に入れ、gif()関数に入れればそれだけである。fpsオプションでは、秒間フレーム数を指定してアニメGIFの速さを調整できる。\nusing Plots\rrandom\\_walk = cumsum(rand(100).-.5)\ranim = @animate for t in 1:100\rplot(random\\_walk[1:t], legend = :none)\rend; gif(anim, \u0026#34;example.gif\u0026#34;, fps = 10) 別のパスを指定しなければ、ドキュメントに保存されることに注意しよう。これをうまく利用すれば、下に示すような素晴らしいアニメGIFを作ることもできる。\n","id":1863,"permalink":"https://freshrimpsushi.github.io/jp/posts/1863/","tags":null,"title":"ジュリアでGIFを作る方法"},{"categories":"선형대수","contents":"定義 ベクター空間 $V$の部分集合$M$に対して、次の式が成り立つ場合、$M$を凸集合convex setと言う。\n$$ \\lambda x +(1-\\lambda)y \\in M,\\quad \\forall \\lambda\\in[0,1],\\ \\forall x,y \\in M $$\n説明 この式を言葉で解くなら、「$M$が凸集合だとは、$M$に含まれる任意の二つのベクターの間にある全てのベクターもまた$M$に属している」という意味だ。また、$M$が部分空間であれば、足し算とスカラーの掛け算に対して閉じているので、凸集合である。\n","id":1914,"permalink":"https://freshrimpsushi.github.io/jp/posts/1914/","tags":null,"title":"ベクトル空間における凸集合"},{"categories":"알고리즘","contents":"定義 次の五つの塩基を主要な塩基Canonical Baseと呼ぶ。\nプリン塩基: アデニンAdenin $A$, グアニンGuanine $G$ ピリミジン塩基: シトシンCytosine $C$, チミンThymine $T$, ウラシルUracil $U$ 説明 チミンはDNAでのみ使われ、ウラシルはRNAでのみ使われる。したがって、データで $T$ か $U$ のどちらが使われているかだけ確認することで、それがDNAかRNAの塩基配列か分かる。\n水素結合が可能な二つの塩基が繋がれたものを塩基対Base Pairと言う。プリン塩基とピリミジン塩基からそれぞれ一つずつ選ばれ、その中で可能なケースは $A-T, A-U, G-C$ 三つがある。\n$A-T$ と $A-U$ は2つの水素結合で、$G-C$ は3つの水素結合で繋がれている。DNAは塩基対によって二重らせんの構造をとっている。そのため、片方の鎖に $A$ があれば、反対側の鎖には $T$ があることが分かる。 $$ A-T \\\\ C-G \\\\ C-G \\\\ G-C \\\\ T-A \\\\ T-A \\\\ A-T \\\\ C-G $$ 例えば、上のようなDNAサンプルがあると、片方の鎖だけを知っていればいい。だから、データ取得時には左側だけを読んで、次のように記録してもいい:ACCGTTAC二重らせん構造の意義は、「バックアップ」そのものだ。実際にRNAは一本鎖で不安定な構造をしており、問題を起こすことが多いが、DNAは片側の鎖に問題が生じても、反対側の鎖を参照することで、安定して遺伝情報を子孫に伝えられる。\n","id":1832,"permalink":"https://freshrimpsushi.github.io/jp/posts/1832/","tags":null,"title":"バイオインフォマティクスにおける主要な塩基と塩基対"},{"categories":"힐베르트공간","contents":"定義1 $\\left( X, \\left\\langle \\cdot, \\cdot \\right\\rangle \\right)$を内積空間としよう。二つの元 $\\mathbf{x}, \\mathbf{y}\\in X$が$\\left\\langle \\mathbf{x}, \\mathbf{y} \\right\\rangle =0$を満たすなら、$\\mathbf{y}$と$\\mathbf{x}$は互いに直交すると言い、以下のように表記する。\n$$ \\mathbf{x} \\perp \\mathbf{y} $$\n元の集合$X$、$\\left\\{ \\mathbf{x}_{k} \\right\\}_{k\\in \\mathbb{N}}$が次の式を満たすなら、直交システムあるいは直交集合と呼ぶ。\n$$ \\left\\langle \\mathbf{x}_{k}, \\mathbf{x}_{\\ell} \\right\\rangle =0\\quad \\forall k\\ne \\ell $$\n直交システム$\\left\\{ \\mathbf{x}_{k} \\right\\}_{k\\in \\mathbb{N}}$が次の式を満たす場合、正規直交システムあるいは正規直交集合と呼ぶ。\n$$ \\left\\| \\mathbf{x}_{k} \\right\\| =1\\quad \\forall k\\in \\mathbb{N} $$\n説明 内積空間で、ノルムは$\\left\\| \\cdot \\right\\|:=\\sqrt{\\left\\langle \\cdot,\\cdot \\right\\rangle }$として定義されるので、正規直交システムの定義を再記すれば以下のようになる。\n$$ \\left\\| \\mathbf{x}_{k} \\right\\| = \\left\\langle \\mathbf{x}_{k},\\mathbf{x}_{\\ell} \\right\\rangle = \\begin{cases} 1 \u0026amp; \\text{if}\\ k=\\ell \\\\ 0 \u0026amp; \\text{if}\\ k\\ne \\ell \\end{cases} $$\nまた、直交システムが可算集合に対して定義される必要は特にない。\n定義2 $A$を任意のインデックス集合、$\\alpha$、$\\beta$を$A$のインデックスとしよう。元の集合$X$、$\\left\\{ \\mathbf{x}_{\\alpha} \\right\\}_{\\alpha \\in A}$が次の式を満たすなら、直交システムあるいは直交集合と呼ぶ。\n$$ \\left\\langle \\mathbf{x}_{\\alpha}, \\mathbf{x}_{\\beta} \\right\\rangle =0\\quad \\forall \\alpha \\ne \\beta $$\n直交システム$\\left\\{ \\mathbf{x}_{\\alpha} \\right\\}_{\\alpha \\in A}$が次の式を満たす場合、正規直交システムあるいは正規直交集合と呼ぶ。\n$$ \\left\\| \\mathbf{x}_{\\alpha} \\right\\| =1\\quad \\forall \\alpha \\in A $$\n説明 従って、正規直交システム$\\left\\{ \\mathbf{x}_{\\alpha} \\right\\}_{\\alpha \\in A}$に対して、以下の式を得る。\n$$ \\left\\langle \\mathbf{x}_{\\alpha},\\mathbf{x}_{\\beta} \\right\\rangle =\\begin{cases} 1 \u0026amp; \\text{if}\\ \\alpha=\\beta \\\\ 0 \u0026amp; \\text{if}\\ \\alpha \\ne \\beta \\end{cases} $$\nOle Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010), p66-67\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWalter Rudin, Real and Complex Analysis (3rd Edition, 1987), p82-83\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1912,"permalink":"https://freshrimpsushi.github.io/jp/posts/1912/","tags":null,"title":"内積空間における直交性、直交集合、正規直交集合"},{"categories":"알고리즘","contents":"ビルドアップ 化学的合成によって単量体が繰り返し連結された高分子をポリマーPolymerと言う。 リン酸Phosphoric Acidは無機酸素酸の一種で、化学式は$H_{3}PO_{4}$である。 5つの炭素原子を持つ単糖類をペントースPentoseと言う。 遺伝情報の基本単位として機能する分子を窒素塩基Nitrogenous base、または簡単に塩基Baseと言う。 リン酸-ペントース-塩基からなり、核酸の単位体になる分子をヌクレオチドNucleotideと言う。 生命現象が発生する上で必須の生体高分子として、ヌクレオチドのポリマーを核酸Nucleic Acidと言う。 リボースというペントースを基盤として鎖構造を形成する核酸を**リボ核酸(Ribo Nucleic Acid, RNA)**と言う。 ヌクレオチドのポリマーの二つの長い鎖が互いに絡み合って二重螺旋構造を成す核酸を**デオキシリボ核酸(Deoxyribo Nucleic Acid, DNA)**と言う。 DNAまたはRNAを遺伝物質Genetic Materialと言う。 染色体との比較 これらの説明を定義と受け入れれば、DNAとRNAは遺伝物質である以前に実在する高分子であり、ヌクレオチドがどのような塩基で構成されているかによって、その組み合わせは無限にあるだろう。RNAは順序がある鎖の形を取り、DNAはその鎖の二つが塩基対を形成し二重螺旋の構造を持つ。これは、遺伝物質を覗き込みその順序を記録することで遺伝情報を得られるということだ。\nこうした情報化を起点として、私たちは化学と生物学からの決別を告げる。染色体との違いに関心を持つなら、DNAやRNAが撚り合わさって固まったものが染色体であり、情報を読み取りメモを取る行為が塩基配列である。染色体は物理的なもの、塩基配列はデータ的なものと見ても良い。\n定義 遺伝物質の塩基を記号で順番に並べたものを塩基配列Nucleic Sequenceと言う。 主要な塩基と文字 生命情報工学の文脈では、主要な塩基は5つの文字$A, T, G, C, U$に限定される。これらは前から順にアデニン、チミン、グアニン、シトシン、ウラシルを意味し、コンピュータ科学的に見れば、情報化された塩基配列はこれら5つの文字からなる文字列（String）となる。データ分析者が扱うのは、正確にはDNAやRNAそのものではなく、それらの塩基配列であるということをはっきりさせよう。\n塩基配列を扱う際の最も原始的な問題は、これらの配列のサイズが決して小さくないということだ。例えば、人間のゲノムは、そのサイズがなんと33億塩基対に及び、これを分析して意味ある結果を出すためには、単に前から後ろへ読むだけの単純なアプローチよりも賢い方法が必要であろう。\nまた、塩基配列の方向は上流と下流を確認することで特定できるので、塩基配列が逆転していることを心配する必要はない。\n","id":1828,"permalink":"https://freshrimpsushi.github.io/jp/posts/1828/","tags":null,"title":"生物情報学における塩基配列"},{"categories":"푸리에해석","contents":"概要 フーリエ変換の定義や記法は、作者のニーズや好みによってさまざまに表れる。だから、教科書や講義、論文などでフーリエ変換を扱う前に、定義や記法をしっかりと把握しておくことが多い。知っている概念だと思って定義を省略して読んでいると、式がおかしいと感じることがあるので、よく確認する必要がある。もちろん、最も重要なのは、これらの定義が本質的に全て同じであることなので、記法や定義自体について大きく心配する必要はない。この文書では、各定義の長所と短所、そして違いについて紹介する。\n説明1 フーリエ変換は、周期が実数全体である関数のフーリエ級数を考える過程から自然に導かれる。その過程で、フーリエ変換とフーリエ逆変換は次のように定義される。\nフーリエ変換 フーリエ逆変換 $\\displaystyle \\hat{f}(\\xi):=\\int _{-\\infty} ^{\\infty}f(x)e^{-i \\xi x}dx$ $\\displaystyle f(x):=\\frac{1}{2\\pi}\\int _{-\\infty} ^{\\infty}\\hat{f}(\\xi)e^{i \\xi x}d\\xi$ ここで、$\\hat{}$は「ハット」と読む。$\\hat{f}(\\xi)$は「エフハット クシ」と読む。作用素としての感じ、積分変換としての感じを強調したいときや、微分を意味する${}^{\\prime}$記号と$\\hat{}$記号を一緒に使う必要があるとき、または混乱を避けるために、以下のような記法で書くこともある。\nフーリエ変換 フーリエ逆変換 $\\mathcal{F}:L^{1} \\to L^{1}$ $\\mathcal{F}^{-1}:L^{1} \\to L^{1}$ $\\displaystyle \\mathcal{F}f(\\xi):=\\int _{-\\infty} ^{\\infty}f(x)e^{-i \\xi x}dx$ $\\displaystyle \\mathcal{F}^{-1}f(x):=\\frac{1}{2\\pi}\\int _{-\\infty} ^{\\infty}f(\\xi)e^{i \\xi x}d\\xi$ $\\mathscr{F}$も使われることがある。記法の違いはあるが、フーリエ変換自体の定義も以下のように異なる場合がある。\nフーリエ変換 フーリエ逆変換 $\\displaystyle \\hat{f}(\\xi):=\\frac{1}{\\sqrt{2\\pi}}\\int _{-\\infty} ^{\\infty}f(x)e^{-i \\xi x}dx$ $\\displaystyle f(x):=\\frac{1}{\\sqrt{2\\pi}}\\int _{-\\infty} ^{\\infty}\\hat{f}(\\xi)e^{i \\xi x}d\\xi$ $\\displaystyle \\hat{f}(\\xi):=\\int _{-\\infty} ^{\\infty}f(x)e^{-2\\pi i \\xi x}dx$ $\\displaystyle f(x):=\\int _{-\\infty} ^{\\infty}\\hat{f}(\\xi)e^{2\\pi i \\xi x}d\\xi$ 説明の便宜のため、上記の各定義を以下のように表記しよう。\n$$ \\tilde{f}(\\xi):=\\frac{1}{\\sqrt{2\\pi}}\\int _{-\\infty} ^{\\infty}f(x)e^{-i \\xi x}dx \\quad \\text{and} \\quad \\check{f}(\\xi):=\\int _{-\\infty} ^{\\infty}f(x)e^{-2\\pi i \\xi x}dx $$\nフーリエ変換の定義が多様な理由は、下記の表から見ることができる。\nプランシェレルの定理 畳み込み 導関数のフーリエ変換 $\\| \\hat{f} \\|^{2} =2\\pi\\left\\| f \\right\\|^{2}$ $(f \\ast g)\\hat{}=\\hat{f}\\hat{g}$ $(f^{\\prime})\\hat{} (\\xi)=i\\xi \\hat{f}(\\xi)$ $\\| \\tilde{f} \\|^{2}=\\left\\| f \\right\\|^{2}$ $(f \\ast g)\\tilde{}=\\sqrt{2\\pi}\\tilde{f}\\tilde{g}$ $(f^{\\prime})\\tilde{} (\\xi)=i\\xi \\tilde{f}(\\xi)$ $\\| \\check{f} \\|^{2}=\\left\\| f \\right\\|^{2}$ $(f \\ast g)\\check{}=\\check{f}\\check{g}$ $(f^{\\prime})\\check{} (\\xi)=2\\pi i\\xi \\check{f}(\\xi)$ この表から見てわかるように、定義によっては、定数$2\\pi$が出現する式が異なる場合がある。したがって、どのような式を簡単にしたいかによって、定義が異なる場合がある。経験的には、信号や画像処理分野では$\\check{f}$のような定義が多く使われる。また、フーリエ変換の定義では、指数にマイナス$(-)$がない場合もある。その場合は、逆変換側に付いているので、知っている定義と異なると混乱しないように注意しよう。\nGerald B. Folland, Fourier Analysis and Its Applications (1992), p223-224\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1898,"permalink":"https://freshrimpsushi.github.io/jp/posts/1898/","tags":null,"title":"フーリエ変換の複数の定義と記法"},{"categories":"알고리즘","contents":"ビルドアップ 化学的合成を通じたモノマーの連続した結合によってつくられる高分子をポリマーPolymerと言う。 リン酸Phosphoric Acidは無機酸素酸の一種であり、化学式は$H_{3}PO_{4}$である。 5個の炭素原子を持つ単糖をペントースPentoseと言う。 遺伝情報の基本単位として機能する分子を窒素塩基Nitrogenous base、あるいは単に塩基Baseと呼ぶ。 リン酸－ペントース－塩基から成り、核酸のユニットとなる分子をヌクレオチドNucleotideという。 生命現象の発生に不可欠な生体高分子として、ヌクレオチドのポリマーを核酸Nucleic Acidと言う。 リボースというペントースを基本として鎖構造をもつ核酸を**リボ核酸(Ribo Nucleic Acid, RNA)**と言う。 ヌクレオチドのポリマーである２つの長い鎖が互いにねじれた二重螺旋構造をもつ核酸を**デオキシリボ核酸(Deoxyribo Nucleic Acid, DNA)**と言う。 DNAまたはRNAを遺伝物質Genetic Materialと言う。 DNAとRNAの違い これらの言葉は、それ自体で生命情報工学では大きな意味を持たないかもしれないが、知らなければ馬鹿になった気がするので、暇なときにウィキでも探しておくことをお勧めする。\nDNAとRNAの違いは以下の通り:\nDNAは塩基対があり、二本鎖でありRNAは一本鎖である。したがって、RNAよりDNAの方がずっと安定である。 DNAは$A,T,C,G$を主要な塩基とし、RNAは$A,U,C,G$を使う。 定義 細胞の遺伝物質が核に散らばっている染色質の形のまま凝縮されたものを染色体Chromosomeという。 塩基配列との比較 染色体は真核生物の細胞分裂過程で観察され、通常、親から一方ずつ受け継ぎ、対を成す。人間の場合、22対の常染色体と1対のXY型性染色体を持つ。\n塩基配列の違いに興味があれば、DNAやRNAがねじれて固まって染色体になり、その一方で、情報を読み取って書き留めたメモが塩基配列であると思ってもいい。染色体は物理的なもの、塩基配列はデータ的なものだと見ることができる。\n生命情報工学では、特に物理的な構造に関心がない限り、データ分析家が染色体を扱うことはないかもしれない。しかし、DNA、RNA、遺伝子、ゲノムなどと共に、何が何であるかを正確に知らないと非常に混乱するため、知っておくのが良い。\n","id":1827,"permalink":"https://freshrimpsushi.github.io/jp/posts/1827/","tags":null,"title":"生命医療情報学におけるDNA、RNA、染色体"},{"categories":"줄리아","contents":"概要 距離行列Distance Matrixは、パーティクルダイナミクスParticle DynamicsやムービングエージェントMoving Agentベースのシミュレーションなどによく使用されるが、実際には準備された関数がなく、自分で計算するコードを書くことは大変なことが多い。Juliaでは、pairwise()やDistancesパッケージのEuclidean()関数を使用して、以下のように簡単に距離行列を計算できる1。\ndimsオプションを使用すると、行と列の方向を指定できる。見ての通り、$\\mathbb{R}^{5 \\times 3}$行列が与えられたときに、$5$次元ベクトルの$3$個の距離を計算したり、$3$次元ベクトルの$5$個の距離を計算することができる。\nコード using Distances\rcoordinate = [2 3 4; 5 1 3; 1 7 5; 1 7 6; 2 4 3]\rpairwise(Euclidean(), coordinate; dims=1)\rpairwise(Euclidean(), coordinate; dims=2) 上記のコードを実行した結果は、以下の通りである。\njulia\u0026gt; using Distances\rjulia\u0026gt; coordinate = [2 3 4; 5 1 3; 1 7 5; 1 7 6; 2 4 3]\r5×3 Array{Int64,2}:\r2 3 4\r5 1 3\r1 7 5\r1 7 6\r2 4 3\rjulia\u0026gt; pairwise(Euclidean(), coordinate; dims=1)\r5×5 Array{Float64,2}:\r0.0 3.74166 4.24264 4.58258 1.41421\r3.74166 0.0 7.48331 7.81025 4.24264\r4.24264 7.48331 0.0 1.0 3.74166\r4.58258 7.81025 1.0 0.0 4.3589\r1.41421 4.24264 3.74166 4.3589 0.0\rjulia\u0026gt; pairwise(Euclidean(), coordinate; dims=2)\r3×3 Array{Float64,2}:\r0.0 9.64365 7.07107\r9.64365 0.0 3.31662\r7.07107 3.31662 0.0 最適化 距離行列計算の最適化方法 https://discourse.julialang.org/t/pairwise-distances-from-a-single-column-or-vector/29415/3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1799,"permalink":"https://freshrimpsushi.github.io/jp/posts/1799/","tags":null,"title":"ジュリアで距離行列を計算する方法"},{"categories":"줄리아","contents":"コード サイズ指定 julia\u0026gt; empty = Array{Float64, 2}(undef, 3, 4)\r3×4 Array{Float64,2}:\r3.39519e-313 3.18299e-313 4.66839e-313 1.061e-313\r4.03179e-313 5.51719e-313 1.6976e-313 4.24399e-314\r2.97079e-313 4.66839e-313 7.00259e-313 5.0e-324 上のコードを実行すると、空の配列が作成される。たまに1.76297e-315のような変な値が入っているように見えるが、これは0に非常に近い値で、初期化には大きな問題がない。\nArray{X, Y}(undef, ...)はデータ型XでY次元配列を、該当するデータ型の未定値でサイズ...だけ埋めた配列になる。ここでのポイントはundefだ。\n可変配列 一次元配列の場合、括弧の中に何も入れずに、簡単に空の配列を作ることができる。\njulia\u0026gt; empty = Array{Float64, 1}()\rFloat64[]\rjulia\u0026gt; empty = Array{Float64, 2}()\rERROR: MethodError: no method matching Array{Float64,2}()\rClosest candidates are:\rArray{Float64,2}(::UndefInitializer, ::Int64, ::Int64) where T at boot.jl:408\rArray{Float64,2}(::UndefInitializer, ::Int64...) where {T, N} at boot.jl:412\rArray{Float64,2}(::UndefInitializer, ::Tuple{Int64,Int64}) where T at boot.jl:416\r...\rStacktrace:\r[1] top-level scope at REPL[85]:1 しかし、同じ方法で二次元配列を作ろうとすると、上述のようにMethodErrorが発生する。もちろん自然な二次元配列ではないが、一次元配列の一次元配列を作るような形で空の配列を作ることは可能だが、速度の面ではネイティブな文法をそのまま使うことを推奨する。\njulia\u0026gt; empty = Array{Array{Float64, 1}, 1}()\rArray{Float64,1}[] もっと簡単な方法 下のように波括弧を使うと、もっと簡単に配列を作ることができる。\njulia\u0026gt; empty = Float64[]\rFloat64[]\rjulia\u0026gt; empty = Array{Float64, 1}[]\rArray{Float64,1}[]\rjulia\u0026gt; empty = Array{Float64, 2}[]\rArray{Float64,2}[] 環境 OS: Windows julia: v1.5.0 ","id":1797,"permalink":"https://freshrimpsushi.github.io/jp/posts/1797/","tags":null,"title":"ジュリアで空の配列を作成する方法"},{"categories":"수리물리","contents":"定義 3次元のスカラー関数 $f=f(x,y,z)$のグラディエントのダイバージェンスを $f$のラプラシアンLaplacianと言い、$\\nabla^{2}$で表される。\n$$ \\nabla ^{2} f := \\nabla \\cdot(\\nabla f)= \\frac{ \\partial^{2} f}{ \\partial x^{2} }+\\frac{ \\partial^{2} f}{ \\partial y^{2}}+\\frac{ \\partial^{2} f}{ \\partial z^{2}} $$\n説明 ラプラシアンという名称はフランスの数学者ラプラスから取られている。$\\nabla^{2}$という表記は便宜上使われているものだ。数学（偏微分方程式論）では$\\Delta$という表記をもっと多く使う。ラプラシアンを一言で言うならば、2階の微分の拡張である。グラディエントが1階の微分を3次元に拡張したものであれば、ラプラシアンは2階の微分を3次元に拡張したものだ。高校の微分の時間にこんな内容を学んだはずだ。\n1\n1階の微分は単純に関数$f$が増えているのか減っているのかの情報しか与えないが、2階の微分はどう増えたり減ったりしているのかの情報を与える。$f$のラプラシアンを求める式は上で示された通り、ダイバージェンスを求める式に微分が一回増えただけだ。\n導出 導出らしいことはない。\n$$ \\begin{align*} \\nabla \\cdot (\\nabla f) \u0026amp;= \\nabla \\cdot \\left( \\frac{ \\partial f}{ \\partial x },\\frac{ \\partial f}{ \\partial y},\\frac{ \\partial f}{ \\partial z} \\right) \\\\ \u0026amp;= \\frac{ \\partial ^{2} f }{ \\partial x^{2} }+\\frac{ \\partial ^{2} f }{ \\partial y^{2} } + \\frac{ \\partial ^{2}f }{ \\partial z^{2} } \\end{align*} $$\n■\n関連項目を見る デル演算子 $\\nabla$ グラディエント $\\nabla f$ ダイバージェンス $\\nabla \\cdot \\mathbf{F}$ カール $\\nabla \\times \\mathbf{F}$ ラプラシアン $\\nabla^{2} f$ EBS 2021学年度 大学入試特講 微積分 p.70\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1879,"permalink":"https://freshrimpsushi.github.io/jp/posts/1879/","tags":null,"title":"三次元デカルト座標系におけるスカラー関数のラプラシアン"},{"categories":"수리통계학","contents":"定義 1 確率変数 $X$ と確率変数のシーケンス $\\left\\{ X_{n} \\right\\}$ が次を満たすとき、$n \\to \\infty$ のとき $X$ に確率収束すると言い、$X_{n} \\overset{P}{\\to} X$ と示される。 $$ \\forall \\varepsilon \u0026gt; 0 , \\lim_{n \\to \\infty} P \\left[ \\left| X_{n} - X \\right| \u0026lt; \\varepsilon \\right] = 1 $$\n説明 確率収束の条件は、文字通り確率のセンスで収束を定義したもので、簡単に言えば、$n$が大きくなると二つの確率変数が非常に小さい誤差 $\\varepsilon$ をもって同じになる確率が $100%$ だと見ればいい。まさに確率収束の言葉とピッタリ合う。数式に使われるときは、同等でありながらより便利な次の表現をよく使う。 $$ \\forall \\varepsilon \u0026gt; 0 , \\lim_{n \\to \\infty} P \\left[ \\left| X_{n} - X \\right| \\ge \\varepsilon \\right] = 0 $$ 確率変数はサンプル空間から実数への関数として知られているが、二つの関数を比較した時、その差が $\\varepsilon$ に比較されるので、解析学的なセンスでは関数の一様収束に相当するだろう。このような類似性は、一様収束すれば点収束するように、確率収束すれば分布収束するという事実にも繋がる。急に出てきた エプシロンが嬉しくないなら、今からでも慣れるか数理統計学を諦めるか選ばないといけない。統計学で $n$ が大きくなるとは、単に任意の数を無限大に送るという意味ではなく、サンプルが十分に多いという仮定を数学的に表現したものだから、確率論を使って理論を展開する数理統計学でサンプルの数を議論できなければ、まさに言うことができないだろう。解析学がどれほど不得意でも、このポストでの証明[3]のPart 1.くらいは読んで理解できるように努力した方がいい。確率収束について次のような常識的な性質を紹介する。\n定理 $X_{n} \\overset{P}{\\to} X$ とする。\n[1] 連続写像定理: 連続関数 $g$ に対して $$ g\\left( X_{n} \\right) \\overset{P}{\\to} g (X) $$ $$ X_{n} \\overset{P}{\\to} X \\implies X_{n} \\overset{D}{\\to} X $$ [3]: $a \\in \\mathbb{R}$ が定数で $ Y_{n} \\overset{P}{\\to} Y$ ならば $$ aX_{n} \\overset{P}{\\to} a X \\\\ X_{n} + Y_{n} \\overset{P}{\\to} X + Y \\\\ X_{n} Y_{n} \\overset{P}{\\to} XY $$ 証明 [1] 学部レベルを超える証明があって、わざわざ数理統計学のレベルまで落ちて知る必要はない。受け入れて先に進んでも構わない。\n■\n2 直接演繹する。\n■\n[3] Part 1. $aX_{n} \\overset{P}{\\to} a X $\n連続写像定理の直接的な結論としても得ることができるが、解析学的な証明の例として直接演繹を試みる。$a = 0$ の場合は自明に成立するので、$a \\ne 0$ としよう。\n$\\varepsilon \u0026gt; 0$ とすると、確率 $P$ の中の式から $|a|$ を割ることによって次の等式を得る。 $$ \\begin{align*} P \\left( \\left| a X_{n} - aX \\right| \\ge \\varepsilon \\right) =\u0026amp; P \\left( |a| \\left| X_{n} - X \\right| \\ge \\varepsilon \\right) \\\\ =\u0026amp; P \\left( \\left| X_{n} - X \\right| \\ge {{ \\varepsilon } \\over { |a| }} \\right) \\end{align*} $$ $n \\to \\infty$ の時 $X_{n} \\overset{P}{\\to} X$ と仮定したので、最後の項は $n \\to \\infty$ の時 $0$ に収束し、したがって最初の項に極限をとると次を得る。 $$ \\lim_{n \\to \\infty} P \\left( \\left| a X_{n} - aX \\right| \\ge \\varepsilon \\right) = 0 $$\nPart 2. $X_{n} + Y_{n} \\overset{P}{\\to} X + Y$\n不等式の向きを間違えなければ、それほど難しくない。三角不等式によると $$ \\left| \\left( X_{n} - X \\right) + \\left( Y_{n} - Y \\right) \\right| \\le \\left| X_{n} - X \\right| + \\left| Y_{n} - Y \\right| $$ である。以下の図に従って 二つの事象の包含関係 $$ \\color{blue}{\\left( \\left| X_{n} - X \\right| + \\left| Y_{n} - Y \\right| \\ge \\varepsilon \\right) } \\subset \\color{orange}{ \\left[ \\left( \\left| X_{n} - X \\right| \\ge \\varepsilon / 2 \\right) \\cup \\left( \\left| Y_{n} - Y \\right| \\ge \\varepsilon / 2 \\right) \\right] } $$ が成立することがわかる。今、$\\varepsilon \\le \\left| \\left( X_{n} - X \\right) + \\left( Y_{n} - Y \\right) \\right|$ と仮定すると $$ \\begin{align*} P \\left[ \\left| \\left( X_{n} + Y_{n} \\right) - \\left( X + Y \\right) \\right| \\ge \\varepsilon \\right] =\u0026amp; P \\left[ \\left| \\left( X_{n} - X \\right) + \\left( Y_{n} - Y \\right) \\right| \\ge \\varepsilon \\right] \\\\ \\le \u0026amp; P \\left[ \\color{blue}{ \\left| X_{n} - X \\right| + \\left| Y_{n} - Y \\right| \\ge \\varepsilon } \\right] \\\\ \\le \u0026amp; P \\left[ \\color{orange}{ \\left( \\left| X_{n} - X \\right| \\ge \\varepsilon / 2 \\right) \\cup \\left( \\left| Y_{n} - Y \\right| \\ge \\varepsilon / 2 \\right) } \\right] \\\\ \\le \u0026amp; P \\left[ \\left| X_{n} - X \\right| \\ge \\varepsilon / 2 \\right] + P \\left[ \\left| Y_{n} - Y \\right| \\ge \\varepsilon / 2 \\right] \\end{align*} $$ $n \\to \\infty$ の時最後の項が $0$ に収束するので、次を得る。 $$ \\lim_{n \\to \\infty} P \\left[ \\left| \\left( X_{n} + Y_{n} \\right) - \\left( X + Y \\right) \\right| \\ge \\varepsilon \\right] \\le 0 $$\nPart 3. $X_{n} Y_{n} \\overset{P}{\\to} XY$\n$$ g(x) := x^{2} $$ は連続関数なので、定理 [1] により $X_{n} \\overset{P}{\\to} X$ であり、 $$ \\begin{align*} X_{n} Y_{n} =\u0026amp; {{ 1 } \\over { 2 }} X_{n}^{2} + {{ 1 } \\over { 2 }} Y_{n}^{2} - {{ 1 } \\over { 2 }} \\left( X_{n} - Y_{n} \\right)^{2} \\\\ \u0026amp;\\overset{P}{\\to}\u0026amp; {{ 1 } \\over { 2 }} X^{2} + {{ 1 } \\over { 2 }} Y^{2} - {{ 1 } \\over { 2 }}\\left( X - Y \\right)^{2} \\\\ =\u0026amp; XY \\end{align*} $$\n■\n厳密な定義 測度論で定義される確率収束 Hogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p295.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1789,"permalink":"https://freshrimpsushi.github.io/jp/posts/1789/","tags":null,"title":"数理統計学における確率収束"},{"categories":"확률론","contents":"統計 1 次は、連続写像定理の測度論的記述です。\n距離空間 $\\left( S , d \\right)$ と $\\left( S' , d\u0026rsquo; \\right)$ について、$g : S \\to S'$ が $C_{g} \\subset S$ から連続だとしましょう。$S$ の確率要素 $X$ に対して、$P \\left( X \\in C_{g} \\right) = 1$ ならば $X$ に収束する確率要素のシーケンス $\\left\\{ X_{n} \\right\\}_{n \\in \\mathbb{N}}$ に対して次のように成り立ちます。 $$ X_{n} \\overset{D}{\\to} X \\implies g \\left( X_{n} \\right) \\overset{D}{\\to} g(X) \\\\ X_{n} \\overset{P}{\\to} X \\implies g \\left( X_{n} \\right) \\overset{P}{\\to} g(X) \\\\ X_{n} \\overset{\\text{a.s.}}{\\to} X \\implies g \\left( X_{n} \\right) \\overset{\\text{a.s.}}{\\to} g(X) $$\n$C_{g} \\subset S$ は関数 $g$ が連続である点の集合を示します。 $\\overset{P}{\\to}$、$\\overset{D}{\\to}$、$\\overset{\\text{a.s.}}{\\to}$ はそれぞれ確率収束、分布収束、殆ど確実に収束を意味します。 説明 連続関数をかけても収束性が保証される性質は、収束の定義方法に関わらず、数学全般で一般的に見られる現象です。しかし、Continuous Mapping Theoremという名称は、主に確率論で使用されます。有名な系として、学部の数理統計学のレベルでステートメント程度にのみ紹介されるスラッツキーの定理Slutsky\u0026rsquo;s Theoremがあります。\nスラッツキーの定理2： 定数 $a,b$ と確率変数 $A_{n}, B_{n} ,X_{n} , X$ について、$a_{n} \\overset{P}{\\to} a $、$ B_{n} \\overset{P}{\\to} b $、$ X_{n} \\overset{D}{\\to} X $ の場合、 $$ A_{n} + B_{n} X_{n} \\overset{D}{\\to} a + b X $$\n事実自体は、学部の数理統計学のような基礎的な科目でも使われると仮定されますが、背景知識なしに理解しやすい証明を見つけるのは難しく、そのために測度論が引用された証明が紹介されました。もし自分が学部生で実解析の理解が足りないなら、証明を理解できなくても普通であり、失望する必要はありません。むしろ、もっと難しい数学を学ぶ必要があると思う、とりあえず事実としてうまく使おうという気持ちで十分です。\n証明 分布収束 混合定理の系として得られる。\n■\n確率収束 $\\varepsilon \u0026gt; 0$ を固定し、任意の $\\delta \u0026gt; 0$ に対して次の集合 $C_{g}^{\\delta} \\subset C_{g}$ を定義しましょう。 $$ C_{g}^{\\delta}:= \\left\\{ x \\in C_{g} \\mid \\exists y : y \\in B \\left( x;\\delta \\right) \\land g(y) \\notin B ' \\left( g(x) ; \\varepsilon \\right) \\right\\} $$ この集合は、$g$ が連続である点 $x$ を集め、$g(y)$ と $g(x)$ から十分に遠い $y$ を選ぶことができ、半径 $\\delta$ 内にあります。もちろん、$\\delta \u0026gt; 0$ が小さくなるほど、そのような $y$ が半径の中に存在する可能性は減りますが、自明で $\\displaystyle \\lim_{\\delta \\to 0} C_{g}^{\\delta} = \\emptyset$ です。今 $d\u0026rsquo; \\left( g(X) , g \\left( X_{n} \\right) \\right) \\ge \\varepsilon$ と仮定してみると、少なくとも次の3つのうち1つは真である必要があります：\n(1): $d \\left( X , X_{n} \\right) \u0026gt; \\delta$：最初から $X$ と $X_{n}$ が遠すぎるため、$g$ が連続であっても $g(X)$ と $ g \\left( X_{n} \\right) $ も遠い。 (2): $X \\in C_{g}^{\\delta}$：$X$ は連続だが、$\\delta$ 半径内の $X_{n}$ に対しては、$g(X_{n})$ と $g (X)$ の距離が遠い。 (3): $X \\notin C_{g}$：$X$ が連続でないため、$g(X)$ と $g \\left( X_{n} \\right)$ が遠い。 これを確率を使って数式で表すと $$ P \\left( d\u0026rsquo; \\left( g \\left( X_{n} \\right) , g(X) \\right) \u0026gt; \\varepsilon \\right) \\le P \\left( d \\left( X_{n} , X \\right) \\ge \\delta \\right) + P \\left( X \\in C_{g}^{\\delta} \\right) + P \\left( X \\notin C_{g} \\right) $$ 右辺の各項は\n(1): 前提から $X_{n} \\overset{P}{\\to} X$ なので、全ての $\\delta \u0026gt;0$ に対して $$ \\lim_{n \\to \\infty} P \\left( d \\left( X_{n} , X \\right) \\ge \\delta \\right) = 0 $$ (2): 上記で $\\displaystyle \\lim_{\\delta \\to 0} C_{g}^{\\delta} = \\emptyset$ としたので $$ \\lim_{\\delta \\to 0} P \\left( X \\in C_{g}^{\\delta} \\right) = 0 $$ (3): 前提から $P \\left( X \\in C_{g} \\right) = 1$ なので $$ P \\left( X \\notin C_{g} \\right) = P \\left( X \\in C_{g}^{c} \\right) = 0 $$ まとめると $$ \\lim_{n \\to \\infty} P \\left( d\u0026rsquo; \\left( g \\left( X_{n} \\right) , g (X) \\right) \u0026gt; \\varepsilon \\right) = 0 $$\n■\n殆ど確実に収束 $g$ が連続である点 $\\omega \\in C_{g}$ に対して、 $$ \\lim_{n \\to \\infty} X_{n} (\\omega) = X (\\omega) \\implies \\lim_{n \\to \\infty} g \\left( X_{n} (\\omega) \\right) = g \\left( X (\\omega) \\right) $$ 事象として見て、包含関係で示すと $$ \\left[ \\lim_{n \\to \\infty} X_{n} (\\omega) = X (\\omega) \\right] \\subset \\left[ \\lim_{n \\to \\infty} g \\left( X_{n} (\\omega) \\right) = g \\left( X (\\omega) \\right) \\right] $$ 前提から $X_{n} \\overset{\\text{a.s.}}{\\to} X$、すなわち $\\displaystyle P \\left( \\lim_{n \\to \\infty } X_{n} = X , X \\in C_{g} \\right) = 1$ なので $$ \\begin{align*} P \\left[ \\lim_{n \\to \\infty} g \\left( X_{n} (\\omega) \\right) = g \\left( X (\\omega) \\right) \\right] \\ge \u0026amp; P \\left[ \\lim_{n \\to \\infty} g \\left( X_{n} (\\omega) \\right) = g \\left( X (\\omega) \\right) , X \\in C_{g} \\right] \\\\ \\ge \u0026amp; P \\left[ \\lim_{n \\to \\infty} X_{n} (\\omega) = X (\\omega) , X \\in C_{g} \\right] \\\\ =\u0026amp; 1 \\end{align*} $$\n■\nhttps://en.wikipedia.org/wiki/Continuous_mapping_theorem\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p306.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1787,"permalink":"https://freshrimpsushi.github.io/jp/posts/1787/","tags":null,"title":"連続写像定理の証明"},{"categories":"해석개론","contents":"要約 1 $F$、$G$が区間$[a,b]$で微分可能であり、$F^{\\prime}=f$、$G^{\\prime}=g$が積分可能であるとしよう。すると、次の式が成立する。\n$$ \\begin{align*} \\int _{a} ^{b} F(x)g(x)dx \u0026amp;= F(b)G(b)-F(a)G(a)-\\int _{a} ^{b}f(x)G(x)dx \\\\ \u0026amp;= \\left[ F(x)G(x) \\right]_{a}^{b} -\\int _{a} ^{b}f(x)G(x)dx \\end{align*} $$\n説明 この結果は部分積分法と呼ばれる。積分 - 微分 - 積分[積分]- $\\int$ 微分 積分と覚えると簡単だ。積分するものはそのまま左右に書き、微分するものは前にそのまま書き、後ろには微分して書く。\n$$ \\begin{align*} \\int Fg \u0026amp;= \\left[ FG \\right] - \\int fG \\\\ \u0026amp;= \\left[ \\text{그냥}\\cdot\\text{적분} \\right] - \\int \\text{미분}\\cdot\\text{적분} \\end{align*} $$\n証明 微分可能なら連続であり、連続なら積分可能であるので、$F, G$も積分可能だ。今、$H(x)=F(x)G(x)$とする。すると、積の微分法則により、次が成立する。\n$$ H^{\\prime}(x)=F(x)g(x)+f(x)G(x) $$\n積分は線形であり、関数の積は積分可能性を保持するので、$H^{\\prime}$は積分可能だ。すると、微分積分学の基本定理2によって、$H^{\\prime}$の定積分は次のように計算される。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\int _{a} ^{b}H^{\\prime}(x)dx \u0026amp;= H(b)-H(a) \\\\ \\implies \u0026amp;\u0026amp; \\int _{a} ^{b}H^{\\prime}(x)dx \u0026amp;= F(b)G(b)-F(a)G(a) \\\\ \\implies \u0026amp;\u0026amp; \\int _{a} ^{b}F(x)g(x) + f(x)G(x) dx \u0026amp;= F(b)G(b)-F(a)G(a) \\\\ \\implies \u0026amp;\u0026amp; \\int _{a} ^{b}F(x)g(x)dx \u0026amp;=F(b)G(b)-F(a)G(a)-\\int _{a} ^{b}f(x)G(x) \\end{align*} $$\n■\nウォルター・ルーディン, 数学解析の原理 (第3版, 1976), p134\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1867,"permalink":"https://freshrimpsushi.github.io/jp/posts/1867/","tags":null,"title":"部分積分法"},{"categories":"해석개론","contents":"要約1 関数 $f$が区間 $[a,b]$でリーマン積分可能であり、$F^{\\prime}=f$を満たす$[a,b]$で微分可能な関数 $F$が存在するとする。その場合、以下が成り立つ。\n$$ \\int_{a}^{b} f(x) dx= F(b)-F(a) $$\n解説 この定理は、微分積分学の基本定理2としてよく知られている。FTC2Funcamental Theorem of Calculus1と略されることが多い。$f$の定積分は、不定積分である$F$の両端の値の差で表されることを意味する。\n証明 $\\varepsilon \u0026gt;0$が与えられたとする。すると$f$は$[a,b]$で積分可能であるので、必要十分条件により、以下を満たす区間$[a,b]$の分割 $P=\\left\\{a= x_{0}, \\cdots, x_{n}=b \\right\\}$が存在する。\n$$ U(P,f)-L(P,f) \u0026lt; \\varepsilon $$\n$F$が微分可能であると仮定すると、連続である。そのため、平均値の定理により、以下を満たす $t_{i}\\in [x_{i-1},x_{i}]$が存在する。\n$$ F(x_{i})-F(x_{i-1})=f(t_{i})\\Delta x_{i},\\quad (i=1,\\dots,n) $$\n上の式を全ての $i$について加えると、以下のようになる。\n$$ \\begin{align*} \\sum \\limits _{i=1} ^{n} f(t_{i})\\Delta x_{i}\u0026amp;=\\left( F(b)-F(x_{n-1}) \\right)+\\cdots+\\left( F(x_{1})-F(a) \\right) \\\\ \u0026amp;= F(b) -F(a) \\end{align*} $$\n補助定理\n$$ \\left| \\sum \\limits_{i=1} ^{n} f(t_{i})\\Delta \\alpha_{i} - \\int _{a} ^{b}f (x)d\\alpha (x) \\right| \u0026lt; \\varepsilon $$\n上の補助定理により、以下が成立する。\n$$ \\begin{align*} \\left| \\sum \\limits _{i=1} ^{n} f(t_{i})\\Delta \\alpha_{i} - \\int _{a} ^{b}f (x)d\\alpha (x) \\right| \u0026amp;= \\left|\\big( F(b)-F(a) \\big) - \\int _{a} ^{b}f (x)d\\alpha (x) \\right| \\\\ \u0026amp;\u0026lt; \\varepsilon \\end{align*} $$\nここで、$\\varepsilon$は任意の正数であるので、以下を得る。\n$$ \\int _{a} ^{b}f (x)d\\alpha (x)=F(b)-F(a) $$\n■\n参照 FTC1\n微分積分学におけるFTC2\nWalter Rudin, Principles of Mathmatical Analysis (3rd Edition, 1976), p134\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1866,"permalink":"https://freshrimpsushi.github.io/jp/posts/1866/","tags":null,"title":"解析学における微分積分学の基本定理2"},{"categories":"머신러닝","contents":"定義 1 ロジスティック関数は、微分方程式の解として$y ' = y(1-y)$で求められるものである。 $$ y(t) = {{ 1 } \\over { 1 + e^{-t} }} $$\n説明 もっと一般的な形では、$\\displaystyle f(x) := {{ L } \\over { 1 + e^{-k(x-x_{0})} }}$のような形も使われる。ロジスティック関数はシグモイド関数であり、動力学、統計学、ディープラーニング、生物学など、多くの分野で言及され、多用されている関数だ。\nロジスティック？ 問題は、なぜ「ロジスティック関数」と呼ばれるのか、ということだ。Logisticの意味を調べてみると、「物流の」、「軍需の」、あるいは「記号論理学の」という言葉が出てくるが、関数の形を見てもそれらと関連があるようにはとても見えない。ロジスティック回帰は、回帰分析を応用しロジスティック関数を導入する分類技法で、これも軍需や記号論理学とは関係がない。\n一番説得力のある推測は、まさにこのロジスティック関数が出てきた背景であるロジスティック微分方程式 $y\u0026rsquo;=y(1-y)$ の意味を考えることだった。ロジスティック微分方程式は、主に人口増加モデルとして言及され、多くの分野で広範囲に使用されている。ある集団の個体数が爆発的に増加し、食糧不足などの理由で成長が鈍る様子を描いている。\nここで「食糧」とは、就職者にとっては仕事であり、好気性細菌にとっては酸素となるなど、状況に応じて何でも扱うことができる。おそらく「その何か」を適切に表現できる上品な表現がLogisticではなかったかと思われる。軍需というのは単に食糧を意味するのではなく、戦争に必要なもの全般を指し、物流も特定の物品を指すわけではない。科学技術の外の世界でのロジスティックの意味と合致するなら、おそらくこのように理解されるのが妥当だろう。\nhttps://en.wikipedia.org/wiki/Logistic_function\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1775,"permalink":"https://freshrimpsushi.github.io/jp/posts/1775/","tags":null,"title":"ロジスティック関数とは？"},{"categories":"동역학","contents":"정리 $2$次元の多様体 $\\mathcal{P}$ と関数 $f,g \\in C^{r} \\left( \\mathcal{P} \\right)$ に対して、次のようなベクトル場が微分方程式として与えられているとする。 $$ \\dot{x} = f(x,y) \\\\ \\dot{y} = g(x,y) $$ $\\mathcal{M}$ このベクトル場が有限個の不動点を持つ不変集合である場合、$p \\in \\mathcal{M}$ のオメガリミットセット $\\omega (p)$ は次の三つのうちの一つを満たす：\n(1): $\\omega (p)$ は単元素集合である。つまり、ただ一つの不動点のみを含む。 (2): $\\omega (p)$ は閉じた軌道である。 (3): $\\omega (p)$ は有限個の不動点 $p_{1} , \\cdots , p_{n}$ のいくつかの $i,j \\in [1,n]$ に対して次を満たす軌道 $\\gamma$ から構成される。 $$ \\alpha ( \\gamma ) = \\left\\{ p_{i} \\right\\} \\\\ \\omega ( \\gamma ) = \\left\\{ p_{j} \\right\\} $$ 説明 距離空間は当然 $T_{1}$ 空間であり、$T_{1}$ では単元素集合が閉集合であることが保証されるため、$\\omega (p) = \\left\\{ p \\right\\}$ は当然閉軌道と言えるが、ステートメントの文脈上、一つの不動点のみを含む場合は別のものとして区別しよう。\n実際、ポアンカレ・ベンディクソンの定理では、カオスというものは定義される必要もなく、カオスが起こらないというステートメント自体は系に近い。定理が言うのは単にオメガリミットセットの分類であり、それが正確に我々が知っているもので構成されているため、カオスが起こることはないという事実が導かれるのである。しかし、このような定理があることにより、カオス理論の関心は$2$次元を確実に超えることができるようになる。\n定理の直観的な理解はそれほど難しくない。$\\mathcal{M}$ がバウンドされていない場合はそもそもカオスにならず、バウンドされている場合は永遠に伸び続けることはできず、フローが狭まって回るか広がって回るかしなければならない。しかし、$3$次元とは異なり、$2$次元では線が平面を二つの領域に分けてしまうため、そのうちの一つの領域を諦めなければならない状況が常に起こる。これは、限られた空間である$\\mathcal{M}$ の残りの部分を時が経つにつれて捨てていくようなものと見ることができる。まだ通過していない領域を使用するために、すでに通過したフローを通過しようとすると、その瞬間それは閉じた軌道となり、結局閉じた軌道か不動点に収束することになり、カオスを引き起こすことはできない。\n証明 1 戦略: ポアンカレの名前が付いた定理らしく、位相数学的である。$\\mathcal{P}$ 内部で連続で連結なアーク(continuous, connected arc)を一つ$\\Sigma$としよう。\n$\\Sigma$のすべての点での法線ベクトルとベクトル場の内積が$0$でなく、符号も変わらない場合、$\\Sigma$は$\\mathcal{P}$上のベクトル場を横切るTransverseと言う。この概念は一点に対してのみ考えることもできるが、その点ではベクトル場と$\\Sigma$は接触しないだろう。フローの観点からは、一点で出会うだけでなく、$\\Sigma$を貫通することになる。\n与えられたベクトル場で作られるフローを$\\phi_{t}$、フロー$\\phi_{t}$の下で一点$p \\in \\mathcal{P}$の正の時間に対する軌道を$O_{+}(p)$として表そう。一点$p_{i}$がフロー$\\phi_{t}$の下で時間$t$の流れに従って$p_{j}$に到達するまでの軌道を$\\widehat{p_{i} p_{j}} \\subset O_{+} (p)$として表そう。また、オメガリミットセットを表す$\\omega ( \\cdot )$は元々与えられた一点に対して定義されていたが、ある集合$X$に対する$\\omega \\left( X \\right)$は次のように考えればよい。\n$$ \\omega (X) := \\bigcup_{x \\in X} \\omega (x) $$ これはアルファリミットセット$\\alpha ( \\cdot )$も同様に定義されたと考えればよい。\nその他、次のような補助定理を続けて使用することになる。\n補助定理(オメガリミットセットの性質): 全体空間がユークリッド空間$X = \\mathbb{R}^{n}$であり、フロー$\\phi_{t} ( \\cdot )$でコンパクト不変集合$\\mathcal{M}$の一点$p \\in \\mathcal{M}$が与えられているとする：\n[1]: $\\omega (p) \\ne \\emptyset$ [2]: $\\omega (p)$は閉集合である。 [3]: $\\omega (p)$はフローに不変である。つまり、$\\omega (p)$は軌道の合併である。 [4]: $\\omega (p)$は連結空間である。 まず、$2$次元で生じるオメガリミットセットは何らかの面積を持つ形状ではないため、以降言及されるオメガリミットセットは何らかの曲線の形状と考えればよい。\nPart 1.\n$\\Sigma \\subset \\mathcal{M}$がベクトル場を横切るアークである場合、$\\mathcal{M}$が$2$次元ベクトル場の不変集合であるため、$\\Sigma$がベクトル場の流れに逆らって$\\mathcal{M}$の外へ出ることはできない。したがって、任意の$p \\in \\mathcal{M}$に対して、$O_{+} (p)$と$\\Sigma$が交わる$k$番目の点を$p_{k}$とすると、$p_{k}\\subset \\widehat{p_{k-1} p_{k+1}} \\subset O_{+} (p)$でなければならない。つまり、フローが$\\mathcal{M}$内部に向かって収束していくが、その過程で$\\Sigma$と交わる交点が近づいてまた遠ざかることは起こらないということである。\nPart 2. $p \\in \\mathcal{M}$のオメガリミットセット$\\omega (p)$は$\\Sigma$と多くとも一点でしか交差しない。\n背理法で示す。$\\omega (p)$と$\\Sigma$が異なる二点$q , \\overline{q}$で交差すると仮定してみる。\nその場合、オメガリミットセットの定義により、$n \\to \\infty$のとき $$ q_{n} \\to q \\\\ \\overline{q}_{n} \\to \\overline{q} $$ を満たすシーケンス$\\left\\{ q_n \\right\\}_{n \\in \\mathbb{N}} , \\left\\{ \\overline{q}_n \\right\\}_{n \\in \\mathbb{N}} \\subset O_{+} (p)$が存在する。しかし、Part 1によれば、これらの交点はある順序$p_{1} , p_{2} , \\cdots$に並べられるため、仮定に矛盾する。したがって、$\\omega (p)$と$\\Sigma$は最初から交差しないか、交差するとしてもただ一点でのみ交差する。[ 注: トーラスの場合には、この論理をそのまま適用することはできないが、いくつかの部分に分けて$\\mathcal{M}$と同じ形状にすることで同じ結論を得ることができる。 ]\nPart 3. $\\omega (p)$が不動点を含まない場合、閉じた軌道である。\n$q \\in \\omega (p)$の軌道$O_{+}(q)$が閉じた軌道であることを示し、その後$\\omega (p) = O_{+} (q)$であることを示せばよい。\nPart 3-1. 軌道$O_{+}(q)$は閉じている。 点$x \\in \\omega (q)$を一つ選んでみると、補助定理[2]により$\\omega (p)$が閉じており、不動点を持たない軌道の合併であるため、$x$も不動点であってはならない。$p,q$が混乱しないように、仮定は$\\omega (p)$が不動点を持たないことであり、$x$は$x \\in \\omega (q)$であるため、必ずしも$x \\in \\omega (p)$である保証はないが、いずれにせよ不動点ではないと言える。この不動点でない一点$x$のベクトル場を横切る一つのアーク$\\Sigma_{x}$を選ぼう。**Part 1.**によれば、$\\Sigma_{x}$と$O_{+} (q)$の交点のシーケンス$\\left\\{ q_{n} \\right\\}_{n \\in \\mathbb{N}}$は$n \\to \\infty$のとき$q_{n} \\to x$であり、$x \\in \\mathcal{M}$であるため、**Part 2.**により$\\forall n \\in \\mathbb{N}$に対して$q_{n} = x$でなければならない。$x$は不動点ではないため、$O_{+} (q)$が$x$と交差する場合、離れた後に再び戻って交差しなければならない。ここで$x \\in \\omega (q)$としたので、$O_{+}(q)$は$x$に近づいて止まることなく、実際に$x$と交差し、したがって$O_{+}(q)$は閉じた軌道となる。 Part 3-2. $O_{+}(q) = \\omega (p)$ 点$q \\in \\omega (p)$からベクトル場を横切る一つのアーク$\\Sigma_{q}$を選んでみると、Part 2により$\\omega (p)$と$\\Sigma_{q}$はただ$q$でのみ出会う。補助定理[3]により$\\omega (p)$は軌道の合併であるため、$q \\in \\omega (p)$であれば$O_{+} (q) \\subset \\omega (p)$であるが、$\\omega (p)$は不動点を含まず連結空間であるため、正確に$O_{+}(q) = \\omega (p)$でなければならない。 Part 4. $p \\in \\mathcal{M}$に対して異なる$p_{1} , p_{2} \\in \\omega (p)$がベクトル場の不動点である場合、$\\alpha (\\gamma) = \\left\\{ p_{1} \\right\\}$と$\\omega (\\gamma) = \\left\\{ p_{2} \\right\\}$を満たす軌道$\\gamma \\subset \\omega (p)$は多くても一つしか存在しない。\n背理法で示す。二点を結ぶ異なる二つの軌道があれば、その二つの軌道の間に面積を持つ何らかの領域$\\mathcal{K}$が生じるだろう、そこから矛盾を導く。次の条件を満たす異なる二つの軌道$\\gamma_{1} , \\gamma_{2} \\subset \\omega (p)$が存在すると仮定しよう。 $$ \\alpha \\left( \\gamma_{i} \\right) = \\left\\{ p_{1} \\right\\} \\\\ \\omega \\left( \\gamma_{i} \\right) = \\left\\{ p_{2} \\right\\} $$ これらの軌道から一点ずつ$q_{1} \\in \\gamma_{1}$、$q_{2} \\in \\gamma_{2}$を選び、$q_{1}$と$q_{2}$からベクトル場を横切るアークを$\\Sigma_{1}, \\Sigma_{2}$として選ぶ。\n$\\gamma_{1} , \\gamma_{2} \\subset \\omega (p)$であるため、Part 2により、$O_{+} (p)$が$\\Sigma_{1}$と一点$a$で交差した後、$\\Sigma_{2}$は一点$b$で交差するとしよう。すると、$2$次元多様体上で次のような経路に囲まれた部分領域$\\color{red}{\\mathcal{K}}$が生じるだろう。\n$$ q_{1} \\overset{\\Sigma_{1}}{\\to} a \\overset{ O_{+} (p) }{ \\to } b \\overset{\\Sigma_{2}}{\\to} q_{2} \\overset{ \\omega (\\gamma) }{ \\to } p_{2} \\overset{ \\gamma_{1} }{ \\gets } q_{1} $$ 記法$\\displaystyle x \\overset{\\mathcal{C}}{\\to} y$は点$x,y$がカーブ$\\mathcal{C}$に繋がれたことを意味して使用された。$\\color{red}{\\mathcal{K}}$から始まったフローは$\\gamma_{1} , \\gamma_{2}$を超えることができないため、$\\color{red}{\\mathcal{K}}$は不変集合となる。しかし、$p$から始まった軌道$O_{+}(p)$が$\\color{red}{\\mathcal{K}}$に入ると、二度と出ることはできないということは、$\\gamma_{1}$や$\\gamma_{2}$が$\\omega (p)$に属することはできないということである。例えば$\\gamma_{2}$を考えると、$q_{2} \\overset{\\gamma_{2}}{\\to} p_{2}$は$\\omega (p)$に属することができるかもしれないが、その前部分である$p_{1} \\overset{\\gamma_{2}}{\\to} q_{2}$には行けない。したがって、$\\gamma_{2}$全体が$\\omega (p)$に属するという主張はできず、$\\gamma_{1} , \\gamma_{2} \\subset \\omega (p)$と矛盾する。\nPart 5.\nこのパートでは、不動点でない点を正則点Regular Pointと呼ぼう。必ずしもこのパートに限定する必要はないが、不動点の否定という文脈が頻繁に出てこないのに対し、正則Regularという表現は学問を問わず頻繁に使用されるため、注意や警告なしに使用すると大きな混乱を引き起こす可能性があるためである。\nケース1. $\\omega (p)$が不動点のみを持つ場合 $\\mathcal{M}$は有限個の不動点を持ち、$\\omega (p)$は連結空間であるため、ただ一つの不動点のみを持たなければならない。 ケース2. $\\omega (p)$が正則点のみを持つ場合 Part 3により、$\\omega (p)$は閉じた軌道である。 ケース3. $\\omega (p)$が不動点と正則点の両方を持つ場合 正則点のみからなる軌道$\\gamma \\subset \\omega (p)$を考える。 $\\gamma$は正則点のみからなっているため、Part 3により、$\\omega ( \\gamma )$と$\\alpha (\\gamma)$は閉じた軌道であるが、その一方で不動点を持たなければならない。しかし、補助定理[4]により、$\\omega ( \\gamma )$は連結空間であるため、閉じた軌道と不動点が離れていることはできず、不動点は閉じた軌道のどこかに位置していなければならないが、これはすなわち$\\omega ( \\gamma )$が不動点のみを含む単元素集合であるということである。同じ議論を$\\alpha ( \\gamma )$で繰り返すと、$\\omega (p)$のすべての正則点はそのオメガリミットポイントとアルファリミットポイントとして不動点を持つことがわかる。\n$\\omega (p)$は上記の三つのケースのいずれかに属していなければならない。これで証明は終わりである。\n■\nWiggins. (2003). Introduction to Applied Nonlinear Dynamical Systems and Chaos Second Edition(2nd Edition): 118~120.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1788,"permalink":"https://freshrimpsushi.github.io/jp/posts/1788/","tags":null,"title":"プアンカレ-ベンディクソン定理の証明"},{"categories":"선형대수","contents":"定義 積分変換 $J$と二つの関数 $f$、$g$が与えられたとする。以下の条件を満たす関数 $f \\ast g$を$J$に対する$f$と$g$のコンボリューションconvolution, 合成積と定義する。\n$$ J(f \\ast g)=(Jf)(Jg) $$\n説明 定義によると、積の積分変換であるコンボリューションは、積分変換の積に分けられる。すなわち、一つの積分で束ねられていた二つの関数を二つの積分に分けることができるという意味である。積分で表されるforward operatorの逆変換を求める時に役立つテクニックである。\n一般的にコンボリューションと言った場合は、具体的に言及しなくても、フーリエ変換のコンボリューションを指す。\nフーリエ変換 $$ (f \\ast g)(x) =\\int _{-\\infty} ^{\\infty}f(y)g(x-y)dy $$\nラプラス変換 $$ (f \\ast g)(x) = \\int_{0}^{x}f(x-y)g(y)dy $$\nメリン変換 $$ ( f\\times g)(x)=\\int _{0} ^{\\infty} f(y)g \\left( \\frac{x}{y} \\right)\\frac{dy}{y} $$\n","id":1848,"permalink":"https://freshrimpsushi.github.io/jp/posts/1848/","tags":null,"title":"畳み込みの一般的な定義"},{"categories":"선형대수","contents":"定義 関数空間から関数空間へのマップ$J$が下のような積分で定義される場合、$J$を積分変換integral transformという。\n$$ (Jf) (x) = \\int_{a}^{b} K(x,t)f(t)dt $$\n$$ J : f(\\cdot) \\mapsto \\int_{a}^{b} K(\\cdot,t)f(t)dt $$\nこの時、$K$を$J$のカーネルkernelという。$Jf$から$f$へのマップが存在する場合、これを$J^{-1}$と表示し、$J$の逆変換inverse transformという。\n説明 積分は線型だから、積分変換は線型変換だ。\n積分領域が必ずしも有界である必要はない。$a=-\\infty$であろうと$b=\\infty$であろうと、またはその両方であろうと関係ない。積分変換は上の定義に従ってどう作っても構わないが、適切な意味を持つためには、与えられた問題を$f$で解くより$Jf$で解くほうが簡単であるか、または逆変換が存在し、$Jf$と$f$を自由に変えられる必要がある。積分変換の例としては次のようなものがある。\nフーリエ変換 $\\mathcal{F}$:\n$$ \\mathcal{F}f(\\xi)=\\int _{-\\infty} ^{\\infty} f(x)e^{i \\xi x}dx,\\quad K(x,\\xi)=e^{i\\xi x} $$\nラプラス変換 $\\mathcal{L}$:\n$$ \\mathcal{L}f(s)=\\int _{0} ^{\\infty}f(t)e^{-st}dt,\\quad K(t,s)=e^{-st} $$\nメリン変換 $\\mathcal{M}$:\n$$ \\mathcal{M}f(s)=\\int_{0}^{\\infty} f(x)x^{s-1}dx,\\quad K(x,s)=x^{s-1} $$\nラドン変換 $\\mathcal{R}$:\n$$ \\mathcal{R}f(s,\\theta)=\\int_{-\\infty}^{\\infty}f(s\\cos\\theta-t\\sin\\theta, s\\sin\\theta+t\\cos\\theta)dt $$\n参照 畳み込み ","id":1847,"permalink":"https://freshrimpsushi.github.io/jp/posts/1847/","tags":null,"title":"積分変換"},{"categories":"힐베르트공간","contents":"まとめ1 $(H, \\langle \\cdot ,\\cdot \\rangle)$を内積空間としよう。すると、以下の不等式が成り立ち、これをコーシー・シュワルツの不等式Cauchy-Schwarz inequalityという。\n$$ \\left| \\langle x,y \\rangle \\right| \\le \\langle x,x \\rangle^{1/2} \\langle y,y \\rangle ^{1/2},\\quad \\forall x,y \\in H $$\n説明 内積からノルムを定義できるので、次の式で表すこともできる。\n$$ \\left| \\left\\langle x, y \\right\\rangle \\right| \\le \\left\\| x \\right\\| \\left\\| y \\right\\|,\\quad \\forall x,y\\in H $$\n証明 ケース 1. $x=0$ あるいは $y=0$\n一般性を失わずに$x=0$としよう。すると、内積の定義により\n$$ \\left| \\langle 0,y \\rangle \\right| = \\left| \\langle 0x,y \\rangle \\right| =0\\left| \\langle x,y\\rangle \\right|=0 $$\nよって成立する。\nケース 2. $x\\ne0$, $y\\ne0$で、$\\langle x,y \\rangle \\in \\mathbb{R}$の場合\n内積の定義により\n$$ \\begin{align*} 0 \\le\u0026amp; \\left\\langle x-\\frac{\\langle x,y \\rangle}{\\langle y,y \\rangle} y, x-\\frac{\\langle x,y \\rangle}{\\langle y,y \\rangle} y \\right\\rangle \\\\ =\u0026amp;\\ \\langle x,x \\rangle - \\frac{\\langle x,y \\rangle}{\\langle y,y \\rangle} \\langle x,y \\rangle -\\frac{\\langle x,y \\rangle}{\\langle y,y \\rangle} \\langle y,x \\rangle +\\frac{\\langle x,y \\rangle^{2}}{\\langle y,y \\rangle^{2}}\\langle y,y \\rangle \\end{align*} $$\nこの時、$\\langle x,y \\rangle \\in \\mathbb{R}$なので、$\\langle x,y\\rangle=\\overline{\\langle y,x \\rangle}=\\langle y,x \\rangle$である。したがって\n$$ \\begin{align*} 0 \\le\u0026amp; \\langle x,x \\rangle - 2\\frac{\\langle x,y \\rangle}{\\langle y,y \\rangle} \\langle x,y \\rangle +\\frac{\\langle x,y \\rangle^{2}}{\\langle y,y \\rangle^{2}}\\langle y,y \\rangle \\\\ =\u0026amp;\\ \\langle x,x \\rangle - 2\\frac{\\langle x,y \\rangle^{2}}{\\langle y,y \\rangle} +\\frac{\\langle x,y \\rangle^{2}}{\\langle y,y \\rangle} \\\\ =\u0026amp;\\ \\langle x,x \\rangle - \\frac{\\langle x,y \\rangle^{2}}{\\langle y,y \\rangle} \\end{align*} $$\nこの時、$\\langle y,y \\rangle \u0026gt;0$なので、両辺にかければ\n$$ \\begin{align*} \u0026amp;\u0026amp; 0 \\le\u0026amp; \\langle x,x \\rangle \\langle y,y \\rangle - \\langle x,y \\rangle ^{2} \\\\ \\implies \u0026amp;\u0026amp; \\langle x,y \\rangle ^{2} \\le\u0026amp; \\langle x,x \\rangle \\langle y,y \\rangle \\\\ \\implies \u0026amp;\u0026amp; \\left| \\langle x,y \\rangle \\right| \\le\u0026amp; \\langle x,x \\rangle ^{1/2} \\langle y,y \\rangle ^{1/2} \\end{align*} $$\nケース 3. $x\\ne0$, $y\\ne 0$で、$\\langle x,y\\rangle \\in \\mathbb{C}$の場合\n$\\left| \\lambda \\right| =1$で、$\\lambda \\langle x,y \\rangle\\in [0,\\infty)$を満たす$\\lambda \\in \\mathbb{C}$を一つ選ぼう。すると\n$$ \\left| \\langle x,y \\rangle \\right| =\\left| \\lambda \\right| \\left| \\langle x,y \\rangle \\right|=\\left| \\lambda \\langle x,y \\rangle \\right|= \\lambda\\langle x,y \\rangle =\\langle \\lambda x,y \\rangle $$\nよって、ケース 2により\n$$ \\begin{align*} \\left| \\langle x,y \\rangle \\right| =\u0026amp;\\ \\langle \\lambda x,y \\rangle \\\\ \\le\u0026amp; \\langle \\lambda x, \\lambda x \\rangle ^{1/2} \\langle y,y \\rangle ^{1/2} \\\\ =\u0026amp;\\ \\langle x,x\\rangle^{1/2}\\langle y,y\\rangle ^{1/2} \\end{align*} $$\n■\nOle Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010), p62-23\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1843,"permalink":"https://freshrimpsushi.github.io/jp/posts/1843/","tags":null,"title":"内積空間とコーシー・シュワルツの不等式"},{"categories":"힐베르트공간","contents":"定義1 $X$をベクトル空間とする。$\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in X$ 及び $\\alpha, \\beta \\in \\mathbb{C}$(または $\\mathbb{R}$)に対して、次の条件を満たす関数\n$$ \\langle \\cdot , \\cdot \\rangle : X \\times X \\to \\mathbb{C} $$\nを内積と定義し、$\\left( X, \\langle \\cdot ,\\cdot \\rangle \\right)$を内積空間と呼ぶ。\n線形性: $$\\langle \\alpha \\mathbf{x} + \\beta \\mathbf{y} ,\\mathbf{z} \\rangle =\\alpha \\langle \\mathbf{x},\\mathbf{z}\\rangle + \\beta \\langle \\mathbf{y},\\mathbf{z}\\rangle$$ 共役対称性: $$\\langle \\mathbf{x},\\mathbf{y} \\rangle = \\overline{ \\langle \\mathbf{y},\\mathbf{x} \\rangle}$$ 正定値性: $$\\langle \\mathbf{x},\\mathbf{x} \\rangle \\ge 0 \\quad \\text{and} \\quad \\langle \\mathbf{x},\\mathbf{x} \\rangle = 0\\iff \\mathbf{x}=0$$ 説明 線形性と共役対称性から、次の式が得られる。\n$$ \\begin{align*} \\langle \\mathbf{x},\\alpha \\mathbf{y}+\\beta \\mathbf{z} \\rangle =\u0026amp;\\ \\overline{\\langle \\alpha \\mathbf{y}+\\beta \\mathbf{z} ,\\mathbf{x} \\rangle} \\\\ =\u0026amp;\\ \\overline{\\alpha \\langle \\mathbf{y},\\mathbf{x} \\rangle +\\beta \\langle \\mathbf{z},\\mathbf{x} \\rangle} \\\\ =\u0026amp;\\ \\overline{\\alpha}\\overline{\\langle \\mathbf{y},\\mathbf{x} \\rangle}+\\overline{\\beta} \\overline{\\langle \\mathbf{z},\\mathbf{x} \\rangle} \\\\ =\u0026amp;\\ \\overline{\\alpha}\\langle \\mathbf{x},\\mathbf{y}\\rangle + \\overline{\\beta} \\langle \\mathbf{x},\\mathbf{z} \\rangle \\end{align*} $$\nこれは二番目の要素に対してアンチリニアであることを意味する。物理学、工学等では、内積が少しだけ異なって定義されることもある。たとえば、第一成分に対してアンチリニアで、第二成分に対してはリニアに定義されることもある。一方で内積空間では、以下のようにコーシー・シュワルツの不等式が成り立つ。\n$(X, \\langle \\cdot ,\\cdot \\rangle)$が内積空間であるとする。すると、以下の不等式が成り立ち、これをコーシー・シュワルツの不等式と呼ぶ。\n$$ \\left| \\langle \\mathbf{x},\\mathbf{y} \\rangle \\right| \\le \\langle \\mathbf{x},\\mathbf{x} \\rangle^{1/2} \\langle \\mathbf{y},\\mathbf{y} \\rangle ^{1/2},\\quad \\forall \\mathbf{x},\\mathbf{y} \\in X $$\nまた、内積から以下のようにノルムを定義できる。\n$$ \\left\\| \\mathbf{x} \\right\\| := \\sqrt{\\langle \\mathbf{x},\\mathbf{x} \\rangle},\\quad \\mathbf{x}\\in X $$\nこのように内積から自然に導出されたノルムをassociated normとも呼ぶ。また、ノルムが与えられた場合には、ノルムから距離を定義できるので、距離空間の性質である完備性についても語ることができる。完備な内積空間をヒルベルト空間と呼ぶ。\n特性 コーシー・シュワルツの不等式: 任意の $\\mathbf{x},\\mathbf{y}\\in X$に対して、\n$$ \\left| \\langle \\mathbf{x},\\mathbf{y} \\rangle \\right| \\le \\left\\| \\mathbf{x} \\right\\| \\left\\| \\mathbf{y} \\right\\| $$\n平行四辺形の法則: 任意の $\\mathbf{x},\\mathbf{y}\\in X$に対して、\n$$ \\left\\| \\mathbf{x} + \\mathbf{y} \\right\\|^{2} + \\left\\| \\mathbf{x} - \\mathbf{y} \\right\\|^{2} = 2 \\left( \\left\\| \\mathbf{x} \\right\\| ^{2}+ \\left\\| \\mathbf{y} \\right\\| ^{2} \\right) $$\n複素内積空間における偏極アイデンティティ: 複素内積空間 $X$及び任意の $\\mathbf{x},\\mathbf{y}\\in X$に対して、\n$$ \\langle \\mathbf{x},\\mathbf{y} \\rangle = \\frac{1}{4} \\Big( \\left\\| \\mathbf{x} + \\mathbf{y} \\right\\|^{2} - \\left\\| \\mathbf{x} - \\mathbf{y} \\right\\|^{2} + i \\left( \\left\\| \\mathbf{x} + iy \\right\\|^{2} - \\left\\| \\mathbf{x} - iy \\right\\|^{2} \\right) \\Big) $$\n実内積空間における偏極アイデンティティ: 実内積空間 $X$及び任意の $\\mathbf{x},\\mathbf{y}\\in X$に対して、\n$$ \\langle \\mathbf{x},\\mathbf{y}\\rangle = \\frac{1}{4} \\left( \\left\\| \\mathbf{x}+\\mathbf{y} \\right\\|^{2} - \\left\\| \\mathbf{x}-\\mathbf{y} \\right\\| ^{2} \\right) $$\nノルム対内積: 任意の $\\mathbf{x} \\in X$に対して、\n$$ \\left\\| \\mathbf{x} \\right\\| =\\sup \\left\\{ \\left| \\langle \\mathbf{x},\\mathbf{y} \\rangle \\right| : \\mathbf{y}\\in X, \\left\\| \\mathbf{y} \\right\\| =1 \\right\\} $$\nOle Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010), p61-65\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1842,"permalink":"https://freshrimpsushi.github.io/jp/posts/1842/","tags":null,"title":"内積空間"},{"categories":"힐베르트공간","contents":"説明 内積空間 $\\left( X, \\langle\\cdot, \\cdot\\rangle \\right)$が与えられているとする。そうすると、下記のように内積から自然にノルムを定義できる。\n$$ \\begin{equation} \\left\\| x \\right\\| := \\sqrt{ \\langle x, x\\rangle},\\quad x\\in X \\end{equation} $$\nしたがって、内積空間ならばノルム空間だ。続いてこのように定義されたノルムから距離を定義できる。\n$$ \\begin{equation} d(x,y):=\\left\\| x -y \\right\\| =\\sqrt{ \\langle x-y, x-y \\rangle},\\quad x,y\\in X \\end{equation} $$\nしたがって、内積空間ならばノルム空間であり、距離空間でもある。教科書の中には距離空間が与えられているといいながらノルムや内積の概念を使用するものがあるが、まさにこのためだ。距離空間と言っても実際には内積空間が与えられているとみなすわけだ。\n逆に、「内積空間 $X$が与えられている」という言葉は、「距離空間 $X$が与えられている」「ノルム空間 $X$が与えられている」と同じ意味だ。また、完備性は距離空間で定義される概念だが、内積やノルムから距離を定義できるため、ノルム空間や内積空間が完備であると言える理由はこれだ。証明は定義を使えば難しくないので、$(1)$についてだけ紹介する。\n定理 内積空間ならばノルム空間だ。\n証明 内積空間 $X$が与えられているとする。そして、$x,y\\in X$であり、$c\\in \\mathbb{C}$とする。すると内積の定義により、\n$$ \\left\\| x \\right\\| \\ge 0 $$\nが成り立つ。また、内積の定義により、\n$$ \\left\\| x \\right\\| =0 \\iff x=0 $$\nが成り立つ。同様に内積の定義により、\n$$ \\begin{align*} \\left\\| cx \\right\\| =\u0026amp;\\ \\sqrt{ \\langle cx,cx\\rangle } \\\\ =\u0026amp;\\ \\sqrt{ \\left| c \\right| ^{2} \\langle x,x \\rangle} \\\\ =\u0026amp;\\ \\left| c \\right| \\sqrt{\\langle x,x \\rangle} \\\\ =\u0026amp;\\ \\left| c \\right| \\left\\| x \\right\\| \\end{align*} $$\nが成り立つ。最後の条件も内積の定義により、\n$$ \\begin{align*} \\left\\| x + y \\right\\|^{2} =\u0026amp;\\ \\langle x+y,x+y \\rangle \\\\ =\u0026amp;\\ \\langle x,x+y\\rangle +\\langle y,x+y \\rangle \\\\ =\u0026amp;\\ \\langle x,x\\rangle + \\langle x,y\\rangle + \\langle y,x\\rangle + \\langle y,y\\rangle \\\\ =\u0026amp;\\ \\left\\| x \\right\\|^{2}+\\langle x,y \\rangle +\\overline{ \\langle x,y \\rangle}+ \\left\\| y \\right\\| ^{2} \\\\ \\le\u0026amp; \\left\\| x \\right\\| ^{2} + 2 \\left| \\langle x,y \\rangle \\right| + \\left\\| y \\right\\| ^{2} \\\\ \\le\u0026amp; \\left\\| x \\right\\|^{2} +2\\langle x,x \\rangle ^{1/2}\\langle y,y \\rangle^{1/2} + \\left\\| y \\right\\|^{2} \\\\ =\u0026amp;\\ \\left\\| x \\right\\|^{2}+2\\left\\| x \\right\\|\\left\\| y \\right\\| +\\left\\| y \\right\\|^{2} \\\\ =\u0026amp;\\ \\left( \\left\\| x \\right\\| + \\left\\| y \\right\\| \\right)^{2} \\end{align*} $$\n任意の複素数 $c\\in \\mathbb{C}$に対して、$c+\\overline{c} \\in \\mathbb{R}$であるため、第五行は成立する。第六行はコーシー・シュワルツの不等式によって成り立つ。したがって、\n$$ \\left\\| x \\right\\| := \\sqrt{\\langle x,x \\rangle} $$\nとして定義された $\\left\\| \\cdot \\right\\|$は、ノルムの定義を満たす。■\n","id":1840,"permalink":"https://freshrimpsushi.github.io/jp/posts/1840/","tags":null,"title":"内積空間、ノルム空間、距離空間の関係"},{"categories":"수리통계학","contents":"定義 1 $\\theta$ の推定量 $T$ が次を満たす場合、$T$ は $\\theta$ の不偏推定量Unbiased Estimatorと呼ばれる。 $$ E T = \\theta $$\n説明 特に、$\\theta$ に対する不偏推定量の中で分散が最も小さい場合、最小分散不偏推定量Minimum Variance Unbiased Estimator, MVUEと呼ばれる。\n不偏性とは、偏りを持たない性質のことを言う。例えば、$X_{i} \\sim \\left( \\mu , \\sigma^{2} \\right)$ とする時、$\\mu$ の推定量として標本平均 $\\displaystyle \\overline{X} = {{ 1 } \\over { n }} \\sum_{i} X_{i}$ を使用する場合、$\\displaystyle E \\overline{X} = \\mu$ であるため、$\\overline{X}$ は $\\mu$ の不偏推定量になる。これは一見当たり前に見えるが、推定量が母数を正確に示すことは非常に重要な性質であり、当たり前のことではない。例えば、分散と標本分散について見てみよう。\n例 $$ X_{i} \\sim \\left( \\mu , \\sigma^{2} \\right) $$ とする場合、分散の不偏推定量は次のようになる。 $$ S^{2} := {{1} \\over {n-1}} \\sum_{i=1}^{n} \\left( X_{i} - \\overline{X} \\right)^{2} $$ 知られているように、標本平均とは異なり、標本分散は偏差の二乗をすべて加算した後、$n$ ではなく $n-1$ で割る。標本分散を求める際に $n-1$ で割る理由には、聞く人のレベルに応じて様々な説明ができるが、最も正確な式で説明するならば、「標本分散が不偏推定量となるため」である。\n証明 2 $$ \\mu := E \\overline{X} \\\\ \\sigma^{2} := E X_{i} ^{2} - \\mu^{2} $$ とすると、 $$ \\begin{align*} E \\left( \\overline{X}^{2} \\right) - \\mu^{2} =\u0026amp; E \\left( \\overline{X}^{2} \\right) - \\left( E \\overline{X} \\right)^{2} \\\\ =\u0026amp; \\text{Var} \\overline{X} \\\\ =\u0026amp; \\text{Var} \\left( {{1} \\over {n}} \\sum_{i=1}^{n} X_{i} \\right) \\\\ =\u0026amp; {{1} \\over {n^{2}}} \\sum_{i=1}^{n} \\text{Var} X_{i} \\\\ =\u0026amp; {{1} \\over {n^{2}}} n \\sigma^{2} \\\\ =\u0026amp; {{\\sigma^{2}} \\over {n}} \\end{align*} $$ よって、標本分散 $S^{2}$ の期待値は $$ \\begin{align*} E S^{2} =\u0026amp; (n-1)^{-1} E \\sum_{i=1}^{n} \\left( X_{i} - \\overline{X} \\right)^{2} \\\\ =\u0026amp; (n-1)^{-1} \\left[ \\sum_{i=1}^{n} E X_{i}^{2} - \\sum_{i=1}^{n} E \\overline{X} ^{2} \\right] \\\\ =\u0026amp; (n-1)^{-1} \\left[ \\sum_{i=1}^{n} \\left( \\sigma^{2} + \\mu^{2} \\right) - n \\left( \\mu^{2} + {{\\sigma^{2}} \\over {n}} \\right) \\right] \\\\ =\u0026amp; (n-1)^{-1} \\left[ n\\sigma^{2} + n \\mu^{2} - n \\mu^{2} - \\sigma^{2} \\right] \\\\ =\u0026amp; (n-1)^{-1} (n-1) \\sigma^{2} \\\\ =\u0026amp; \\sigma^{2} \\end{align*} $$\n■\nHogg et al. (2013). 「数理統計学の概要」(第7版): p208.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHogg et al. (2013). 「数理統計学の概要」(第7版): p137.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1745,"permalink":"https://freshrimpsushi.github.io/jp/posts/1745/","tags":null,"title":"不偏推定量"},{"categories":"동역학","contents":"定義1 空間$X$と関数$f,g : X \\to X$について、ベクトル場とマップが次のように表現されるとする。 $$ \\dot{x} = f(x) \\\\ x \\mapsto g(x) $$ $S \\subset X$としよう。\n(V): $\\forall x_{0} \\in S$がすべての$t \\in \\mathbb{R}$に対して以下を満たす場合、ベクトル場$\\dot{x}=f(x)$下での不変集合という。 $$ x(t,x_{0}) \\in S $$ (M): $\\forall x_{0} \\in S$がすべての$n \\in \\mathbb{Z}$に対して以下を満たす場合、マップ$x \\mapsto g(x)$下での不変集合という。 $$ g^{n} (x_{0}) \\in S $$ 不変集合は、条件に応じて以下のように呼ばれることもある。\n不変集合$S$の時間が$t \\ge 0$または$n \\ge 0$までのみ考慮される場合、正不変集合と言い、逆に$t \\le 0$または$n \\le 0$までのみ考慮される場合、負不変集合と言う。 不変集合$S$が$C^{r}$で微分可能なマニホールドの構造を形成している場合、$C^{r}$不変マニホールドと言う。 説明 不変集合とは、過去であれ未来であれ、抜け出すことのできない集合のことを言う。過去に抜け出すことができないというのは、言い換えると、不変集合の外部から入ってくることが許されないということだ。すべての時間$\\mathbb{R}$が考慮されるため、動的な「動き」よりも、既に決定された「空間」を想像する方が適切だ。\nマニホールドが言及されるだけでなく、その空間自体の探求について言及される点で、位相数学との関連性を思い浮かべる人も多いだろう。歴史的にも動力学と位相数学は同じ根から出たからで、双方からよく知られているものが頻繁に登場するのは避けられない。「プアンカレ予想」でも有名なアンリ・プアンカレHenri Poincaréなど、両方で大きな業績を残した学者もいるが、当時はこれらが区別されていなかったため、両方で業績を残したと表現するのは適切ではないかもしれない。1900年代初頭は位相数学と動力学の始まりの時代であり、プアンカレのような学者によって発展した理論は、各自の関心に沿って分化したと見るべきだ。\n与えられたシステムで不変集合の存在を見つける主要な方法には、ハダマードの方法とリアプノフ-ペロンの方法の二つがあり、その安定性や微分可能性に対しても多くの関心が持たれている。\nWiggins. (2003). Introduction to Applied Nonlinear Dynamical Systems and Chaos Second Edition(2nd Edition): p28.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1079,"permalink":"https://freshrimpsushi.github.io/jp/posts/1079/","tags":null,"title":"力学における不変集合"},{"categories":"해석개론","contents":"定義 距離空間 $X$で定義された関数 $f :X \\to \\mathbb{R}$が与えられたとする。もし $f$が $x\\in X$で連続ではない場合、$f$は $x$で不連続だと言ったり、$x$で不連続性を持つと言う。 $f: (a,b) \\to \\mathbb{R}$とする。\n任意の点 $x$に対して、$a \\le x \u0026lt;b$とする。$\\left\\{ t_{n} \\right\\}$を $x$に収束する$(x,b)$の点の数列とする。もし全ての$\\left\\{ t_{n} \\right\\}$に対して $$ \\lim \\limits_{n \\to \\infty}f(t_{n})=q $$ が成り立つなら、$f(x+)=q$と記し、$q$を**$x$での $f$の右極限**right-hand-limitと呼ぶ。\n任意の点 $x$に対して、$a\u0026lt; x \\le b$とする。$\\left\\{ t_{n} \\right\\}$を $x$に収束する$(a,x)$の点の数列とする。もし全ての$\\left\\{ t_{n} \\right\\}$に対して $$ \\lim \\limits_{n\\to\\infty} f(t_{n})=q $$ が成り立つなら、$f(x-)=q$と記し、$q$を**$x$での $f$の左極限**left-hand-limitと呼ぶ。\n説明 不連続性について詳しく話すために、上記のように左極限、右極限という概念を定義する。左極限、右極限は不連続点だけでなく、任意の点に対して定義され得ることに注意が必要だ。\n解析学の言葉で厳密に定義しただけで、概念自体は高校で学んだ左極限、右極限と変わらない。$x$より大きい点だけからなる$x$に収束する数列で、関数値の数列が収束する場合、それを右極限、反対の場合を左極限と呼ぼうという話だ。上の定義によると、以下の事実が成立することが自明であると分かる。高校では、以下の命題が連続の定義だった。\n定理 任意の点 $x\\in (a,b)$に対して、極限 $\\lim \\limits_{t \\to x}f(t)$が存在することは、\n$$ f(x+)=f(x-)=\\lim \\limits_{t\\to x }f(t) $$\nが成立することと同値である。\n","id":1830,"permalink":"https://freshrimpsushi.github.io/jp/posts/1830/","tags":null,"title":"解析学において厳密に定義される左極限と右極限"},{"categories":"함수","contents":"定義 関数 $f:[a,b] \\rightarrow \\mathbb{R}$が与えられたとする。$x_{1}$、$x_{2}$、$\\in [a,b]$に対して\n$$ x_{1} \\lt x_{2} \\ \\implies f(x_{1}) \\le f(x_{2}) $$\nを満たす場合、$f$は 単調に増加monotonically increasingすると言い、$f$を 単調増加関数monotone increasing functionと呼ぶ。逆に\n$$ x_{1} \\lt x_{2} \\ \\implies f(x_{1}) \\ge f(x_{2}) $$\nを満たす場合、$f$は 単調に減少monotonically decreasingすると言い、$f$を 単調減少関数monotone decreasing functionと呼ぶ。\n$f$が単調増加関数か単調減少関数であれば、$f$を 単調関数monotoneと呼ぶ。\n説明 単調に増加するとは、変数が大きくなるにつれて、関数の値が 少なくとも減少しないということを意味する。逆に、単調に減少するとは、少なくとも増加しないということを意味する。\n定義 以下の式\n$$ x_{1} \\lt x_{2} \\implies f(x_{1}) \\lt f(x_{2}) $$\nを満たす$f$を 厳密に増加関数strictly increasing functionと呼ぶ。逆に\n$$ x_{1} \\lt x_{2} \\implies f(x_{1}) \\gt f(x_{2}) $$\nを満たす$f$を 厳密に減少関数strictly increasing functionと呼ぶ。\n","id":848,"permalink":"https://freshrimpsushi.github.io/jp/posts/848/","tags":null,"title":"単調関数、増加関数、減少関数"},{"categories":"해석개론","contents":"要約1 $f :[a,b] \\to \\mathbb{R}$が連続関数であり、$x\\in [a,b]$で微分可能だとしよう。$g : f([a,b])\\to \\mathbb{R}$が$f (x)\\in f([a,b])$で微分可能だとしよう。そして、$h : [a,b] \\to \\mathbb{R}$を次のようにする。\n$$ h(t)=g\\left( f(t) \\right)\\quad (a\\le t \\le b) $$\nそうすると、$h$は$x$で微分可能であり、その値は以下の通りである。\n$$ h^{\\prime}(x)=g^{\\prime}(f(x))f^{\\prime}(x) $$ 合成関数記号を使用すると、次のようになる。 $$ ( g \\circ f)^{\\prime}(x)=g^{\\prime}(f(x))f^{\\prime}(x) $$\n説明 この結果は一般に連鎖律と呼ばれる。\nここで、$f^{\\prime}(x)$を内部微分とも呼ぶ。$y=f(x)$、$z=g(y)$と置いて、ライプニッツ記号で表すと、次のようになる。 $$ \\frac{dz}{dx}=\\frac{dz}{dy}\\frac{dy}{dx} $$\nライプニッツ記号が便利な理由は、上の式の左辺がまるで右辺を約分したかのように見えるからである。$\\dfrac{dy}{dx}$は「dxのdに対する割合」ではなく$y$の微分だが、それを分数として扱っても、その意味はピッタリ合っている。\n証明 まず、以下のように関数$G$を定義しよう。\n$$ G(f(t)) :=\\begin{cases} \\frac{g(f(x))-g(f(t))}{f(x)-f(t)} -g^{\\prime}(f(x)) \u0026amp; f(t) \\ne f(x) \\\\ 0 \u0026amp; f(t)=f(x)\\end{cases},\\quad (t\\in[a,b]) $$\nすると、すべての$f(t)$に対して、次が成り立つ。\n$$ \\lim \\limits_{ f(s) \\to f(t) } G(f(s))=G(f(t)) $$\nこれは連続の同値条件であるため、$G$は連続関数である。さらに、次が成り立つ。\n$$ h(x)-h(t) = g(f(x))-g(f(t))=\\Big( f(x)-f(t) \\Big) \\Big( g^{\\prime}(f(x))+G(f(t)) \\Big) $$\nすると、極限の性質により、以下の式が成り立つ。\n$$ \\begin{align*} h^{\\prime}(x) =\u0026amp;\\ \\lim \\limits_{t \\to x} \\frac{ h(x)-h(t)}{x-t} \\\\ =\u0026amp;\\ \\lim \\limits_{t \\to x} \\frac{ \\Big( f(x)-f(t) \\Big) \\Big( g^{\\prime}(f(x))+G(f(t)) \\Big)}{x-t} \\\\ =\u0026amp;\\ \\lim \\limits_{t \\to x} \\left[ g^{\\prime}(f(x))\\frac{ f(x)-f(t) }{x-t}+G(f(t))\\frac{f(x)-f(t) }{x-t} \\right] \\\\ =\u0026amp;\\ \\lim \\limits_{t \\to x} \\left[ g^{\\prime}(f(x))\\frac{ f(x)-f(t) }{x-t}\\right]+\\lim \\limits_{t \\to x} \\left[G(f(t))\\frac{ f(x)-f(t) }{x-t} \\right] \\\\ =\u0026amp;\\ \\lim \\limits_{t \\to x} g^{\\prime}(f(x))\\lim \\limits_{t \\to x}\\frac{ f(x)-f(t) }{x-t}+\\lim \\limits_{t \\to x}G(f(t))\\lim \\limits_{t \\to x}\\frac{ f(x)-f(t) }{x-t} \\\\ =\u0026amp;\\ g^{\\prime}(f(x))f^{\\prime}(x)+0\\cdot f^{\\prime}(x) \\\\ =\u0026amp;\\ g^{\\prime}(f(x))f^{\\prime}(x) \\end{align*} $$\n■\nウォルター・ルーディン, 数学分析の原理 (第3版, 1976), p105\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1823,"permalink":"https://freshrimpsushi.github.io/jp/posts/1823/","tags":null,"title":"解析学における微分の連鎖律"},{"categories":"해석개론","contents":"要約1 $f, g : [a,b] \\to \\mathbb{R}$としよう。もし$f,g$が$x\\in [a,b]$で微分可能であれば、$f+g$、$fg$、$f/g$も$x$で微分可能であり、以下の式が成立する。\n$$ \\begin{align} (f+g)^{\\prime}(x) \u0026amp;=f^{\\prime}(x)+g^{\\prime}(x) \\\\ (fg)^{\\prime}(x) \u0026amp;= f^{\\prime}(x)g(x)+f(x)g^{\\prime}(x) \\\\ \\left( \\frac{f}{g} \\right)^{\\prime}(x) \u0026amp;= \\frac{f^{\\prime}(x)g(x)-f(x)g^{\\prime}(x)}{g^{2}(x)} \\end{align} $$\nただし、$(3)$は$g(x)\\ne 0$のときに成立する。\n説明 $(2)$は一般に乗法の微分法則と呼ばれる。\n証明 $(1)$ 微分の定義と関数の極限の性質により、以下が成立する。\n$$ \\begin{align*} (f+g)^{\\prime}(x) \u0026amp;=\\lim \\limits_{t \\to x} \\frac{(f+g)(x)-(f+g)(t)}{x-t} \\\\ \u0026amp;= \\lim \\limits_{t \\to x} \\frac{(f(x)+g(x))-(f(t)+g(t))}{x-t} \\\\ \u0026amp;= \\lim \\limits_{t \\to x} \\frac{(f(x)-f(t))+(g(x)+g(t))}{x-t} \\\\ \u0026amp;= \\lim \\limits_{t \\to x} \\left[ \\frac{f(x)-f(t)}{x-t}+\\frac{g(x)+g(t)}{x-t} \\right] \\\\ \u0026amp;= \\lim \\limits_{t \\to x} \\frac{f(x)-f(t)}{x-t}+ \\lim \\limits_{t \\to x}\\frac{g(x)+g(t)}{x-t} \\\\ \u0026amp;= f^{\\prime}(x)+g^{\\prime}(x) \\end{align*} $$\n■\n$(2)$ 微分の定義と関数の極限の性質により、以下が成立する。\n$$ \\begin{align*} (fg)^{\\prime}(x) \u0026amp;= \\lim \\limits_{t \\to x} \\frac{(fg)(x)-(fg)(t)}{x-t} \\\\ \u0026amp;= \\lim \\limits_{t \\to x}\\frac{f(x)g(x)-f(t)g(t)}{x-t} \\\\ \u0026amp;= \\lim \\limits_{t \\to x}\\frac{f(x)g(x) {\\color{blue}-f(t)g(x)+f(t)g(x)}-f(t)g(t)}{x-t} \\\\ \u0026amp;= \\lim \\limits_{t \\to x}\\left[ \\frac{f(x)g(x) -f(t)g(x)}{x-t} + \\frac{f(t)g(x)-f(t)g(t)}{x-t} \\right] \\\\ \u0026amp;= \\lim \\limits_{t \\to x}\\left[ \\frac{f(x) -f(t)}{x-t}g(x) + f(t)\\frac{g(x)-g(t)}{x-t} \\right] \\\\ \u0026amp;= \\lim \\limits_{t \\to x} \\left[\\frac{f(x) -f(t)}{x-t}g(x)\\right] + \\lim \\limits_{t \\to x} \\left[ f(t)\\frac{g(x)-g(t)}{x-t} \\right] \\\\ \u0026amp;= \\lim \\limits_{t \\to x}\\frac{f(x) -f(t)}{x-t}\\lim \\limits_{t \\to x}g(x) + \\lim \\limits_{t \\to x} f(t)\\lim \\limits_{t \\to x}\\frac{g(x)-g(t)}{x-t} \\\\ \u0026amp;= f^{\\prime}(x)g(x)+f(x)g^{\\prime}(x) \\end{align*} $$\n■\n$(3)$ $(2)$と同様の方法で証明される。\n$$ \\begin{align*} \\left( \\frac{f}{g} \\right)^{\\prime}(x) \u0026amp;= \\lim \\limits_{ t \\to x } \\frac{(f/g)(x) -(f/g)(t)}{x-t} \\\\ \u0026amp;= \\lim \\limits_{ t \\to x } \\frac{f(x)/g(x) -f(t)/g(t)}{x-t} \\\\ \u0026amp;= \\lim \\limits_{ t \\to x } \\frac{f(x)/g(x) {\\color{blue}-f(x)/g(t)+f(x)/g(t)}- f(t)/g(t)}{x-t} \\\\ \u0026amp;= \\lim \\limits_{ t \\to x } \\left[ \\frac{f(x)/g(x) - f(x)/g(t) }{x-t}+\\frac{f(x)/g(t)-f(t)/g(t)}{x-t} \\right] \\\\ \u0026amp;= \\lim \\limits_{ t \\to x } \\left[ \\frac{\\frac{f(x){\\color{blue}g(t)}}{g(x){\\color{blue}g(t)}} -\\frac{f(x){\\color{blue}g(x)}}{g(t){\\color{blue}g(x)}} }{x-t}+\\frac{\\frac{f(x){\\color{blue}g(x)}}{g(t){\\color{blue}g(x)}}-\\frac{f(t){\\color{blue}g(x)}}{g(t){\\color{blue}g(x)}}}{x-t} \\right] \\\\ \u0026amp;= \\lim \\limits_{ t \\to x } \\frac{1}{g(x)g(t)} \\left[ {\\color{red}\\frac{f(x)g(t)-f(x)g(x) }{x-t}}+\\frac{f(x)g(x)-f(t)g(x)}{x-t} \\right] \\\\ \u0026amp;= \\lim \\limits_{ t \\to x }\\frac{1}{g(x)g(t)} \\left[\\frac{f(x)g(x)-f(t)g(x)}{x-t}{\\color{red}-\\frac{f(x)g(x)-f(x)g(t) }{x-t}} \\right] \\\\ \u0026amp;= \\lim \\limits_{ t \\to x }\\frac{1}{g(x)g(t)} \\left[\\frac{f(x)-f(t)}{x-t}g(x)-f(x)\\frac{g(x)-g(t) }{x-t} \\right] \\\\ \u0026amp;= \\lim \\limits_{ t \\to x }\\frac{1}{g(x)g(t)} \\lim \\limits_{ t \\to x } \\left[\\frac{f(x)-f(t)}{x-t}g(x)-f(x)\\frac{g(x)-g(t) }{x-t} \\right] \\\\ \u0026amp;= \\frac{1}{g^{2}(x)}\\left[ f^{\\prime}(x)g(x)-f(x)g^{\\prime}(x) \\right] \\\\ \u0026amp;= \\frac{f^{\\prime}(x)g(x)-f(x)g^{\\prime}(x)}{g^{2}(x)} \\end{align*} $$\n■\nWalter Rudin, Principles of Mathmatical Analysis (3rd Edition, 1976), p104-105\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1821,"permalink":"https://freshrimpsushi.github.io/jp/posts/1821/","tags":null,"title":"微分可能な関数の性質"},{"categories":"동역학","contents":"定義 空間 $X$ と関数 $f \\in C^{1}(X,X)$ に対して、以下のようなベクトル場が微分方程式として与えられているとしよう。 $$ \\dot{x} = f(x) $$ $\\overline{x}$ がこの自律系の一つの固定点であり、$D f \\left( \\overline{x} \\right)$ の固有値を $\\lambda_{1} , \\cdots , \\lambda_{m}$ とする。\nハイパーボリック：双曲固定点1 ハイパーボリック：$D f \\left( \\overline{x} \\right)$ の全ての固有値の実部が $0$ でなければ、$\\overline{x}$ はハイパーボリックだと言われる。 $$ \\text{Re} \\left( \\lambda_{1} \\right) \\ne 0 , \\cdots , \\text{Re} \\left( \\lambda_{m} \\right) \\ne 0 $$ サドル：$\\overline{x}$ がハイパーボリックであり、$D f \\left( \\overline{x} \\right)$ が実部が正の固有値と負の固有値を少なくとも一つずつ持つ場合、$\\overline{x}$ はサドルだという。 $$ \\exists i, j \\in [1,m] : \\text{Re} \\left( \\lambda_{i} \\right) \u0026gt; 0 \\land \\text{Re} \\left( \\lambda_{j} \\right) \u0026lt; 0 $$ シンク：$D f \\left( \\overline{x} \\right)$ の全ての固有値の実部が負ならば、$\\overline{x}$ は安定Stableだと言われ、シンクという。 $$ \\text{Re} \\left( \\lambda_{1} \\right) \u0026lt; 0 , \\cdots , \\text{Re} \\left( \\lambda_{m} \\right) \u0026lt; 0 $$ ソース：$D f \\left( \\overline{x} \\right)$ の全ての固有値の実部が正ならば、$\\overline{x}$ は不安定Untableだと言われ、ソースという。 $$ \\text{Re} \\left( \\lambda_{1} \\right) \u0026gt; 0 , \\cdots , \\text{Re} \\left( \\lambda_{m} \\right) \u0026gt; 0 $$ エリプティック：楕円固定点2 エリプティック、センター：$D f \\left( \\overline{x} \\right)$ の全ての固有値が純虚数ならば、$\\overline{x}$ はエリプティックだと言われ、センターという。 $$ \\text{Im} \\left( \\lambda_{1} \\right) = \\lambda_{1} , \\cdots , \\text{Im} \\left( \\lambda_{m} \\right) = \\lambda_{m} $$ $\\Re$ と$\\Im$ はそれぞれ複素数から実部と虚部を取り出す関数である。 説明 定義ではそう言われていないけれども、ハイパーボリックかどうかは、端的に言って、システムが単純かどうかとほぼ一致する。システムを動力学的に分析する場合、主な問題はほとんどの場合、固有値が $0$ である場合に生じる。ハイパーボリックであれば、そのような厄介ごとを心配せずに済むため、分析も単純になる。\n例 例として、ダフィング振り子を考えてみよう： $$ \\begin{align*} \\dot{x} =\u0026amp; y \\\\ \\dot{y} =\u0026amp; x - x^{3} - \\delta y \\qquad , \\delta \\ge 0 \\end{align*} $$ ダフィング振り子の固定点は $(x,y) = (0,0) , (\\pm 1 , 0)$ であり、ヤコビアンは $$ D \\mathbb{f} = \\begin{bmatrix} 0 \u0026amp; 1 \\\\ 1 - 3 x^{2} \u0026amp; - \\delta \\end{bmatrix} $$ であるため、固定点でのヤコビアンは $$ D \\mathbb{f} (0,0) = \\begin{bmatrix} 0 \u0026amp; 1 \\\\ 1 \u0026amp; - \\delta \\end{bmatrix} \\\\ D \\mathbb{f} (\\pm1,0) = \\begin{bmatrix} 0 \u0026amp; 1 \\\\ -2 \u0026amp; - \\delta \\end{bmatrix} $$ である。これらの固有値を計算すると、$(0,0)$ の時 $$ \\det ( D \\mathbb{f} (0,0) - \\lambda E ) = \\det \\begin{bmatrix} -\\lambda \u0026amp; 1 \\\\ 1 \u0026amp; -\\lambda - \\delta \\end{bmatrix} = \\lambda^{2} + \\delta \\lambda - 1 $$ であり、二次方程式の解によって $$ \\lambda_{1,2} = {{ -\\delta \\pm \\sqrt{\\delta^{2} + 4 } } \\over { 2 }} $$ である。$D \\mathbb{f} (0,0)$ の固有値は常に正数と負数の一つずつを持つため、固定点 $(0,0)$ はサドルである。同様に、$D \\mathbb{f} (\\pm1,0)$ の固有値を計算すると $$ \\lambda_{1,2} = {{ -\\delta \\pm \\sqrt{\\delta^{2} - 8 } } \\over { 2 }} $$ であるため、$D \\mathbb{f} (\\pm1,0)$ の固有値は $\\delta \u0026gt; 0$ の時、全部負であり、固定点 $(\\pm1,0)$ はシンクである。しかし、$\\delta = 0$ の時は純虚数 $\\lambda_{1,2} = \\pm \\sqrt{2} i$ であるため、固定点 $(\\pm1,0)$ はセンターである。\n上記の例では、各固定点でヤコビアンを求め、パラメーター $\\delta$ の設定によって安定性がどのように変化するかを見た。このような分析は、システムがベクトル場で表されていれば、動力学のどの論文でも同じように使われる方法である。少なくとも一度は、必ず自分で試してみよう。\nWiggins. (2003). Introduction to Applied Nonlinear Dynamical Systems and Chaos Second Edition(2nd Edition): p12.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWiggins. (2003). Introduction to Applied Nonlinear Dynamical Systems and Chaos Second Edition(2nd Edition): p12.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1733,"permalink":"https://freshrimpsushi.github.io/jp/posts/1733/","tags":null,"title":"自律システムにおける固定点の分類"},{"categories":"푸리에해석","contents":"式 区間$[-L,\\ L)$で定義された関数$f$の複素フーリエ級数complex Fourier seriesは次の通りである。\n$$ f(t) = \\sum \\limits_{n=-\\infty}^{\\infty} c_{n} e^{i\\frac{n\\pi t}{L}} $$\nここで複素フーリエ係数は次のようになる。\n$$ c_{n} = \\dfrac{1}{2L}\\int_{-L}^{L}f(t)e^{-i\\frac{n \\pi t}{L} }dt $$\nフーリエ係数は以下の式を満たす。\n$$ \\begin{align*} a_{0} \u0026amp; = 2 c_{0} \\\\ a_{n} \u0026amp;= c_{n}+c_{-n} \\\\ b_{n} \u0026amp;= i(c_{n}-c_{-n}) \\\\ c_{n} \u0026amp;= \\frac{1}{2} (a_{n}-ib_{n}) \\\\ c_{-n} \u0026amp;= \\frac{1}{2} (a_{n}+ib_{n}) \\end{align*} $$\n三角関数の形よりも単純で、より頻繁に使用される形である。\n証明 フーリエ級数\n$$ \\begin{equation} f(t) = \\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{\\infty} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L} + b_{n}\\sin\\dfrac{n\\pi t }{L} \\right) \\end{equation} $$\n$$ \\begin{align*} \\text{where}\\quad a_{0} =\u0026amp;\\ \\dfrac{1}{L}\\int_{-L}^{L}f(t)dt \\\\ a_{n} =\u0026amp;\\ \\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{n\\pi t}{L} dt \\\\ b_{n} =\u0026amp;\\ \\dfrac{1}{L}\\int_{-L}^{L}f(t)\\sin\\dfrac{n\\pi t}{L}dt \\\\ \\end{align*} $$\nオイラーの公式を利用して、コサイン関数とサイン関数を複素指数関数で表すと次のようになる。\n$$ \\begin{align*} \\cos \\dfrac{n\\pi t}{L} \u0026amp;= \\dfrac{e^{i\\frac{n\\pi t}{L}} + e^{-i\\frac{n\\pi t}{L}} }{2} \\\\ \\sin \\dfrac{n \\pi t}{L} \u0026amp;= \\dfrac{e^{i\\frac{n\\pi t}{L}} - e^{-i\\frac{n\\pi t}{L}} }{2i} \\end{align*} $$\nこれを$(1)$に代入すると次のようになる。\n$$ f(t)=\\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{\\infty} \\left( a_{n} \\dfrac{e^{i\\frac{n\\pi t}{L}} + e^{-i\\frac{n\\pi t}{L}} }{2} + b_{n}\\dfrac{e^{i\\frac{n\\pi t}{L}} - e^{-i\\frac{n\\pi t}{L}} } {2i} \\right) $$\n指数関数を基準に項をまとめると\n$$ f(t) = \\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{\\infty} \\left( \\dfrac{1}{2}\\left(a_{n}-ib_{n} \\right)e^{i\\frac{n\\pi t}{L}} +\\dfrac{1}{2}\\left(a_{n} + ib_{n} \\right) e^{-i\\frac{n\\pi t}{L} } \\right) $$\n次に、$c_{0}=\\dfrac{a_{0}}{2}$、$c_{n}=\\dfrac{1}{2}\\left(a_{n}-ib_{n} \\right)$、$c_{-n}=\\dfrac{1}{2}\\left(a_{n} + ib_{n} \\right)$とおくと、下記の通りになる。\n$$ f(t) =c_{0}+\\sum \\limits_{n=1}^{\\infty} \\left( c_{n}e^{i\\frac{n\\pi t}{L}} +c_{-n} e^{-i\\frac{n\\pi t}{L} } \\right) $$\nインデックスを一つにまとめて整理すると\n$$ f(t) = \\sum \\limits_{n=-\\infty}^{\\infty} c_{n} e^{i\\frac{n \\pi t}{L} } $$\nさらに、$c_{0}$、$c_{n}$、$c_{-n}$を計算すると\n$$ \\begin{align*} c_{0} \u0026amp;=\\dfrac{a_{0}}{2}=\\dfrac{1}{2L}\\int_{-L}^{L}f(t)dt \\\\ c_{n}\u0026amp;=\\dfrac{1}{2}\\left(a_{n}-ib_{n} \\right)=\\dfrac{1}{2L}\\int_{-L}^{L}f(t)e^{-i\\frac{n \\pi t}{L} }dt \\quad (n=1,\\ 2,\\ \\cdots ) \\\\ c_{-n}\u0026amp;=\\dfrac{1}{2}\\left(a_{n} + ib_{n} \\right)=\\dfrac{1}{2L}\\int_{-L}^{L}f(t)e^{i\\frac{n \\pi t}{L} }dt \\quad (n=-1,\\ -2,\\ \\cdots) \\end{align*} $$\nしたがって\n$$ c_{n} = \\dfrac{1}{2L}\\int_{-L}^{L}f(t)e^{-i\\frac{n \\pi t}{L} }dt \\quad (n=0,\\ \\pm 1,\\ \\pm 2,\\ \\cdots) $$\n■\n","id":964,"permalink":"https://freshrimpsushi.github.io/jp/posts/964/","tags":null,"title":"複素数表示のフーリエ級数"},{"categories":"편미분방정식","contents":"定義1 偏微分方程式 自然数$k \\in \\mathbb{N}$と、開集合$U \\subset \\mathbb{R}^{n}$に対して、次の表現を**$k$次の偏微分方程式**と呼ぶ。\n$$ \\begin{equation} F(D^{k}u(x), D^{k-1}u(x),\\cdots,Du(x),u(x),x)=0\\quad (x\\in U) \\end{equation} $$\nここで、$D^{k}u$は多重指数表記である。$F$は次のように与えられ、未知数$u$は次のようである。\n$$ F : {\\mathbb{R}}^{n^{k}}\\times{\\mathbb{R}}^{n^{k-1}}\\times \\cdots \\times \\mathbb{R}^{n}\\times \\mathbb{R}\\times U \\to \\mathbb{R} \\\\ u : U \\to \\mathbb{R} $$\n偏微分方程式の連立 与えられた$\\mathbf{F} : {\\mathbb{R}}^{mn^{k}}\\times{\\mathbb{R}}^{mn^{k-1}}\\times \\cdots \\times \\mathbb{R}^{mn}\\times \\mathbb{R}^{m}\\times U \\to \\mathbb{R}^{m}$と未知数$\\mathbf{u}:U \\to \\mathbb{R}^{m}$、$\\mathbf{u}=(u^{1},\\cdots,u^{m})$に対して、下記の表現\n$$ \\mathbf{F}(D^{k}\\mathbf{u}(x),D^{k-1}\\mathbf{u}(x),\\cdots,D\\mathbf{u}(x),\\mathbf{u}(x),x)=\\mathbf{0}\\quad (x\\in U) $$\nを**$k$次の偏微分方程式システム**と呼ぶ。\n説明 偏微分方程式は、よくPDEと略される。PDEを解くことは、$(1)$を満たす$u$を全て見つけ出すことを意味し、そのような$u$をソリューションと呼ぶ。\nソリューションを見つけることは、\n理想的には、簡単で明示的なソリューションを見つけることを意味する、 それが不可能なときは、解の存在や他の特徴を明らかにすることを意味する。 ほとんどの場合、偏微分方程式での$U, \\Omega \\subset \\mathbb{R}^{n}$は開集合を意味し、変数$t$は常に時間を意味し、$t\\ge 0$である。また、\n$$ Du=D_{x}u=(u_{x_{1}},\\cdots,u_{x_{n}}) $$\nは$u$のグラディエントを意味する。このとき、$x=(x_{1},\\cdots,x_{n})$である。\n分類 偏微分方程式は、線形性に基づいて以下のように分類できる。\n線形 偏微分方程式$(1)$が、与えられた関数$a_{\\alpha}, f$に対して、次の式を満たす場合、線形と言われる。\n$$ \\sum _{| \\alpha | \\le k} a_{\\alpha}(x) D^{\\alpha} u = f(x) $$\n$f=0$の場合、同質の線形PDEと呼ぶ。線形ではない場合、非線形と呼ぶ。2次の線形偏微分方程式はさらに以下のように分類される。\n双曲型PDE 放物型PDE 楕円型PDE 半線形 偏微分方程式$(1)$が次を満たす場合、半線形と呼ぶ。\n$$ \\sum _{| \\alpha | = k} a_{\\alpha}(x) D^{\\alpha} u + a_{0}\\left( D^{k-1}u, \\dots, Du, u, x \\right) = 0 $$\n言い換えると、半線形pdeは、オーダーが$k$最も高いオーダーの導関数の係数が$x$にのみ依存する偏微分方程式を意味する。例としては、\n反応-拡散方程式 $$ u_{t} - \\Delta u = f(u) \\qquad (\\text{e.g. } f(u) = u^{2}) $$ 準線形 偏微分方程式$(1)$が次を満たす場合、準線形と呼ぶ。\n$$ \\sum _{| \\alpha | = k} a_{\\alpha}(D^{k-1}u, \\dots, Du, u, x)D^{\\alpha} u + a_{0}\\left( D^{k-1}u, \\dots, Du, u, x \\right) = 0 $$\n例としては、\n粘性のないバーガース方程 $$ u_{t} + uu_{xx} = 0 $$ 完全非線形 準線形ではない非線形方程式を完全非線形と言う。\nLawrence C. Evans, Partial Differential Equations (2nd Edition, 2010), p1-3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1818,"permalink":"https://freshrimpsushi.github.io/jp/posts/1818/","tags":null,"title":"偏微分方程式"},{"categories":"수리통계학","contents":"定義 1 確率密度関数 $f (x; \\theta)$ を持つ確率変数 $X$ のサンプル $X_{1} , \\cdots , X_{n}$ と信頼係数Confidence Coefficient $\\alpha \\in (0,1)$ が与えられているとしよう。 $$ L := L \\left( X_{1} , \\cdots , X_{n} \\right) \\\\ U := U \\left( X_{1} , \\cdots , X_{n} \\right) $$ 統計量 $L \u0026lt; U$ が上述のように定義されているとき、次を満たす区間 $(L,U) \\subset \\mathbb{R}$ を母数 $\\theta$ に対する $( 1 - \\alpha)100 \\%$ 信頼区間という。 $$ 1-\\alpha = P \\left[ \\theta \\in \\left( L,U \\right) \\right] $$\n説明 実際、信頼区間は$\\mathbb{R}$ より広い空間で信頼領域として一般化することができ、基礎統計学を学ぶ立場からは、上記のように数式で定義することは無駄に難しく感じるかもしれない。しかし、今や確率変数、サンプル、統計量の定義が数理的にしっかりしているので、もう少し厳密な議論を進めることができる。\nこの信頼区間の定義で重要なのは、$L$ と$U$ が統計量であることだ。信頼区間が区間として与えられているので、空間的な感覚で信頼区間を見ることもできるだろうが、実際には数理統計学以前の統計学で求めていた信頼区間は、その上限$U$ と下限$L$ を求めることで見つけられたはずだ。例えば$X$ が標準正規分布 $N(0,1)$ に従っているならば、$\\mu$ に対する$1-\\alpha$ 信頼区間は以下のように求められた。 $$ L := \\overline{X} - {{ S } \\over { \\sqrt{n} }} z_{\\alpha/2} \\\\ U := \\overline{X} + {{ S } \\over { \\sqrt{n} }} z_{\\alpha/2} $$ このように、$\\mu$ に対する区間推定で実際に「動く」もの、つまり区間の両端が$L,U$ であることに注意しよう。一見すると、区間が与えられていて$\\mu$ がランダムに変化して信頼区間内に入る確率に関心があるように見えるが、わざわざ$L,U$ を「統計量」と呼ぶこと自体がそのような誤読を解消するためである。\n同時に見る 信頼区間の難しい定義 Hogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p218.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1732,"permalink":"https://freshrimpsushi.github.io/jp/posts/1732/","tags":null,"title":"信頼区間の簡単な定義"},{"categories":"수리물리","contents":"要約 曲線座標系でのベクトル関数$\\mathbf{F}=\\mathbf{F}(q_{1},q_{2},q_{3})=F_{1}\\hat{\\mathbf{q}}_{1}+F_{2}\\hat{\\mathbf{q}}_{2}+F_{3}\\hat{\\mathbf{q}}_{3}$のダイバージェンスは以下の通りである。\n$$ \\nabla \\cdot \\mathbf{F}=\\frac{1}{h_{1}h_{2}h_{3}}\\left[ \\frac{ \\partial }{ \\partial q_{1} }(h_{2}h_{3}F_{1})+\\frac{ \\partial }{ \\partial q_{2} }(h_{1}h_{3}F_{2})+\\frac{ \\partial }{ \\partial q_{3} }(h_{1}h_{2}F_{3}) \\right] $$\n$h_{i}$はスケールファクターである。\n公式 直交座標系:\n$$ h_{1}=h_{2}=h_{3}=1 $$\n$$ \\begin{align*} \\nabla \\cdot \\mathbf{F} =\\frac{\\partial F_{x}}{\\partial x}+\\frac{\\partial F_{y}}{\\partial y}+\\frac{\\partial F_{z}}{\\partial z} \\end{align*} $$\n円筒座標系:\n$$ h_{1}=1,\\quad h_{2}=\\rho,\\quad h_{3}=1 $$\n$$ \\begin{align*} \\nabla \\cdot \\mathbf{F} \u0026amp;= \\frac{1}{\\rho} \\left( \\frac{\\partial (\\rho F_{\\rho})}{\\partial \\rho} + \\frac{\\partial (F_{\\phi})}{\\partial \\phi} + \\frac{\\partial (\\rho F_{z})}{\\partial z} \\right) \\\\ \u0026amp;= \\frac{1}{\\rho} \\frac{\\partial (\\rho F_{\\rho})}{\\partial \\rho} + \\frac{1}{\\rho}\\frac{\\partial F_{\\phi}}{\\partial \\phi} + \\frac{\\partial F_{z}}{\\partial z} \\end{align*} $$\n球座標系: $$ h_{1}=1,\\quad h_{2}=r\\quad, h_{3}=r\\sin\\theta $$\n$$ \\begin{align*} \\nabla \\cdot \\mathbf{F} \u0026amp;= \\frac{1}{r^{2}\\sin\\theta}\\left( \\frac{\\partial (r^{2}\\sin\\theta F_{r})}{\\partial r}+\\frac{\\partial (r\\sin\\theta F_{\\theta})}{\\partial \\theta}+\\frac{\\partial (rF_{\\phi})}{\\partial \\phi} \\right) \\\\ \u0026amp;= \\frac{1}{r^{2}}\\frac{\\partial (r^{2} F_{r})}{\\partial r}+\\frac{1}{r\\sin\\theta}\\frac{\\partial (\\sin\\theta F_{\\theta})}{\\partial \\theta}+\\frac{1}{r\\sin\\theta}\\frac{\\partial F_{\\phi}}{\\partial \\phi} \\end{align*} $$\n導出 3次元デカルト座標系で、ベクトル関数$\\mathbf{F}$のダイバージェンス$\\nabla \\cdot \\mathbf{F}$は$\\mathbf{F}$がどのように流れるかを教えてくれた。曲線座標系でも同じ方法でダイバージェンスを得ることができる。まずは$q_{1}$軸方向だけで計算してみよう。$d\\mathbf{a}_{1}$と$d\\mathbf{a}_{2}$を通過する量は、2つのベクトルの内積で計算することができる。計算はデカルト座標系と同じなので、一部は省略する。\n$$ \\begin{align*} \\mathbf{F}(q_{1}+dq_{1},q_{2},q_{3})\\cdot d\\mathbf{a}_{1} \u0026amp;= F_{1}(q_{1}+dq_{1},q_{2},q_{3})h_{2}h_{3}dq_{2}dq_{3} \\\\ \\mathbf{F}(q_{1},q_{2},q_{3})\\cdot d\\mathbf{a}_{2} \u0026amp;=- F_{1}(q_{1},q_{2},q_{3})h_{2}h_{3}dq_{2}dq_{3} \\end{align*} $$\nそして、これら2つの和が流入量（流出量）だ。\n$$ \\begin{align*} \u0026amp;F_{1}(q_{1}+dq_{1},q_{2},q_{3})h_{2}h_{3}dq_{2}dq_{3}- F_{1}(q_{1},q_{2},q_{3})h_{2}h_{3}dq_{2}dq_{3} \\\\ =\u0026amp; \\frac{F_{1}(q_{1}+dq_{1},q_{2},q_{3})h_{2}h_{3}- F_{1}(q_{1},q_{2},q_{3})h_{2}h_{3} }{dq_{1}}dq_{1}dq_{2}dq_{3} \\\\ \\approx \u0026amp;\\frac{ \\partial (F_{1}h_{2}h_{3})}{ \\partial q_{1}}dq_{1}dq_{2}dq_{3} \\end{align*} $$\n同じ方法で$q_{2}$と$q_{3}$について計算すると次のようになる。\n$$ \\frac{ \\partial (F_{2}h_{1}h_{3})}{ \\partial q_{2}}dq_{1}dq_{2}dq_{3}\\quad \\text{and} \\quad \\frac{ \\partial (F_{3}h_{1}h_{2})}{ \\partial q_{3}}dq_{1}dq_{2}dq_{3} $$\nこれらをすべて加えると$\\mathbf{F}$が入ってくるか出ていく量になり、体積$dV=h_{1}h_{2}h_{3}dq_{1}dq_{2}dq_{3}$で割ると単位体積あたりの流入量（流出量）になる。\n$$ \\begin{align*} \u0026amp; \\frac{ \\partial (F_{1}h_{2}h_{3})}{ \\partial q_{1}}dq_{1}dq_{2}dq_{3}+\\frac{ \\partial (F_{2}h_{1}h_{3})}{ \\partial q_{2}}dq_{1}dq_{2}dq_{3}+\\frac{ \\partial (F_{3}h_{1}h_{2})}{ \\partial q_{3}}dq_{1}dq_{2}dq_{3} \\\\ =\u0026amp;\\ \\left( \\frac{ \\partial (F_{1}h_{2}h_{3})}{ \\partial q_{1}}+\\frac{ \\partial (F_{2}h_{1}h_{3})}{ \\partial q_{2}}+\\frac{ \\partial (F_{3}h_{1}h_{2})}{ \\partial q_{3}} \\right)dq_{1}dq_{2}dq_{3} \\\\ \\frac{1}{dV}\\times \\implies \u0026amp;\\frac{1}{h_{1}h_{2}h_{3}}\\left( \\frac{ \\partial (F_{1}h_{2}h_{3})}{ \\partial q_{1}}+\\frac{ \\partial (F_{2}h_{1}h_{3})}{ \\partial q_{2}}+\\frac{ \\partial (F_{3}h_{1}h_{2})}{ \\partial q_{3}} \\right) \\end{align*} $$\n■\n参照 曲線座標系でのグラディエント、ダイバージェンス、カール、ラプラシアンの公式 グラディエント（傾き） ダイバージェンス（発散） カール（回転） ラプラシアン ","id":1817,"permalink":"https://freshrimpsushi.github.io/jp/posts/1817/","tags":null,"title":"曲線座標系におけるベクトル関数の発散"},{"categories":"수리물리","contents":"概要 曲線座標系でスカラー関数の$f=f(q_{1},q_{2},q_{3})$の勾配は次の通りだ。\n$$ \\nabla f= \\frac{1}{h_{1}}\\frac{ \\partial f }{ \\partial q_{1} } \\hat{\\mathbf{q}}_{1} + \\frac{1}{h_{2}}\\frac{ \\partial f }{ \\partial q _{2}}\\hat{\\mathbf{q}}_{2}+\\frac{1}{h_{3}}\\frac{ \\partial f }{ \\partial q_{3} } \\hat{\\mathbf{q}}_{3}=\\sum \\limits _{i=1} ^{3}\\frac{1}{h_{i}}\\frac{ \\partial f}{ \\partial q_{i}}\\hat{\\mathbf{q}}_{i} $$\n$h_{i}$はスケール因子だ。\n公式 直交座標系:\n$$ h_{1}=h_{2}=h_{3}=1 $$\n$$ \\nabla f= \\frac{\\partial f}{\\partial x}\\mathbf{\\hat{\\mathbf{x}} }+ \\frac{\\partial f}{\\partial y}\\mathbf{\\hat{\\mathbf{y}}} + \\frac{\\partial f}{\\partial z}\\mathbf{\\hat{\\mathbf{z}}} $$\n円筒座標系:\n$$ h_{1}=1,\\quad h_{2}=\\rho,\\quad h_{3}=1 $$\n$$ \\nabla f = \\frac{\\partial f}{\\partial \\rho}\\boldsymbol{\\hat \\rho} + \\frac{1}{\\rho}\\frac{\\partial f}{\\partial \\phi}\\boldsymbol{\\hat \\phi} + \\frac{\\partial f}{\\partial z}\\mathbf{\\hat{\\mathbf{z}}} $$\n球座標系:\n$$ h_{1}=1,\\quad h_{2}=r\\quad, h_{3}=r\\sin\\theta $$\n$$ \\nabla f= \\frac{\\partial f}{\\partial r} \\mathbf{\\hat r} + \\frac{1}{r}\\frac{\\partial f}{\\partial \\theta} \\boldsymbol{\\hat \\theta} + \\frac{1}{r\\sin\\theta}\\frac{\\partial f}{\\partial \\phi}\\boldsymbol{\\hat \\phi} $$\n導出 三次元デカルト座標系で、以下の式を満たす$\\mathbf{a}$を$f$の勾配と名付け、$\\nabla f$と記されると定義した。\n$$ d f =\\mathbf{a} \\cdot d\\mathbf{r} $$\n任意の曲線座標系でもこのように定義する。$f$の全微分は下の通りだ。\n$$ d f = \\frac{ \\partial f}{ \\partial q_{1} }dq_{1}+\\frac{ \\partial f}{ \\partial q_{2}}dq_{2}+\\frac{ \\partial f}{ \\partial q_{3}}dq_{3} $$\n曲線座標系での位置ベクトル$\\mathbf{r}$の微小変化量は次の通りだ。\n$$ d\\mathbf{r}=h_{1}dq_{1}\\hat{\\mathbf{q}}_{1}+h_{2}dq_{2}\\hat{\\mathbf{q}}_{2}+h_{3}dq_{3}\\hat{\\mathbf{q}}_{3} $$\nこれから、以下の式を満たす$\\mathbf{a}$を探求する。\n$$ \\begin{equation} df=\\mathbf{a} \\cdot d\\mathbf{r} \\end{equation} $$\n$\\mathbf{a}=a_{1}\\hat{\\mathbf{q}}_{1}+a_{2}\\hat{\\mathbf{q}}_{2}+a_{3}\\hat{\\mathbf{q}}_{3}$とすると、$(1)$は下の通りだ。\n$$ \\frac{ \\partial f}{ \\partial q_{1} }dq_{1}+\\frac{ \\partial f}{ \\partial q_{2}}dq_{2}+\\frac{ \\partial f}{ \\partial q_{3}}dq_{3} = a_{1}h_{1}dq_{1}+a_{2}h_{2}dq_{2}+a_{3}h_{3}dq_{3} $$\nゆえに$a_{i}=\\dfrac{1 }{h_{i}}\\dfrac{ \\partial f}{ \\partial q_{i} }$であり、次が成立する。\n$$ \\quad \\mathbf{a}=\\frac{1 }{h_{1}}\\frac{ \\partial f}{ \\partial q_{1} }\\hat{\\mathbf{q}}_{1}+\\frac{1 }{h_{2}}\\frac{ \\partial f}{ \\partial q_{2} }\\hat{\\mathbf{q}}_{2}+\\frac{1 }{h_{3}}\\frac{ \\partial f}{ \\partial q_{3} }\\hat{\\mathbf{q}}_{3} $$\nこれで上記のベクトル$\\mathbf{a}$を$f$の勾配と定義し、$\\nabla f$と表記する。\n■\n関連項目 曲線座標系での勾配、発散、回転、ラプラシアンの公式 勾配(傾き) 発散 回転 ラプラシアン ","id":1816,"permalink":"https://freshrimpsushi.github.io/jp/posts/1816/","tags":null,"title":"曲線座標系でのスカラー関数の勾配"},{"categories":"푸리에해석","contents":"定義1 関数 $f:\\mathbb{R} \\to \\mathbb{R}$ が 区間ごとの多項式piecewise polynomialである場合、$f$を $\\mathbb{R}$上の スプラインsplineと呼ぶ。多項式が変わる点を ノットknotという。\n説明 定義からわかるように、スプラインが連続関数である必要はない。以下の関数 $f$はスプラインの一例である。\n$$ f(x) = \\begin{cases} 0 \u0026amp; x\\in[\\infty,0] \\\\ 2x^{2}\u0026amp;x\\in(0,1] \\\\ 2-x \u0026amp; x\\in (1,4] \\\\ \\frac{1}{16}x^{3} \u0026amp; x\\in(4,\\infty] \\end{cases} $$\n上の場合で、$x=0$、$x=1$、$x=4$がノットである。B-スプラインはスプラインの中で良い特徴を持つものである。$B$-B-スプライン $N_{1}$を以下のように区間$[0,1]$上の特性関数を用いて定義する。\n$$ N_{1}(x) :=\\chi_{[0,1]}(x)\\quad , x\\in \\mathbb{R} $$\nそして、$m \\in \\mathbb{N}$に対してB-スプライン $N_{m+1}$を以下のように定義する。\n$$ \\begin{equation} N_{m+1}(x) := (N_{m} * N_{1})(x)\\end{equation} $$\nこの時、$\\ast$は畳み込みである。$m$をB-スプライン $N_{m}$のオーダーorderと言う。定義 $(1)$により、次が成り立つ。\n$$ \\begin{align*} N_{m} =\u0026amp;\\ N_{m-1}*N_{1} \\\\ =\u0026amp;\\ N_{m-2}*N_{1}*N_{1} \\\\ =\u0026amp;\\ N_{m-3}*N_{1}*N_{1}*N_{1} \\\\ =\u0026amp;\\ \\underbrace{N_{1}N_{1}N_{1}\\cdotsN_{1}}_{m} \\end{align} $$\nまた、$N_{1}$と畳み込みの定義により、下の式が成り立つことがわかる。\n$$ N_{m+1}(x)=\\int _{-\\infty} ^{\\infty}N_{m}(x-t)N_{1}(t)dt=\\int_{0}^{1}N_{m}(x-t)dt $$\n下の画像は左から$N_{2}$、$N_{3}$のグラフを描いたものである。\n性質 オーダーが$m\\in \\mathbb{N}$のB-スプラインは次のような性質を持つ。\n(a) $\\mathrm{supp}N_{m}=[0,m]$ $\\text{and}$ $N_{m}(x)\u0026gt;0 \\text{ for } x\\in(0,m) $\n(b) $\\displaystyle \\int _{-\\infty} ^{\\infty} N_{m}(x)dx=1$\n(c) $m\\ge 2$に対して、下の式が成り立つ。\n$$ \\begin{equation} \\sum \\limits_{k \\in \\mathbb{Z}} N_{m}(x-k)=1,\\quad \\forall x\\in \\mathbb{R} \\end{equation} $$\n(c\u0026rsquo;) $m=1$の時、上の式は$x\\in \\mathbb{R}\\setminus \\mathbb{Z}$に対して成り立つ。\n参照 数値解析におけるB-スプライン Ole Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010), p203-204\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1815,"permalink":"https://freshrimpsushi.github.io/jp/posts/1815/","tags":null,"title":"解析学におけるスプライン、B-スプライン"},{"categories":"수리통계학","contents":"定義 12 確率変数 $X$ のサンプル $X_{1} , \\cdots , X_{n}$ の関数 $T$ を統計量Statisticと言う。 $$ T := T \\left( X_{1} , \\cdots , X_{n} \\right) $$ $X$ の分布関数が $f(x; \\theta)$ あるいは $p(x; \\theta)$ のように表される時、$T$ が $\\theta$ を把握するための統計量であれば、$T$ を$\\theta$ の推定量Estimatorと言う。 統計量の確率分布をサンプリング分布Sampling Distributionと言う。 説明 推定量(Estimator)の実現を推定値Estimateと言う。パラメータは通常スカラー $\\theta \\in \\mathbb{R}$ の場合が多く、この場合は$T$を$\\theta$の点推定量Point Estimatorとも言う。例えば、正規分布 $N \\left( \\mu, \\sigma^{2} \\right)$ に従うランダムサンプルがあるとき、母平均 $\\mu$ の推定量は次の通りである。 $$ \\overline{X} := {{ 1 } \\over { n }} \\sum_{k = 1}^{n} X_{k} $$ 実際のデータ $x_{1} , \\cdots , x_{n}$ がある場合、$\\mu$ の推定値は次の通りである。 $$ \\overline{x} := {{ 1 } \\over { n }} \\sum_{k = 1}^{n} x_{k} $$\n参考文献 基礎統計学での統計量 基礎統計学ではサンプルの関数とは言わずもっと直感的に「計算されたもの」という表現を使って定義している。本質的には同じ意味だが、数学に馴染みのない新入生や人にとってより良い定義かもしれない。\n統計量の例 平均や分散などを除外して、「統計量」と名前についている統計量には以下のような例がある:\n十分統計量: 分布内のパラメータに関するすべての情報を持つ統計量である。 最小十分統計量: 特定の条件を満たす十分統計量である。 補助統計量: 十分統計量とは反対に、パラメータに関するどんな情報も持たない統計量である。 完全統計量: 統計量としてこのような性質を持っているべきだと言われる時、実際にその性質を持つ統計量である。 推定量の例 推定量には以下のような例がある:\n不偏推定量: 偏りを持たない推定量である。 一致推定量: 極限概念でパラメータを的確に推定する推定量である。 最尤推定量: 尤度Likelihoodを最大化する推定量である。 効率的推定量: 統計量の分散と関連した推定量である。 Hogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p208.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCasella. (2001). Statistical Inference(2nd Edition): p211.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1730,"permalink":"https://freshrimpsushi.github.io/jp/posts/1730/","tags":null,"title":"数理統計学における統計量と推定量"},{"categories":"수리통계학","contents":"定義 1 確率変数 $X$の実際に引き出された結果を実現Realizationと言い、普通、小文字の$x$で表す。 確率変数$X$と同じ確率分布からサンプルサイズSample Size$n$分の確率変数をサンプルSampleと呼び、次のように表す。 $$ X_{1} , X_{2} , \\cdots , X_{n} $$ 確率変数$X_{1} , \\cdots , X_{n}$がiidであれば、サイズ$n$のランダムサンプルと呼ぶ。 説明 これらの定義により、数理統計学は実際の統計解析との接点を持つことになる。ランダムサンプルの実現を扱うことは、統計解析に該当し、数理統計学はそのデータをどのように扱うかについての大きな灯台となる。関心を持つデータ、得たい結論、利用する方法は異なるかもしれないが、その下には数理統計学が理論的な基盤として支えていなければならない。\n実際には、数理統計学の教科書を離れると、実現という表現はあまり使われず、通常はその実現を直接言及する言葉がある。たとえばランダムサンプルの実現は、値、データ、観測値などと呼ばれる。しかし、確率変数は大文字、データは小文字という慣習は、ほとんどすべての統計学教科書で守られている。\n参照 統計入門におけるデータの定義 学部1〜2年生向けの統計学入門では、実験単位や試行などで実際に測定された結果の集まりをデータと定義している。\nHogg et al. (2013).《数理統計学入門》(第7版): p208.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1715,"permalink":"https://freshrimpsushi.github.io/jp/posts/1715/","tags":null,"title":"数理統計学におけるランダムサンプリング"},{"categories":"머신러닝","contents":"オーバーフィッティング トレーニングロスは減っていくが、テストロス（バリデーションロス）が減らない、あるいは増えてしまう現象をオーバーフィッティングover fitting, 過剰適合と呼ぶ。\n説明 これとは反対のアンダーフィッティングという言葉もあるが、正直意味がない言葉で、経験上あまり使われない。\n機械学習で重要な点は、持っているデータで学習した関数が新しいデータに対してもよく機能しなければならないこと。だから、学習に使わなかったデータに対する性能を言う一般化性能という言葉があり、オーバーフィッティングが起きたということは一般化性能が低いということと同じ。入試で模擬試験を解くのは、結局のところ、大学入試をうまく受けるためで、模擬試験は満点を取り、大学入試をダメにする学生は模擬試験の問題にオーバーフィッティングした学生と見なせる。模擬試験で満点までではなくてもいい成績を取り、大学入試でも同様にいい成績を取る学生は、一般化性能がいいということだ。\nレギュラリゼーション （トレーニングロスではなく）テストロスを減らすためにアルゴリズムを修正するすべての方法をレギュラリゼーションと呼ぶ。1\nGoodfellowはレギュラリゼーションを「学習アルゴリズムに対して行う、訓練エラーではなく、一般化エラーを減らすことを目的としたあらゆる修正」と定義している。\nつまり、オーバーフィッティングを防ぐためのすべての方法をまとめてレギュラリゼーションと呼ぶ。機械学習やディープラーニングを勉強するときに、最初に接するのは通常ドロップアウトだ。\n種類 $\\ell_{1}$ レギュラリゼーション $\\ell_{2}$ レギュラリゼーション Weight decay Early stopping ドロップアウトdropout Batch normalization Label smoothing データオーグメンテーションdata augmentation フラッディング 一緒に見ておくべきこと 標準化：通常、統計学でデータの平均を$0$、分散を$1$に合わせるプロセスをいう。 正規化：通常、データを特定の範囲に配置するプロセスをいう。 正則化：通常、機械学習で過剰適合を防ぐプロセスをいう。 Ian Goodfellow, Yoshua Bengio, アンド Aaron Courville. (2016) Deep Learning. MIT Press\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1807,"permalink":"https://freshrimpsushi.github.io/jp/posts/1807/","tags":null,"title":"マシンラーニングにおけるオーバーフィッティングと正則化とは？"},{"categories":"초함수론","contents":"定義 以下の二つの条件を満たす関数$\\phi : \\mathbb{R}^{n} \\to \\mathbb{C}$の集合をシュワルツ空間Schwartz spaceと呼び、$\\mathcal{S}(\\mathbb{R}^{n})$で表す。シュワルツ空間の元$\\phi$をシュワルツ関数Schwartz functionという。\n(a) $\\phi \\in $ $C^{\\infty}$\n(b) すべての多重指数$\\alpha$, $\\beta$に対して$\\left| \\mathbf{x}^{\\beta}D^{\\alpha}\\phi (\\mathbf{x}) \\right| \u0026lt;\\infty$が成り立つ。この時、$\\beta=(\\beta_{1}, \\beta_{2},\\dots,\\beta_{n})$に対して\n$$ \\mathbf{x}^{\\beta}=x_{1}^{\\beta_{1}}x_{2}^{\\beta_{2}}\\dots x_{n}^{\\beta_{n}} $$\n(b) を再記述すると、次のようになる。\n$$ \\mathbf{x}^{ \\beta}D^{\\alpha}\\phi (\\mathbf{x})\\to 0 \\text{ as } \\left| \\mathbf{x} \\right|\\to \\pm \\infty \\quad \\forall \\alpha, \\beta $$\n説明 超関数の掛け算、微分等の様々な操作をテスト関数に適応することで定義した。そうすると、このような意味で超関数のフーリエ変換を下記のように定義しようとする試みができる。\n$$ \\widehat{T^{}}(\\phi):=T \\big( \\hat{\\phi} \\big) $$\nしかし、$\\phi$がテスト関数だとしても、コンパクトサポートを持たないため、テスト関数ではなくなる可能性がある。そのため、超関数のフーリエ変換がうまく定義できない。そこで、フーリエ変換がうまく定義できるようにテスト関数空間を拡張して新たに定義した空間がシュワルツ空間である。\n(a) を見ると、テスト関数の条件と異なり、コンパクトサポートを持つ必要がないという条件がない。これが**(b)** という条件が加えられた理由である。テスト関数は強い条件であるコンパクトサポートを持つため、関数の形に制限を設ける必要がなかった。これに対してシュワルツ関数はコンパクトサポートを持つ必要がないため、数直線の端での関数の値が任意の多項式よりも速く減少するような条件、つまり**(b)** が必要なのである。実際に、テスト関数空間がシュワルツ関数空間の真部分集合であることを示して、うまく拡張されたことがわかる。\n$$ \\mathcal{D}(\\mathbb{R}^{n}) \\subsetneq \\mathcal{S}(\\mathbb{R}^{n}) $$\n性質1 $\\phi, \\psi \\in \\mathcal{S}(\\mathbb{R}^{n})$としよう。\n$\\mathcal{S}(\\mathbb{R}^{n})$はベクトル空間である。 $\\mathcal{S}(\\mathbb{R}^{n})$は乗算に対して閉じている。 $$ \\psi \\phi \\in \\mathcal{S}(\\mathbb{R}^{n}) $$ $\\mathcal{S}(\\mathbb{R}^{n})$は多項式$P$との乗算に対して閉じている。 $$ P\\phi \\in \\mathcal{S}(\\mathbb{R}^{n}) $$ $\\mathcal{S}(\\mathbb{R}^{n})$は微分に対して閉じている。 $$ \\phi^{\\prime} \\in \\mathcal{S}(\\mathbb{R}^{n}) $$ $\\mathcal{S}(\\mathbb{R}^{n})$は平行移動と、複素指数関数との乗算に対して不変である。 $$ \\phi (x+y), \\phi (x)e^{i\\xi \\cdot x} \\in \\mathcal{S}(\\mathbb{R}^{n}) $$ シュワルツ関数は積分可能である。 $$ \\int\\limits_{\\mathbb{R}^{n}} \\left| \\phi (x) \\right| dx \\lt \\infty $$ Robert Strichartz, A Guide to Distribution Theory and Fourier Transforms (1994), p30\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1805,"permalink":"https://freshrimpsushi.github.io/jp/posts/1805/","tags":null,"title":"シュワルツ空間とシュワルツ関数"},{"categories":"수리물리","contents":"ビルドアップ 3次元空間で位置を表現する最も一般的な方法はデカルト座標系だ。デカルトによって考案されたのでこの名前が付けられ、また直交座標系ともよく呼ばれる。しかし、特定の状況下ではデカルト座標系で位置を表現するのが難しい場合がある。例えば、2次元平面で回転運動をしている物体があるとする。その場合、この物体の位置を$(x,y)$で表現するよりも$(r,\\theta)$で表現した方がずっと簡単だ。特定の状況と言ったが、実際には物理的な問題を解決する際にこれらの状況が思った以上によく現れる。そのため、そのような問題を解決するためにデカルト座標系以外の座標系を導入する必要性があり、それについて数学的によく整理されたものが曲線座標系だ。3次元空間のどんな座標系が使用するには、以下のような条件を満たさなければならない。\n(a) デカルト座標系と変換可能でなければならない。 (b) 各座標の方向は互いに直交しなければならない。 既存の座標系と互いに変換できない場合、新しい座標系を作る意味がないため (a) は必須だ。また、内積が数学的にどれだけ便利な演算かを考えれば、(b) を満たさない座標系もまた良い座標系になり得ない。今度は新しい曲線座標系の座標を$q_{1}$、$q_{2}$、$q_{3}$で表そう。その場合、条件 (a) によって次のように表される必要がある。\n$$ \\begin{equation*} \\begin{aligned} x \u0026amp;= x(q_{1},q_{2},q_{3}) \\\\ y \u0026amp;= y(q_{1},q_{2},q_{3}) \\\\ z \u0026amp;= z(q_{1},q_{2},q_{3}) \\end{aligned} \\quad \\text{and} \\quad \\begin{aligned} q_{1} \u0026amp;= q_{1}(x,y,z) \\\\ q_{2} \u0026amp;= q_{2}(x,y,z) \\\\ q_{3} \u0026amp;= q_{3}(x,y,z) \\end{aligned} \\end{equation*} $$\nまた、各座標の単位ベクトルを$\\hat{\\mathbf{q}}_{1}$、$\\hat{\\mathbf{q}}_{2}$、$\\hat{\\mathbf{q}}_{3}$と表す。さらに、各座標の順序は右手の法則に従うとしよう。これを条件 (b) のように表すと、次のようになる。\n$$ \\hat{\\mathbf{q}}_{i}\\cdot \\hat{\\mathbf{q}}_{j}=\\delta_{ij} $$\n$$ (\\hat{\\mathbf{q}}_{1}\\times \\hat{\\mathbf{q}}_{2})\\cdot \\hat{\\mathbf{q}}_{3}\u0026gt;0 $$\n面積=長さ$^{2}$、体積=長さ$^{3}$なので、曲線座標系での長さがどのように表されるかを見てみよう。ある位置ベクトル$\\mathbf{r}$の座標がわずかに変わったとしよう。しかし、さまざまな座標系を考えると、変数が必ずしも長さの単位である必要はない。例えば、極座標系で角度である$\\theta$が変わるたびに、位置が変わる距離は弧の長さ$l=r\\theta$で表される。そのため、$d\\mathbf{r}$の成分が長さの次元を持つように補正する何かを$h_{i}$とすると、$d\\mathbf{r}$は次のように表されることができる。\n$$ d\\mathbf{r}=h_{1}dq_{1}\\hat{\\mathbf{q}_{1}}+h_{2}dq_{2}\\hat{\\mathbf{q}_{2}}+h_{3}dq_{3}\\hat{\\mathbf{q}_{3}} $$\nしたがって、微小長さの二乗は次のようになる。\n$$ \\begin{equation} ds^{2}=d\\mathbf{r}\\cdot d\\mathbf{r}=(h_{1}dq_{1})^{2}+(h_{2}dq_{2})^{2}+(h_{3}dq_{3})^{2} \\end{equation} $$\nまた、微小体積は次のようになる。\n$$ dV=h_{1}h_{2}h_{3}dq_{1}dq_{2}dq_{3} $$\nここで、各成分に対して長さの次元を持たせるための補正をするその何か、つまり$h_{i}$がどのように表されるかを見てみよう。一方、$\\mathbf{r}=\\mathbf{r}(q_{1},q_{2},q_{3})$の変化量である全微分は次のようになる。\n$$ d\\mathbf{r}= \\frac{ \\partial \\mathbf{r}}{ \\partial q_{1}}dq_{1}+\\frac{ \\partial \\mathbf{r}}{ \\partial q_{2}}dq_{2}+\\frac{ \\partial \\mathbf{r}}{ \\partial q_{3}}dq_{3} $$\nすると$\\mathbf{r}$が変化した分の長さの二乗は次のようになる。\n$$ \\begin{equation} \\begin{aligned} ds^{2} \u0026amp;= d \\mathbf{r}^{2}=d\\mathbf{r}\\cdot d\\mathbf{r} \\\\ \u0026amp;= \\left( \\frac{ \\partial \\mathbf{r}}{ \\partial q_{1}}dq_{1}+\\frac{ \\partial \\mathbf{r}}{ \\partial q_{2}}dq_{2}+\\frac{ \\partial \\mathbf{r}}{ \\partial q_{3}}dq_{3} \\right) \\cdot \\left( \\frac{ \\partial \\mathbf{r}}{ \\partial q_{1}}dq_{1}+\\frac{ \\partial \\mathbf{r}}{ \\partial q_{2}}dq_{2}+\\frac{ \\partial \\mathbf{r}}{ \\partial q_{3}}dq_{3} \\right) \\\\ \u0026amp;= \\frac{ \\partial \\mathbf{r}}{ \\partial q_{1} }\\cdot\\frac{ \\partial \\mathbf{r}}{ \\partial q_{1}}dq_{1}dq_{1} + \\frac{ \\partial \\mathbf{r}}{ \\partial q_{1} }\\cdot\\frac{ \\partial \\mathbf{r}}{ \\partial q_{2}}dq_{1}dq_{2}+\\frac{ \\partial \\mathbf{r}}{ \\partial q_{1} }\\cdot\\frac{ \\partial \\mathbf{r}}{ \\partial q_{3}}dq_{1}dq_{3} \\\\ \u0026amp;\\quad + \\frac{ \\partial \\mathbf{r}}{ \\partial q_{2} }\\cdot\\frac{ \\partial \\mathbf{r}}{ \\partial q_{1}}dq_{2}dq_{1} + \\frac{ \\partial \\mathbf{r}}{ \\partial q_{2} }\\cdot\\frac{ \\partial \\mathbf{r}}{ \\partial q_{2}}dq_{2}dq_{2}+\\frac{ \\partial \\mathbf{r}}{ \\partial q_{2} }\\cdot\\frac{ \\partial \\mathbf{r}}{ \\partial q_{3}}dq_{2}dq_{3} \\\\ \u0026amp;\\quad + \\frac{ \\partial \\mathbf{r}}{ \\partial q_{3} }\\cdot\\frac{ \\partial \\mathbf{r}}{ \\partial q_{1}}dq_{3}dq_{1} + \\frac{ \\partial \\mathbf{r}}{ \\partial q_{3} }\\cdot\\frac{ \\partial \\mathbf{r}}{ \\partial q_{2}}dq_{3}dq_{2}+\\frac{ \\partial \\mathbf{r}}{ \\partial q_{3} }\\cdot\\frac{ \\partial \\mathbf{r}}{ \\partial q_{3}}dq_{3}dq_{3} \\end{aligned} \\end{equation} $$\nこの時、以下が成り立つ。\n$$ \\begin{align*} \\frac{ \\partial \\mathbf{r}}{ \\partial q_{i} } \u0026amp;= \\frac{ \\partial (x\\hat{\\mathbf{x}}+y\\hat{\\mathbf{y}}+\\hat{z\\mathbf{z}})}{ \\partial q_{i} } \\\\ \u0026amp;= \\frac{ \\partial x}{ \\partial q_{i}}\\hat{\\mathbf{x}}+\\frac{ \\partial y}{ \\partial q_{i}}\\hat{\\mathbf{y}}+\\frac{ \\partial z}{ \\partial q_{i}}\\hat{\\mathbf{z}} \\end{align*} $$\nしたがって、次を得る。\n$$ \\frac{ \\partial \\mathbf{r}}{ \\partial q_{i}}\\cdot \\frac{ \\partial \\mathbf{r}}{ \\partial q_{j}} = \\frac{ \\partial x}{ \\partial q_{i}}\\frac{ \\partial x}{ \\partial q_{j}}+ \\frac{ \\partial y}{ \\partial q_{i}}\\frac{ \\partial y}{ \\partial q_{j}}+\\frac{ \\partial z}{ \\partial q_{i}}\\frac{ \\partial z}{ \\partial q_{j}} $$\nこの式を簡単に$g_{ij}=\\frac{ \\partial \\mathbf{r}}{ \\partial q_{i}}\\cdot \\frac{ \\partial \\mathbf{r}}{ \\partial q_{j}}$で表すと$(2)$は次のようになる。\n$$ \\begin{equation} \\begin{aligned} ds^{2} \u0026amp;= g_{11}dq_{1}^{2}+g_{12}dq_{1}dq_{2}+g_{13}dq_{1}dq_{3} \\\\ \u0026amp;\\quad + g_{21}dq_{2}dq_{1}+g_{22}dq_{2}^{2}+g_{23}dq_{2}dq_{3} \\\\ \u0026amp;\\quad + g_{31}dq_{3}dq_{1}+g_{32}dq_{3}dq_{2}+g_{33}dq_{3}^{2} \\\\ \u0026amp;= \\sum \\limits _{i,j=1}^{3}g_{ij}dq_{i}dq_{j} \\end{aligned} \\end{equation} $$\nこれで$(1)$と$(3)$を比較してみると、以下の結果が得られる。\n$$ g_{ij}=\\begin{cases} h_{i}^{2}, \u0026amp;i=j \\\\ 0, \u0026amp;i\\ne j \\end{cases} $$\nこの時、$h_{i}=\\sqrt{g_{ii}}=\\sqrt{\\dfrac{ \\partial \\mathbf{r}}{ \\partial q_{i} }\\cdot \\dfrac{ \\partial \\mathbf{r}}{ \\partial q_{i} }}$をスケールファクターscale factor と呼ぶ。長さの単位に変えるために非長さの変数に掛けられる役割をする。\n座標空間の座標系を曲線座標系に一般化すると、デカルト座標系は$h_{1}=h_{2}=h_{3}=1$の曲線座標系と考えることができる。主に使用される各曲線座標系のスケールファクターは以下の通り。\n証明\n極座標系:\n$$ h_{1}=1,\\quad h_{2}=r $$\n円筒座標系:\n$$ h_{1}=1, \\quad h_{2}=\\rho,\\quad h_{3}=1 $$\n球座標系:\n$$ h_{1}=1,\\quad h_{2}=r,\\quad h_{3}=r\\sin\\theta $$\n","id":1774,"permalink":"https://freshrimpsushi.github.io/jp/posts/1774/","tags":null,"title":"三次元空間の曲線座標系"},{"categories":"초함수론","contents":"ビルドアップ 超関数の微分を定義するアイデアを思い出そう。$u \\in {L}_{\\mathrm{loc}}^1(\\Omega)$に対して、正則超関数$T_{u}$が存在する。$u$が微分可能であれば、部分積分法により、次の式が成り立ち、$T_{u}$の導関数を$u$の導関数である$u^{\\prime}$に対応する$T_{u^{\\prime}}$として定義した。\n$$ \\begin{align*} T_{u}^{\\prime}(\\phi) \u0026amp;:= T_{u^{\\prime}}(\\phi) \\\\ \u0026amp;= \\int u^{\\prime}(x)\\phi (x)dx \\\\ \u0026amp;= \\left[ u(x) \\phi (x) \\right]_{-\\infty}^{\\infty} -\\int u(x)\\phi ^{\\prime} (x) dx \\\\ \u0026amp;= -\\int u(x)\\phi ^{\\prime} (x) dx \\\\ \u0026amp;= -T_{u}(\\phi^{\\prime}) \\end{align*} $$\nでも、もし$u(x)$が$\\Omega$で微分可能ではないとしたら。それでも、$u$に対応する超関数$T_{u}$は定義により、次のような導関数を持つ。\n$$ T_{u}^{\\prime}(\\phi) = T_{u}(\\phi^{\\prime}) $$\n従って、次の式を満たす$v(x)$が存在するなら、これを$u(x)$の導関数として扱うことができるだろう。\n$$ -T_{u}(\\phi^{\\prime}) = -\\int u(x)\\phi ^{\\prime} (x) dx = \\int v(x)\\phi (x)dx = T_{v}(\\phi) $$\nこれを多指数$\\alpha$に対して拡張すると、次のようになる。\n$$ (-1)^{|\\alpha|} \\int_{\\Omega} u(x){D}^{\\alpha}\\phi (x)dx = \\int_{\\Omega}v_{\\alpha}(x)\\phi (x)dx, \\quad \\forall\\ \\phi \\in \\mathcal{D}(\\Omega) $$\n定義1 $u \\in {L}_{\\mathrm{loc}}^1(\\Omega)$としよう。次のような式を満たす$v_{\\alpha}$が存在するなら、これを$u$の弱導関数weak derivativeまたは超関数的導関数distributional derivativeと呼ぶ。\n$$ \\begin{align*} T_{{v}_{\\alpha}} \u0026amp;= {D}^{\\alpha}T_{u} \u0026amp; \\text{in } \\mathcal{D}^{\\ast}(\\Omega) \\\\ \\int_{\\Omega}v_{\\alpha}(x)\\phi (x)dx \u0026amp;= (-1)^{|\\alpha|} \\int_{\\Omega} u(x){D}^{\\alpha}\\phi (x)dx \u0026amp; \\forall\\ \\phi \\in \\mathcal{D}(\\Omega) \\end{align*} $$\n説明 簡単な説明はここを参照してくれ。\n例 区間$(-1, 1)$で、$u$と$v$が下のように定義されているとしよう。\n$$ u(x) = |x| \\quad \\text{and} \\quad v(x) = \\begin{cases} 1 \u0026amp; 0 \\lt x \\lt 1 \\\\ 0 \u0026amp; x=0 \\\\ -1 \u0026amp; -1 \\lt x \\lt 0 \\end{cases} $$ すると、$u$は$x=0$で微分不可能なので、$(-1,1)$で導関数を定義することはできないが、$v$が$u$の弱導関数になる。$v$が$u$の弱導関数であることは、下の過程で確認できる。$\\phi \\in \\mathcal{D}(\\Omega)$としよう。すると、次の式が成り立つ。\n$$ \\begin{align*} -\\int_{-1}^1 u(x) \\phi^{\\prime}(x)dx \u0026amp;= -\\int_{-1}^{0} |x| \\phi^{\\prime}(x) dx -\\int_{0}^{1} |x| \\phi^{\\prime}(x) dx \\\\ \u0026amp;= -\\int_{-1}^{0} -x \\phi^{\\prime}(x) dx -\\int_{0}^{1} x \\phi^{\\prime}(x) dx \\\\ \u0026amp;= -\\left( [-x\\phi (x)]_{-1}^{0} +\\int_{-1}^{0}\\phi (x)dx \\right) - \\left( [x\\phi (x)]_{0}^1-\\int_{0}^1 \\phi (x)dx \\right) \\\\ \u0026amp;= \\int_{-1}^{0} -1 \\cdot \\phi (x) dx + \\int_{0}^{1}\\ 1 \\cdot \\phi (x) dx \\\\ \u0026amp;= \\int_{-1}^1v(x)\\phi (x) dx \\end{align*} $$\n実際に$v(x)$の値は$x \\ne 0$の所では全て$u^{\\prime}(x)$と同じで、$x=0$では、$u(x)$の左右の微分係数の中間値を持つ。従って、$v(x)$を$u(x)$の導関数として扱っても問題ないことがわかる。\nRobert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003), p22\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1085,"permalink":"https://freshrimpsushi.github.io/jp/posts/1085/","tags":null,"title":"近似導関数"},{"categories":"바나흐공간","contents":"定義 $(X, \\left\\| \\cdot \\right\\|)$をノルム空間と呼ぼう。$X$の列$\\left\\{ x_{n} \\right\\}$が\n$$ \\lim \\limits_{n \\to \\infty} \\left\\| x - x_{n} \\right\\| = 0,\\quad x\\in X $$\nを満たせば、列$\\left\\{ x_{n} \\right\\}$が$x$に収束するconvergeと言い、以下のように表されます。\n$$ x_{n} \\to x \\text { as } n \\to \\infty \\quad \\text{or} \\quad x=\\lim \\limits_{n\\to\\infty}x_{n} $$\n説明 収束を定義するためには距離が必要ですが、ノルム空間では$d(x,y)=\\left\\| x - y \\right\\|$として自然に距離を定義できるため、距離空間の定義とは距離をノルムに置き換えたものと同じです。\n全ての$\\epsilon \u0026gt;0$に対して、以下の式を満たす自然数$N\\in \\mathbb{N}$が存在すれば、列$\\left\\{ x_{n} \\right\\}$が$x$に収束すると言います。\n$$ \\left\\| x - x_{n} \\right\\|\u0026lt;\\epsilon \\quad \\forall n \\ge N $$\n弱収束と比べて強く収束するとも言います。\n$$ \\begin{align*} \u0026amp; x_{n} \\text{ converges to } x \\\\ =\u0026amp;\\ x_{n} \\text{ converges in norm to } x \\\\ =\u0026amp;\\ x_{n} \\text{ converges strongly to } x \\end{align*} $$\n","id":1800,"permalink":"https://freshrimpsushi.github.io/jp/posts/1800/","tags":null,"title":"ノルム空間内の数列の収束"},{"categories":"수리물리","contents":"定義 多変数関数$f : \\mathbb{R}^{n} \\to \\mathbb{R}$が与えられたとしよう。変数$\\mathbf{x} = (x_{1}, x_{2}, \\dots, x_{n})$の変化に伴う$f(\\mathbf{x})$の変化を以下のように$df$と表し、これを$f$の全微分total differentialまたは完全微分exact differentialという。\n$$ \\begin{equation} df = \\frac{ \\partial f}{ \\partial x_{1} }dx_{1} + \\frac{ \\partial f}{ \\partial x_{2} }dx_{2} + \\cdots + \\frac{ \\partial f}{ \\partial x_{n} }dx_{n} \\label{1} \\end{equation} $$\n説明 上記の定義は、変数$\\mathbf{x}$の変化に伴う$f$の値の変化を\n各成分の変化量$dx_{i}$に、各成分の変化に伴う$f$の変化率$\\dfrac{\\partial f}{\\partial x_{i}}$を掛けた$\\dfrac{ \\partial f}{ \\partial x_{i} }dx_{i}$を全て足し合わせること と考えるという意味だ。以下の式を通して、この表記が直観的で便利であることが分かる。$f=f(x,y,z)$とするとき、\n$$ \\dfrac{df}{dx} = \\frac{ \\partial f}{ \\partial x}\\dfrac{dx}{dx} + \\frac{ \\partial f}{ \\partial y}\\dfrac{dy}{dx} + \\frac{ \\partial f}{ \\partial z}\\dfrac{dz}{dx} = \\dfrac{\\partial f}{\\partial x} $$\n物理学では、次のような形でも頻繁に登場する。$\\left( x(t), y(t), z(t) \\right)$について、\n$$ \\dfrac{d f}{d t} = \\frac{ \\partial f}{ \\partial x}\\dfrac{dx}{dt} + \\frac{ \\partial f}{ \\partial y}\\dfrac{dy}{dt} + \\frac{ \\partial f}{ \\partial z}\\dfrac{dz}{dt} $$\n導出 2変数関数について、次のような方法で$\\eqref{1}$を導出できる。$z=f(x,y)$が与えられたとしよう。$z$の全微分は変数$x$、$y$が変化するときの$z$の変化量であるから、以下のように表される。\n$$ dz = f(x+dx,y+dy)-f(x,y) $$\nここで、右辺に$f(x,y+dy)$を引いて足してから式を整理すると、以下のようになる。 $$ \\begin{align*} dz \u0026amp;= f(x+dx,y+dy) {\\color{blue}-f(x,y+dy)+f(x,y+dy)}-f(x,y) \\\\ \u0026amp;= [f(x+dx,y+dy) -f(x,y+dy)]+[f(x,y+dy)-f(x,y)] \\\\ \u0026amp;= \\frac{f(x+dx,y+dy) -f(x,y+dy)}{dx}dx+\\frac{f(x,y+dy)-f(x,y)}{dy}dy \\\\ \u0026amp;\\approx \\frac{ \\partial f}{ \\partial x}dx + \\frac{ \\partial f}{ \\partial y }dy \\\\ \u0026amp;= \\frac{ \\partial z}{ \\partial x}dx+\\frac{ \\partial z}{ \\partial y}dy \\end{align*} $$\n■\n","id":1773,"permalink":"https://freshrimpsushi.github.io/jp/posts/1773/","tags":null,"title":"全微分、完全微分"},{"categories":"확률분포론","contents":"定義 以下の確率密度関数を持つ連続確率分布をコーシー分布と呼ぶ。 $C$ $$ f(x) = {1 \\over \\pi} {1 \\over {x^2 + 1}} \\qquad , x \\in \\mathbb{R} $$\n説明 全ての確率分布に平均や分散があると思われがちだけど、実際にはそうではない。その代表例がコーシー分布で、一見正規分布に似ているが、両側の裾が厚い形状をしている。パラメータに関係なく、モーメント生成関数が存在しないので、平均であれ分散であれモーメントを含んだ全てのものは存在できない。\nもちろん、母平均があろうと無かろうと、標本平均は計算できる。実際に$x$軸において$\\theta$だけ平行移動したコーシー分布では、$\\theta$のmle$\\hat{\\theta}$が標本平均として現れる。\n一方、t-分布の確率密度関数は $$ g(y) = {{\\Gamma ( (n+1)/2 ) } \\over { \\sqrt{\\pi n} \\Gamma (n/2) }} { {1} \\over {(1 + y^{2} / n)^{(n+1)/2} } } $$ であり、コーシー分布は自由度$n=1$のt-分布と見なすことができる。\n要約 コーシー分布のモーメント生成関数は存在しない。\n証明1 コーシー分布の確率分布関数は$\\displaystyle f(x) = {1 \\over \\pi} {1 \\over {x^2 + 1}}, -\\infty \u0026lt; x \u0026lt; \\infty$で与えられる。モーメント生成関数$\\displaystyle E(e^{tx}) = \\int_{-\\infty}^{\\infty} e^{tx} {1 \\over \\pi} {1 \\over {x^2 + 1}} dx$が発散することを示すために、$t\u0026gt;0$のとき、平均値の定理により、 $$ {{e^{tx} - e^0} \\over {tx - 0}} = { { e^{tx} - 1 } \\over {tx} } = e^{\\xi} \\ge e^0 = 1 $$ を満たす$0\u0026lt; \\xi \u0026lt; tx$が存在する。この式を少し整理すると、次の不等式を得る。 $$ e^{tx} \\ge 1 + tx \\ge tx $$ 再び積分に戻ると、 $$ \\begin{align*} E(e^{tx}) \\ge\u0026amp; \\int_{-\\infty}^{\\infty} e^{tx} {1 \\over \\pi} {1 \\over {x^2 + 1}} dx \\\\ \\ge\u0026amp; \\int_{0}^{\\infty} e^{tx} {1 \\over \\pi} {1 \\over {x^2 + 1}} dx \\\\ \\ge\u0026amp; \\int_{0}^{\\infty} {1 \\over \\pi} {tx \\over {x^2 + 1}} dx \\\\ =\u0026amp; { t \\over {2 \\pi} } \\left[ \\ln (x^2+1) \\right]_{0}^{\\infty} \\\\ =\u0026amp; \\infty \\end{align*} $$ したがって、コーシー分布のモーメント生成関数は存在しない。\n■\nコード 以下はコーシー分布、t-分布、コーシー分布の確率密度関数を示すJuliaのコードだ。\n@time using LaTeXStrings\r@time using Distributions\r@time using Plots\rcd(@__DIR__)\rx = -4:0.1:4\rplot(x, pdf.(Cauchy(), x),\rcolor = :red,\rlabel = \u0026#34;Cauchy\u0026#34;, size = (400,300))\rplot!(x, pdf.(TDist(3), x),\rcolor = :orange,\rlabel = \u0026#34;t(3)\u0026#34;, size = (400,300))\rplot!(x, pdf.(TDist(30), x),\rcolor = :black, linestyle = :dash,\rlabel = \u0026#34;t(30)\u0026#34;, size = (400,300))\rplot!(x, pdf.(Normal(), x),\rcolor = :black,\rlabel = \u0026#34;Standard Normal\u0026#34;, size = (400,300))\rxlims!(-4,5); ylims!(0,0.5); title!(L\u0026#34;\\mathrm{pdf\\,of\\, t}(\\nu)\u0026#34;)\rpng(\u0026#34;pdf\u0026#34;) Hogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p63.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":147,"permalink":"https://freshrimpsushi.github.io/jp/posts/147/","tags":null,"title":"コーシー分布：平均が存在しない分布"},{"categories":"수리물리","contents":"定義 ベクトル関数 $\\mathbf{F}(x,y,z)=F_{x}\\hat{\\mathbf{x}}+F_{y}\\hat{\\mathbf{y}} + F_{z}\\hat{\\mathbf{z}}$について、以下のようなスカラー関数を$\\mathbf{F}$のダイバージェンスdivergence, 発散と定義し、$\\nabla \\cdot \\mathbf{F}$と表記する。\n$$ \\begin{equation} \\nabla \\cdot \\mathbf{F} := \\frac{ \\partial F_{x}}{ \\partial x} + \\frac{ \\partial F_{y}}{ \\partial y }+ \\frac{ \\partial F_{z}}{ \\partial z} \\label{divergence} \\end{equation} $$\n説明 幾何学的に$\\nabla \\cdot \\mathbf{F}\u0026gt;0$の場合、$\\mathbf{F}$が広がり出る、外へ出る形をしていることを意味し、$\\nabla \\cdot \\mathbf{F}\u0026lt;0$の場合は$\\mathbf{F}$が集まる、内へ入る形をしていることを意味し、$\\nabla \\cdot \\mathbf{F}=0$の場合は$\\mathbf{F}$が広がりも集まりもしない、出入りの量が同じ形をしていることを意味する。\nダイバージェンスは発散と翻訳される。生エビ寿司屋では、勾配をグラディエント、回転をカールと使うため、統一感のために発散ではなくダイバージェンスと表記する。\n定義で$\\dfrac{ \\partial F_{x}}{ \\partial x} + \\dfrac{ \\partial F_{y}}{ \\partial y }+ \\dfrac{ \\partial F_{z}}{ \\partial z}$という値を$\\nabla \\cdot \\mathbf{F}$で表記することに注意しよう。$\\nabla$をデル演算子と呼ぶことはあるが、これ自体に何か意味を持つと考えると$\\nabla \\cdot \\mathbf{F}$や$\\nabla \\times \\mathbf{F}$を内積と外積と間違えやすい。だから$\\nabla$は単なる便利な表記法程度にしか理解しないほうがよく、グラディエント、ダイバージェンス、カールをまとめてデル演算子たちと呼んだり、むしろデル演算子=グラディエントと考えるほうがよいかもしれない。詳細は以下で続ける。\n注意点 $\\nabla \\cdot \\mathbf{F}$は$\\nabla$と$\\mathbf{F}$の内積ではない $\\nabla \\cdot \\mathbf{F}$は絶対に$\\nabla$と$\\mathbf{F}$の内積ではない。\r内積は基本的に二つのベクトル同士の演算である。$\\nabla \\cdot \\mathbf{F}$を内積と考えることは、$\\nabla$を以下のようなベクトルと見なすことである。\n$$ \\nabla \\overset{?}{=}\\dfrac{ \\partial }{ \\partial x}\\hat{\\mathbf{x}} +\\dfrac{ \\partial }{ \\partial y}\\hat{\\mathbf{y}}+\\dfrac{ \\partial }{ \\partial z}\\hat{\\mathbf{z}} = \\left( \\dfrac{ \\partial }{ \\partial x},\\ \\dfrac{ \\partial }{ \\partial y},\\ \\dfrac{ \\partial }{ \\partial z} \\right) $$\n確かにこのように考えると、次のようにダイバージェンスの定義$(1)$通りに計算がうまくいくので便利であることは事実である。\n$$ \\nabla \\cdot \\mathbf{F} = \\left( \\dfrac{ \\partial }{ \\partial x},\\ \\dfrac{ \\partial }{ \\partial y},\\ \\dfrac{ \\partial }{ \\partial z} \\right) \\cdot \\left( F_{x}, F_{y}, F_{z} \\right) = \\frac{ \\partial F_{x}}{ \\partial x} + \\frac{ \\partial F_{y}}{ \\partial y }+ \\frac{ \\partial F_{z}}{ \\partial z} $$\nしかし、これが実際に内積であれば、内積は交換法則が成立するため、以下のような等式が成立するという奇妙な結論になる。\n$$ \\mathbf{F} \\cdot \\nabla = F_{x} \\dfrac{\\partial }{\\partial x} + F_{y} \\dfrac{\\partial }{\\partial y} + F_{z} \\dfrac{\\partial }{\\partial z} \\overset{?}{=} \\frac{ \\partial F_{x}}{ \\partial x} + \\frac{ \\partial F_{y}}{ \\partial y }+ \\frac{ \\partial F_{z}}{ \\partial z} = \\nabla \\cdot \\mathbf{F} $$\n実際には$\\nabla \\cdot$の実体はベクトル関数 $\\mathbf{F}(x,y,z)$をスカラー関数 $\\frac{ \\partial F_{x}(x,y,z)}{ \\partial x} + \\frac{ \\partial F_{y}(x,y,z)}{ \\partial y }+ \\frac{ \\partial F_{z}(x,y,z)}{ \\partial z}$に対応させる演算子である。これが何を意味するかというと、$\\operatorname{div}$という関数を次のように定義してみよう。\n$$ \\operatorname{div}(\\mathbf{F}) := \\frac{ \\partial F_{x}}{ \\partial x} + \\frac{ \\partial F_{y}}{ \\partial y }+ \\frac{ \\partial F_{z}}{ \\partial z},\\qquad \\mathbf{F} = (F_{x}, F_{y}, F_{z}) $$\nここには内積だとかそういう言葉は一切ない。$\\operatorname{div}$という関数は単に変数$\\mathbf{F}$が代入されるたびに$\\dfrac{ \\partial F_{x}}{ \\partial x} + \\dfrac{ \\partial F_{y}}{ \\partial y }+ \\dfrac{ \\partial F_{z}}{ \\partial z}$を与える関数である。これを定義してみると、$\\nabla$というものを$\\nabla = \\dfrac{ \\partial }{ \\partial x}\\hat{\\mathbf{x}} +\\dfrac{ \\partial }{ \\partial y}\\hat{\\mathbf{y}}+\\dfrac{ \\partial }{ \\partial z}\\hat{\\mathbf{z}}$と同じベクトルと考えると$\\operatorname{div}$の関数値を表記するのが非常に便利で直感的であることがわかる。だから$\\operatorname{div}(\\mathbf{F})$と表記する代わりに、$\\nabla \\cdot \\mathbf{F}$と表記するのである。実際に専門の数学の教科書では、ダイバージェンスを$\\operatorname{div}$と表記することが容易に見つかるが、これは物理学部と異なり3次元ベクトルを直感的に扱わないためであると考えられる。\nでは$\\nabla \\cdot \\mathbf{F}$を交換法則が成立しない内積として考えてはいけないのか？ いけない。なぜなら$\\nabla \\cdot \\mathbf{F}$で$\\nabla \\cdot$自体が関数(演算子)であり$\\mathbf{F}$が変数である。一方で$\\mathbf{F} \\cdot \\nabla$はそのもの自体が関数(演算子)であるためである。したがって$\\nabla \\cdot \\mathbf{F}$は関数$\\nabla \\cdot$の関数値であり、$\\mathbf{F} \\cdot \\nabla$は（まだ変数が代入されていない）関数である。具体的に$\\mathbf{F} \\cdot \\nabla$という表記は次のような関数$f$を直感的\nに簡単に表記したものである。$f$はベクトル関数$\\mathbf{A}$を変数とする演算子であり、$\\mathbf{A}$の各成分に$\\left( F_{x}\\dfrac{\\partial }{\\partial x} + F_{y}\\dfrac{\\partial }{\\partial y} + F_{z}\\dfrac{\\partial }{\\partial z} \\right)$を適用する関数である。\n$$ \\begin{align*} f (\\mathbf{A}) \\overset{\\text{definition}}{=}\u0026amp; \\left( F_{x}\\dfrac{\\partial A_{x}}{\\partial x} + F_{y}\\dfrac{\\partial A_{x}}{\\partial y} + F_{z}\\dfrac{\\partial A_{x}}{\\partial z} \\right)\\hat{\\mathbf{x}} \\\\ \u0026amp;\\quad+ \\left( F_{x}\\dfrac{\\partial A_{y}}{\\partial x} + F_{y}\\dfrac{\\partial A_{y}}{\\partial y} + F_{z}\\dfrac{\\partial A_{y}}{\\partial z} \\right)\\hat{\\mathbf{y}} \\\\ \u0026amp;\\quad+ \\left( F_{x}\\dfrac{\\partial A_{z}}{\\partial x} + F_{y}\\dfrac{\\partial A_{z}}{\\partial y} + F_{z}\\dfrac{\\partial A_{z}}{\\partial z} \\right)\\hat{\\mathbf{z}} \\\\ \\overset{\\text{notation}}{=}\u0026amp; (\\mathbf{F}\\cdot \\nabla) (\\mathbf{A}) \\end{align*} $$\n何かスカラー関数$\\phi$が変数としてある場合、次のような演算子と考える。\n$$ \\begin{align*} f (\\phi) \\overset{\\text{definition}}{=}\u0026amp; F_{x}\\dfrac{\\partial \\phi}{\\partial x} + F_{y}\\dfrac{\\partial \\phi}{\\partial y} + F_{z}\\dfrac{\\partial \\phi}{\\partial z} \\\\ \\overset{\\text{notation}}{=}\u0026amp; (\\mathbf{F}\\cdot \\nabla) (\\phi) \\end{align*} $$\nしたがって$\\nabla \\cdot \\mathbf{F}$と$\\mathbf{F}\\cdot \\nabla$は$\\nabla$と$\\mathbf{F}$の内積として理解してはいけず、$\\nabla \\cdot$と$\\mathbf{F} \\cdot \\nabla$自体を一つの関数として考えなければならない。これはもちろんダイバージェンスに限った説明ではなく、グラディエント$\\nabla f$やカール$\\nabla \\times \\mathbf{F}$も同様に理解しなければならない。\n導出 まず以下のように3次元空間で微小体積を考えてみよう。\n今、我々の目的は$\\mathbf{F}$がその微小体積内の各座標でどのように見えるかを知ることである。現実に例えるならば$\\mathbf{F}$が熱であればどの方向に、どの速度で流れているかを、$\\mathbf{F}$が水であればこれが蛇口から出ている水なのか、排水溝に入っていく水なのかを知りたいということである。まず$x$軸方向だけを計算してみよう。$\\mathbf{F}$が$d\\mathbf{a}_{1}$を通過する量は二つのベクトルの内積で求めることができる。\n$$ \\begin{align} \\mathbf{F}(x+dx) \\cdot d\\mathbf{a}_{1} \u0026amp;= \\left( F_{x}(x+dx)\\hat{\\mathbf{x}}+F_{y}(x+dx)\\hat{\\mathbf{y}}+F_{z}(x+dx)\\hat{\\mathbf{z}} \\right) \\cdot dydz\\hat{\\mathbf{x}} \\nonumber \\\\ \u0026amp;= F_{x}(x+dx)dydz \\end{align} $$\n$F_{x}(x+dx)dydz \u0026gt;0$の場合、$\\mathbf{F}$が微小体積を抜け出る量であり、$F_{x}(x+dx)dydz\u0026lt;0$の場合は$\\mathbf{F}$が微小体積に入る量である。同様に$\\mathbf{F}$が$d\\mathbf{a}_{2}$を抜け出る量は以下のようである。\n$$ \\begin{equation} \\mathbf{F}(x) \\cdot d \\mathbf{a}_{2} = F_{x}(x)\\hat{\\mathbf{x}} \\cdot(-dydz\\hat{\\mathbf{x}})=-F_{x}(x)dydz \\end{equation} $$\nしたがって$(2) + (3)$は微小体積での$\\mathbf{F}$の$x$軸方向の流入量（流出量）である。\n$$ \\begin{align*} (2) + (3) \u0026amp;=\\left[ F_{x}(x+dx) -F_{x}(x)\\right]dydz \\\\ \u0026amp;= \\frac{F_{x}(x+dx) -F_{x}(x) }{dx}dxdydz \\end{align*} $$\nしかし$dx$が微小距離であるため、$\\dfrac{F_{x}(x+dx) -F_{x}(x) }{dx}\\approx \\dfrac{ \\partial F_{x}}{ \\partial x }$と同様に近似できる。したがって$\\mathbf{F}$が$x$軸方向へ微小体積に入るまたは出る量は以下のように表される。\n$$ \\frac{ \\partial F_{x}}{ \\partial x}dxdydz $$ 同様に$y$軸方向、$z$軸方向について計算すると以下の結果を得る。\n$$ \\frac{ \\partial F_{y}}{ \\partial y}dxdydz \\quad \\text{and} \\quad \\frac{ \\partial F_{z}}{ \\partial z}dxdydz $$\nこれを全て足すと$\\mathbf{F}$が微小体積に入るまたは出る量となり、$dxdydz$で割ると単位体積あたりの流入量（流出量）となる。\n$$ \\frac{ \\partial F_{x}}{ \\partial x}+\\frac{ \\partial F_{y}}{ \\partial y}+\\frac{ \\partial F_{z}}{ \\partial z} $$\nこれからこれを$\\mathbf{F}$のダイバージェンスと呼び、$\\nabla \\cdot \\mathbf{F}$と表記しよう。\n$$ \\nabla \\cdot \\mathbf{F} := \\frac{ \\partial F_{x}}{ \\partial x}+\\frac{ \\partial F_{y}}{ \\partial y}+\\frac{ \\partial F_{z}}{ \\partial z} $$\n■\n導出される過程を見ても分かるように、上で述べた通り$\\nabla \\cdot \\mathbf{F}$は絶対に$\\nabla$と$\\mathbf{F}$の内積ではない。この点に注意しよう。\n関連する公式 線形性:\n積の規則:\n$$ \\nabla \\cdot (f\\mathbf{A}) = f(\\nabla \\cdot \\mathbf{A}) + \\mathbf{A} \\cdot (\\nabla f) $$ $$ \\nabla \\cdot (\\mathbf{A} \\times \\mathbf{B}) = \\mathbf{B} \\cdot (\\nabla \\times \\mathbf{A}) - \\mathbf{A} \\cdot (\\nabla \\times \\mathbf{B}) $$\n二階導関数:\n$$ \\nabla \\cdot (\\nabla T) = \\dfrac{\\partial^{2} T}{\\partial x^{2}} + \\dfrac{\\partial ^{2} T} {\\partial y^{2}} + \\dfrac{\\partial ^{2} T}{\\partial z^{2}} $$ $$ \\nabla (\\nabla \\cdot \\mathbf{A} ) $$ $$ \\nabla \\cdot (\\nabla \\times \\mathbf{A})=0 $$\nガウスの定理 (発散定理)\n$$ \\int_\\mathcal{V} \\nabla \\cdot \\mathbf{ F} dV = \\oint _\\mathcal{S} \\mathbf{F} \\cdot d \\mathbf{S} $$\n積分公式\n$$ \\int_{\\mathcal{V}} \\left[ T \\nabla^{2} U + (\\nabla T) \\cdot (\\nabla U) \\right] d \\tau = \\oint_{\\mathcal{S}} (T \\nabla U) \\cdot d \\mathbf{a} $$ $$ \\int_{\\mathcal{V}} \\left( T \\nabla^{2} U - U \\nabla^{2} T \\right) d \\tau = \\oint_{\\mathcal{S}} \\left( T \\nabla U - U \\nabla T \\right) \\cdot d \\mathbf{a} $$\n部分積分\n$$ \\int_{\\mathcal{V}}\\mathbf{A} \\cdot (\\nabla f)d\\tau = \\oint_{\\mathcal{S}}f\\mathbf{A} \\cdot d \\mathbf{a}-\\int_{\\mathcal{V}}f(\\nabla \\cdot \\mathbf{A})d\\tau $$\n参照 デル演算子 $\\nabla$ グラディエント $\\nabla f$ ダイバージェンス $\\nabla \\cdot \\mathbf{F}$ カール $\\nabla \\times \\mathbf{F}$ ラプラシアン $\\nabla^{2} f$ ","id":1796,"permalink":"https://freshrimpsushi.github.io/jp/posts/1796/","tags":null,"title":"直交座標系におけるベクトル関数の発散"},{"categories":"초함수론","contents":"ビルドアップ 配布物超関数は、その定義域が関数空間であるため、実数空間で定義された関数のように微分することはできません。しかし、正則超関数の場合には、対応する局所的に積分可能な関数$u\\in L_{\\mathrm{loc}}^{1}$が存在し、以下のように表されます。\n$$ T_{u}(\\phi) =\\int u(x)\\phi (x) dx,\\quad \\phi \\in \\mathcal{D} $$\nしたがって、$u$に対するある作用$S$によって$Su=u^{\\prime}$を得ることができるだろうが、$u^{\\prime}$が依然として局所的に積分可能な関数であるならば、それに対応する超関数$T_{u^{\\prime}}$が存在します。したがって、$u$に対する作用$S$を、$T_{u}$に対する作用であるかのように考えるわけです。このようなアイデアを超関数全体に拡張して、超関数の微分を定義しようとしています。\n以下では、$u\\in C^{\\infty}$と仮定していますが、必ずしもそうする必要はありません。$u \\in C^{n}$とし、$n$階の導関数まで話してもかまいません。\nあるスムーズ関数$u\\in C^{\\infty}$が与えられたとしましょう。テスト関数$\\phi$はコンパクトサポートを持つので、テスト関数のサポートを含むあるコンパクトな集合$K$上で$u$が定義されていると考えても問題ありません。コンパクトな集合上でのスムーズな関数は局所的に積分可能なので、$u$に対応する正規超関数$T_{u}$を考えることができます。\n一方、$u$はスムーズな関数なので微分可能であり、$u^{\\prime}$も局所的に積分可能なので、対応する正則超関数$T_{u^{\\prime}}$が存在します。すると、テスト関数$\\phi \\in \\mathcal{D}$に対して部分積分法を使って以下のように表現できます。\n$$ \\begin{align*} T_{u^{\\prime}}(\\phi) \u0026amp;= \\int u^{\\prime}(x)\\phi (x)dx \\\\ \u0026amp;= \\left[ u(x) \\phi (x) \\right]_{-\\infty}^{\\infty} -\\int u(x)\\phi ^{\\prime} (x) dx \\end{align*} $$\nここで、$\\phi$はコンパクトサポートを持っているので、第一項は$0$です。したがって、次のことを得ます。\n$$ T_{u^{\\prime}}(\\phi)=-\\int u(x)\\phi ^{\\prime} (x) dx=-T_{u}(\\phi^{\\prime}) $$\n定義1 超関数$T$の導関数を、以下のように定義します。\n$$ (DT)(\\phi):= -T(D\\phi) $$\nここで、$D$は微分演算子です。マルチインデックス$\\alpha$については、次のようになります。\n$$ (D^{\\alpha}T)(\\phi):= \\left| -1 \\right|^{\\left| \\alpha \\right| } T(D^{\\alpha}\\phi) $$\nテスト関数の導関数もテスト関数であることから、定義域に問題はなく、その点を除けば、単に定数項$\\left| -1 \\right|^{\\left| \\alpha \\right|}$が乗じられたことに過ぎず、したがって$D^{\\alpha}T$もまた超関数になることを知ることができます。もちろん、超関数の定義を利用して証明することができますが、必ずしもそうする必要はありません。\nGerald B. Folland, Fourier Analysis and Its Applications (1992), p308-309\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1084,"permalink":"https://freshrimpsushi.github.io/jp/posts/1084/","tags":null,"title":"超関数の微分"},{"categories":"함수","contents":"定義 $A \\subset X$ について、以下のように定義される 関数 $\\chi_{A} : X \\to \\mathbb{R}$ を 特性関数 または 指示関数 と言う。\n$$ \\chi _{A}(x) := \\begin{cases} 1, \u0026amp; x\\in A \\\\ 0 ,\u0026amp; x \\notin A \\end{cases} $$\n説明 $\\chi$ はギリシャ文字の カイ だ。学生時代、数学の先生がxを $\\chi$ で書くなって言ったのは、まさに $\\chi$ がxではないからだ。特に、こんなに強い意味を持っているから、適当に使うべきではない。\n数学科では、特に特性関数と呼ばれることはほとんどなく、そのまま [キャラクタリスティック ファンクション] と読む。特に定積分が含まれる方程式で、積分範囲を変更するトリックのためによく使われる。例えば、以下のような場合だ。 $$ \\int _{a} ^{b}f(x)g(x) dx=\\int _{-\\infty}^{\\infty}\\chi_{[a,b]}f(x)g(x)dx $$\n科目によっては、以下のように太字の1で表されることもある。どちらがより多く使われるかは、はっきりしていないが、$\\chi$ は様々な分野でそれぞれ独自の意味を持っているのに対し、$\\mathbf{1}$ は主に指示関数のみを指すので、論文では書籍よりも $\\mathbf{1}$ が $\\chi$ よりも多く使われるようだ。 $$ \\mathbf{1}_{A} = \\chi _{A}(x) $$\n","id":1790,"permalink":"https://freshrimpsushi.github.io/jp/posts/1790/","tags":null,"title":"特性関数、指示関数"},{"categories":"확률분포론","contents":"定義 1 自由度$\\nu \u0026gt; 0$に対して、次の確率密度関数を持つ連続確率分布$t \\left( \\nu \\right)$をt-分布という。 $$ f(x) = {{ \\Gamma \\left( {{ \\nu + 1 } \\over { 2 }} \\right) } \\over { \\sqrt{\\nu \\pi} \\Gamma \\left( {{ \\nu } \\over { 2 }} \\right) }} \\left( 1 + {{ x^{2} } \\over { \\nu }} \\right)^{- {{ \\nu + 1 } \\over { 2 }}} \\qquad ,x \\in \\mathbb{R} $$\n$\\Gamma (\\nu)$はガンマ関数だ。 説明 t-分布は、今でもビールで有名なギネス醸造所で働いていたウィリアム・ゴセットWilliam S. Gossetによって発見され、公表された分布である。その当時、企業に所属していたため、彼は学生という筆名で投稿し、それが学生t-分布とも呼ばれるようになった。統計学の新入生は、当初、標本が正規分布に従うと仮定されるが、実際には30個に達しない小さなサンプルで使用される分布に最初に遭遇する。$\\nu \\ge 30$のときは、ほぼ正規分布に収束したとみなされる。\n一方、特に$\\nu = 1$のときの分布はコーシー分布と呼ばれる。\n基本的な性質 モーメント生成関数 平均と分散 [2]: $X \\sim t (\\nu)$であれば $$ \\begin{align*} E(X) =\u0026amp; 0 \u0026amp; \\qquad , \\nu \u0026gt;1 \\\\ \\text{Var}(X) =\u0026amp; {{ \\nu } \\over { \\nu - 2 }} \u0026amp; \\qquad , \\nu \u0026gt; 2 \\end{align*} $$ 定理 二つの確率変数$W,V$が独立であり、$W \\sim N(0,1)$、$V \\sim \\chi^{2} (r)$であるとする。\n$k$次モーメント [a]: $k \u0026lt; r$であれば$\\displaystyle T := { {W} \\over {\\sqrt{V/r} } }$は$k$次のモーメントが存在し $$ E T^{k} = E W^{k} {{ 2^{-k/2} \\Gamma \\left( {{ r } \\over { 2 }} - {{ k } \\over { 2 }} \\right) } \\over { \\Gamma \\left( {{ r } \\over { 2 }} \\right) r^{-k/2} }} $$ 標準正規分布とカイ二乗分布から導かれる [b]: $${ {W} \\over {\\sqrt{V/r} } } \\sim t(r)$$ スチューデントt分布の極限分布として標準正規分布を導く [c]: $T_n \\sim t(n)$であれば $$ T_n \\ \\overset{D}{\\to} N(0,1) $$ F分布を導く [d]: 自由度$\\nu \u0026gt; 0$のt-分布に従う確率変数$X \\sim t(\\nu)$について、次のように定義された$Y$はF分布$F (1,\\nu)$に従う。 $$ Y := X^{2} \\sim F (1,\\nu) $$ $N \\left( \\mu , \\sigma^{2} \\right)$は平均が$\\mu$で分散が$\\sigma^{2}$の正規分布だ。 $\\chi^{2} \\left( r \\right)$は自由度$r$のカイ二乗分布だ。 証明 1 確率変数のモーメント生成関数が存在するとは、すべての$k \\in \\mathbb{N}$に対して$k$次のモーメントが存在することを意味する。しかし、定理[a]によれば、t分布の$k$次モーメントは$k \u0026lt; r$のときに存在するため、モーメント生成関数は存在しない。\n■\n[2] モーメント公式[a]を使用する。\n■\n[a] カイ二乗分布のモーメント: $X \\sim \\chi^{2} (r)$とする。$k \u0026gt; - r/ 2$であれば$k$次モーメントが存在し $$ E X^{k} = {{ 2^{k} \\Gamma (r/2 + k) } \\over { \\Gamma (r/2) }} $$\n$k \u0026lt; r$の両辺に$-1/2$を掛けると$-k/2 \u0026gt; -r/2$となるので $$ \\begin{align*} E T^{k} =\u0026amp; E \\left[ W^{k} \\left( {{ V } \\over { r }} \\right)^{-k/2} \\right] \\\\ =\u0026amp; E W^{k} E \\left( {{ V } \\over { r }} \\right)^{-k/2} \\\\ =\u0026amp; E W^{k} {{ 2^{-k/2} \\Gamma \\left( {{ r } \\over { 2 }} - {{ k } \\over { 2 }} \\right) } \\over { \\Gamma \\left( {{ r } \\over { 2 }} \\right) r^{-k/2} }} \\end{align*} $$\n■\n[b] 結合密度関数から直接導く。\n■\n[c] 確率密度関数にスターリング近似を使用する。\n■\n[d] カイ二乗分布の比によって迂回する。\n■\nコード 以下はコーシー分布、t分布、コーシー分布の確率密度関数を表示するJuliaのコードだ。\n@time using LaTeXStrings @time using Distributions @time using Plots cd(@__DIR__) x = -4:0.1:4 plot(x, pdf.(Cauchy(), x), color = :red, label = \u0026#34;Cauchy\u0026#34;, size = (400,300)) plot!(x, pdf.(TDist(3), x), color = :orange, label = \u0026#34;t(3)\u0026#34;, size = (400,300)) plot!(x, pdf.(TDist(30), x), color = :black, linestyle = :dash, label = \u0026#34;t(30)\u0026#34;, size = (400,300)) plot!(x, pdf.(Normal(), x), color = :black, label = \u0026#34;Standard Normal\u0026#34;, size = (400,300)) xlims!(-4,5); ylims!(0,0.5); title!(L\u0026#34;\\mathrm{pdf\\,of\\, t}(\\nu)\u0026#34;) png(\u0026#34;pdf\u0026#34;) Hogg et al. (2013). Introduction to Mathematical Statistics(7th Edition): p191.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1667,"permalink":"https://freshrimpsushi.github.io/jp/posts/1667/","tags":null,"title":"t-分布"},{"categories":"초함수론","contents":"要約1 すべての$u \\in L_{\\mathrm{loc} }^1(\\Omega) $に対して、次のように定義される超関数 $T_{u} \\in D^{\\ast}(\\Omega)$が存在する。\n$$ T_{u} (\\phi) := \\int_{\\Omega} u(x)\\phi (x)dx, \\quad \\phi \\in D(\\Omega) $$\n説明 $\\mathcal{D}(\\Omega)$はテスト関数空間である。このように定義される超関数を正則超関数regular distributionと呼ぶ。また、上の式は内積空間の観点から見ると$u$と$\\phi$の内積と同じであるため、以下のように表記することもある。\n$$ T_{u}(\\phi)=\\langle u , \\phi \\rangle $$\n上の定理によれば、局所積分可能な関数を超関数として扱ってもよい。このような理由で、超関数を一般化された関数と呼ぶことがある。\n証明 上で定義した$T_{u}$が超関数であるかどうかは、$\\mathcal{D}$の連続性と線形性を持つ汎関数であることを示すことだ。線形性は積分によって定義されているため自明であり、連続性を示せば良い。このときの連続性とはテスト関数空間での収束による連続性であることを忘れないで欲しい。\n$\\phi_{j} \\rightarrow \\phi\\ \\ \\mathrm{in}\\ D(\\Omega)$と仮定する。すると、収束の定義により、次のような$K \\Subset\\Omega$が存在する。\n$$ \\mathrm{supp}(\\phi_{j}-\\phi) \\subset K\\quad \\forall\\ j $$\nすると、$u$は局所積分可能であるため、$M\u0026gt;0$に対して以下の式が成り立つ。\n$$ \\begin{align*} \\left| T_{u}(\\phi_{j})-T_{u}(\\phi) \\right| \u0026amp;= \\left| \\int_{K} u(x)\\left( \\phi_{j}(x) -\\phi (x) \\right) dx \\right| \\\\ \u0026amp; \\le \\sup \\limits_{x\\in K} \\left| \\phi_{j} (x) - \\phi (x) \\right| \\int_{K} |u(x)|dx \\\\ \u0026amp;\\le \\sup \\limits_{x\\in K} \\left| \\phi_{j} (x) - \\phi (x) \\right|M \\end{align*} $$\nこのとき、仮定により$\\phi_{j}(x) \\rightrightarrows \\phi (x)$であるため、次が成り立つ。\n$$ \\sup \\limits_{x\\in K} \\left| \\phi_{j} (x) - \\phi (x) \\right|M \\to 0 \\quad \\text{as } j \\rightarrow \\infty $$\nしたがって、次が成り立つ。\n$$ T_{u}( \\phi_{j} ) \\rightarrow T_{u}(\\phi) \\quad \\text{as } j \\rightarrow \\infty $$\nゆえに、$T_{u}$は$\\mathcal{D}$で連続である。\n■\nすべての超関数が上に述べた形であれば扱いやすいが、残念ながらそうではない。\nRobert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003), p20-21\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1078,"permalink":"https://freshrimpsushi.github.io/jp/posts/1078/","tags":null,"title":"すべての局所可積分関数が超関数に拡張可能であることを証明"},{"categories":"초함수론","contents":"定義1 開集合 $\\Omega \\subset \\mathbb{R}^{n}$と関数 $\\phi : \\Omega \\to \\mathbb{C}$が与えられているとする。$\\phi$が無限に微分可能で、その導関数が全部連続であり、コンパクトサポートを持っていれば、テスト関数と呼ばれる。テスト関数の関数空間は$C_{c}^{\\infty}(\\Omega)$または単純に$\\mathcal{D}(\\Omega)$と表される。\n説明 test functionまたはtesting functionとも呼ばれる。$\\phi$がテスト関数と名づけられたのは、$\\phi$自体を扱いたいわけではなく、何か他の関数を定義し、その関数の性質を研究するために使いたいからである。具体的には、テスト関数は数学的に曖昧な関数、例えばディラックのデルタ関数などを厳密に定義するために使用される。テスト関数の具体的な例にはモーリファイアがある。\n定理2 $\\phi$がテスト関数ならば、その導関数もテスト関数である。\n$$ \\phi \\in \\mathcal{D}(\\Omega) \\implies \\frac{ \\partial \\phi}{ \\partial x_{i}} \\in \\mathcal{D}(\\Omega) (i=1,\\cdots,n) $$\nこの場合、$x=(x_{1},\\cdots,x_{n})\\in \\mathbb{R}^{n}$である。\n証明 テスト関数の定義により$\\dfrac{ \\partial \\phi}{ \\partial x_{i}} \\in C^{\\infty}$は自明である。 $x_{0} \\notin \\mathrm{supp} \\phi$としよう。すると、$x_{0} \\in \\left( \\mathrm{supp} \\phi \\right)^{c}$であり、サポートは閉集合であるため$(\\mathrm{supp} \\phi)^{c}$は開集合である。したがって、開集合の定義により、$x_{0}$を含む何らかの近傍 $N_{x_{0}}$が存在する。また、サポートの定義により、$N_{x_{0}}$上で$\\phi=0$であり、当然$\\dfrac{ \\partial \\phi}{ \\partial x_{i}}=0$である。これは$x_{0} \\notin \\mathrm{supp} \\dfrac{ \\partial \\phi}{ \\partial x_{i}}$であることを意味する。したがって、以下が成り立つ。\n$$ \\mathrm{supp} \\frac{ \\partial \\phi}{ \\partial x_{i} } \\subset \\mathrm{supp} \\phi $$\nコンパクト集合の閉部分集合はコンパクトであるため、$\\mathrm{supp} \\dfrac{ \\partial \\phi}{ \\partial x_{i}}$はコンパクトである。■\n系 $\\phi,\\phi_{1},\\phi_{2} \\in \\mathcal{D}(\\mathbb{R}^{n})$、$x_{0}\\in \\mathbb{R}^{n}$、$a \\in \\mathbb{R}\\setminus \\left\\{ 0 \\right\\}$、$\\psi \\in C^{\\infty}(\\mathbb{R}^{n})$としよう。すると、以下が成り立つ：\n(a) $\\phi (x-x_{0})$、$\\phi (-x)$、$\\phi (ax)\\in \\mathcal{D}(\\mathbb{R}^{n})$\n(b) $\\psi \\phi \\in \\mathcal{D}(\\mathbb{R}^{n})$\n(c) $\\phi_{1} * \\phi_{2} \\in \\mathcal{D}$\n明らかなので、証明は省略する。\nRobert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003), p19-20\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDaniel Eceizabarrena perez, Distribution Theory and Fundamental Solutions of Differential Operators (2015), p1-3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1782,"permalink":"https://freshrimpsushi.github.io/jp/posts/1782/","tags":null,"title":"テスト関数とテスト関数空間"},{"categories":"초함수론","contents":"テスト関数空間では、「収束」を特別な方法で定義する。ある空間 $X$が与えられたとき、通常は$X$で定義されたノルムや距離を使用して収束を定義する。しかし、テスト関数空間では、超関数をうまく定義し扱うために、より強力な条件で収束を定義する。\n定義 $\\Omega \\subset \\mathbb{R}^n$が開集合で、$\\left\\{ \\phi _{j} \\right\\}$がテスト関数の数列であるとする。$\\left\\{ \\phi_{j} \\right\\}$が以下の二つの条件を満たすとき、$\\mathcal{D}(\\Omega)$のセンス で$0$に収束 するとし、次のように表記する。\n$$ \\phi_{j} \\overset{\\mathcal{D}}{\\to} 0 $$\n(a) $\\mathrm{supp} (\\phi_{j}) \\subset K\\quad \\forall\\ j$を満たす$K \\Subset \\Omega$が存在する。\n(b) 各マルチインデックス $\\alpha$に対して、$D^{\\alpha}\\phi_{j}$が$0$に一様収束する。\n$$ D^{\\alpha}\\phi_{j} \\rightrightarrows 0 $$\nこの時、$\\mathrm{supp}$はサポートを意味する。\n説明 著者によって用語が少し異なる場合があるが、用語自体が重要なわけではない。\n$\\mathcal{D}$空間のセンスで収束する: $\\mathcal{D}$空間のセンスで収束する1\n$\\mathcal{D}$で収束する: $\\mathcal{D}$で収束する2\nもちろん、特定の教科書や講義の文脈で混乱の余地がない場合は、単に$\\phi_{j} \\to 0$として表記することができる。定義 (b) によれば、$\\mathcal{D}$で収束すれば、一般的な意味での収束も満たされる。上記の定義を$0$ではなく、全ての$\\phi$について一般的に記述すると、以下の通りである。\n$\\Omega \\subset \\mathbb{R}^n$が開集合で、$\\left\\{ \\phi _{j} \\right\\}$がテスト関数の数列であるとする。$\\left\\{ \\phi_{j} \\right\\}$が以下の二つの条件を満たす場合、$\\mathcal{D}(\\Omega)$のセンスで$\\phi$に収束するとし、$\\phi_{j} \\to \\phi \\text{ in } D(\\Omega)$と表記する。\n(a) $\\mathrm{supp} (\\phi_{j}-\\phi) \\subset K\\quad \\forall\\ j$を満たす$K \\Subset \\Omega$が存在する。\n(b) 各マルチインデックス $\\alpha$に対して、$D^{\\alpha}\\phi_{j}$が$D^{\\alpha} \\phi$に一様収束する。 $$ D^{\\alpha}\\phi_{j} \\rightrightarrows D^{\\alpha}\\phi $$\nRobert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003), p19-20\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDaniel Eceizabarrena perez, Distribution Theory and Fundamental Solutions of Differential Operators (2015), p3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1077,"permalink":"https://freshrimpsushi.github.io/jp/posts/1077/","tags":null,"title":"テスト関数の空間における収束"},{"categories":"르벡공간","contents":"定義 $\\Omega \\subset \\mathbb{R}^{n}$を開集合と呼ぶ。\n定義１1 すべての有界な可測集合 $K \\subset \\Omega$に対して、\n$$ \\int_{K} \\left| u(x) \\right| dx \\lt \\infty $$\nを満たす関数 $u : \\Omega \\to \\mathbb{C}$を（ルベーグ測度において）局所的に積分可能であると言う。\n定義２2 関数$u$が$\\Omega$上のほぼ至る所で定義された関数だとする。全ての開集合$U \\Subset \\Omega$に対して$u \\in L^{1}(U)$の時、$u$を$\\Omega$上で局所的に積分可能であると言う。\n表記 局所的に積分可能な関数の集合を次のように表す。\n$$ L_{\\text{loc}}^{1}(\\Omega) := \\left\\{ u : \\Omega \\to \\mathbb{C} \\Big| u \\text{ is locally integrable.}\\right\\} $$\n説明 定義により、以下の包含関係が自明に成立する。\n$$ \\href{../592}{L^{1}(\\Omega)} \\subset L_{\\text{loc}}^{1}(\\Omega) $$\n$$ \\href{../1594}{C(\\Omega)} \\subset L_{\\text{loc}}^{1}(\\Omega) $$\n性質 局所的に積分可能な関数は超関数への拡張が可能である。 Gerald B. Folland, Real Analysis: Modern Techniques and Their Applications (2nd Edition, 1999), p95\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRobert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003), p20\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1783,"permalink":"https://freshrimpsushi.github.io/jp/posts/1783/","tags":null,"title":"局所積分可能な関数"},{"categories":"초함수론","contents":"定義1 2 $\\Omega \\subset \\mathbb{R}^{n}$が開集合だとしよう。テスト関数空間の連続な線形汎関数 $T : \\mathcal{D}(\\Omega) \\to \\mathbb{C}$を超関数distributionと定義する。つまり、超関数はテスト関数空間の双対空間の要素だ。だから、\n$$ T \\in \\mathcal{D}^{\\ast} $$\nと表記し、$D^{\\ast}$を**（シュワルツ）超関数空間**(Schwartz) distribution spaceと呼ぶ。\n説明 distributionという名前は、質量が一点に集中した点質量などを表現するために考案されたディラックのデルタ関数の影響を受けているようだ。直訳すれば分布だが、数学者は超関数と呼ぶ。他の名称であるgeneralized functionは、厳密には関数ではないディラックのデルタ関数のようなものを厳密に定義した概念で、そのために付けられた。超関数の定義で重要な部分は連続性である。連続関数になる同値条件によって、超関数$T$が連続であるということは以下を意味する。\n$$ \\phi_{j} \\to \\phi \\implies T(\\phi_{j}) \\to T(\\phi) $$\nしかし、テスト関数空間での収束を少し特別に定義した。したがって、具体的に超関数の定義を再記載すると、以下の通りになる。\nテスト関数空間 $\\mathcal{D}(\\Omega)$の汎関数 $T : \\mathcal{D}(\\Omega) \\to \\mathbb{C}$が線形(a)であり、かつ連続(b)である場合、これを超関数という。\n(a) $T(a\\phi + b \\psi ) = aT(\\phi)+bT(\\psi)\\quad (\\phi,\\psi\\in \\mathcal{D},\\ a,b\\in\\mathbb{C})$\n(b) $\\phi_{j} \\to \\phi \\text{ in } \\mathcal{D} \\implies T(\\phi_{j}) \\to T(\\phi)$\n以下の条件を満たす$\\mathcal{D}(\\Omega)$の関数列 $\\left\\{ \\phi_{j} \\right\\}$に対して、$\\phi_{j} \\to \\phi \\text{ in } \\mathcal{D}$と定義する。\n(c) $\\mathrm{supp} (\\phi_{j}-\\phi) \\subset K\\quad \\forall\\ j$を満たす$K \\Subset \\Omega$が存在する。\n(d) 各多重指数 $\\alpha$について、$D^{\\alpha}\\phi_{j}$が$D^{\\alpha} \\phi$に一様収束する。\n超関数の定義に従って、ディラックのデルタ関数は以下のように定義できる。\n$$ \\begin{align*} \\delta_{a} : \\mathcal{D} \u0026amp;\\to \\mathbb{C} \\\\ \\phi \u0026amp;\\mapsto \\phi (a) \\end{align*} $$\nRobert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003), p19-20\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGerald B. Folland, Fourier Analysis and Its Applications (1992), p306-307\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1009,"permalink":"https://freshrimpsushi.github.io/jp/posts/1009/","tags":null,"title":"超関数、一般化された関数"},{"categories":"바나흐공간","contents":"関数解析学は英語でfunctional analysisです。function analysisではなくfunctionalは一体何を意味しているのか、疑問に思うかもしれません。まず、functionalという単語を見ると、function+alで構成されているように見えます。つまり、functionの形容詞形のように見え、この感じで解釈すれば、functionalは「関数的な（もの）」や「関数のような（もの）」程度の意味を含んでいるように思われます。この感じは、別の名前であるgeneralized functionでも見つけることができます。なぜ関数ではなく、関数のようなものと名付けられたのか、以下のfunctionalの一般的な定義を見ながら考えてみましょう。\nベクトル空間$X$に対して以下のような関数$f$をfunctionalと呼びます。\n$$ f : X \\to \\mathbb{C} $$\nこの定義を見て「定義上は$f$はfunctionなのに、なぜfunctionalという名前を付けたのか？」と思うかもしれません。上記のような条件を満たす関数に特別な名前を付けるのは納得できるものの、なぜその名前がfunctional（関数的なもの）でなければならないのかは、納得がいかないかもしれません。\n関数の定義によれば、上の$f$は関数ですが、なぜ「関数的なもの」という名前を付けたのかを理解するためには、関数解析学が発展し始めた時代の数学について知る必要があります。現代に生まれ、数学を学ぶ人は、関数を以下のように知っています。\n全ての$x_{1}, x_{2} \\in X$に対して$x_{1} = x_{2} \\implies f(x_{1}) = f(x_{2})$を満たす$f(x_{1})$と$f(x_{2})$が$Y$に存在するならば、対応$f$を以下のように表記し、$X$から$Y$への関数と言います。\n$$ f : X \\to Y $$\n集合論で厳密に定義すると、以下のようになります。\n空集合でない二つの集合$X$、$Y$が与えられたとします。二項関係$f \\subset (X,Y)$が以下を満たすならば、関数と呼び、$f : X \\to Y$のように表されます。\n$$ (x ,y_{1}) \\in f \\land (x,y_{2}) \\in f \\implies y_{1} = y_{2} $$\n上の定義からわかるように、二つの集合$X$、$Y$には何の条件もありません。したがって、$X$が$\\mathbb{R}$であろうと、関数空間であろうと、何の問題もありません。しかし、19世紀後半の数学者にとって、関数とは上記のようではありませんでした。当時の数学者は、関数を値から値へのマッピング、つまり、$f:\\mathbb{R} \\to \\mathbb{R}$に限定して考えていました1。値を与えると、ルールに従って別の値を与える「公式」のように扱ったのです。これは、中学校で初めて関数を学ぶ時の受け入れ方と同じです。\nなぜ関数をそのように考えたのか疑問に思うかもしれませんが、ある意味で当然です。関数の厳密な定義は、上記で見たように集合論を通じて作られました。現代集合論の創始者であるカントールが1845年生まれであることを思い出せば、19世紀後半〜20世紀初頭の数学者までもが関数を数字と数字の間の公式程度に考えていた事実は全く不思議ではありません。元々、関数をなぜ機能functionと呼んだのでしょうか。\n以下のような関数を考えてみましょう。\n微分可能な関数$f$に対して、閉区間$[a,b]$での曲線$y=f(x)$の長さは以下のようになります。\n$$ L(f)=\\int_{a}^{b} \\sqrt{1+ f^{\\prime}(x)^{2}}dx $$\n当時の数学者にとって、$L$は関数ではありませんでした。値を値へ送るのではなく、関数を値へ送るためです。したがって、$L$を「関数の関数」と呼ばなければならないのですが、関数ではないため、用語に関する曖昧さが存在しました。そのため、ボルテラVolterraはfunctions of linesとも呼びました。この時、フランスの数学者アダマールHadamardがこのような「関数ではないが関数のような関数の関数」をfoncionnellesと呼ぶことを初めて提案しました。これは後に英語表現でfunctionalとなりました。\nもちろん、集合論で関数を厳密に定義した後は、functionalもfunctionになりましたが、定義域が関数空間であることが明確になるため、functionalという表現が続けられたようです。集合の集合、set of setsをcollectionやfamillyと表現するのと同様です。functionalがfunctionであることに概念的な問題はなくても、関数の関数という表現は混乱を招きやすいため、functionalという用語が生き残らなかったのではないでしょうか。この学問の名前がfunctional analysisと固まったのも影響があるでしょう。functionalは後に一般化され、ベクトル空間から複素数空間へのマッピングを意味するようになりました。\nDistribution Theory 上述のように、最初にfunctionalは関数ではないが関数のようなものを指すために作られた言葉です。最終的に関数が集合論を通じて定義された後は、functionalも関数になりましたが。面白いことに、このようなfunctionalが実際に「関数ではないが関数のようなもの」を説明するために使われるようになったのです。ディラックのデルタ関数は、ポアソンとコーシーがフーリエ解析を研究する過程で最初に考案され、理論物理学者のポール・ディラックが量子力学で広く使用されることで有名になりました。2 3 デルタ関数のナイーブnaiveな定義は、以下の条件を満たす関数です。\n$$ \\delta (x)=\\begin{cases} \\infty, \u0026amp; x=0 \\\\ 0, \u0026amp; x\\ne 0\\end{cases} \\quad \\\u0026amp; \\quad \\int_{-\\infty}^{\\infty}\\delta (x)dx=1 $$\nしかし、発散するということは値ではなく状態であるため、厳密に言えばデルタ関数は関数ではありませんでした。しかし、単に関数として扱って良い結果を得られました。1935年4にこの概念を知ったフランスの数学者ローラン-モワーズ・シュワルツLaurent-moise Schwartzが15年間の研究の末、1950年5にTheorie des distributionsという本でデルタ関数を数学的に厳密に定義しました。6 いくつかの良い条件を持つスムース関数をテスト関数と呼び、テスト関数の空間を$\\mathcal{D}$と表記します。distributionは$\\mathcal{D}$から$\\mathbb{C}$への写像であり、これはfunctionalになります。functionalという名前は、当初は関数だと思われていなかったものに付けられましたが、時間が経つにつれて、実際に関数ではないが関数のように扱うものに対する理論を立てるのに使われるようになりました。驚くべき偶然です。\nhttps://courses.mai.liu.se/GU/TATM85/FA-history.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Dirac_delta_function\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Dirac_delta_function\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n21歳でした。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n36歳でした。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://horizon.kias.re.kr/11905/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1780,"permalink":"https://freshrimpsushi.github.io/jp/posts/1780/","tags":null,"title":"ファンクショナルがファンクショナルと名付けられた理由"},{"categories":"확률분포론","contents":"要約 二つの確率変数 $W,V$ が独立で、$W \\sim N(0,1)$、$V \\sim \\chi^{2} (r)$ である場合、 $$ T = { {W} \\over {\\sqrt{V/r} } } \\sim t(r) $$\n$N \\left( \\mu , \\sigma^{2} \\right)$ は平均が $\\mu$ で、分散が $\\sigma^{2}$ の正規分布だ。 $\\chi^{2} \\left( r \\right)$ は自由度 $r$ のカイ二乗分布だ。 $t(r)$ は自由度 $r$ のt-分布だ。 説明 この定理を統計学だけで接近するなら、実用性や歴史的に見てむしろt-分布の定義に近い。\n導出1 戦略：ジョイント確率密度関数で直接演繹する。\n正規分布の定義：$\\mu \\in \\mathbb{R}$ と $\\sigma \u0026gt; 0$ に対して、以下の確率密度関数を持つ連続確率分布 $N \\left( \\mu,\\sigma^{2} \\right)$ を正規分布という。 $$ f(x) = {{ 1 } \\over { \\sqrt{2 \\pi} \\sigma }} \\exp \\left[ - {{ 1 } \\over { 2 }} \\left( {{ x - \\mu } \\over { \\sigma }} \\right)^{2} \\right] \\qquad, x \\in \\mathbb{R} $$\nカイ二乗分布の定義：自由度 $r \u0026gt; 0$ に対して、以下の確率密度関数を持つ連続確率分布 $\\chi^{2} (r)$ をカイ二乗分布という。 $$ f(x) = {{ 1 } \\over { \\Gamma (r/2) 2^{r/2} }} x^{r/2-1} e^{-x/2} \\qquad , x \\in (0, \\infty) $$\n$W,V$ の確率密度関数 $f_{1} , f_{2}$ が以下のように与えられるため、 $$ f_1 (w) := { {1} \\over {\\sqrt{2 \\pi }} } e ^{- w^{2} / 2} \\\\ \\displaystyle f_2 (v) ={ 1 \\over { \\Gamma ({r \\over 2}) 2^{r \\over 2} } } v^{ {r \\over 2} - 1 } e^{-{{v} \\over 2}} $$ $W$ と $V$ の[ジョイント]確率密度関数(../1449) $h$ は $w \\in \\mathbb{R}$、$v \\in (0,\\infty)$ に対して以下の通りである。 $$ h(w,v) = { {1} \\over {\\sqrt{2 \\pi }} } e ^{- w^{2} / 2} { 1 \\over { \\Gamma ({r \\over 2}) 2^{r \\over 2} } } v^{ {r \\over 2} - 1 } e^{-{{v} \\over 2}} $$ 今、$\\displaystyle T := { {W} \\over {\\sqrt{V/r} } }$ そして $U := V$ とすると、$w = t\\sqrt{u} / \\sqrt{r}$ そして $v = u$ であるため、 $$ \\left| J \\right| = \\begin{vmatrix} {{\\sqrt{u}} \\over {\\sqrt{r}}} \u0026amp; 0 \\\\ {{t} \\over {2 \\sqrt{ur}}} \u0026amp; 1 \\end{vmatrix} = \\sqrt{{{ u } \\over { r }}} $$ 従って、$T, U$ のジョイント確率密度関数は $$ \\begin{align*} g(t,u) =\u0026amp; h({ {w} \\over {\\sqrt{v/r} } },u) |J| \\\\ =\u0026amp; { {1} \\over {\\sqrt{2 \\pi } \\Gamma (r/2) 2^{r/2} } } u^{r/2 -1} \\exp \\left\\{ -{{u} \\over {2} } \\left( 1 + { {t^2} \\over {r} } \\right) \\right\\} { {\\sqrt{u} } \\over {\\sqrt{r} } } \\end{align*} $$ $T$ のマージナル確率密度関数は $$ \\begin{align*} g(t) =\u0026amp; \\int_{-\\infty}^{\\infty} g(t,u) du \\\\ =\u0026amp; \\int_{0}^{\\infty} { {1} \\over {\\sqrt{2 \\pi r} \\Gamma (r/2) 2^{r/2} } } u^{(r+1)/2 -1} \\exp \\left\\{ -{{u} \\over {2} } \\left( 1 + { {t^2} \\over {r} } \\right) \\right\\} du \\end{align*} $$ $\\displaystyle z := {{u} \\over {2}} \\left( 1 + {{t^2} \\over {r}} \\right)$ に置換えると、 $$ \\begin{align*} g(t) =\u0026amp; \\int_{0}^{\\infty} { {1} \\over {\\sqrt{2 \\pi r} \\Gamma (r/2) 2^{r/2} } } \\left( { {2z} \\over {1 + t^2 / r} }\\right)^{(r+1)/2-1} e^{-z} \\left( { {2} \\over {1+ t^2 / r} } \\right) dz \\\\ =\u0026amp; { {1} \\over {\\sqrt{\\pi r} \\Gamma (r/2) } } \\int_{0}^{\\infty} {{ 1 } \\over { \\sqrt{2} 2^{r/2} }}z^{(r+1)/2-1} \\left( { {2} \\over {1 + t^2 / r} }\\right)^{(r+1)/2-1} e^{-z} \\left( { {2} \\over {1+ t^2 / r} } \\right) dz \\\\ =\u0026amp; { {1} \\over {\\sqrt{\\pi r} \\Gamma (r/2) } } \\int_{0}^{\\infty} {{ 1 } \\over { 2^{(r+1)/2} }}z^{(r+1)/2-1} \\left( { {2} \\over {1 + t^2 / r} }\\right)^{(r+1)/2} e^{-z} dz \\\\ =\u0026amp; { {1} \\over {\\sqrt{\\pi r} \\Gamma (r/2) } } \\int_{0}^{\\infty}z^{(r+1)/2-1} \\left( { {1} \\over {1 + t^2 / r} }\\right)^{(r+1)/2} e^{-z} {{ \\Gamma \\left( (r+1)/2 \\right) } \\over { \\Gamma \\left( (r+1)/2 \\right) }} dz \\\\ =\u0026amp; { {\\Gamma \\left( (r+1)/2 \\right)} \\over {\\sqrt{\\pi r} \\Gamma (r/2) } } \\left( { {1} \\over {1 + t^2 / r} }\\right)^{(r+1)/2} \\int_{0}^{\\infty} {{ 1 } \\over { \\Gamma \\left( (r+1)/2 \\right) }} z^{(r+1)/2-1} e^{-z} dz \\\\ =\u0026amp; { {\\Gamma \\left( (r+1)/2 \\right)} \\over {\\sqrt{\\pi r} \\Gamma (r/2) } } \\left( { {1} \\over {1 + t^2 / r} }\\right)^{(r+1)/2} \\cdot 1 \\end{align*} $$ 積分関数がガンマ分布 $\\Gamma \\left( {{ r + 1 } \\over { 2 }} , 1 \\right) $ の確率密度関数となり、複雑な計算を回避できる。整理すると、 $$ g(t) = {{\\Gamma ( (r+1)/2 ) } \\over { \\sqrt{\\pi r} \\Gamma (r/2) }} { {1} \\over {(1 + t^{2} / r)^{(r+1)/2} } } $$ これは自由度$r$ のt-分布の確率密度関数であるため、 $$ T = { {W} \\over {\\sqrt{V/r} } } \\sim t(r) $$\n■\nHogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): 191-192.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":204,"permalink":"https://freshrimpsushi.github.io/jp/posts/204/","tags":null,"title":"独立な正規分布およびカイ二乗分布からのスチューデントのt分布の導出"},{"categories":"수리물리","contents":"定義 スカラー関数 $f=f(x,y,z)$に対して、以下のようなベクトル関数を $f$のグラディエントgradient, 勾配と定義し、$\\nabla f$と表記する。\n$$ \\nabla f := \\frac{ \\partial f}{ \\partial x }\\hat{\\mathbf{x}}+\\frac{ \\partial f}{ \\partial y}\\hat{\\mathbf{y}}+\\frac{ \\partial f}{ \\partial z}\\hat{\\mathbf{z}} = \\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right) $$\n説明 グラディエントは勾配、坂、水勾配などと翻訳される。坂、水勾配はグラディエントの古い翻訳で、最近ではあまり使われない。また、坂は勾配の漢字語であるため、勾配と同じ意味である。グラディエントは実際にベクトルであるため、勾配という言葉はグラディエントが持つ意味をすべて含むには不十分であるように思われる。生しらす寿司店では、勾配という言葉の代わりにグラディエントと統一する。\n幾何学的には $\\nabla f$は $f$が最も急激に変化する方向を意味する。つまり点 $(x,y,z)$で $f$の増加率が最も大きい方向はベクトル $\\left( \\dfrac{\\partial f(x,y,z)}{\\partial x}, \\dfrac{\\partial f(x,y,z)}{\\partial y}, \\dfrac{\\partial f(x,y,z)}{\\partial z} \\right)$であるということである。これは微分係数を多次元に拡張したものに過ぎない。$f$が増加していれば微分係数が正、$f$が減少していれば微分係数が負であるという概念と同じである。\n一方で定義で $\\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right)$という値を $\\nabla f$と表記するとしたことに注意しよう。$\\nabla$をデル演算子と呼ぶことはあるが、これ自体に何か意味を持つと考えると$\\nabla \\cdot \\mathbf{F}$や$\\nabla \\times \\mathbf{F}$を内積や外積と誤解するのにちょうどよい。したがって、$\\nabla$は単なる便利な表記法としてのみ理解するべきであり、グラディエント、ダイバージェンス、カールをまとめてデル演算子と呼んだり、デル演算子=グラディエントと考える方が良いかもしれない。詳細は以下で続く。\n注意点 $\\nabla f$は $\\nabla$と $f$の積ではない グラディエントを理解する上で重要なのは、$\\nabla f$がベクトル $\\nabla = (\\frac{\\partial }{\\partial x}, \\frac{\\partial }{\\partial y}, \\frac{\\partial }{\\partial z})$とスカラー $f$の積ではないという事実である。もちろん、そう考えると直感的で良さそうだが、実際は逆である。$\\nabla$を $(\\frac{\\partial }{\\partial x}, \\frac{\\partial }{\\\npartial y}, \\frac{\\partial }{\\partial z})$というベクトルとして説明することで、ベクトルとスカラーの積のように見えるようにするのである。もし $\\nabla f$がベクトル $\\nabla$とスカラー $f$の積であれば、ベクトルとスカラーの積は交換可能であるため、次のような奇妙な数式が成り立つことになる。\n$$ \\nabla f = \\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right) \\overset{?}{=} \\left( f\\dfrac{\\partial }{\\partial x}, f\\dfrac{\\partial }{\\partial y}, f\\dfrac{\\partial }{\\partial z} \\right) = f\\nabla $$\nこの奇妙な数式が飛び出したのは、実際には $\\nabla$はベクトルではなく、$\\nabla f$はベクトルとスカラーの積ではないためである。$\\nabla$はベクトルではなく、$f(x,y,z)$というスカラー関数を $\\left( \\frac{\\partial f(x,y,z)}{\\partial x}, \\frac{\\partial f(x,y,z)}{\\partial y}, \\frac{\\partial f(x,y,z)}{\\partial z} \\right)$というベクトル関数に対応させる演算子である。関数自体を変数とする $\\operatorname{grad}$という関数を次のように定義してみよう。\n$$ \\begin{equation} \\operatorname{grad} (f) = \\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right), \\quad f=f(x,y,z) \\end{equation} $$\nこの定義から、ベクトルとスカラーの積という説明は必要ない。$\\operatorname{grad}$は単に変数として $f$が入力されると、$(1)$の規則に従って関数値を持つ関数（演算子）に過ぎない。しかし $\\operatorname{grad} (f)$の関数値をよく見ると、$\\operatorname{grad} = \\nabla$と表記し、これを $\\nabla = (\\frac{\\partial }{\\partial x}, \\frac{\\partial }{\\partial y}, \\frac{\\partial }{\\partial z})$というベクトルとして説明すると直感的で便利な表記法になるのである。\nこれは本質的な意味を正確に説明するものではないが、計算や理解の便利さのために使われる他の表記法には微分のライプニッツ表記法がある。$\\dfrac{dy}{dx}$という表記法を採用し、分数のように扱うと、変化率という意味を理解するのに便利で、無意識に掛け算や約分などの計算をしても実際の結果とピタリと合う。しかし、皆さんは $\\dfrac{dy}{dx}$は分数ではないことを知っている。そう見えるだけで、そう扱うと計算が便利なだけである。$\\nabla f$も同様に、ベクトルとスカラーの積に見えるだけで、そう扱うと計算が便利なのであって、実際にそうであるわけではない。\nでは $f\\nabla$は何か？ 上の説明に従えば、$\\nabla$は一つの関数であるため、$\\nabla f = \\nabla(f)$は $\\nabla$という関数に $f$という変数を代入したときに得られる関数値である。一方で $f \\nabla$はそれ自体が一つの関数であり、$g$という関数を変数として代入したときに以下のように関数値を対応させる関数（演算子）である。\n$$ (f\\nabla) (g) = f\\left( \\dfrac{\\partial g}{\\partial x}, \\dfrac{\\partial g}{\\partial y}, \\dfrac{\\partial g}{\\partial z} \\right) = \\left( f\\dfrac{\\partial g}{\\partial x}, f\\dfrac{\\partial g}{\\partial y}, f\\dfrac{\\partial g}{\\partial z} \\right) $$\nもちろん、$f \\nabla g$\nという関数値を見たときには、$f \\nabla$に $g$を代入したものと考えても良いし、スカラー関数 $f$とベクトル関数 $\\nabla g$の積と見ても良い。\n導出 1次元 上の図を見よう。$f_{1}$の点 $x=2$での微分係数は $4$である。$4$という値は関数 $f_{1}$が点 $x=2$でどれほど傾いているかを教えてくれる量だけでなく、それだけではない。$4$の前にある $+$という符号が $f_{1}$のグラフは $x$が増加する方向に増加するという事実も教えてくれる。したがって、微分係数 $4$は単なるスカラーではなく、1次元ベクトル $4\\hat{\\mathbf{x}}$として理解すべきである。\n同様に、$f_{2}$の $x=2$での微分係数は $-3$であり、これは傾きの程度が $3$であることと、$x$が増加する方向に進むと $f_{2}$のグラフが減少するという意味も含んでいる。つまり、符号を方向と考えた場合、微分係数の方向は関数のグラフが大きくなる方向を向いているという話である。別の言い方をすると、微分係数が指し示す方向に進めば、グラフの頂点を見つけることができるということである。\n3次元に拡張する前に、$y$の $x$での微分係数 $\\dfrac{ d y}{ d x}=a$をまるで分数のように扱えることを思い出そう。これは微分を数学的に厳密に扱う方法ではないが、幾何学的な意味を理解する上での助けとなり、その利点がある。ライプニッツは $dy$、$dx$を $y$と $x$の非常に小さな変化量、微分素と考え、その変化量の比率を微分係数と呼んだ。1\n$$ dy=adx $$\n余談だが、このように考えるとなぜ $a$を微分 \u0026lsquo;係数\u0026rsquo;と呼ぶのか理解できる。\n3次元 ここで3次元スカラー関数 $f=f(x,y,z)$と位置ベクトル $\\mathbf{r}=x\\hat{\\mathbf{x}}+y\\hat{\\mathbf{y}}+z\\hat{\\mathbf{z}}$が与えられたとしよう。$f$の変化量は全微分で表される。\n$$ \\begin{equation} df=\\frac{ \\partial f}{ \\partial x }dx + \\frac{ \\partial f}{ \\partial y}dy+\\frac{ \\partial f}{ \\partial z}dz \\end{equation} $$\n$\\mathbf{r}$の変化量は以下のようである。\n$$ d\\mathbf{r}=dx\\hat{\\mathbf{x}}+dy\\hat{\\mathbf{y}}+dz\\hat{\\mathbf{z}} $$\nこれで1次元の時と同じように、$df$と $d\\mathbf{r}$の間の比率を表す何かを探してみよう。しかし、$df$はスカラーで $d\\mathbf{r}$はベクトルであるため、その \u0026lsquo;何か\u0026rsquo;はベクトルであり、$df$はその何かと $d\\mathbf{r}$の内積として表現されることを想像できる。したがって\n、とりあえずその何かを $\\mathbf{a}=a_{1}\\hat{\\mathbf{x}}+a_{2}\\hat{\\mathbf{y}}+a_{3}\\hat{\\mathbf{z}}$と表記して、以下のように表現してみよう。\n$$ \\begin{align*} df=\\mathbf{a}\\cdot d\\mathbf{r}\u0026amp;=(a_{1}\\hat{\\mathbf{x}}+a_{2}\\hat{\\mathbf{y}}+a_{3}\\hat{\\mathbf{z}})\\cdot(dx\\hat{\\mathbf{x}}+dy\\hat{\\mathbf{y}}+dz\\hat{\\mathbf{z}}) \\\\ \u0026amp;= a_{1}dx+a_{2}dy+a_{3}dz \\end{align*} $$\nこれを $(2)$と比較すると、以下の結果を得る。\n$$ \\mathbf{a}=\\frac{ \\partial f}{ \\partial x}\\hat{\\mathbf{x}}+\\frac{ \\partial f}{ \\partial y}\\hat{\\mathbf{y}}+\\frac{ \\partial f}{ \\partial z}\\hat{\\mathbf{z}} $$\nこれから、このベクトル $\\mathbf{a}$を $\\nabla f$と表記し、$f$のグラディエントと呼ぶことにしよう。グラディエントの方向は関数 $f$のグラフが最も大きく増加する方向を指し、その大きさはその程度を示す。\n関連する公式 線形性:\n$$ \\nabla (f + g) = \\nabla f + \\nabla g $$\n積の規則:\n$$ \\nabla{(fg)}=f\\nabla{g}+g\\nabla{f} $$ $$ \\nabla(\\mathbf{A} \\cdot \\mathbf{B}) = \\mathbf{A} \\times (\\nabla \\times \\mathbf{B}) + \\mathbf{B} \\times (\\nabla \\times \\mathbf{A})+(\\mathbf{A} \\cdot \\nabla)\\mathbf{B}+(\\mathbf{B} \\cdot \\nabla) \\mathbf{A} $$\n2次導関数:\n$$ \\nabla \\cdot (\\nabla T) = \\dfrac{\\partial^{2} T}{\\partial x^{2}} + \\dfrac{\\partial ^{2} T} {\\partial y^{2}} + \\dfrac{\\partial ^{2} T}{\\partial z^{2}} $$ $$ \\nabla \\times (\\nabla T)= \\mathbf{0} $$ $$\\nabla (\\nabla \\cdot \\mathbf{A} ) $$\n勾配の基本定理\n$$ T(b)-T(a) = \\int _{a}^{b} (\\nabla T) \\cdot d\\mathbf{l} $$\n積分公式\n$$ \\int_{\\mathcal{V}} (\\nabla T) d \\tau = \\oint_{\\mathcal{S}} T d \\mathbf{a} $$ $$ \\int_{\\mathcal{V}} \\left[ T \\nabla^{2} U + (\\nabla T) \\cdot (\\nabla U) \\right] d \\tau = \\oint_{\\mathcal{S}} (T \\nabla U) \\cdot d \\mathbf{a} $$ $$ \\int_{\\mathcal{V}} \\left( T \\nabla^{2} U - U \\nabla^{2} T \\right) d \\tau = \\oint_{\\mathcal{S}} \\left( T \\nabla U - U \\nabla T \\right) \\cdot d \\mathbf{a} $$ $$ \\int_{\\mathcal{S}} \\nabla T \\times d \\mathbf{a} = - \\oint_{\\mathcal{P}} T d \\mathbf{l} $$\n部分積分\n$$ \\int_{\\mathcal{V}}\\mathbf{A} \\cdot (\\nabla f)d\\tau = \\oint_{\\mathcal{S}}f\\mathbf{A} \\cdot d \\mathbf{a}-\\int_{\\mathcal{V}}f(\\nabla \\cdot \\mathbf{A})d\\tau $$ $$ \\int_{\\mathcal{S}} f \\left( \\nabla \\times \\mathbf{A} \\right)\\mathbf{A} \\cdot d \\mathbf{a} = \\int_{\\mathcal{S}} \\left[ \\mathbf{A} \\times \\left( \\nabla f \\right) \\right] \\cdot d\\mathbf{a} + \\oint_{\\mathcal{P}} f\\mathbf{A} \\cdot d\\mathbf{l} $$\n一緒に見る デル演算子 $\\nabla$ グラディエント $\\nabla f$ ダイバージェンス $\\nabla \\cdot \\mathbf{F}$ カール $\\nabla \\times \\mathbf{F}$ ラプラシアン $\\nabla^{2} f$ https://pomp.tistory.com/941?category=37772\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1778,"permalink":"https://freshrimpsushi.github.io/jp/posts/1778/","tags":null,"title":"3次元デカルト座標系におけるスカラー関数の勾配"},{"categories":"확률분포론","contents":"まとめ $X \\sim N(\\mu,\\sigma ^2)$ならば $$ V=\\left( { X - \\mu \\over \\sigma} \\right) ^2 \\sim \\chi ^2 (1) $$\n$N \\left( \\mu , \\sigma^{2} \\right)$は平均が$\\mu$で分散が$\\sigma^{2}$の正規分布だ。 $\\chi^{2} \\left( 1 \\right)$は自由度$1$のカイ二乗分布だ。 説明 一般的に、これを一般化したスチューデントの定理がよく使われる。\n統計学を勉強する者なら、標準正規分布の二乗がカイ二乗分布に従うというのを当然として知っていなければならない。何かのデータが正規分布に従うと仮定できる時、標準化されたデータの分散が過度に高かったり低かったりするなら、何か問題があるとすぐに推測できる。当然、多くの統計的検定に応用され、それについての理論的な直感があるかないかは、天と地の差だ。\n一方で逆に考えると、カイ二乗分布の定義を先に思い浮かべてその性質を探るよりも、最初から標準正規分布に従うデータの二乗、たぶん残差の二乗がどんな分布に従うかを研究していてカイ二乗分布を発見する方がもっと常識的だ。\n証明 1 $\\displaystyle W := {(X-\\mu) \\over \\sigma }$とすると$W \\sim N(0,1)$になる。\n標準正規分布の定義: 次のような確率密度関数を持つ正規分布$N \\left( 0,1^{2} \\right)$を標準正規分布という。 $$ f(z) = {{ 1 } \\over { \\sqrt{2 \\pi} }} \\exp \\left[ - {{ z^{2} } \\over { 2 }} \\right] $$\n$V$の累積分布関数を$F$とすると $$ \\begin{align*} F(v) =\u0026amp; P(V \\le v) \\\\ =\u0026amp; P \\left( W^2 \\le v \\right) \\\\ =\u0026amp; P \\left( \\sqrt{v} \\le W \\le \\sqrt{v} \\right) \\\\ =\u0026amp; \\int_{-\\sqrt{v}}^{\\sqrt{v}} { 1 \\over \\sqrt{ 2 \\pi } } e^{-{{w^2} \\over 2}} dw \\\\ =\u0026amp; 2 \\int_{0}^{\\sqrt{v}} { 1 \\over \\sqrt{ 2 \\pi } } e^{-{{w^2} \\over 2}} dw \\end{align*} $$ $w := \\sqrt{x}$と置き換えると $$ F(v) = 2\\int_{0}^{v} { 1 \\over \\sqrt{ 2 \\pi } } e^{-{{x} \\over 2}} {1 \\over {2 \\sqrt{x} } } dx $$ 積分学の基本定理により、$v$の確率密度関数$f$は $$ f(v) = F ' (v) = { 1 \\over {\\sqrt{ 2 \\pi } } } e^{-{{v} \\over 2}}{ 1 \\over {v^{1 \\over 2}} } $$\nオイラーの反射公式: $$ {\\Gamma (1-x) \\Gamma ( x )} = { {\\pi} \\over {\\sin \\pi x } } $$\n反射公式により$\\displaystyle \\sqrt{\\pi} = \\Gamma \\left( {{ 1 } \\over { 2 }} \\right)$なので $$ f(v) = { 1 \\over { \\Gamma ({1 \\over 2}) 2^{1 \\over 2} } } v^{ - {1 \\over 2} } e^{-{{v} \\over 2}} $$\nガンマ分布の定義: $k, \\theta \u0026gt; 0$に対して次のような確率密度関数を持つ連続確率分布$\\Gamma ( k , \\theta )$をガンマ分布という。 $$ f(x) = {{ 1 } \\over { \\Gamma ( k ) \\theta^{k} }} x^{k - 1} e^{ - x / \\theta} \\qquad , x \u0026gt; 0 $$\n結論として、$V$はガンマ分布$\\displaystyle \\Gamma \\left( {{ 1 } \\over { 2 }} , 2 \\right)$の確率密度関数を持つ。\nガンマ分布とカイ二乗分布の関係: $$ \\Gamma \\left( { r \\over 2 } , 2 \\right) \\iff \\chi ^2 (r) $$\nしたがって、$\\displaystyle \\Gamma \\left( {1 \\over 2}, 2 \\right) \\sim \\chi^2 (1)$であり $$ \\left( { X - \\mu \\over \\sigma} \\right) ^2 \\sim \\chi ^2 (1) $$\n■\nHogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): 175-176.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":148,"permalink":"https://freshrimpsushi.github.io/jp/posts/148/","tags":null,"title":"標準正規分布の二乗は、自由度1のカイ二乗分布に従うことを証明"},{"categories":"매트랩","contents":"方法 clear コマンド コマンドウィンドウにclearと入力すると、作業スペースが初期化される。\n作業スペースを消去する(Alt+T+O) 作業スペースウィンドウを右クリックすると、\u0026lsquo;作業スペースを消去する(O)\u0026lsquo;を選択できる。押すと作業スペースが初期化される。これはショートカットAlt+T+Oでも実行できるが、エディターが開いている状態ではできない。\n直接選択して削除 全体をドラッグして選択するか、Ctrl+aで全選択してDeleteを押すと削除できる。\n他言語 Rで ","id":1758,"permalink":"https://freshrimpsushi.github.io/jp/posts/1758/","tags":null,"title":"MATLABで作業スペースを初期化し、すべての変数を削除する方法"},{"categories":"확률분포론","contents":"定義 平均 $\\mu \\in \\mathbb{R}$ と分散 $\\sigma^{2} \u0026gt; 0$ に対し、以下のような確率密度関数を持つ連続確率分布 $N \\left( \\mu,\\sigma^{2} \\right)$ を正規分布Normal Distributionという。\n$$ f(x) = {{ 1 } \\over { \\sqrt{2 \\pi} \\sigma }} \\exp \\left[ - {{ 1 } \\over { 2 }} \\left( {{ x - \\mu } \\over { \\sigma }} \\right)^{2} \\right] \\qquad, x \\in \\mathbb{R} $$\n特に、以下のような確率密度関数を持つ正規分布 $N \\left( 0,1^{2} \\right)$ を標準正規分布という。\n$$ f(z) = {{ 1 } \\over { \\sqrt{2 \\pi} }} \\exp \\left[ - {{ z^{2} } \\over { 2 }} \\right] $$\n説明 正規分布の別名はガウス分布Gaussian Distributionだ。歴史的には、ガウスが1809年に最小二乗法に関する研究で正規分布を紹介したことで広く知られるようになった。正規分布の本質を最初に理解した人がガウスであると断言することはできないが、ガウスは正規分布の異名を持つにふさわしい人物である。\n1794年、たった17歳のガウスは、日常や研究で遭遇する測定値から真値を求める方法についてのインスピレーションを得た。ガウスは頻繁に通る道で自分の歩数を数え、そのデータを収集してグラフに描き、鐘型の曲線を得た。それはヒストグラムという概念がなかった時代の発見だったが、ガウス自身はこれらの正規分布と最小二乗法の概念がすでに広く知られていて、誰もが使用している技術だと思っていた1。まさに圧倒的な天才性だ。また、正規分布に関連する多くの計算にガウス積分が使われることもある。\nその後、正規分布は広く研究され、科学全般になくてはならないツールになった。それほど馴染み深いため、一般人は統計学とは、結局のところ、データが正規分布に従うと仮定して平均分散を求めるだけではないかという誤解を持つことがある。そのような過小評価が統計学への進学につながった場合、それは残念なことだが、非専門家にはその程度の説明で十分かもしれない。それほど正規分布が重要で強力であるという意味での話だ。\n基本性質 モーメント生成関数 [1]: $$m(t) = \\exp \\left( \\mu t + {{ \\sigma^{2} t^{2} } \\over { 2 }} \\right) \\qquad , t \\in \\mathbb{R}$$ 平均と分散 [2] : $X \\sim N\\left( \\mu , \\sigma^{2} \\right)$ の場合 $$ \\begin{align*} E(X) =\u0026amp; \\mu \\\\ \\text{Var} (X) =\u0026amp; \\sigma^{2} \\end{align*} $$ 十分統計量と最尤推定量 [3] : 正規分布に従うランダムサンプル $\\mathbf{X} := \\left( X_{1} , \\cdots , X_{n} \\right) \\sim N \\left( \\mu , \\sigma^{2} \\right)$ が与えられたとする。 十分統計量 $T$ と最尤推定量 $\\left( \\hat{\\mu}, \\widehat{\\sigma^{2}} \\right)$ は以下の通りである。 $$ \\begin{align*} T =\u0026amp; \\left( \\sum_{k} X_{k}, \\sum_{k} X_{k}^{2} \\right) \\\\ \\left( \\hat{\\mu}, \\widehat{\\sigma^{2}} \\right) =\u0026amp; \\left( {{ 1 } \\over { n }} \\sum_{k} X_{k}, {{ 1 } \\over { n }} \\sum_{k} \\left( X_{k} - \\overline{X} \\right)^{2} \\right) \\end{align*} $$\nエントロピー [4] : (自然対数を選んだ場合)正規分布のエントロピーは以下の通りである。 $$ H = \\ln \\sqrt{2\\pi e \\sigma^{2}} $$ 定理 正規分布の具体的な重要性を長々と説明する必要はなく、以下のように単に定理を並べるだけで十分である。見てみよう。\n中心極限定理 [a]: $\\left\\{ X_{k} \\right\\}_{k=1}^{n}$ がiid 確率変数で、確率分布 $\\left( \\mu, \\sigma^2 \\right) $ に従うとすると、$n \\to \\infty$ の時 $$ \\sqrt{n} {{ \\overline{X}_n - \\mu } \\over {\\sigma}} \\overset{D}{\\to} N (0,1) $$ カイ二乗分布との関係 [b]: $X \\sim N(\\mu,\\sigma ^2)$ ならば $$ V=\\left( { X - \\mu \\over \\sigma} \\right) ^2 \\sim \\chi ^2 (1) $$ 二項分布の極限分布としての標準正規分布の導出 [c]: $X_i \\sim B(1,p)$ であり、$Y_n = X_1 + X_2 + \\cdots + X_n$ の場合、$Y_n \\sim B(n,p)$ である $$ { { Y_n - np } \\over {\\sqrt{ np(1-p) } } }\\overset{D}{\\to} N(0,1) $$ ポアソン分布の極限分布としての標準正規分布の導出 [d]: $X_{n} \\sim \\text{Poi} \\left( n \\right)$ であり、$\\displaystyle Y_{n} := {{ X_{n} - n } \\over { \\sqrt{n} }}$ の場合 $$ Y_{n} \\overset{D}{\\to} N(0,1) $$ スチューデントのt分布の極限分布としての標準正規分布の導出 [e]: $T_n \\sim t(n)$ の場合 $$ T_n \\ \\overset{D}{\\to} N(0,1) $$ 正規分布とカイ二乗分布からt分布の導出 [f]: 二つの確率変数 $W,V$ が独立であり、$W \\sim N(0,1)$、$V \\sim \\chi^{2} (r)$ の場合 $$ T = { {W} \\over {\\sqrt{V/r} } } \\sim t(r) $$ 証明 戦略：ガウス積分が使用できるように指数部分を完全平方形にして標準正規分布のモーメント生成関数から導き出し、置換により正規分布のモーメント生成関数を得る。\nガウス積分: $$ \\int_{-\\infty}^{\\infty} e^{-x^2} dx= \\sqrt{\\pi} $$\n[1] 2 $\\displaystyle Z := {{ X - \\mu } \\over { \\sigma }} \\sim N(0,1)$ とすると、そのモーメント生成関数は\n$$ \\begin{align*} m_{Z}(t) =\u0026amp; \\int_{-\\infty}^{\\infty} \\exp (tz) {{ 1 } \\over { \\sqrt{2 \\pi} }} \\exp \\left[ - {{ 1 } \\over { 2 }} z^{2} \\right] dz \\\\ =\u0026amp; {{ 1 } \\over { \\sqrt{\\pi} }} \\int_{-\\infty}^{\\infty} {{ 1 } \\over { \\sqrt{2} }} \\exp \\left[ - {{ 1 } \\over { 2 }} z^{2} + tz \\right] dz \\\\ =\u0026amp; {{ 1 } \\over { \\sqrt{\\pi} }} \\int_{-\\infty}^{\\infty} {{ 1 } \\over { \\sqrt{2} }} \\exp \\left[ - {{ 1 } \\over { 2 }} \\left( z - t \\right)^{2} + {{ t^{2} } \\over { 2 }} \\right] dz \\\\ =\u0026amp; {{ 1 } \\over { \\sqrt{\\pi} }} \\int_{-\\infty}^{\\infty} {{ 1 } \\over { \\sqrt{2} }} \\exp \\left[ - {{ 1 } \\over { 2 }} \\left( z - t \\right)^{2} \\right] \\exp \\left[ {{ t^{2} } \\over { 2 }} \\right] dz \\\\ =\u0026amp; \\exp \\left[ {{ t^{2} } \\over { 2 }} \\right] {{ 1 } \\over { \\sqrt{\\pi} }} \\int_{-\\infty}^{\\infty} {{ 1 } \\over { \\sqrt{2} }} \\exp \\left[ - w^{2} \\right] \\sqrt{2} dw \\\\ =\u0026amp; \\exp \\left[ {{ t^{2} } \\over { 2 }} \\right] \\end{align*} $$\nすると、$X \\sim N \\left( \\mu , \\sigma^{2} \\right)$ のモーメント生成関数は\n$$ \\begin{align*} m_{X}(t) =\u0026amp; E \\left[ \\exp ( t X ) \\right] \\\\ =\u0026amp; E \\left[ \\exp \\left( t (\\sigma Z + \\mu) \\right) \\right] \\\\ =\u0026amp; \\exp(\\mu t) E \\left[ \\exp \\left( t \\sigma Z \\right) \\right] \\\\ =\u0026amp; \\exp(\\mu t) \\exp \\left( {{ t^{2} \\sigma^{2} } \\over { 2 }} \\right) \\\\ =\u0026amp; \\exp \\left( \\mu t + {{ \\sigma^{2} t^{2} } \\over { 2 }} \\right) \\end{align*} $$\n■\n[2] モーメント生成関数を使用して直接導く。\n■\n[3] 直接導く。\n■\n[4] 直接導く。\n■\n[a] モーメント法を応用する。\n■\n[b] 確率密度関数を直接導く。ガンマ関数とガンマ分布、カイ二乗分布との関係が使われる。\n■\n[c] 中心極限定理を使用して証明される。\n■\n[d] モーメント生成関数を使用して証明される。\n■\n[e] 難しい。スターリング近似を通じて確率密度関数が収束することを証明する。\n■\n[f] 簡単だが複雑。確率密度関数を直接導く。\n■\nコード 以下はコーシー分布、t分布、コーシー分布の確率密度関数を示すJuliaのコードである。\n@time using LaTeXStrings\r@time using Distributions\r@time using Plots\rcd(@__DIR__)\rx = -4:0.1:4\rplot(x, pdf.(Cauchy(), x),\rcolor = :red,\rlabel = \u0026#34;Cauchy\u0026#34;, size = (400,300))\rplot!(x, pdf.(TDist(3), x),\rcolor = :orange,\rlabel = \u0026#34;t(3)\u0026#34;, size = (400,300))\rplot!(x, pdf.(TDist(30), x),\rcolor = :black, linestyle = :dash,\rlabel = \u0026#34;t(30)\u0026#34;, size = (400,300))\rplot!(x, pdf.(Normal(), x),\rcolor = :black,\rlabel = \u0026#34;Standard Normal\u0026#34;, size = (400,300))\rxlims!(-4,5); ylims!(0,0.5); title!(L\u0026#34;\\mathrm{pdf\\,of\\, t}(\\nu)\u0026#34;)\rpng(\u0026#34;pdf\u0026#34;) フーベルト・マニア. (2010). 熱中すること (冷たい数字の世界で絶対的な秩序を見つけ出した、ガウスの伝記): p69~72.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nホッグ他. (2013). Introduction to Mathematical Statistics(第7版): p171~172.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1645,"permalink":"https://freshrimpsushi.github.io/jp/posts/1645/","tags":null,"title":"正規分布"},{"categories":"수리물리","contents":"定義 ベクトル関数 $\\mathbf{F}(x,y,z)=(F_{x},F_{y},F_{z})=F_{x}\\hat{\\mathbf{x}} + F_{y}\\hat{\\mathbf{y}} + F_{z}\\hat{\\mathbf{z}}$に対して、以下のようなベクトルを$\\mathbf{F}$のカールcurlと定義し、$\\nabla \\times \\mathbf{F}$と表記する。\n$$ \\begin{align} \\nabla \\times \\mathbf{F} \u0026amp;= \\left( \\dfrac{ \\partial F_{z}}{ \\partial y }-\\dfrac{ \\partial F_{y}}{ \\partial z} \\right)\\hat{\\mathbf{x}}+ \\left( \\dfrac{ \\partial F_{x}}{ \\partial z }-\\dfrac{ \\partial F_{z}}{ \\partial x} \\right)\\hat{\\mathbf{y}}+ \\left( \\dfrac{ \\partial F_{y}}{ \\partial x }-\\dfrac{ \\partial F_{x}}{ \\partial y} \\right)\\hat{\\mathbf{z}} \\label{def1} \\\\ \u0026amp;=\\begin{vmatrix} \\hat{\\mathbf{x}} \u0026amp; \\hat{\\mathbf{y}} \u0026amp; \\hat{\\mathbf{z}} \\\\ \\dfrac{ \\partial }{ \\partial x} \u0026amp; \\dfrac{ \\partial }{ \\partial y } \u0026amp; \\dfrac{ \\partial }{ \\partial z} \\\\ F_{x} \u0026amp; F_{y} \u0026amp;F_{z}\\end{vmatrix} \\label{def2} \\end{align} $$\n$(2)$は$\\mathbf{F}$のカールを簡単に覚えるための公式である。行列式と考えてそのまま展開すればよい。 説明 カールは回転と翻訳される。しかし、回転という言葉は日常的すぎる上に、カールではなくrotationと誤解される可能性があるため、생새우초밥집では回転の代わりにカールを使用する。\n$\\nabla \\times \\mathbf{F}$は$\\mathbf{F}$という物理量がどの方向に回転しているかを教えてくれるベクトルである。$\\nabla \\times \\mathbf{F}$の方向を軸(親指)にして右手の法則を適用すると、右手が包む方向と$\\mathbf{F}$が回転する方向が一致する。ベクトル$\\nabla \\times \\mathbf{F}$の大きさは回転の程度を示す。\nアインシュタインの表記法とレヴィ-チヴィタ記号を使用すれば、以下のように表すことができる。$\\nabla_{j} = \\dfrac{\\partial }{\\partial x_{j}}$と表記するなら、\n$$ \\nabla \\times \\mathbf{F} = \\epsilon_{ijk}\\hat{\\mathbf{e}}_{i}\\nabla_{j}F_{k} $$\n一方、定義で$(1)$という値を$\\nabla \\times \\mathbf{F}$と表記するとしたことに注意しよう。$\\nabla$をデル演算子と呼ぶことはあるが、これ自体が何かの意味を持つと考えると、$\\nabla \\cdot \\mathbf{F}$や$\\nabla \\times \\mathbf{F}$を内積や外積と誤解することになりかねない。したがって、$\\nabla$は便利な表記法程度にしか理解してはならず、勾配、ダイバージェンス、カールをまとめてデル演算子と呼ぶこともあるし、むしろデル演算子=勾配と考える方が良いかもしれない。詳細は以下で続く。\n注意点 $\\nabla \\times \\mathbf{F}$は$\\nabla$と$\\mathbf{F}$の外積ではない $\\nabla \\times \\mathbf{F}$は絶対に$\\nabla$と$\\mathbf{F}$の外積ではない。\r単に$\\nabla \\times \\mathbf{F}$は$\\mathbf{F}$に関する何らかの情報を含むベクトルである。$\\nabla$を$\\nabla = \\dfrac{ \\partial }{ \\partial x}\\hat{\\mathbf{x}} + \\dfrac{ \\partial }{ \\partial y}\\hat{\\mathbf{y}} + \\dfrac{ \\partial }{ \\partial z}\\hat{\\mathbf{z}}$のようなベクトルと考えて計算すると、結果が$(1)$と完全に一致するため、便宜上$\\nabla \\times \\mathbf{F}$と表記しているだけである。もし$\\nabla$を実際のベクトルと仮定すると、おかしな結果になる。\n二つのベクトル$\\mathbf{A}, \\mathbf{B}$に対して次の式が成り立つ。\n$$ \\nabla \\times (\\mathbf{A} \\times \\mathbf{B}) = (\\mathbf{B} \\cdot \\nabla)\\mathbf{A} - (\\mathbf{A} \\cdot \\nabla)\\mathbf{B} + \\mathbf{A} (\\nabla \\cdot \\mathbf{B}) - \\mathbf{B} (\\nabla \\cdot \\mathbf{A}) $$\nもし$\\nabla$が本当にベクトルだったら、上の公式に代入することができ、次の結果が得られるだろう。\n$$ \\nabla \\times (\\nabla \\times \\mathbf{F})=(\\mathbf{F} \\cdot \\nabla)\\nabla - (\\nabla \\cdot \\nabla)\\mathbf{F} + \\nabla (\\nabla \\cdot \\mathbf{F}) - \\mathbf{F} (\\nabla \\cdot \\nabla) $$\nしかし、正しい結果は次のようになる。\n$$ \\nabla \\times (\\nabla \\times \\mathbf{F})=\\nabla(\\nabla \\cdot \\mathbf{F})-\\nabla ^{2} \\mathbf{F} $$\n他にも例がある。ベクトルの外積は反交換性を持つため、$\\nabla \\times \\mathbf{F}$が外積であるならば、次の式が成り立つはずだ。\n$$ \\nabla \\times \\mathbf{F} \\overset{?}{=} - \\mathbf{F} \\times \\nabla $$\nしたがって、$\\nabla$はベクトルではなく、$\\nabla \\times \\mathbf{F}$を$\\nabla$と$\\mathbf{F}$の外積ではないことが分かる。ベクトルではなく、$\\nabla \\times$自体を一つの関数と考えるべきだ。このように関数を変数とする関数を物理学では演算子と呼ぶ。\nでは $\\nabla \\times \\mathbf{F}$と$\\mathbf{F} \\times \\nabla$の違いは？ $\\nabla \\times$はベクトル関数を変数とする、次のように定義される演算子である。\n$$ \\nabla \\times (\\mathbf{F}) = \\left( \\dfrac{ \\partial F_{z}}{ \\partial y }-\\dfrac{ \\partial F_{y}}{ \\partial z} \\right)\\hat{\\mathbf{x}}+ \\left( \\dfrac{ \\partial F_{x}}{ \\partial z }-\\dfrac{ \\partial F_{z}}{ \\partial x} \\right)\\hat{\\mathbf{y}}+ \\left( \\dfrac{ \\partial F_{y}}{ \\partial x }-\\dfrac{ \\partial F_{x}}{ \\partial y} \\right)\\hat{\\mathbf{z}} $$\nつまり $\\nabla \\times \\mathbf{F}$は $\\nabla \\times$という演算子（関数）に$\\mathbf{F}$という変数を代入したときの関数値である。もちろんこれは再び$(x,y,z)$を変数とするベクトル関数である。$\\nabla \\times \\mathbf{F}$が$\\nabla \\times$の関数値であるのに対し、$\\mathbf{F} \\times \\nabla$はそれ自体が一つの演算子である。よく使われる数式ではないが、定義するなら次のような微分演算子であると言える。\n$$ \\begin{align*} \\mathbf{F} \\times \\nabla \u0026amp;= \\begin{vmatrix} \\hat{\\mathbf{x}} \u0026amp; \\hat{\\mathbf{y}} \u0026amp; \\hat{\\mathbf{z}} \\\\ F_{x} \u0026amp; F_{y} \u0026amp;F_{z} \\\\ \\dfrac{ \\partial }{ \\partial x} \u0026amp; \\dfrac{ \\partial }{ \\partial y } \u0026amp; \\dfrac{ \\partial }{ \\partial z} \\end{vmatrix} \\\\ \u0026amp;= \\left( F_{y}\\dfrac{ \\partial }{ \\partial z} - F_{z}\\dfrac{ \\partial }{ \\partial y} \\right)\\hat{\\mathbf{x}} + \\left( F_{z}\\dfrac{ \\partial }{ \\partial x} - F_{x}\\dfrac{ \\partial }{ \\partial z} \\right)\\hat{\\mathbf{y}} + \\left( F_{x}\\dfrac{ \\partial }{ \\partial y} - F_{y}\\dfrac{ \\partial }{ \\partial x} \\right)\\hat{\\mathbf{z}} \\end{align*} $$\n導出 ここで、ベクトル関数が回転する方向（時計回りか反時計回りか）を示す関数について考えてみましょう。重要なのは、回転面内のどの方向も回転の方向を特定できないということです。下の図を見てください。\nベクトル $-\\hat{\\mathbf{x}}$は点 $A$での動きは説明できますが、$B$での動きは説明できません。 ベクトル $\\hat{\\mathbf{y}}$は点 $C$での動きは説明できますが、$D$での動きは説明できません。 ベクトル $\\hat{\\mathbf{x}} + \\hat{\\mathbf{y}}$は経路 $F$を説明できますが、$G$を説明できません。 これは時計回りの場合にも同じです。回転方向を特定するためには回転面を離れる必要があることが理解できるでしょう。実際、これを決定するための良い方法が既にあります。それは、右手の法則を使うことです。右手が巻き込む方向の回転軸を親指の方向として決定します。したがって、$xy$平面で反時計回りに回る回転の軸（方向）は$\\hat{\\mathbf{z}}$であり、時計回りに回る回転の軸（方向）は$-\\hat{\\mathbf{z}}$です。\nそれでは、$\\mathbf{F}$が$xy$平面で反時計回りに回っている場合、$\\hat{\\mathbf{z}}$方向を示す値、つまり正の値を見つけてみましょう。回転は簡単に以下のように四角形で表現しましょう。\n経路①は点 $a$から点 $b$まで動き、$\\mathbf{F}(a) = (1,0,0)$, $\\mathbf{F}(b) = (0,1,0)$としましょう。すると、点 $a$から点 $b$まで$x$は$+1$だけ変化し、$F_{y}$も$+1$だけ変化するので、次のようになります。\n$$ \\dfrac{\\partial F_{y}}{\\partial x} \\gt 0 $$\n同様に、点 $b$から点 $c$までの経路で、$y$は$+1$だけ変化し、$F_{x}$は$-1$だけ変化します。4つの経路すべてを確認すると、\n$$ \\dfrac{\\partial F_{y}}{\\partial x} \\gt 0 \\quad \\text{in path $\\textcircled{1}$, $\\textcircled{3}$} $$\n$$ \\dfrac{\\partial F_{x}}{\\partial y} \\lt 0 \\quad \\text{in path $\\textcircled{2}$, $\\textcircled{4}$} $$\nしたがって、上記のように反時計回りに回転するベクトル $\\mathbf{F}$に対して、以下の値は常に正です。\n$$ \\dfrac{\\partial F_{y}}{\\partial x} - \\dfrac{\\partial F_{x}}{\\partial y} \\gt 0 $$\n逆に、$\\mathbf{F}$が時計回りに回転している場合、上記の値は常に負です。それでは、ベクトル関数 $\\mathbf{F}$を代入すると、$xy$平面で回転する方向と大きさを示す演算子 $\\operatorname{curl}_{xy}$を次のように定義できます。\n$$ \\operatorname{curl}_{xy} (\\mathbf{F}) = \\left( \\dfrac{\\partial F_{y}}{\\partial x} - \\dfrac{\\partial F_{x}}{\\partial y} \\right) \\hat{\\mathbf{z}} $$\nこの関数の $\\hat{\\mathbf{z}}$ 成分の符号は、$\\mathbf{F}$が$xy$平面で回転する方向を示す。 $+$の場合、$\\mathbf{F}$は$xy$平面で反時計回りに回転する。 $-$の場合、$\\mathbf{F}$は$xy$平面で時計回りに回転する。 $0$の場合、回転しない。 この関数の $\\hat{\\mathbf{z}}$ 成分の大きさは、$\\mathbf{F}$が$xy$平面でどれだけ速く回転しているかを示す。 このような議論を$yz$平面と$zx$平面にも適用することで、$\\mathbf{F}$が3次元空間で回転している方向と大きさを示すベクトル$\\nabla \\times \\mathbf{F}$を次のように定義することができます。\n$$ \\nabla \\times \\mathbf{F} := \\left( \\dfrac{\\partial F_{z}}{\\partial y} - \\dfrac{\\partial F_{y}}{\\partial z} \\right)\\hat{\\mathbf{x}} + \\left( \\dfrac{\\partial F_{x}}{\\partial z} - \\dfrac{\\partial F_{z}}{\\partial x} \\right)\\hat{\\mathbf{y}} + \\left( \\dfrac{\\partial F_{y}}{\\partial x} - \\dfrac{\\partial F_{x}}{\\partial y} \\right)\\hat{\\mathbf{z}} $$\n■\n関連する公式 リニアリティ: $$ \\nabla \\times (\\mathbf{A} + \\mathbf{B}) = \\nabla \\times \\mathbf{A} + \\nabla \\times \\mathbf{B} $$\n乗算規則:\n$$ \\nabla \\times (f\\mathbf{A}) = f(\\nabla \\times \\mathbf{A}) - \\mathbf{A} \\times (\\nabla f) $$\n$$ \\nabla \\times (\\mathbf{A} \\times \\mathbf{B}) = (\\mathbf{B} \\cdot \\nabla)\\mathbf{A} - (\\mathbf{A} \\cdot \\nabla)\\mathbf{B} + \\mathbf{A} (\\nabla \\cdot \\mathbf{B}) - \\mathbf{B} (\\nabla \\cdot \\mathbf{A}) $$\n二次関数:\n$$ \\nabla \\times (\\nabla f) = \\mathbf{0} $$\n$$ \\nabla \\times (\\nabla \\times \\mathbf{F}) = \\nabla (\\nabla \\cdot \\mathbf{F}) - \\nabla^{2} \\mathbf{F} $$\nストークスまとめ $$ \\int_{\\mathcal{S}} (\\nabla \\times \\mathbf{v} )\\cdot d\\mathbf{a} = \\oint_{\\mathcal{P}} \\mathbf{v} \\cdot d\\mathbf{l} $$\n積分式 $$ \\int_{\\mathcal{V}} (\\nabla \\times \\mathbf{v}) d \\tau = - \\oint_{\\mathcal{S}} \\mathbf{v} \\times d \\mathbf{a} $$\n$$ \\int_{\\mathcal{S}} \\nabla T \\times d \\mathbf{a} = - \\oint_{\\mathcal{P}} T d \\mathbf{l} $$\n部分積分 $$ \\int_{\\mathcal{S}} f \\left( \\nabla \\times \\mathbf{A} \\right)\\mathbf{A} \\cdot d \\mathbf{a} = \\int_{\\mathcal{S}} \\left[ \\mathbf{A} \\times \\left( \\nabla f \\right) \\right] \\cdot d\\mathbf{a} + \\oint_{\\mathcal{P}} f\\mathbf{A} \\cdot d\\mathbf{l} $$\n$$ \\int_{\\mathcal{V}} \\mathbf{B} \\cdot \\left( \\nabla \\times \\mathbf{A} \\right) d\\tau = \\int_{\\mathcal{V}} \\mathbf{A} \\cdot \\left( \\nabla \\times \\mathbf{B} \\right) d\\tau + \\oint_{\\mathcal{S}} \\left( \\mathbf{A} \\times \\mathbf{B} \\right) \\cdot d \\mathbf{a} $$\n証明 線形性 アインシュタイン表記法, レヴィ・チビタ記号を使います。$\\nabla_{j} = \\dfrac{\\partial }{\\partial x_{j}}$ とすると、\n$$ \\begin{align*} \\left[ \\nabla \\times (\\mathbf{A} + \\mathbf{B}) \\right]_{i} \u0026amp;= \\epsilon_{ijk} \\nabla_{j} (\\mathbf{A} + \\mathbf{B})_{k} \\\\ \u0026amp;= \\epsilon_{ijk} \\nabla_{j} (A_{k} + B_{k}) \\\\ \u0026amp;= \\epsilon_{ijk} \\nabla_{j}A_{k} + \\epsilon_{ijk} \\nabla_{j}B_{k} \\\\ \u0026amp;= [\\nabla \\times \\mathbf{A}]_{i} + [\\nabla \\times \\mathbf{B}]_{i} \\\\ \\end{align*} $$\n第三の等号は、$\\dfrac{\\partial (A_{k} + B_{k})}{\\partial x_{j}} = \\dfrac{\\partial A_{k}}{\\partial x_{j}} + \\dfrac{\\partial B_{k}}{\\partial x_{j}}$であるため成立します。\n■\n参照 デル演算子 $\\nabla$ グラディエント $\\nabla f$ ダイバージェンス $\\nabla \\cdot \\mathbf{F}$ カール $\\nabla \\times \\mathbf{F}$ ラプラシアン $\\nabla^{2} f$ ","id":1752,"permalink":"https://freshrimpsushi.github.io/jp/posts/1752/","tags":null,"title":"3次元デカルト座標系におけるベクトル関数のカール(回転)"},{"categories":"확률분포론","contents":"まとめ 二つの確率変数$U,V$が独立であり、$U \\sim \\chi^{2} ( r_{1})$、$V \\sim \\chi^{2} ( r_{2})$とするならば $$ {{ U / r_{1} } \\over { V / r_{2} }} \\sim F \\left( r_{1} , r_{2} \\right) $$\n説明 二つのデータがカイ二乗分布に従い、独立である場合、その比率を分布理論で説明することができるかもしれない。統計学全般では、標準化された残差の二乗がカイ二乗分布に従うと仮定されるため、F検定を好んで使用する。証明自体が重要なわけではないが、多くの分析でなぜF検定を使用するのかについての洞察を与えるため、数理統計学を学ぶ統計学生にとっては非常に重要な事実である。\n導出1 戦略: カイ二乗分布のジョイント密度関数を直接演繹する。\nカイ二乗分布の定義: 自由度$r \u0026gt; 0$に対して次のような確率密度関数を持つ連続確率分布$\\chi^{2} (r)$はカイ二乗分布と呼ばれる。 $$ f(x) = {{ 1 } \\over { \\Gamma (r/2) 2^{r/2} }} x^{r/2-1} e^{-x/2} \\qquad , x \\in (0, \\infty) $$\nF分布の定義: 自由度$r_{1}, r_{2} \u0026gt; 0$に対して次のような確率密度関数を持つ連続確率分布$F \\left( r_{1} , r_{2} \\right)$はF分布と呼ばれる。 $$ f(x) = {{ 1 } \\over { B \\left( r_{1}/2 , r_{2} / 2 \\right) }} \\left( {{ r_{1} } \\over { r_{2} }} \\right)^{r_{1} / 2} x^{r_{1} / 2 - 1} \\left( 1 + {{ r_{1} } \\over { r_{2} }} x \\right)^{-(r_{1} + r_{2}) / 2} \\qquad , x \\in (0, \\infty) $$\n$U,V$が独立なのでジョインット密度関数は$u,v \\in (0,\\infty)$に対して次のようになる。 $$ h(u,v) = {{ 1 } \\over { \\Gamma \\left( {{ r_{1} } \\over { 2 }} \\right) \\Gamma \\left( {{ r_{2} } \\over { 2 }} \\right) 2^{(r_{1} + r_{2})/2} }} u^{r_{1}/2 - 1} v^{r_{2}/2 - 1} e^{-(u+v)/2} $$ 今、$\\displaystyle W:= {{ U/r_{1} } \\over { V / r_{2} }}$と$Z := V$とするならば$u = (r_{1}/r_{2})zw$であり、$v = z$なので $$ \\left| J \\right| = \\begin{vmatrix} (r_{1}/r_{2})z \u0026amp; (r_{1}/r_{2})w \\\\ 0 \u0026amp; 1 \\end{vmatrix} = (r_{1}/r_{2})z \\ne 0 $$ 従って、$W,Z$のジョイント密度関数は$w,z \\in (0,\\infty)$に対して $$ g(w,z) = {{ 1 } \\over { \\Gamma \\left( {{ r_{1} } \\over { 2 }} \\right) \\Gamma \\left( {{ r_{2} } \\over { 2 }} \\right) 2^{(r_{1} + r_{2})/2} }} \\left( {{ r_{1} z w } \\over { r_{2} }} \\right)^{{{ r_{1} - 2 } \\over { 2 }}} z^{{{ r_{2} - 2 } \\over { 2 }}} \\exp \\left[ - {{ z } \\over { 2 }} \\left( {{ r_{1} w } \\over { r_{2} }} + 1 \\right) \\right] {{ r_{1} z } \\over { r_{2} }} $$ $W$のマージナル密度関数$g_{1}$は$\\displaystyle y:= {{ z } \\over { 2 }} \\left( {{ r_{1} w } \\over { r_{2} }} + 1 \\right)$として $$ \\begin{align*} g_{1} (w) =\u0026amp; \\int_{-\\infty}^{\\infty} g(w,z) dz \\\\ =\u0026amp; \\int_{-\\infty}^{\\infty} {{ 1 } \\over { \\Gamma \\left( {{ r_{1} } \\over { 2 }} \\right) \\Gamma \\left( {{ r_{2} } \\over { 2 }} \\right) 2^{(r_{1} + r_{2})/2} }} \\left( {{ r_{1} z w } \\over { r_{2} }} \\right)^{{{ r_{1} - 2 } \\over { 2 }}} z^{{{ r_{2} - 2 } \\over { 2 }}} \\exp \\left[ - {{ z } \\over { 2 }} \\left( {{ r_{1} w } \\over { r_{2} }} + 1 \\right) \\right] {{ r_{1} z } \\over { r_{2} }} dz \\\\ =\u0026amp; \\int_{-\\infty}^{\\infty} {{ (r_{1} / r_{2})^{r_{1} / 2} w^{r_{1}/2 - 1} } \\over { \\Gamma \\left( {{ r_{1} } \\over { 2 }} \\right) \\Gamma \\left( {{ r_{2} } \\over { 2 }} \\right) 2^{(r_{1} + r_{2})/2} }} \\left( {{ 2y } \\over { {{ r_{1} } \\over { r_{2} }} w + 1 }} \\right)^{{{ r_{1} + r_{2} } \\over { 2 }} - 1} e^{-y} \\left( {{ 2 } \\over { {{ r_{1} } \\over { r_{2} }} w + 1 }} \\right) dy \\\\ =\u0026amp; {{ \\Gamma \\left( {{ r_{1} + r_{2} } \\over { 2 }} \\right) \\left( {{ r_{1} } \\over { r_{2} }} \\right)^{r_{1} / 2} } \\over { \\Gamma \\left( {{ r_{1} } \\over { 2 }} \\right) \\Gamma \\left( {{ r_{2} } \\over { 2 }} \\right) }} {{ w^{r_{1}/2 - 1} } \\over { \\left( 1 + {{ r_{1} } \\over { r_{2} }} w \\right)^{(r_{1} + r_{2}) / 2} }} \\\\ =\u0026amp; {{ 1 } \\over { B \\left( r_{1}/2 , r_{2} / 2 \\right) }} \\left( {{ r_{1} } \\over { r_{2} }} \\right)^{r_{1} / 2} w^{r_{1} / 2 - 1} \\left( 1 + {{ r_{1} } \\over { r_{2} }} w \\right)^{-(r_{1} + r_{2}) / 2} \\end{align*} $$ 従って $$ W \\sim F (r_{1} , r_{2}) $$\n■\nHogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): 193-194.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1643,"permalink":"https://freshrimpsushi.github.io/jp/posts/1643/","tags":null,"title":"独立した二つのカイ二乗分布からF分布を導出する"},{"categories":"고전역학","contents":"減衰調和振動1 バネ定数を$k$とするとき、単振動の運動方程式は次のようになる。\n$$ m \\ddot {x}+kx=0 $$\n単振動ではバネの復元力のみを考慮する。しかし、実際には摩擦力などの他の外力が物体の運動に影響を与えるため、これを無視することはできない。そこで、速度に比例する摩擦力があると仮定してみよう。この力を減速力retarding forceと言う。この減速力が働く振子の運動を減衰調和振動damped harmonic oscillationと言う。具体的な例には、空気抵抗などがある。減速力が$-c\\dot{x}$のようであれば、運動方程式は以下のようになる。\n$$ \\begin{align*} \u0026amp;\u0026amp; m \\ddot{x} +c \\dot{x} +kx\u0026amp;=0 \\\\ \\implies \u0026amp;\u0026amp; \\ddot{x} +\\frac{c}{m} \\dot{x} +\\frac{k}{m}x\u0026amp;=0 \\end{align*} $$\nここで、${\\omega_{0}}^{2}=\\frac{k}{m}$で代替すると、$\\gamma = \\frac{c}{2m}$とする。このとき、$\\omega_{0}$を固有角振動数、$\\gamma $を減衰係数damping factorと呼ぶ。すると、運動方程式は下のようになる。\n$$ \\ddot{x} + 2\\gamma \\dot{x} + {\\omega_{0}} ^{2} x=0 $$\nこのような微分方程式は微分演算子$D:=\\frac{ d }{ d t}$を利用して簡単に解ける。微分演算子を上の運動方程式に適用すると以下のようになる。\n$$ \\begin{align*} \u0026amp;\u0026amp; D^{2}x + 2\\gamma D x + {\\omega_{0}} ^{2} x \u0026amp;=0 \\\\ \\implies \u0026amp;\u0026amp; \\left( D^{2} +2\\gamma D + {\\omega_{0}} ^{2} \\right)x \u0026amp;=0 \\end{align*} $$\nしたがって、$D^{2}+2\\gamma D +{\\omega_{0}}^{2}=0$を解けばよい。この方程式の解は根の公式によって下のようになる。\n$$ D=-\\gamma \\pm \\sqrt{\\gamma ^{2} -{\\omega_{0}} ^{2} } $$\nしたがって、\n$$ Dx = \\left( -\\gamma \\pm \\sqrt{\\gamma ^{2} -{\\omega_{0}} ^{2}} \\right)x $$\nこれは単純な1階微分方程式なので、解は以下のように求めることができる。\n$$ \\begin{equation} x(t)=Ae^{(-\\gamma + \\sqrt{\\gamma ^{2} -{\\omega_{0}}^{2}})t }+Be^{(-\\gamma - \\sqrt{\\gamma ^{2} -{\\omega_{0}}^{2}})t } \\label{eq1} \\end{equation} $$\nこのとき、$A$、$B$は定数である。指数に根が含まれているため、上記の式のグラフは$\\gamma ^{2}-{\\omega_{0}}^{2}$の値によって異なる。\n過減衰overdamping $$ \\gamma ^{2} - {\\omega_{0}}^{2}\u0026gt;0 $$\n平衡点からバネがついた物体を引っ張って放したとき、物体は平衡点に戻るが、図のように減衰力が強いために振動は起こらない。\n臨界減衰critical damping $$ {\\gamma} ^{2} -{\\omega_{0}}^{2}=0 $$\nこの場合、$\\eqref{eq1}$の二つの解が同じである。したがって、第二の解を求める必要があり以下のように知られている。\n$$ x(t)=Ae^{-\\gamma t} +Bte^{-\\gamma t} $$\n過減衰と同様に振動は起こらないが、過減衰に比べて非常に速く平衡点近くまで到達することが分かる。\n未減衰underdamping $$ \\gamma^{2} -{\\omega_{0}}^{2} \u0026lt;0 $$\n簡単にするために、$i\\omega_{d}=\\sqrt{\\gamma^{2} - {\\omega_{0}} ^{2}}$としよう。dはdampedの最初の文字から来ている。すると運動方程式は以下のようになる。\n$$ x(t) = e^{-\\gamma t}\\left( A e^{i\\omega_{d}t} + Be^{-i\\omega_{d}t} \\right) $$\nこのとき位置$x$は実数でなければならず、$x^{\\ast}(t)=x(t)$を満たさなければならない。$^{\\ast}$は共役複素数を意味する。これにより、以下の条件を得る。\n$$ Ae^{i\\omega_{d}t}+Be^{-i\\omega_{d}t}=A^{\\ast}e^{-i\\omega_{d}t}+B^{\\ast}e^{i\\omega_{d}t} \\implies B=A^{\\ast} $$\nしたがって、運動方程式は以下のようになる。\n$$ x(t) = e^{-\\gamma t}\\left( A e^{i\\omega_{d}t} + A^{\\ast}e^{-i\\omega_{d}t} \\right) $$\nそして、$A=a+ib$とすると、以下の式が成立する。\n$$ x(t) = e^{-\\gamma t}\\left[ a\\left( e^{i\\omega_{d}t}+e^{-i\\omega_{d}t}\\right) + ib\\left( e^{i\\omega_{d}t}-e^{-i\\omega_{d}t}\\right) \\right] $$\nオイラーの公式を適用すると、以下のようになる。\n$$ x(t) = e^{-\\gamma t}\\left[ 2a\\cos (\\omega_{d}t)-2b\\sin (\\omega_{d}t) \\right] $$\nすると、三角関数の加法定理により、ある実数$A$、$\\phi_{0}$に対して以下の式が成立する。\n$$ x(t)=e^{-\\gamma t}A \\cos(\\omega_{d}t+\\phi_{0}) $$\n$e^{-\\gamma t}$によって、グラフの大きさは指数関数的に減少しつつ、前の二つの場合とは異なり$\\cos$が含まれるために振動する。\nシミュレーション 減衰調和振動子の場合、振動数と減衰係数の差によって、過減衰、臨界減衰、未減衰に分けられる。各状態で振動子がどのように動くかを視覚的に見ることができれば、理解するのに大きな助けになる。Juliaでは、単にグラフを描くことを超えて、非常に簡単にgifファイルを作成して保存することができる。以下は、減衰調和振動子のアニメーションを作成して保存するコードと、実際の実行画面である。\nusing Plots\rO_γ=3\rO_ω=1\rfunction Overdamping(x)\r0.5exp((-O_γ+sqrt(O_γ^2-O_ω^2))*x)+0.5exp((-O_γ-sqrt(O_γ^2-O_ω^2))*x)\rend\rC_γ=1\rC_ω=1\rfunction Criticaldamping(x)\rexp(-C_γ*x)+x*exp(-C_γ*x)\rend\rU_γ=1\rU_γ=U_γ^2\rU_ω=5\rU_ω=U_ω^2\rfunction Underdamping(x)\rreal(exp.(-U_γ*x).*cos.(1im*sqrt(Complex(U_γ-U_ω))*x))\rend\rp = plot([Overdamping, Criticaldamping, Underdamping], zeros(0),label=[\u0026#34;Overdamping\u0026#34; \u0026#34;Criticaldamping\u0026#34; \u0026#34;Underdamping\u0026#34;], xlim=(0,15), ylim=(-0.7,1.2))\ranim = Animation()\rfor x = range(0, stop=15, length = 200)\rpush!(p, x, Float64[Overdamping(x), Criticaldamping(x), Underdamping(x)])\rframe(anim)\rend\rgif(anim,\u0026#34;Damping_fps30.gif\u0026#34;,fps=30) 参照 単振動 強制振動 複数バネ振動 結合振動 Grant R. Fowles and George L. Cassiday, Analytical Mechanics (7th Edition, 2005), p96-100\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1736,"permalink":"https://freshrimpsushi.github.io/jp/posts/1736/","tags":["줄리아"],"title":"減衰調和振動"},{"categories":"르벡공간","contents":"定義1 関数 $f$ のフーリエ変換\n$$ \\widehat{f} (\\gamma ) := \\int_{\\mathbb{R}} f(x) e^{-2 \\pi i x \\gamma} dx, \\quad \\gamma \\in \\mathbb{R} $$\nは、以下の作用素 $\\mathcal{F}$ としても表される。\n$$ (\\mathcal{F} f) (\\gamma ) := \\widehat{f} ( \\gamma ) $$\n説明 フーリエ変換は解析学全般で広く使用されており、二つの表現 $\\widehat{f}$ と $\\mathcal{F} f$ は本質的に違いはないが、記号を使用する際のニュアンスが少し異なる。実際の計算や公式、速記に重点を置く場合は $\\widehat{f}$ が好まれ、作用素としての性質や演算順序が重要な場合は $\\mathcal{F}$ が好まれる。\n$f,g \\in L^{1}$ としよう。\n$a \\in \\mathbb{R}$ に対して $$ \\mathcal{F} T_{a} = E_{-a} \\mathcal{F} $$\n$b \\in \\mathbb{R}$ に対して $$ \\mathcal{F} E_{b} = T_{b} \\mathcal{F} $$\n$c \\ne 0$ に対して $$ \\mathcal{F} D_{c} = D_{1/c} \\mathcal{F} $$\nコンボリューション: $$ \\mathcal{F} ( f \\ast\\ g) = (\\mathcal{F} f \\cdot \\mathcal{F} g) $$\n1~3: $T_{a}, E_{b}, D_{c}$ はトランスレーション、モジュレーション、ディレーションである。\n4: $\\ast$ はコンボリューション畳み込みで、$\\cdot$ は単に関数の積を意味する。つまり、$\\gamma \\in \\mathbb{R}$ に対して\n$$ \\widehat{f \\ast\\ g} (\\gamma) = \\widehat{f} (\\gamma) \\widehat{g} (\\gamma) $$\n$f , g \\in L^{2}$ としよう。\nノルム: $$ \\left\\| \\mathcal{F} f \\right\\|_{2} = \\left\\| f \\right\\|_{2} $$\n内積:\n$$ \\langle \\mathcal{F} f , \\mathcal{F} g \\rangle = \\langle f , g \\rangle $$\n上記の性質はフーリエ解析で広く知られているものを、作用素論の言葉で再び表わしたものである。\n証明 1~4の証明については、こちらを参照。\n■\n関連項目 フーリエ変換の導出 フーリエ変換の様々な定義と表記法 フーリエ変換の様々な意味 Ole Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010), p126-127\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1640,"permalink":"https://freshrimpsushi.github.io/jp/posts/1640/","tags":null,"title":"フーリエ変換としての作用素"},{"categories":"거리공간","contents":"要約 $(X,d_{X})$がコンパクト距離空間で、$(Y,d_{Y})$が距離空間で、 $f:X\\to Y$が連続だとする。すると、$f$は$X$で一様連続である。\n説明 コンパクトという条件は外せない。\n証明 任意の正数$\\varepsilon \u0026gt;0$が与えられたとする。$f$が連続であると仮定すると、定義により、各点$p\\in X$に対して以下の式を満たす正数$\\delta_{p}$が存在する。\n$$ \\forall q\\in X,\\quad d_{X}(p,q)\u0026lt;\\delta_{p} \\implies d_{Y}(f(p),f(q))\u0026lt;\\frac{\\varepsilon}{2} $$\n次に、以下のような集合を考える。\n$$ N_{p}:= \\left\\{ q : d_{X}(p,q)\u0026lt;\\frac{1}{2}\\delta_{p} \\right\\} $$\n$p \\in N_{p}$であるから、全ての$N_{p}$のコレクションは$X$のオープンカバーになる。$X$はコンパクトと仮定されているので、以下の式を満たす$p_{1},\\cdots,p_{n}$が存在する。\n$$ \\begin{equation} X \\subset N_{p_{1}}\\cup \\cdots \\cup N_{p_{n}} \\tag{2} \\label{eq1} \\end{equation} $$\n今、$\\delta=\\frac{1}{2} \\min (\\delta_{p_{1}},\\cdots,\\delta_{p_{n}})$とする。すると、$\\delta$は明らかに正である。今、$d_{X}(p,q)\u0026lt;\\delta$を満たす二点$p,q \\in X$を考える。すると、$\\eqref{eq1}$により、$p \\in N_{p_{m}}$を満たす$m(1\\le m \\le n)$が存在する。従って、以下が成り立つ。\n$$ d_{X}(p,p_{m}) \\le \\frac{1}{2}\\delta_{p_{m}} $$\nすると、以下の式が成り立つ。\n$$ d_{X}(q,p_{m}) \\le d_{X}(q,p) + d_{X}(p,p_{m}) \u0026lt; \\delta + \\frac{1}{2}\\delta_{p_{m}} \\le \\delta _{p_{m}} $$\n従って、\n$$ d_{X}(p,q)\u0026lt;\\delta \\implies d_{Y}(f(p),f(q))\\le d_{Y}(f(p),f(p_{m})) + d_{Y}(f(p_{m}),f(q))\u0026lt;\\frac{1}{2}\\varepsilon+\\frac{1}{2}\\varepsilon=\\varepsilon $$\nが成り立つので、$f$は一様連続である。\n■\n","id":1727,"permalink":"https://freshrimpsushi.github.io/jp/posts/1727/","tags":null,"title":"コンパクト距離空間上の連続関数が一様連続であることの証明"},{"categories":"거리공간","contents":"要旨 $X$をコンパクトな距離空間、$f : X \\to \\mathbb{R}$を連続とする。そうしたら、次のようになる。\n$$ M = \\sup \\limits_{x\\in X} f(x),\\quad m=\\inf \\limits_{x \\in X}f(x) $$\nそうすると、\n$$ M=f(p),\\quad m=f(q) $$\nを満たす$q,p\\in X$が存在する。つまり、すべての$x$に対して、\n$$ f(q)\\le f(x) \\le f(p) $$\nを満たす$q,p \\in X$が存在する。これを最大最小定理extreme value theoremという。\n説明 コンパクトな条件は省略できない。\n$f(X)$が$f$の最大値、最小値を含むことを保証する定理だ。何の条件もなければ、上限と下限の定義により$M$、$m$が$f(X)$に含まれる保証はないが、$X$がコンパクトで、$f$が連続であるという仮定により$M,m\\in f(X)$が成立する。\n証明 $f$がコンパクト空間で連続だから、$f(X)$はコンパクトである。ユークリッド空間でのコンパクトの同値条件によって、$f(X)$は閉じていて有界な実数集合である。\n補助定理\n$E$を空でない実数の集合で、上に有界とする。そして$y=\\sup E$とする。すると、$y \\in \\overline{E}$である。また、$E$が閉じていれば、$y \\in E$である。\nそうすると、補助定理によって証明完了。\n■\n参考 位相空間における最大最小定理 ","id":1725,"permalink":"https://freshrimpsushi.github.io/jp/posts/1725/","tags":null,"title":"距離空間における最大最小定理"},{"categories":"거리공간","contents":"要約 $X$をコンパクト距離空間、$Y$を距離空間、$f:X\\to Y$が連続だとする。すると、$f(X)$はコンパクトである。\nコンパクトという条件は省略できない。\n証明 $\\left\\{ O_\\alpha \\right\\}$を$f(X)$の開被覆とする。$f$が連続であるため、同値条件により、各逆像$f^{-1}(O_{\\alpha})$も$X$で開集合である。従って、$\\left\\{ f^{-1}(O_{\\alpha}) \\right\\}$は$X$の開被覆であり、$X$がコンパクトであるため、\n$$ X \\subset f^{-1}(O_{\\alpha_{1}})\\cup \\cdots \\cup f^{-1}(O_{\\alpha_{n}}) $$\nを満たす$\\alpha_{1},\\cdots,\\alpha_{n}$が存在する。従って、逆像の定義により、以下が成り立つ。\n$$ f(X) \\subset O_{\\alpha_{1}}\\cup \\cdots \\cup O_{\\alpha_{n}} $$\n従って、$f(X)$はコンパクトである。\n■\n結果 $X$をコンパクト距離空間、$\\mathbf{f} :X\\to \\mathbb{R}^{k}$が連続だとする。すると、$\\mathbf{f}(X)$は閉じていて有界である。また、$\\mathbf{f}$も有界である。\n定義 実数値関数$\\mathbf{f}: E \\to \\mathbb{R}^{k}$が与えられたとする。全ての$x \\in E$に対して、\n$$ \\left|\\mathbf{f}(x) \\right| \\le M $$\nを満たす実数$M$が存在するならば、$\\mathbf{f}$を有界という。\n証明 ユークリッド空間でのコンパクトの同値条件と上記の定理により、$\\mathbf{f}(X)$は閉じており有界である。$\\mathbf{f}(X)$が有界であるため、$\\mathbf{f}$も有界である。\n■\n","id":1724,"permalink":"https://freshrimpsushi.github.io/jp/posts/1724/","tags":null,"title":"距離空間における連続性とコンパクト性"},{"categories":"거리공간","contents":"定義 二つの距離空間$\\left( X , d_{X} \\right)$、$\\left( Y , d_{Y} \\right)$と部分集合$E\\subset X$に対して、関数$f : E \\to Y$を定義しよう。\n$p \\in E$としよう。ある$\\varepsilon \u0026gt; 0$に対して、\n$$ x \\in E \\quad \\text{and} \\quad d_{X}(p, x ) \u0026lt; \\delta \\implies d_{Y}(f(p) , f(x) ) \u0026lt; \\varepsilon $$\nを満たす$\\delta\u0026gt;0$が存在するならば、$f$は$p \\in E$で連続であるという。$f$が$E$の全ての点で連続ならば、$f$を$E$上での連続関数continuous functionという。\nある$ \\varepsilon \u0026gt; 0$に対して、\n$$ d_{X}(x_{1}, x_{2} ) \u0026lt; \\delta \\land x_{1}, x_{2} \\in E \\implies d_{Y}(f(x_{1}) , f(x_{2}) ) \u0026lt; \\varepsilon $$\nを満たす$\\delta\u0026gt;0$が存在するならば、$f$が$E$上で一様連続uniformly continuousであるという。\n$\\land$は論理的に「そして」を表す論理積の記号だ。 説明 連続と一様連続は、$\\mathbb{R}$を超えて距離空間に対しても定義できる。$\\mathbb{R}$の連続と異なる点は、$d_{1}$と$d_{2}$を変えての一般化が可能であることだ。\n一方で、もっと難しい表現を使って、ある$B_{d_{Y}} (f(p) , \\varepsilon )$に対して$f(B_{d_{X}} (p , \\delta)) \\subset B_{d_{Y}} (f(p) , \\varepsilon )$を満たす$B_{d_{X}} (p , \\delta)$が存在する時、$f$が$p \\in X$で連続であるとも言える。初めは抽象的すぎて避けがちだが、見ているうちにこの表現の方が便利になるかもしれない。位相空間への一般化を考えれば、早めに慣れておいた方が良いかもしれない。\n定理: 連続関数である同値条件 関数$f:X \\to Y$に対して、以下の条件は互いに同値である。\n$f : X \\to Y$は連続である。\n$\\forall x \\in X,\\ \\displaystyle \\lim_{n \\to \\infty} p_{n} = p \\implies \\lim_{n \\to \\infty} f(p_{n}) = f(p)$\n$Y$の全ての開集合$O$に対して、$f^{-1} ( O )$は$X$で開集合である。\n$Y$の全ての閉集合$C$に対して、$f^{-1} ( C )$は$X$で閉集合である。\nこれらの性質は与えられた関数が連続であることを証明するのに役立つことがある。\n上の図を見ると、一見、四番目の条件の反例に見える。閉区間$[c,d]$に対してその逆像$f^{-1} [c,d]$が$(a,b)$であり、知っての通り、$(a,b)$は開区間である。しかし、$f : (a,b) \\to \\mathbb{R}$なので、$(a,b)$は全空間になり、全空間は閉集合であるため、命題に反していない。\n","id":384,"permalink":"https://freshrimpsushi.github.io/jp/posts/384/","tags":null,"title":"距離空間における連続性と一様連続性"},{"categories":"거리공간","contents":"定理2 二つの距離空間$(X,d_{X})$と$(Y,d_{Y})$について、$f : X \\to Y$とする。すると、以下の三つの命題は同値である。\n(2a) $f$が$X$で連続である。\n(2b) すべての$Y$の開集合$O_{Y}$に対して$f^{-1}(O_{Y})$は$X$で開集合である。\n(2c) すべての$Y$の閉集合$C_{Y}$に対して$f^{-1}(C_{X})$は$X$で閉集合である。ここで、$f^{-1}$は逆関数ではなく逆像を意味する。\n証明 (2a) $\\implies$ (2b)\n$f$が$X$で連続だと仮定する。$O_{Y}$は$Y$で開集合である。開集合の定義により$f^{-1}(O_{Y})$の全ての点は$f^{-1}(O_{Y})$の内点であることが示される。任意の$f(p) \\in O_{Y}$を考える。すると$p \\in f^{-1}(O_{Y})$である。$O_{Y}$が開いているため、$f(p)$は$O_{Y}$の内点である。したがって、\n$$ \\begin{equation} d_{Y}(y,f(p)) \u0026lt; \\varepsilon \\implies y \\in O_{Y} \\label{eq1} \\end{equation} $$\nである正の$\\varepsilon$が存在する。すると、$f$が$X$で連続であるため、この$\\varepsilon$に対して\n$$ \\begin{equation} d_{X}(x,p) \u0026lt; \\delta \\implies d_{Y}(f(x),f(p))\u0026lt;\\varepsilon \\label{eq2} \\end{equation} $$\nであるある$\\delta \u0026gt;0$が存在する。しかし、$\\eqref{eq1}$、$\\eqref{eq2}$により\n$$ d_{X}(x,p) \u0026lt; \\delta \\implies d_{Y}(f(x),f(p))\u0026lt;\\varepsilon \\implies f(x)\\in O_{Y} $$\nであるため、$x \\in f^{-1}(O_{Y})$である。そのため、何らかの正の$\\delta$に対して\n$$ d_{X}(x,p) \u0026lt; \\delta \\implies x \\in f^{-1}(O_{Y}) $$\nが成り立ち、$p$は$f^{-1}(O_{Y})$の内点であり、$f^{-1}(O_{Y})$は開いている。\n■\n(2b) $\\implies$ (a2)\n(2b) を仮定する。任意の$p \\in X$と$\\varepsilon \u0026gt;0$を選択する。そして、集合$O_{Y}$を次のようにする。\n$$ O_{Y} =\\left\\{ y : d_{Y}(y,f(p))\u0026lt;\\varepsilon \\right\\} $$\nすると、$O_{Y}$は$Y$で開集合である。すると、仮定により$f^{-1}(O_{Y})$は$X$で開集合である。したがって、\n$$ d_{X}(x,p) \u0026lt;\\delta \\implies x \\in f^{-1}(O_{Y}) $$\nを満たす正の$\\delta \u0026gt;0$が存在する。すると、\n$$ x\\in f^{-1}(O_{Y}) \\quad \\text{and} \\quad d_{X}(x,p)\u0026lt; \\delta \\implies d_{Y}(f(x),f(p))\u0026lt;\\varepsilon $$\nが成り立つため、連続の定義により$f$は全ての$p \\in X$で連続である。\n■\n(2b) $\\iff$ (2c)\n(2b) $\\implies$ (2c) これを証明すれば、反対の側も同じ論理で証明できるため、残りは省略する。(2b) を仮定する。$Y$での開集合$O_{Y}$については、以下のようである。\n$$ O_{Y}\\mathrm{\\ is\\ open\\ in\\ } Y \\implies f^{-1}(O_{Y})\\mathrm{\\ is\\ open\\ in\\ }X $$\n開集合は閉集合の補集合であるため、$O_{Y}=(C_{Y})^{c}$として上記の文は以下のようである。\n$$ C_{Y}\\mathrm{\\ is\\ closed\\ in\\ } Y \\implies f^{-1}((C_{Y})^{c})\\mathrm{\\ is\\ open\\ in\\ }X $$\nしかし、$f^{-1}((C_{Y})^{c})=(f^{-1}(C_{Y}))^{c}$であるため、以下が成り立つ。\n$$ C_{Y}\\mathrm{\\ is\\ closed\\ in\\ } Y \\implies (f^{-1}(C_{Y}))^{c}\\mathrm{\\ is\\ open\\ in\\ }X $$\nまた、開集合の補集合は閉集合であるため、以下が成り立つ。\n$$ C_{Y}\\mathrm{\\ is\\ closed\\ in\\ } Y \\implies f^{-1}(C_{Y})\\mathrm{\\ is\\ closed\\ in\\ }X $$\n■\n","id":1722,"permalink":"https://freshrimpsushi.github.io/jp/posts/1722/","tags":null,"title":"距離空間における関数の連続性の同値条件"},{"categories":"거리공간","contents":"定義1 $\\left\\{ p_{n} \\right\\}$が距離空間 $(X,d)$の点の列であるとしよう。以下の条件を満たす点 $p \\in X$が存在するなら、列 $\\left\\{ p_{n} \\right\\}$は$p$に収束するconvergeと言い、$p_{n} \\rightarrow p$または$\\lim \\limits_{n\\to \\infty}p_{n}=p$と表される。\n$$ \\forall \\varepsilon \u0026gt;0,\\ \\exists N\\in \\mathbb{N}\\ \\mathrm{s.t}\\ n\\ge N \\implies d(p_{n},p)\u0026lt;\\varepsilon $$\n$\\left\\{ p_{n} \\right\\}$が収束しないなら発散するdivergeと言う。また、すべての$p_{n}$の集合を$\\left\\{ p_{n} \\right\\}$の値域rangeと言う。$\\left\\{ p_{n} \\right\\}$の値域が有界なら、列$\\left\\{ p_{n} \\right\\}$は有界boundedだと言われる。\n定理 $\\left\\{ p_{n} \\right\\}$を距離空間$(X,d)$の列とする。\n(a) $p_{n}\\to p$の必要十分条件は、すべての$p$の近傍が無限個を除くすべての$\\left\\{ p_{n} \\right\\}$の項を含むことである。\n(b) $p_{n} \\to p$であり、かつ$p_{n} \\to p^{\\prime}$ならば、$p=p^{\\prime}$である。\n(c) $\\left\\{ p_{n} \\right\\}$が収束すれば、有界である。\n(d) $E\\subset X$が与えられたとしよう。$p$が$E$の集積点であれば、$p=\\lim \\limits_{n \\to \\infty}p_{n}$を満たす$E$の列$\\left\\{ p_{n} \\right\\}$が存在する。また、$\\left\\{ p_{n} \\right\\}$が異なる点の集合であれば、逆も成り立つ。\n証明 (a) $(\\implies)$\n$p_{n} \\to p$と仮定しよう。任意の正の数$\\varepsilon \u0026gt;0$が与えられたとする。$V$を$p$の半径が$\\varepsilon$の近傍とする。近傍の定義により、次が成り立つ。\n$$ d(p,q)\u0026lt;\\varepsilon\\quad \\implies q\\in V $$\nしかし、仮定により、与えられた$\\varepsilon$に対して、以下の条件を満たす$N$が存在する。\n$$ \\forall n \\ge N,\\ d(p_{n},p) \u0026lt;\\varepsilon $$\n従って、有限個の点を除くすべての$p_{n}$が$V$に含まれる。\n$(\\impliedby)$\n$p$のすべての近傍が無限個を除くすべての$\\left\\{ p_{n} \\right\\}$を含むと仮定しよう。任意の正の数$\\varepsilon\u0026gt;0$が与えられたとする。$V$を$p$の半径が$\\varepsilon$の近傍とする。すると、仮定により、以下の条件を満たす$N$が存在する。\n$$ n \\ge N \\implies p_{n}\\in V $$\nなので、$V$は$p$の近傍なので、次が成り立つ。\n$$ \\forall n \\ge N,\\quad d(p_{n},p)\u0026lt;\\varepsilon $$\n従って、$p_{n}\\to p$。\n■\n(b) 任意の正の数$\\varepsilon \u0026gt;0$が与えられたとしよう。仮定により、以下の条件を満たす2つの正の数$N$、$N^{\\prime}$が存在する。\n$$ \\begin{align*} n\\ge N \u0026amp; \\implies d(p_{n},p) \u0026lt;\\frac{\\varepsilon}{2} \\\\ n\\ge N^{\\prime} \u0026amp; \\implies d(p_{n},p) \u0026lt;\\frac{\\varepsilon}{2} \\end{align*} $$\nすると、$n \\ge \\max(N,N^{\\prime})$に対して、以下の式が成り立つ。\n$$ d(p,p^{\\prime}) \\le d(p,p_{n}) + d(p_{n},p^{\\prime})\u0026lt;\\varepsilon $$\n$\\varepsilon$は任意の正の数なので、\n$$ d(p,p^{\\prime})=0 $$\nであり、距離の定義により、$p=p^{\\prime}$\n■\n(c) $\\left\\{ p_{n} \\right\\}$が$p$に収束すると仮定しよう。仮定により、以下の式が成り立つ正の数$N$が存在する。\n$$ n \\ge N \\implies d(p_{n},p)\u0026lt;1 $$\n今、\n$$ r=\\max \\left\\{ 1,\\ d(p_{1},p),\\ \\cdots,\\ d(p_{N},p) \\right\\} $$\nとしよう。すると、すべての$n$に対して、\n$$ d(p_{n},p)\\le r $$\nなので、$\\left\\{ p_{n} \\right\\}$は有界である。\n■\n(d) $(\\implies)$\n$E\\subset X$であり、かつ、$p$が$E$の集積点であるとしよう。集積点の定義により、各$n$に対して、\n$$ d(p_{n},p) \u0026lt; \\frac{1}{n} $$\nを満たす$p_{n}\\in E$が存在する。今、任意の正の数$\\varepsilon \u0026gt;0$と$N\\varepsilon\u0026gt;1$を満たす$N$が与えられたとする。すると、$ n \u0026gt;N$に対して、次が成り立つ。\n$$ d(p_{n},p)\u0026lt; \\frac{1}{n}\u0026lt;\\frac{N}{n}\\varepsilon\u0026lt;\\varepsilon $$\n従って、$\\left\\{ p_{n} \\right\\}$は$p$に収束する。\n$(\\impliedby)$\n$p=\\lim \\limits_{n\\to\\infty}p_{n}$を満たす$E$の異なる点の列$\\left\\{ p_{n} \\right\\}$が存在すると仮定しよう。すると、すべての正の数$\\varepsilon \u0026gt;0$に対して、\n$$ n \\ge N \\implies d(p_{n},p)\u0026lt; \\varepsilon $$\nを満たす$N$が存在する。この時、$V_{\\varepsilon}$を$p$の半径が$\\varepsilon$の近傍とする。すると、$V_{\\varepsilon}$は$p$ではない$p_{n} \\in E (n\\ge N)$を含むので、$p$は$E$の集積点である。\n■\nWalter Rudin, Principles of Mathmatical Analysis (第3版, 1976), p47-48, 55-58\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1713,"permalink":"https://freshrimpsushi.github.io/jp/posts/1713/","tags":null,"title":"距離空間における数列の収束"},{"categories":"거리공간","contents":"定義 $\\left\\{ p_{n} \\right\\}$を距離空間$(X,d)$の点の数列とする。全ての正の数$\\varepsilon$に対して\n$$ n\\ge N,\\ m\\ge N \\implies d(p_{n},p_{m})\u0026lt;\\varepsilon $$\nが成り立つ正の数$N$が存在するならば、$\\left\\{ p_{n} \\right\\}$をコーシー数列Cauchy sequenceという。\n距離空間$X$の全てのコーシー数列が$X$の点に収束するならば、$X$を完備空間という。\n説明 以下の定理により、全てのコンパクト距離空間とユークリッド空間は完備であることがわかる。\n定理 (a) 距離空間で、全ての収束する数列はコーシー数列である。\n(b) $X$がコンパクト距離空間で$\\left\\{ p_{n} \\right\\}$が$X$のコーシー数列であるとする。その場合、$\\left\\{ p_{n} \\right\\}$はある$p\\in X$に収束する。\n(c) $\\mathbb{R}^{k}$で、全てのコーシー数列は収束する。\n(a), (b) を一緒に言えば、「コンパクト距離空間で収束する数列とコーシー数列は同値である」となる。\n証明 (a) $p_{n} \\to p$かつ$\\varepsilon \u0026gt;0$が与えられたとする。それならば$\\forall n \\ge N,\\ d(p,p_{n})\u0026lt;\\varepsilon$を満たす$N$が存在する。したがって、次が成り立つ：\n$$ d(p_{n},p_{m}) \\le d(p_{n},p)+d(p,p_{m})\u0026lt;2\\varepsilon,\\quad \\forall m,n\\ge N $$\n従って、定義により$\\left\\{ p_{n} \\right\\}$はコーシー数列である。\n■\n(b) $\\left\\{ p_{n} \\right\\}$をコンパクト距離空間$X$のコーシー数列とする。そして、任意の自然数$N$に対して次のようであるとする：\n$$ E_{N}=\\left\\{ p_{N},p_{N+1},p_{N+2},\\cdots\\right\\} $$\nすると、次が成り立つ：\n$$ \\begin{equation} \\lim \\limits_{N\\to\\infty}\\mathrm{diam\\ }\\overline{E_{N}}=0 \\label{eq1} \\end{equation}$$\nまた、$\\overline{E_{N}}$はコンパクト空間$X$の閉じた部分集合なので、$\\overline{E_{N}}$はコンパクトである。さらに、次の式が成り立つことは自明である：\n$$ E_{N}\\supset E_{N+1} \\quad \\text{and} \\quad \\overline{E_{N}}\\supset \\overline{E_{N+1}} $$\n従って、上記の条件から、$\\forall N \\in \\mathbb{N},\\ p \\in \\overline{E_{N}}$を満たす唯一の$p \\in X$が存在する1ことがわかる。今、$\\varepsilon \u0026gt;0$が与えられたとする。それならば$\\eqref{eq1}$により、\n$$ N \\ge N_{0}\\implies \\mathrm{diam\\ }\\overline{E_{N}}\u0026lt; \\varepsilon $$\nが成り立つ$N_{0}$が存在する。しかし、$\\mathrm{diam\\ }\\overline{E}=\\mathrm{diam\\ }E$かつ$p \\in \\overline{E_{N}}$であるので、全ての$q \\in E_{N}$に対して$d(p,q)\u0026lt;\\varepsilon$が成り立つ。言い換えると、次のようになる：\n$$ n \\ge N_{0} \\implies d(p_{n},p)\u0026lt; \\varepsilon $$\nこれは$p_{n}\\to p$の定義なので、$\\lim \\limits_{n\\to\\infty} p_{n}=p$\n■\n(c) $\\left\\{ \\mathbf{x}_{n} \\right\\}$を$\\mathbb{R}^{k}$でのコーシー数列とする。$E_{N}$を証明 (b) と同じものとする。それならば、\n$$ \\mathrm{diam\\ } E_{N} \u0026lt;1 $$\nを満たす$N$を選んだとする。そして、\n$$ r=\\max \\left\\{ d(\\mathbf{x}_{N},\\mathbf{x}_{1}),\\ d(\\mathbf{x}_{N},\\mathbf{x}_{2}),\\ \\cdots,\\ d(\\mathbf{x}_{N},\\mathbf{x}_{N-1}),\\ 1 \\right\\} $$\nだとするなら、$\\forall m,n \\in \\mathbb{N},\\ d(\\mathbf{x}_{n},\\mathbf{x}_{m}) \u0026lt;r$のため、$\\left\\{ \\mathbf{x}_{n} \\right\\}$は有界である。したがって、$\\overline{ \\left\\{ \\mathbf{x}_{n} \\right\\}}$は閉じていて有界な$\\mathbb{R}^{k}$の部分集合なので、コンパクトである。従って、$\\left\\{ \\mathbf{x}_{n} \\right\\}$はコンパクト空間のコーシー数列であり、したがって**（b）**により、$\\left\\{ \\mathbf{x}_{n} \\right\\}$は収束する。\n■\n定理(b)参照\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1718,"permalink":"https://freshrimpsushi.github.io/jp/posts/1718/","tags":null,"title":"距離空間内のコーシー数列と完備性"},{"categories":"함수","contents":"定義 次のように定義される関数 $\\zeta : \\mathbb{C} \\setminus \\left\\{ 1 \\right\\} \\to \\mathbb{C}$ をリーマン ゼータ関数Riemann zeta Function\u0026lt;/supという。 $$ \\zeta (s) := \\sum_{n \\in \\mathbb{N}} n^{-s} = \\prod_{p : \\text{prime}} \\left( 1- {p^{-s}} \\right)^{-1} $$\n関連定理 [0] ラマヌジャンの和: $\\displaystyle \\sum_{n \\in \\mathbb{N}} x^{n-1} = {{ 1 } \\over { 1-x }}$ が $|x| = 1$ でも成り立つと受け入れるなら $$ \\zeta (0) = 1 + 1 + 1 + 1 + \\cdots = - {{ 1 } \\over { 2 }} $$\n[1] オーレムの証明: $\\zeta (1)$ が定義されない理由は次の通りです。 $$ \\zeta (1) = \\sum_{n \\in \\mathbb{N}} {{ 1 } \\over { n }} = \\infty $$\n[2] オイラーの証明: $$ \\zeta (2) = \\sum_{n \\in \\mathbb{N}} {{ 1 } \\over { n^{2} }} = {{ \\pi^{2} } \\over { 6 }} $$\n[a] ガンマ関数との関係: $\\text{Re} (s) \u0026gt; 1$ ならば $$ \\zeta (s) \\Gamma (s) = \\mathcal{M} \\left[ {{ 1 } \\over { e^{x} - 1 }} \\right] (s) = \\int_{0}^{\\infty} {{ x^{s-1} } \\over { e^{x} - 1 }} dx $$\n[b] ディリクレのエータ関数との関係: $$ \\eta (s) := \\sum_{n \\in \\mathbb{N}} (-1)^{n-1} n^{-s} $$\n説明 ゼータ関数は、実部が$1$より大きい複素数、すなわち$\\text{Re} (s) \u0026gt; 1$である$s$内で収束し、ガンマ関数との関係を持っている。特に整数論と複素解析での関心の対象であり、その悪名高いリーマン予想の主役でもある。\n","id":1626,"permalink":"https://freshrimpsushi.github.io/jp/posts/1626/","tags":null,"title":"リーマンゼータ関数"},{"categories":"거리공간","contents":"定義 $a_i, b_i \\in \\mathbb{R} (1 \\le i \\le k)$に対して、集合$I=[a_{1}, b_{1}] \\times [a_{2}, b_{2}] \\times \\cdots \\times [a_{k}, b_{k}]$を**$k$-セル**と言う。ここで$\\times$は集合のデカルト積である。\n定理1 $\\mathbb{R}$上の閉区間の数列$\\left\\{ I_{n} \\right\\}$が$I_{n} \\supset I_{n+1}\\ (n=1,2,\\cdots)$を満たすとする。すると以下が成立する。\n$$ \\bigcap_{i=1}^{\\infty}I_{n}\\ne \\varnothing $$\n証明 $I_{n}=[a_{n}, b_{n}]$とする。そして$E=\\left\\{ a_{n} : n=1,2,\\cdots \\right\\}$とする。すると$E\\ne \\varnothing$であり、$b_{1}$1によって上限がある。今$x=\\sup E$とする。そして任意の二つの正数$m$、$n$に対して\n$$ a_{n} \\le a_{m+n} \\le b_{m+n} \\le b_{m} $$\nが成立するので、すべての$n$に対して$x\\le b_{n}$である。また$x$が$E$の上限であるため、すべての$n$に対して$a_{n} \\le x$であることは明らかである。したがって、すべての$n$に対して$a_{n}\\le x \\le b_{n}$なので、$x\\in I_{n}\\ \\forall n$である。したがって\n$$ x\\in \\bigcap _{i=1}^{n}I_{n} $$\n■\n定理2 $\\left\\{ I_{n} \\right\\}$が$I_{n}\\supset I_{n+1}(n=1,2,\\cdots)$を満たす$k-$セルの数列であるとする。すると$\\bigcap_{i=1}^{n}I_{n}\\ne\\varnothing$である。\n定理2は定理1を$\\mathbb{R}^{k}$に拡張したものである。\n証明 $I_{n}$を以下のようにする。\n$$ I_{n}=\\left\\{ \\mathbf{x}=(x_{1},\\cdots,x_{k}) : a_{n,j} \\le x_{j} \\le b_{nj},\\quad(1\\le j \\le k;\\ n=1,2,\\cdots) \\right\\} $$\nすなわち$I_{n}=I_{n,1}\\times \\cdots\\times I_{n,k}\\ (I_{n,j}=[a_{n,j},b_{n,j}])$である。すると定理1によって、それぞれの$I_{n,j}$に対して$x_{j}^{\\ast}\\in I_{n,j} \\ (a_{n,j} \\le x_{j}^{\\ast} \\le b_{n,j})$が存在する。したがって\n$$ \\mathbf{x^{\\ast}} =(x_{1}^{\\ast},\\cdots ,x_{k}^{\\ast})\\in I_{n} ,\\quad (n=1,2,\\cdots) $$\n■\n定理3 すべての$k-$セルはコンパクトである。\n証明 $I$を以下のような任意の$k$-セルとする。\n$$ I=I^{1}\\times \\cdots \\times I^{k}=[a_{1},b_{1}]\\times \\cdots \\times [a_{k},b_{k}] $$\nそして以下のようにする。\n$$ \\mathbf{x}=(x_{1},\\cdots,x_{k}) \\quad \\text{and} \\quad a_{j} \\le x_{j} \\le b_{j}(1\\le j \\le k) $$\n今$\\delta$を以下のようにする。\n$$ \\delta =\\left( \\sum \\limits_{j=1}^{k}(b_{j})-a_{j})^{2} \\right)^{{\\textstyle \\frac{1}{2}}}=|\\mathbf{b}-\\mathbf{a}| $$\nこのとき$\\mathbf{a}=(a_{1},\\cdots,a_{n})$、$\\mathbf{b}=(b_{1},\\cdots,b_{n})$である。すると$\\delta$は$\\mathbf{b}$と$\\mathbf{a}$の間の距離と同じである。したがって\n$$ |\\mathbf{x}-\\mathbf{y}| \\le \\delta \\quad \\forall \\mathbf{x},\\mathbf{y}\\in I $$\nが成立する。今から証明が本格的に始まるが、背理法を使用する。つまり$k-$セルがコンパクトでないと仮定する。するとコンパクトの定義によって、$I$のいくつかのオープンカバー$\\left\\{ O_{\\alpha} \\right\\}$が有限部分カバーを持たないと仮定することと同じである。$c_{j}=(a_{j}+b_{j})/2$とする。すると$c_{j}$を使って各$I^{j}$を$[a_{j},c_{j}]$、$[c_{j},b_{j}]$に分けて$2^{k}$個の$1-$セルを作ることができる。これらの和集合は当然$I$になり、仮定によりこれらの中で少なくとも一つは$\\left\\{ O_{\\alpha} \\right\\}$のいくつかの有限部分カバーでカバーされなければならない。そのセルを$I_{1}$とする。すると$I$から$I_{1}$を選んだのと同じ方法で続けて区間を選ぶと、以下の三つの規則を満たす数列$\\left\\{ I_{n} \\right\\}$を得ることができる。\n$(\\mathrm{i})$ $I\\supset I_{1} \\supset I_{2}\\supset \\cdots$\n$(\\mathrm{ii})$ それぞれの$I_{n}$は$\\left\\{ O_{\\alpha} \\right\\}$のいくつかの有限部分カバーでもカバーされない。\n$(\\mathrm{iii})$ $|\\mathbf{x}-\\mathbf{y}|\\le 2^{-n}\\delta,\\quad \\forall \\mathbf{x},\\mathbf{y}\\in I_{n}$\nすると$(\\mathrm{i})$と定理2によって、すべての$n$に対して$\\mathbf{x}^{\\ast}\\in I_{n}$である$\\mathbf{x}^{\\ast}$が存在する。すると$\\left\\{ O_{\\alpha} \\right\\}$が$I$のオープンカバーであるため、いくつかの$\\alpha$に対して$\\mathbf{x}^{\\ast\n}\\in O_{\\alpha}$が成立する。$O_{\\alpha}$が開集合であるため、$|\\mathbf{x}^{\\ast}-\\mathbf{y}|\u0026lt;r \\implies \\mathbf{y}\\in O_{\\alpha}$を満たす$r\u0026gt;0$が存在する。一方で、$n$を十分大きくして$2^{-n}\\delta\u0026lt;r$を満たすようにすることができる。すると$(\\mathrm{iii})$によって$I_{n}\\subset O_{\\alpha}$である。しかし、これは$(\\mathrm{ii})$と矛盾するので、仮定が間違っていることがわかる。したがって、すべての$k-$セルはコンパクトである。\n■\n上記の事実から以下の有用な定理を証明することができる。\nユークリッド空間でコンパクトである同値条件 実数（または複素数）空間の部分集合$E\\subset \\mathbb{R}^{k}(\\mathrm{or}\\ \\mathbb{C}^{k})$に対して、以下の三つの命題は同値である。\n(a) $E$は閉じており有界である。\n(b) $E$はコンパクトである。\n(c) $E$のすべての無限部分集合は集積点 $p \\in E$を持つ。\nここで**(a)、(b)が同値であることはハイネ・ボレルの定理と呼ばれる。(c)を満たす$E$に対して\u0026rsquo;$E$は\u0026rsquo;集積点コンパクトである\u0026rsquo;または\u0026rsquo;$E$は\u0026rsquo;ボルツァーノ-ワイエルシュトラスの性質を持つ\u0026rsquo;と言う。(b)と(c)**が同値であることは距離空間では成立するが、位相空間では一般的には成立しない。\n証明 (a) $\\implies$ (b)\n**(a)**を仮定すると、$E \\subset I$を満たす$k-$セル$I$が存在する。すると$I$がコンパクトであり、コンパクト集合の閉じた部分集合はコンパクトであるため、$E$はコンパクトである。\n(b) $\\implies$ (c)\n背理法で証明する。\n$S$がコンパクト集合$E$の無限部分集合であるとする。そして$S$の集積点が存在しないと仮定する。するとすべての$p\\in E$は、せいぜい$S$の点をただ一つだけ含む$p$の近傍$N_{p}$を持つ。$p \\in S$の場合、そのただ一つの点は$p$である。そしてこれは、オープンカバー$\\left\\{ N_{p} \\right\\}$が$S$をカバーする有限部分カバーを持たないことを意味する。$S \\subset E$なので、同様に$E$をカバーする有限部分カバーも存在しない。これは$E$がコンパクトであるという仮定に矛盾するので、$S$は集積点$p \\in E$を持つ。\n(c) $\\implies$ (a)\n背理法で証明する。\npart 1. $E$は有界である\n$E$は有界ではないと仮定してみる。すると$E$は以下の不等式を満たす点$\\mathbf{x}_{n}$を含む。\n$$ |\\mathbf{x}_{n}| \u0026gt;n\\quad (n=1,2,\\cdots) $$\n今$S=\\left\\{ \\mathbf{x}_{n} : n=1,2,\\cdots\\right\\}$とする。すると$S$は無限集合であり、$\\mathbb{R}^{k}$で集積点を持たないことは明らかである。これは$(c)$に対する矛盾である。したがって$E$は有界である。\npart 2. $E$は閉じている。\n$E$は閉じていないと仮定してみる。すると定義により$E$に含まれない$E$の集積点$\\mathbf{x}_{0}$が存在する。今$n=1,2,\\cdots$に対して$\\mathbf{x}_{n} \\in E$を以下の条件を満たす点とする。\n$$ \\left|\\mathbf{x}_{n}-\\mathbf{x}_{0} \\right| \u0026lt; {\\textstyle \\frac{1}{n}} $$\nそしてこのような$\\mathbf{x}_{n}$の集合を$S$とする。すると$S$は無限集合であり、$\\mathbf{x}_{0}$を集積点として持つ。今$\\mathbf{x}_{0}$が$S$の唯一の集積点であれば、$\\mathbf{x}_{0}\\notin E$であるため$(c)$に矛盾し、$E$は閉じていることがわかる。それでは$\\mathbf{y} \\ne \\mathbf{x}_{0}$である$\\mathbf{y} \\in \\mathbb{R}^{k}$を考える。すると\n$$ \\begin{align*} \\left| \\mathbf{x}_{n} - \\mathbf{y} \\right| \u0026amp; \\ge \\left|\\mathbf{x}_{0} - \\mathbf{y} \\right| - \\left|\\mathbf{x}_{n}-\\mathbf{x}_{0} \\right| \\\\ \u0026amp; \\ge \\left| \\mathbf{x}_{0} - \\mathbf{y} \\right| -\\frac{1}{n} \\end{align*} $$\nこのとき十分に大きな$n$に対して以下の式が成立する。\n$$ \\begin{equation} \\left| \\mathbf{x}_{n} - \\mathbf{y} \\right| \\ge \\left| \\mathbf{x}_{0}- \\mathbf{y} \\right|-\\frac{1}{n} \\ge \\frac{1}{2}\\left|\\mathbf{x}_{0}-\\mathbf{y} \\right| \\label{eq1} \\end{equation} $$\nまた$\\mathbf{x}_{n\n}$の条件により、$n$が大きくなるにつれて$\\mathbf{x}_{n}$は$\\mathbf{x}_{0}$に近づく。この事実と$\\eqref{eq1}$により、$n$を続けて大きくすると$\\mathbf{y}$を含まない$\\mathbf{y}$の近傍を見つけることができる。したがって$\\mathbf{y}$は$S$の集積点ではなく、$\\mathbf{x}_{0}$が$S$の唯一の集積点であることから$(c)$に矛盾し、$E$は閉じている。\n■\nボルツァーノ-ワイエルシュトラスの定理 $\\mathbb{R}^{k}$のすべての有界な無限部分集合は集積点$p \\in \\mathbb{R}^{k}$を持つ。\n証明 $E$を$\\mathbb{R}^{k}$の有界な無限部分集合とする。すると$E$が有界であるため、$E \\subset I$を満たす$k-$セル$I$が存在する。$k-$セルはコンパクトであるため、$I$はコンパクトである。すると$I$がコンパクトである同値条件$(b)\\implies (c)$によって、$E$は集積点$p \\in I \\subset \\mathbb{R}^{k}$を持つ。\n■\n任意の$b_{n}$で問題ない。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1711,"permalink":"https://freshrimpsushi.github.io/jp/posts/1711/","tags":null,"title":"すべてのk-cellはコンパクトである：ユークリッド空間でコンパクトである同値条件。"},{"categories":"르벡공간","contents":"定義1 $a \\in \\mathbb{R}$に対して、以下のように定義される$T_{a} : L^{2} \\to L^{2}$をトランスレーションtranslation, 平行移動と言う。 $$ \\left( T_{a} f \\right) (x) := f(x-a) $$\n$b \\in \\mathbb{R}$に対して、以下のように定義される$E_{b} : L^{2} \\to L^{2}$をモジュレーションmodulation, 変調と言う。 $$ \\left( E_{b} f \\right) (x) := e^{2 \\pi i b x} f(x) $$\n$c \u0026gt; 0$に対して、以下のように定義される$D_{c} : L^{2} \\to L^{2}$をダイレーションdilation, 膨張と言う。 $$ \\left( D_{c} f \\right) (x) := {{ 1 } \\over { \\sqrt{c} }} f \\left( {{ x } \\over { c }} \\right) $$\n説明 上記の線型演算子は$L^{2}$空間でよく使われるものだ。韓国語ではそれぞれ平行移動(translation)、変調(modulation)、膨張(dilation)と翻訳されるけれど、数式的に理解するには英語で直接読む方が楽だろう。\nモジュレーションで掛けられる$e^{2 \\pi i b x}$は文字通り抽象化された回転だ。\nダイレーションで掛けられる$\\displaystyle {{ 1 } \\over { \\sqrt{c} }}$は、ノルム$\\left\\| \\cdot \\right\\|_{2}$に合わせるためにルートが掛けられているとも見れる。特に$c = 1/2$に対して、以下のように定義される$D$は特別な役割をすることもある。\n$$ ( D f ) (x) := \\sqrt{2} f (2x) $$\n便宜上、$D$は$j \\in \\mathbb{Z}$に関して、以下のように書かれる。\n$$ ( D^{j} f ) (x) := \\sqrt{2}^{j} f \\left( 2^{j} x \\right) $$\n性質 全ての$a, b \\in \\mathbb{R}$、$c \u0026gt; 0$及び$f,g \\in L^{1}$に対して、\n$T_{a} , E_{b}, D_{c}$は有界線型演算子だ。\n逆演算子：$T_{a} , E_{b}, D_{c}$はユニタリだ。\n交換関係:\n$$ (T_{a} E_{b} f ) (x) = e^{- 2 \\pi i b a} (E_{b} T_{a} f ) (x) \\\\ (T_{a} D_{c} f ) (x) = (D_{c} T_{a/c} f ) (x) \\\\ (D_{c} E_{b} f ) (x) = (E_{b/c} D_{c} f ) (x) $$\nフーリエ変換との関係:\n$$ \\mathcal{F} T_{a} = E_{-a} \\mathcal{F} \\\\ \\mathcal{F} E_{b} = T_{b} \\mathcal{F} \\\\ \\mathcal{F} D_{c} = D_{1/c} \\mathcal{F} $$\n$D$に関しては、上の定理の系として$j, k \\in \\mathbb{Z}$について、以下を得ることができる。\n$$ T_{k} D^{j} = D^{j} T_{2^{j} k } \\\\ D^{j} T_{k} = T_{2^{-j}k} D^{j} \\\\ \\left( D^{j} \\right)^{ \\ast } = D^{-j} $$\n証明 1. Part 1. 線型\n全ての$f,g \\in L^{2}$及び$\\alpha , \\beta \\in \\mathbb{C}$に対して、\n$$ \\begin{align*} T_{a} \\left( \\alpha f + \\beta g \\right)(x) =\u0026amp; \\left( \\alpha f + \\beta g \\right)(x-a) \\\\ =\u0026amp; \\alpha f (x-a) + \\beta g (x-a) \\\\ =\u0026amp; \\alpha T_{a} f (x) + \\beta T_{a} g (x) \\end{align*} $$\nだから$T_{a}$はリニアだ。\n$$ \\begin{align*} E_{b} \\left( \\alpha f + \\beta g \\right)(x) =\u0026amp; e^{ 2 \\pi i b x } \\left( \\alpha f + \\beta g \\right)(x) \\\\ =\u0026amp; \\alpha e^{ 2 \\pi i b x } f (x) + \\beta e^{ 2 \\pi i b x } g (x) \\\\ =\u0026amp; \\alpha E_{b} f (x) + \\beta E_{b} g (x) \\end{align*} $$\nだから$E_{b}$はリニアだ。\n$$ \\begin{align*} D_{c} \\left( \\alpha f + \\beta g \\right)(x) =\u0026amp; {{ 1 } \\over { \\sqrt{c} }} \\left( \\alpha f + \\beta g \\right) \\left( {{ x } \\over { c }} \\right) \\\\ =\u0026amp; \\alpha {{ 1 } \\over { \\sqrt{c} }} f (x) + \\beta {{ 1 } \\over { \\sqrt{c} }} g (x) \\\\ =\u0026amp; \\alpha D_{c} f (x) + \\beta D_{c} g (x) \\end{align*} $$\nだから$D_{c}$はリニアだ。\nPart 2. 有界\n$t := x - a$のように置換すると、\n$$ \\begin{align*} \\left\\| T_{a} f \\right\\|_{2} =\u0026amp; \\int_{-\\infty}^{\\infty} \\left| T_{a} f \\left( x \\right) \\right|^{2} dx \\\\ =\u0026amp; \\int_{-\\infty}^{\\infty} \\left| f \\left( x - a \\right) \\right|^{2} dx \\\\ =\u0026amp; \\int_{-\\infty}^{\\infty} \\left| f \\left( t \\right) \\right|^{2} dt \\\\ =\u0026amp; \\left\\| f \\right\\|_{2} \\end{align*} $$\nだから$T_{a}$はバウンデッドだ。$\\left| e^{2 \\pi i b x } \\right| =1$なので、\n$$ \\begin{align*} \\left\\| E_{b} f \\right\\|_{2} =\u0026amp; \\int_{-\\infty}^{\\infty} \\left| E_{b} f \\left( x \\right) \\right|^{2} dx \\\\ =\u0026amp; \\int_{-\\infty}^{\\infty} \\left| e^{2 \\pi i b x } f \\left( x \\right) \\right|^{2} dx \\\\ =\u0026amp; \\int_{-\\infty}^{\\infty} 1 \\cdot \\left| f \\left( t \\right) \\right|^{2} dt \\\\ =\u0026amp; \\left\\| f \\right\\|_{2} \\end{align*} $$\nだから$E_{b}$はバウンデッドだ。$t := x/c$のように置換すると、\n$$ \\begin{align*} \\left\\| D_{c} f \\right\\|_{2} =\u0026amp; \\int_{-\\infty}^{\\infty} \\left| D_{c} f \\left( x \\right) \\right|^{2} dx \\\\ =\u0026amp; \\int_{-\\infty}^{\\infty} \\left| {{ 1 } \\over { \\sqrt{c} }} f \\left( {{ x } \\over { c }} \\right) \\right|^{2} dx \\\\ =\u0026amp; \\int_{-\\infty}^{\\infty} {{ 1 } \\over { c }} \\left| f \\left( t \\right) \\right|^{2} c dt \\\\ =\u0026amp; \\int_{-\\infty}^{\\infty} \\left| f \\left( t \\right) \\right|^{2} dt \\\\ =\u0026amp; \\left\\| f \\right\\|_{2} \\end{align*} $$\nだから$D_{c}$はバウンデッドだ。\n■\nOle Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010), p120-122\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1616,"permalink":"https://freshrimpsushi.github.io/jp/posts/1616/","tags":null,"title":"L2空間における変換：平行移動、変調、拡大"},{"categories":"거리공간","contents":"Definition Open Cover Given a metric space $(X,d)$ and a subset $E\\subset X$, a set $\\left\\{ O_{\\alpha} \\right\\}$ of open sets that satisfies the following equation is called an open coveropen cover of $E$.\n$$ E\\subset \\bigcup _{\\alpha} O_{\\alpha} $$\nA subset of an open cover is called a subcover. In particular, a subcover with a finite number of elements is called a finite subcover.\nCompactness Let\u0026rsquo;s assume we have a subset $K$ of a metric space $X$. If every open cover of $K$ has a finite subcover, then $K$ is said to be compactcompact. In other words, if we can still have an open cover by selecting a finite number of open sets, then $K$ is called compact. Expressing this condition with an equation, if for some $\\alpha_{1},\\cdots ,\\alpha_{n}$\n$$ K\\subset O_{\\alpha_{1}}\\cup \\cdots O_{\\alpha_{n}} $$\nis satisfied, then $K$ is compact.\nExplanation The importance of compactness comes from whether a given space retains or loses the property of being compact depending on what the entire space is considered to be. That is to say, compactness is an inherent quality of the set itself. Without going too far, even when observing the concept of openness, there is no guarantee that the property of being open is preserved when the entire space is expanded, hence the term relatively open exists. As one continues to study, it becomes apparent that the condition of being compact plays an important role in various theorems. Compactness is a property bestowed upon a set regardless of the entire space, as confirmed by the theorem below. First, we will use the term compact in $X$ when $K\\subset X$ is compact with respect to the entire space $X$.\nTheorem Consider two metric spaces $X$, $Y$ and suppose $K\\subset Y \\subset X$. Then the following two propositions are equivalent.\n(a) $K$ is compact in $X$.\n(b) $K$ is compact in $Y$.\nProof Lemma\nLet two metric spaces $X$, $Y$ be given, and suppose $E \\subset Y \\subset X$. Then the two following propositions are equivalent:$(d)$ $E$ is relatively open with respect to $Y$.$(e)$ For some open set $O_{X}$ of $X$, $E=Y \\cap O_{X}$ holds.\n(a) $\\Longrightarrow$ (b)\nAssume that $K$ is compact in $X$. Let $\\left\\{ O_{\\alpha}^{Y} \\right\\}$ be a set of open sets in $Y$ that satisfies $K\\subset \\bigcup_{\\alpha} O_{\\alpha}^{Y}$. In other words, assume $\\left\\{ O_{\\alpha}^{Y} \\right\\}$ to be any open cover of $K$ with respect to $Y$. Then, by the lemma,\n$$ O_{\\alpha}^{Y}=Y\\cap O_{\\alpha}^{X},\\quad \\forall \\alpha $$\nan open set $O_{\\alpha}^{X}$ in $X$ exists that satisfies the equation. Then $\\left\\{ O_{\\alpha}^{X} \\right\\}$ forms an open cover of $K$ with respect to $X$. Thus, by assumption, for some $\\alpha_{1},\\cdots,\\alpha_{n}$, the following equation holds:\n$$ K \\subset O_{\\alpha_{1}}^{X}\\cup\\cdots \\cup O_{\\alpha_{n}}^{X} $$\nHowever, since $K\\subset Y$, the following is true:\n$$ \\begin{align*} K \u0026amp; \\subset Y \\cap (O_{\\alpha_{1}}^{X}\\cup\\cdots \\cup O_{\\alpha_{n}}^{X}) \\\\ \u0026amp;= (Y \\cap O _{\\alpha_{1}}^{X})\\cup\\cdots \\cup(Y \\cap O_{\\alpha_{n}}^{X}) \\\\ \u0026amp;= O_{\\alpha_{1}}^{Y}\\cup\\cdots \\cup O_{\\alpha_{n}}^{Y} \\end{align*} $$\nTherefore, any arbitrary open cover $\\left\\{ O_{\\alpha}^{Y} \\right\\}$ of $K$ with respect to $Y$ has a finite subcover satisfying\n$$ K \\subset O_{\\alpha_{1}}^{Y}\\cup\\cdots \\cup O_{\\alpha_{n}}^{Y} $$\nhence, $K$ is compact in $Y$.\n(a) $\\Longleftarrow$ (b)\nSuppose that $K$ is compact in $Y$. Let $\\left\\{ O_{\\alpha}^{X} \\right\\}$ be a set of open sets in $X$ that satisfies $K\\subset \\bigcup_{\\alpha} O_{\\alpha}^{X}$. In other words, take $\\left\\{ O_{\\alpha}^{X} \\right\\}$ as any open cover of $K$ with respect to $X$. Then, set $O_{\\alpha}^{Y}$ as follows:\n$$ O_{\\alpha}^{Y}=Y\\cap O_{\\alpha}^{X},\\quad \\forall \\alpha $$\nBy the lemma, $O_{\\alpha}^{Y}$ becomes an open set in $Y$. Therefore, $\\left\\{ O_{\\alpha}^{Y} \\right\\}$ forms an open cover of $K$. Then, by assumption, for some $\\alpha_{1},\\cdots,\\alpha_{n}$, the following equation holds:\n$$ K \\subset O_{\\alpha_{1}}^{Y}\\cup \\cdots \\cup O_{\\alpha_{n}}^{Y} $$\nHowever, since for each $\\alpha$, $O_{\\alpha}^{Y} \\subset O_{\\alpha}^{X}$ is true, the following holds:\n$$ K\\subset O_{\\alpha_{1}}^{X}\\cup \\cdots \\cup O_{\\alpha_{n}}^{X} $$\nTherefore, as every arbitrary open cover always has a finite subcover, $K$ is compact in $X$.\n{{qed}}\nSee Also Compactness in Topological Spaces === ```markdown ## 定義 ### オープンカバー [距離空間](../381) $(X,d)$と部分集合 $E\\subset X$が与えられたとする。以下の式を満たす[開集合](../1700)の集合 $\\left\\\\{ O\\_{\\alpha} \\right\\\\}$を $E$の**オープンカバー**\u0026lt;sup\u0026gt;open cover\u0026lt;/sup\u0026gt; と言う。 $$\rE\\subset \\bigcup \\_{\\alpha} O\\_{\\alpha}\r$$ --- オープンカバーの部分集合を部分カバーと言う。特に、要素が有限個の部分カバーを有限部分カバーと言う。 ### コンパクト 距離空間 $X$の部分集合 $K$が与えられたとする。もし $K$の全てのオープンカバーに有限部分カバーが存在すれば、$K$は**コンパクト**\u0026lt;sup\u0026gt;compact\u0026lt;/sup\u0026gt;であるという。言い換えると、有限個の集合だけ選んでもまだオープンカバーであれば、$K$をコンパクトと言う。式で表すと、何らかの $\\alpha\\_{1},\\cdots ,\\alpha\\_{n}$に対して $$\rK\\subset O\\_{\\alpha\\_{1}}\\cup \\cdots O\\_{\\alpha\\_{n}}\r$$ が満たされれば、$K$はコンパクトである。 ## 説明 コンパクトが重要である理由は、全体空間を何にするかによって、その集合がコンパクトという性質を得たり失ったりするからである。つまり、**コンパクトはその集合が持つ固有の性質**という意味である。[開放](../1700)という概念を見ても、全体空間を拡大する時に、開かれているという性質が保持される保証がないために、[相対的に開かれている](../1703)という表現がある。学び続けると、[コンパクトという条件が様々な定理で重要な役割を果たす](../1728)ことが分かる。コンパクトは全体空間と無関係に集合に与えられる性質であると、以下の定理を通じて確認できる。まず、$K\\subset X$が全体空間 $X$に対してコンパクトである時、$X$においてコンパクトという表現を使う。 ## 定理 二つの距離空間 $X$、$Y$について $K\\subset Y \\subset X$とする。すると、以下の二つの命題は同値である。 **(a)** $K$は $X$でコンパクトである。 **(b)** $K$は $Y$でコンパクトである。 ## 証明 \u0026gt; [補助定理](../1703) \u0026gt; \u0026gt; 二つの距離空間 $X$、$Y$が与えられたとする。そして $E \\subset Y \\subset X$とする。すると、以下の二つの命題は同値である。$(d)$ $E$が $Y$に対して[相対的に開かれている。](../1703)$(e)$ $X$のある開集合 $O\\_{X}$について、$E=Y \\cap O\\_{X}$が成り立つ。 - **(a)** $\\Longrightarrow$ **(b)** $K$が $X$でコンパクトであると仮定する。$\\left\\\\{ O\\_{\\alpha}^{Y} \\right\\\\}$を $Y$で開かれている集合の集合とし、$K\\subset \\bigcup\\_{\\alpha} O\\_{\\alpha}^{Y}$を満たすとする。つまり、$\\left\\\\{ O\\_{\\alpha}^{Y} \\right\\\\}$を $K$の$Y$に対する任意のオープンカバーとするわけである。すると、補助定理によって $$\rO\\_{\\alpha}^{Y}=Y\\cap O\\_{\\alpha}^{X},\\quad \\forall \\alpha\r$$ $X$で開かれている集合 $O\\_{\\alpha}^{X}$が存在する。すると、$\\left\\\\{ O\\_{\\alpha}^{X} \\right\\\\}$は $K$の$X$に対するオープンカバーとなる。すると、仮定により、何らかの $\\alpha\\_{1},\\cdots,\\alpha\\_{n}$に対して以下の式が成立する。 $$\rK \\subset O\\_{\\alpha\\_{1}}^{X}\\cup\\cdots \\cup O\\_{\\alpha\\_{n}}^{X}\r$$ しかし、$K\\subset Y$なので、次が成立する。 $$\r\\begin{align*}\rK \u0026amp; \\subset Y \\cap (O\\_{\\alpha\\_{1}}^{X}\\cup\\cdots \\cup O\\_{\\alpha\\_{n}}^{X}) \\\\\\ \u0026amp;= (Y \\cap O \\_{\\alpha\\_{1}}^{X})\\cup\\cdots \\cup(Y \\cap O\\_{\\alpha\\_{n}}^{X}) \\\\\\ \u0026amp;= O\\_{\\alpha\\_{1}}^{Y}\\cup\\cdots \\cup O\\_{\\alpha\\_{n}}^{Y}\r\\end{align*}\r$$ したがって、$K$の $Y$に対する任意のオープンカバー $\\left\\\\{ O\\_{\\alpha}^{Y} \\right\\\\}$の有限部分カバーが $$\rK \\subset O\\_{\\alpha\\_{1}}^{Y}\\cup\\cdots \\cup O\\_{\\alpha\\_{n}}^{Y}\r$$ を満たすので、$K$は $Y$でコンパクトである。 - **(a)** $\\Longleftarrow$ **(b)** $K$が $Y$でコンパクトであると仮定する。$\\left\\\\{ O\\_{\\alpha}^{X} \\right\\\\}$を $K\\subset \\bigcup\\_{\\alpha} O\\_{\\alpha}^{X}$を満たす $X$の開集合の集合とする。つまり、$\\left\\\\{ O\\_{\\alpha}^{X} \\right\\\\}$を $K$の$X$に対する任意のオープンカバーとして選ぶわけである。そして、$O\\_{\\alpha}^{Y}$を以下のように設定する。 $$\rO\\_{\\alpha}^{Y}=Y\\cap O\\_{\\alpha}^{X},\\quad \\forall \\alpha\r$$ すると、補助定理によって $O\\_{\\alpha}^{Y}$は $Y$で開かれている集合になる。したがって、$\\left\\\\{ O\\_{\\alpha}^{Y} \\right\\\\}$は $K$のオープンカバーになる。すると、仮定により、何らかの $\\alpha\\_{1},\\cdots,\\alpha\\_{n}$に対して以下の式が成立する。 $$\rK \\subset O\\_{\\alpha\\_{1}}^{Y}\\cup \\cdots \\cup O\\_{\\alpha\\_{n}}^{Y}\r$$ しかし、各 $\\alpha$に対して $O\\_{\\alpha}^{Y} \\subset O\\_{\\alpha}^{X}$なので、次が成立する。 $$\rK\\subset O\\_{\\alpha\\_{1}}^{X}\\cup \\cdots \\cup O\\_{\\alpha\\_{n}}^{X}\r$$ したがって、$K$の任意のオープンカバーが常に有限部分カバーを持つので、$K$は $X$でコンパクトである。 ■\n## 同時に見る - [位相空間でのコンパクト性](../489)[](../489) ","id":1705,"permalink":"https://freshrimpsushi.github.io/jp/posts/1705/","tags":null,"title":"距離空間におけるコンパクト性"},{"categories":"거리공간","contents":"定理1 距離空間 $X$において、コンパクト集合 $K$の[($X$に対して)閉じている]部分集合はコンパクトだ。\n証明 距離空間 $X$で $F\\subset K \\subset X$とし、$F$を$X$で閉集合、$K$をコンパクト集合と仮定する。そして、$\\left\\{ V_{\\alpha}\\right\\}$を$F$の任意の開被覆とする。ここに $F^{c}$を加えて$\\Omega=\\left\\{ V_\\alpha \\right\\}\\cup \\left\\{ F^{c} \\right\\}$とする。すると、$\\Omega$が$K$の開被覆になる。$K$はコンパクトと仮定されているので、$\\Omega$のある有限部分被覆$\\Phi$に関して以下が成り立つ。\n$$ F \\subset K\\subset \\Phi $$\n2つのケースに分けて考えよう。\ncase 1. $F^{c} \\notin \\Phi$\nこの場合、$\\Phi$は$\\left\\{ V_{\\alpha} \\right\\}$の有限部分被覆なので$F$はコンパクトだ。\ncase 2. $F^{c} \\in \\Phi$\n$\\Psi=\\Omega \\setminus \\left\\{ F^{c} \\right\\}$とすると、$F^{c}\\cap F=\\varnothing$より、まだ$F\\subset \\Psi$が成立する。したがって、$\\Psi$は$\\left\\{ V_{\\alpha} \\right\\}$の有限部分集合なので$F$はコンパクトだ。\n■\n結論 距離空間 $X$で、$F$が閉じていて$K$がコンパクトだとする。それならば、$F\\cap K$はコンパクトだ。\n証明 $F \\cap K$は閉集合の交差なので閉集合だ。だから、コンパクト集合$K$の閉じた部分集合で、それ故にコンパクトだ。\n■\nWalter Rudin, Principles of Mathmatical Analysis (3rd Edition, 1976), p37-38\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1706,"permalink":"https://freshrimpsushi.github.io/jp/posts/1706/","tags":null,"title":"距離空間におけるコンパクト集合の閉部分集合はコンパクトである"},{"categories":"거리공간","contents":" $(X,d)$が距離空間だとする。$p \\in X$であり、$E \\subset X$とする。\n$d(q,p)\u0026lt;r$を満たす全ての$q$を含む集合を点$p$の近傍neighborhoodと定義し、$N_{r}(p)$と表記する。この時、$r$を$N_{r}(p)$の半径と呼ぶ。距離を省略できる場合は$N_{p}$のようにも表記される。\n$p$の全ての近傍が$q\\ne p$であり$q\\in E$の$q$を含む場合、$p$を$E$の集積点limit pointと呼ぶ。\n$E$の全ての集積点が$E$に含まれる場合、$E$が閉じているclosedと言う。\n$N\\subset E$を満たす$p$の近傍$N$が存在するなら、$p$を$E$の内点interior pointと呼ぶ。\n$E$の全ての点が$E$の内点である場合、$E$が開いているopenと言う。\n要約 距離空間$X$で、開いた集合のコレクション1を$\\left\\{ O_{\\alpha} \\right\\}$、閉じた集合のコレクションを$\\left\\{ C_{\\alpha} \\right\\}$としよう。そうすると\n(a) 開いた集合の合併$\\bigcup_{\\alpha} O_{\\alpha}$もまた開いた集合だ。\n(b) 閉じた集合の共通部分$\\bigcap_{\\alpha} C_{\\alpha}$もまた閉じた集合だ。\n(c) 開いた集合の有限の共通部分$\\bigcap_{i=1}^{n}O_{i}$もまた開いた集合だ。\n(d) 閉じた集合の有限の合併$\\bigcup _{i=1}^{n} C_{i}$もまた閉じた集合だ。\n$(c)$、$(d)$で有限という条件がなければ成立しない。これは反例を通して示すことができる。\n証明 (a) $O=\\bigcup_{\\alpha} O_{\\alpha}$とする。$p \\in O$ならば、何らかの$\\alpha$について$p \\in O_{\\alpha}$である。従って、開いた集合の定義により、$p$は$O_{\\alpha}$の内点だ。また、内点の定義により、$p$は$O$の内点となる。任意の$p\\in O$について、$p$が$O$の内点であるため、$O$は開いた集合だ。\n■\n(b) ド・モルガンの定理 $\\left\\{ E_{\\alpha}\\right\\}$を集合$E_{\\alpha}$のコレクションとする。すると下記の式が成立する。 $$ \\left( \\bigcup \\limits_{\\alpha}E_{\\alpha} \\right)^{c}=\\bigcap \\limits_{\\alpha} (E_{\\alpha})^{c} $$\n証明は下に紹介する。\nド・モルガンの定理によって、次が成立する。\n$$ \\left( \\bigcap_{\\alpha} C_{\\alpha} \\right)^{c}=\\bigcup_{\\alpha}(C_{\\alpha})^{c} \\tag{1} $$\n$C_{\\alpha}$が閉じた集合なので、$(C_{\\alpha})^{c}$は開いた集合だ。それにより、(a) によって$\\bigcup_{\\alpha}(C_{\\alpha})^{c}=\\left( \\bigcap_{\\alpha} C_{\\alpha} \\right)^{c}$は開いた集合だ。従って、$\\bigcap_{\\alpha} C_{\\alpha}$は開いた集合の補集合なので閉じた集合だ。\n■\n(c) $O=\\bigcap_{i=1}^{n}O_{i}$とする。すると、任意の点$p\\in O$について、全ての$i$に対して$p\\in O_{i}\\ (i=1,\\cdots,n)$が成立する。従って、開いた集合と内点の定義により、それぞれの$i$に対して\n$$ N_{i} \\subset O_{i} \\quad (i=1,\\cdots,n) $$\nを満たす半径が$r_{i}$の$p$の近傍が存在する。この時$r=\\min (r_{1},\\cdots,r_{n})$としよう。そして$N=N_{r}(p)$とする。そのため、$N$は最も小さい半径を持つ近傍なので、次が成立する。\n$$ N\\subset O_{i} \\quad (i=1,\\cdots,n) $$\nよって、$N \\subset O$が成立し、内点の定義により、$p$は$O$の内点だ。任意の$p\\in O$について、$p$は常に$O$の内点なので、$O$は開いた集合だ。\n■\n(d) ド・モルガンの定理によって、次が成立する。\n$$ \\left( \\bigcup_{i=1}^{n}C_{i} \\right)^{c} = \\bigcap _{i=1}^{n} (C_{i})^{c} $$\n$C_{i}$が閉じているため、補助定理2により$(C_{i})^{c}$は開いている。それにより、$(c)$によって$\\bigcap_{i=1}^{n}(C_{i})^{c}=\\left( \\bigcup _{i=1}^{n}C_{i} \\right)^{c}$は開いた集合だ。それで、再び補助定理2によって、$\\bigcup _{i=1}^{n}C_{i}$は閉じた集合だ。\n■\nド・モルガンの定理の証明 真理値表を使った証明\npart 1. $\\left( \\bigcup \\limits_{\\alpha}E_{\\alpha} \\right)^{c} \\subset \\bigcap \\limits_{\\alpha} (E_{\\alpha})^{c}$\n$x\\in \\left( \\bigcup \\limits_{\\alpha}E_{\\alpha} \\right)^{c}$とする。すると、補集合の定義により、次が成立する。\n$$ \\begin{align*} \u0026amp;\u0026amp; x \u0026amp;\\notin \\bigcup \\limits_{\\alpha}E_{\\alpha} \\\\ \\implies\u0026amp;\u0026amp; x\u0026amp;\\notin E_{\\alpha}\\quad \u0026amp;\\forall \\alpha \\\\ \\implies\u0026amp;\u0026amp; x\u0026amp;\\in(E_{\\alpha})^{c}\\quad \u0026amp;\\forall \\alpha \\\\ \\implies\u0026amp;\u0026amp; x\u0026amp;\\in \\bigcap\\limits_{\\alpha}(E_{\\alpha})^{c} \\end{align*} $$\nよって\n$$ \\left( \\bigcup \\limits_{\\alpha}E_{\\alpha} \\right)^{c} \\subset \\bigcap \\limits_{\\alpha} (E_{\\alpha})^{c} $$\npart 2. $\\left( \\bigcup \\limits_{\\alpha}E_{\\alpha} \\right)^{c} \\supset \\bigcap \\limits_{\\alpha} (E_{\\alpha})^{c}$\n$x\\in \\bigcap \\limits_{\\alpha} (E_{\\alpha})^{c}$とする。すると、次が成立する。\n$$ \\begin{align*} \u0026amp;\u0026amp; x \u0026amp;\\in (E_{\\alpha})^{c} \u0026amp;\\forall \\alpha \\\\ \\implies \u0026amp;\u0026amp; x \u0026amp;\\notin E_{\\alpha} \u0026amp;\\forall \\alpha \\\\ \\implies\u0026amp;\u0026amp; x\u0026amp;\\notin \\bigcup \\limits_{\\alpha}E_{\\alpha} \\\\ \\implies \u0026amp;\u0026amp; x \u0026amp;\\in \\left( \\bigcup \\limits_{\\alpha} E_{\\alpha} \\right)^{c} \\end{align*} $$\nよって\n$$ \\left( \\bigcup \\limits_{\\alpha}E_{\\alpha} \\right)^{c} \\supset \\bigcap \\limits_{\\alpha} (E_{\\alpha})^{c} $$\n■\n集合の集まりの意味だ。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1702,"permalink":"https://freshrimpsushi.github.io/jp/posts/1702/","tags":null,"title":"距離空間における開集合と閉集合の性質"},{"categories":"거리공간","contents":"定義 $(X,d)$が距離空間だとしよう。$p \\in X$であり、$E \\subset X$とする。\n$d(q,p)\u0026lt;r$を満たすすべての$q$を含む集合を点$p$の近傍neighborhoodと定義し、$N_{r}(p)$と表記する。このとき$r$を$N_{r}(p)$の半径と呼ぶ。距離を省略できる場合は$N_{p}$のように表記することもある。\n$p$のすべての近傍が$q\\ne p$であり、$q\\in E$の$q$を含む場合、$p$を$E$の集積点limit pointと呼ぶ。\n$p\\in E$でありながら$p$が$E$の集積点でない場合、$p$を$E$の孤立点isolated pointと呼ぶ。\n$E$のすべての集積点が$E$に含まれる場合、$E$が閉じているclosedという。\n$N\\subset E$を満たす$p$の近傍$N$が存在する場合、$p$を$E$の内点interior pointと呼ぶ。\n$E$のすべての点が$E$の内点である場合、$E$が開いているopenという。\n$p \\in X$であり$p \\notin E$のすべての$p$を含む集合を$E$の補集合complementと呼び、$E^{c}$と表記する。\n$E$が閉じており$E$のすべての点が$E$の集積点である場合、$E$が完全perfectであるという。\n$\\forall p\\in E,\\ d(p,q)\u0026lt;M$を満たす点$q\\in X$と実数$M$が存在する場合、$E$を有界boundedと呼ぶ。\n$X$のすべての点が$E$の集積点であるか$E$の点である場合、$E$は$X$で密denseであるという。\n$E$のすべての集積点の集合を$E$の導出集合derived setと呼び、$E^{\\prime}$と表記する。\n$E$と$E^{\\prime}$の和集合を閉包closureと呼び、$\\overline{E}=E\\cup E^{\\prime}$と表記する。\n説明 上で述べる開、集積点、密、内点などは他のステートメントで定義されることもあるが、本質的には同じである。それぞれの概念をなぜ上のように定義し、名前を付けたのかは、1次元、2次元で直接図を描いてみれば感覚が簡単につかめるだろう。孤立点は集積点でない点と定義されるため、孤立点でありながら同時に集積点であることはできない。これとは異なり、開集合と閉集合はそれぞれ独立した条件で定義される。したがって、名前から感じられる直感とは異なり、開いていると同時に閉じている集合や、開いても閉じてもいない集合が存在することがある。前者の例として$\\mathbb{R}^{2}$があり、後者の例として$\\left\\{ {\\textstyle \\frac{1}{n}}\\ |\\ n\\in \\mathbb{N} \\right\\}$がある。内点と近傍の定義をよく考えると、$x$が$E$の内点である条件は\n$$ d(x,p) \u0026lt;\\varepsilon \\implies x \\in E $$\nが成立するようなある正数$\\varepsilon\u0026gt;0$が存在することと同じである。上の概念と関連するいくつかの定理と証明を紹介する。上の定義での表記に従う。\n定理1 すべての近傍は開集合である。\n証明 $E=N_{r}(p)$としよう。また、任意の$q \\in E$を考える。すると、近傍の定義により、以下の式を満たす正の実数$h$が必ず存在する。\n$$ d(p,q)=r-h\u0026lt;r $$\nすると、距離の定義により、$d(q,s)\u0026lt;h$を満たすすべての$s$に対して、以下の式が成立する。\n$$ d(p,s)\\le d(p,q)+d(q,s)\u0026lt;(r-h)+h=r $$\nしたがって、近傍の定義により、$s \\in E$である。これは、▷eq68\n◁の近傍$N_{h}(q)$内の任意の点$s$も$E$の要素であることを示している。したがって、$N_{h}(q) \\subset E$であるため、$q$は$E$の内点である。最初に$q$を$E$の任意の点としたので、$E$のすべての点は内点である。よって、$E$は開集合である。■\n定理2 集合$E$が開集合であることと$E^c$が閉集合であることは同値である。\n証明 $(\\impliedby)$\n$E^c$が閉じていると仮定する。今、任意の$p\\in E$について考える。すると$p \\notin E^c$であり、閉じている定義により$p$は$E^c$の集積点ではない。したがって、$N \\cap E^c=\\varnothing$を満たす$p$の近傍$N$が存在する。これは$N \\subset E$を意味し、内点の定義により$p$は$E$の内点である。任意の$p\\in E$がすべて$E$の内点であるため、定義により$E$は開集合である。\n$(\\implies)$\n$E$が開いていると仮定する。そして、$p$を$E^{c}$の集積点とする。すると、集積点の定義により、$p$のすべての近傍は少なくとも一つの$E^{c}$の点を含む。すると、$p$のすべての近傍は$E$に含まれず、これは$p$が$E$の内点ではないことを意味する。$E$は開いていると仮定したので、$p\\notin E$である。したがって、$E^{c}$のすべての集積点$p$が$E^{c}$に含まれるので、$E^{c}$は閉じている。\n■\n定理3 $p$を$E$の集積点としよう。すると、$p$の近傍は無数に多くの$E$の点を要素として持つ。\nこれを別の言い方をすると、「有限集合は集積点を持たない」「集積点を持つ集合は無限集合である」ということである。\n証明 $p$の近傍$N$が$E$の有限個の要素のみを含むと仮定しよう。そして、$q_{1},q_{2},\\cdots,q_{n}$を$p$ではない$N\\cap E$の点としよう。そして、$p$と$q_{i}$の距離の中で最小値を$r$とする。\n$$ r= \\min \\limits _{1\\le i \\le n}d(p,q_{i}) $$\n各々の$q_{i}$は$p$と異なる点であるため、すべての距離は正であり、正の数の中で最小値を選んでも正であるため、$r\u0026gt;0$である。今、$p$の別の近傍$N_{r}(p)$を考える。すると、近傍と距離の定義により、$N_{r}(p)$にはいかなる$q_{i}$も含まれない。すると、集積点の定義により、$p$は$E$の集積点ではない。これは、$p$が$E$の集積点であるという事実に矛盾する。したがって、帰納法により仮定が間違っていることがわかる。したがって、上の定理は成立する。\n■\n系 有限個の点のみを持つ集合は集積点を持たない。\n定理4 距離空間$(X,d)$と$E \\subset X$に対して、以下の事実が成立する。$(a)$ $\\overline{E}$は閉じている。$(b)$ $E=\\overline{E}$であることと同値は$E$が閉じていることである。$(c)$ $E\\subset F$を満たすすべての閉集合$F\\subset X$に対して$\\overline{E} \\subset F$が成立する。\n$(a)$と$(c)$によって、$\\overline{E}$は$E$を含む最小の$X$の閉部分集合である。\n","id":1700,"permalink":"https://freshrimpsushi.github.io/jp/posts/1700/","tags":null,"title":"メートル空間における近傍、限界点、オープン、クローズド"},{"categories":"거리공간","contents":"定義 $(X,d)$が距離空間であるとする。$p \\in X$であり、$E \\subset X$であるとする。\n$d(q,p)\u0026lt;r$を満たす全ての$q$を含む集合を点$p$の近傍と定義し、$N_{r}(p)$と記す。この時、$r$を$N_{r}(p)$の半径と呼ぶ。距離を省略して良い場合は$N_{p}$とも記す。\n$p$の全ての近傍が$q\\ne p$であり、$q\\in E$である$q$を含んでいれば、$p$を$E$の集積点と呼ぶ。\n$E$の全ての集積点が$E$に含まれる場合、$E$が閉じていると言う。\n$N\\subset E$を満たす$p$の近傍$N$が存在すれば、$p$を$E$の内点と呼ぶ。\n$E$の全ての点が$E$の内点である場合、$E$が開いていると言う。\n$E$の全ての集積点の集合を$E$の導集合と呼び、$E^{\\prime}$と記す。\n$E$と$E^{\\prime}$の合併集合を閉包と呼び、$\\overline{E}=E\\cup E^{\\prime}$と記す。\n定理1 $A,B\\subset X$に対して以下の式が成立する。\n(1a) $A\\subset B \\implies A^{\\prime} \\subset B^{\\prime}$\n(1b) $(A\\cup B)^{\\prime}=A^{\\prime}\\cup B^{\\prime}$\n(1c) $(A \\cap B)^{\\prime} \\subset A^{\\prime}\\cap B^{\\prime}$\n証明 (1a) $A\\subset B$と仮定する。そして$p\\in A^{\\prime}$とする。すると$p$は$A$の集積点であるため、集積点の定義により以下の文が成立する。$p$の全ての近傍$N$は$q\\ne p$であり、$q\\in A$である$q$を含む。この時、$A\\subset B$と仮定したので、上記の文は以下の文を意味する。$p$の全ての近傍$N$は$q\\ne p$であり、$q\\in B$である$q$を含む。したがって、集積点の定義により$p \\in B^{\\prime}$である。\n■\n(1b) 部分 1. $A^{\\prime} \\cup B^{\\prime} \\subset (A\\cup B)^{\\prime}$\n$A\\subset A\\cup B$であり、$B \\subset A\\cup B$であるため、$(a1)$によって以下のようになる。\n$$ A^{\\prime} \\subset (A\\cup B)^{\\prime} \\quad \\text{and} \\quad B^{\\prime} \\subset (A \\cup B)^{\\prime} $$\nしたがって\n$$ A^{\\prime} \\cup B^{\\prime} \\subset (A\\cup B)^{\\prime} $$\n部分 2. $(A\\cup B)^{\\prime} \\subset A^{\\prime}\\cup B^{\\prime}$\n$p \\in (A\\cup B)^{\\prime}$とする。すると集積点の定義により$p$の全ての近傍$N$は$q\\ne p$であり、$q\\in A\\cup B$である$q$を含む。$q\\in A\\cup B$を再び書くと$q\\in A \\text{ or } q\\in B$であるため、これは$p \\in A^{\\prime} \\text{ or } p\\in B^{\\prime}$と同じである。したがって$p\\in A^{\\prime}\\cup B^{\\prime}$であるため、以下のようになる。\n$$ (A\\cup B)^{\\prime} \\subset A^{\\prime}\\cup B^{\\prime} $$\n部分 3.\n上記の結果を総合すると以下のようになる。\n$$ A^{\\prime}\\cup B^{\\prime} = (A\\cup B)^{\\prime} $$\n■\n(1c) $A\\cap B \\subset A$であり、$A\\cap B \\subset B$であるため、(1a) により以下のようになる。\n$$ (A\\cap B)^{\\prime} \\subset A^{\\prime} \\quad \\text{and} \\quad (A\\cap B)^{\\prime} \\subset B^{\\prime} $$\nしたがって\n$$ (A\\cap B)^{\\prime} \\subset A^{\\prime}\\cap B^{\\prime} $$\n■\n定理2 $A,B \\subset X$に対して以下の式が成立する。\n(2a) $A\\subset B \\implies \\overline{A} \\subset \\overline{B}$\n(2b) $\\overline{A\\cup B} = \\overline{A}\\cup \\overline{B}$\n(2c) $\\overline{A\\cap B} \\subset \\overline{A}\\cap \\overline{B}$\n証明 (2a) $A \\subset B$と仮定する。すると**(1a)** により$A^{\\prime} \\subset B^{\\prime}$である。したがって\n$$ \\overline{A} = A\\cup A^{\\prime} \\subset B \\cup B^{\\prime} = \\overline{B} $$\n■\n(2b) 部分 1. $\\overline{A\\cup B}\\subset \\overline{A}\\cup \\overline{B}$\n$p \\in \\overline{A\\cup B}$とする。すると$p\\in A\\cup B$であるか$p \\in (A\\cup B)^{\\prime}$であるという意味である。\nケース 1-1. $p \\in A\\cup B$\nこの場合$p \\in A$であるか$p \\in B$である。しかし$A \\subset \\overline{A}$であり、$B \\subset \\overline{B}$であるため\n$$ p\\in \\overline{A}\\ \\text{or} \\ p \\in \\overline{B}\\implies p \\in \\overline{A}\\cup \\overline{B} $$\nケース 1-2. $p\\in (A\\cup B)^{\\prime}$\n(1b) によって$p\\in (A\\cup B)^{\\prime}=A^{\\prime}\\cup B^{\\prime}$である。これは$p\\in A^{\\prime}$であるか$p\\in B^{\\prime}$であるという意味である。しかし$A^{\\prime} \\subset \\overline{A}$であり、$B^{\\prime} \\subset \\overline{B}$であるため、上記のケースと同様に\n$$ p\\in \\overline{A}\\ \\text{or} \\ p \\in \\overline{B}\\implies p \\in \\overline{A}\\cup\\overline{B} $$\nケース 1-1, 1-2によって以下が成立する。\n$$ \\overline{A\\cup B}\\subset \\overline{A}\\cup \\overline{B} $$\n部分 2. $\\overline{A}\\cup \\overline{B} \\subset \\overline{A\\cup B}$\n$A \\subset A\\cup B$であり、$B\\subset A\\cup B$であるため、$(b1)$によって以下が成立する。\n$$ \\overline{A} \\subset \\overline{A\\cup B}\\quad \\text{and} \\quad \\overline{B}\\subset \\overline{A\\cup B} $$\nしたがって\n$$ \\overline{A}\\cup \\overline{B} \\subset \\overline{A\\cup B} $$\n■\n(2c) $p \\in \\overline{A\\cap B}$とする。すると$p\\in A\\cap B$であるか$p\\in (A \\cap B)^{\\prime}$である。\nケース 1. $p\\in A\\cap B$\nこの場合$p \\in A$でありながら$p \\in B$である。しかし$A\\subset \\overline{A}$であり、$B\\subset \\overline{B}$であるため\n$$ p\\in A \\ \\text{and} \\ \\ p \\in B \\implies p\\in \\overline{A} \\ \\text{and} \\ p \\in \\overline{B} \\implies p\\in \\overline{A}\\cap \\overline{B} $$\nケース 2. $p \\in (A\\cap B)^{\\prime}$\n(1a) によって$(A\\cap B)^{\\prime}\\subset A^{\\prime}$であり、$(A\\cap B)^{\\prime} \\subset B^{\\prime}$である。しかし$A^{\\prime}\\subset \\overline{A}$であり、$B^{\\prime} \\subset \\overline{B}$であるため\n$$ p\\in A^{\\prime} \\ \\text{and} \\ p\\in B^{\\prime} \\implies p\\in \\overline{A}\\quad \\text{and} \\quad p\\in \\overline{B}\\implies p\\in \\overline{A}\\cap \\overline{B} $$\n■\n定理3 距離空間$(X,d)$と$E \\subset X$に対して以下の事実が成立する。\n(3a) $\\overline{E}$は閉じている。\n(3b) $E=\\overline{E}$であることと同等であるのは、$E$が閉じていることである。\n(3c) $E\\subset F$を満たす閉集合$F\\subset X$に対して、$\\overline{E} \\subset F$が成立する。\n(3a) と (3c) によって、$\\overline{E}$は$E$を含む最小の$X$の閉部分集合である。\n証明 (3a) $p \\in X$であり、$p \\notin \\overline{E}$とする。すなわち$p \\in (\\overline{E})^{c}$である。すると$p$は$E$の点でも$E^{\\prime}$の点でもない。したがって、集積点の定義により$p$は少なくとも一つの$N\\cap E=\\varnothing$である近傍$N$を持つ。したがって$N\\subset (\\overline{E})^{c}$であり、$p$は$(\\overline{E})^{c}$の任意の点であったので、内点の定義により$(\\overline{E})^{c}$の全ての点が内点であり、これは$(\\overline{E})^{c}$が開集合であることを意味する。$(\\overline{E})^{c}$が開集合であるため、$\\overline{E}$は閉集合である。1\n■\n(3b) $(\\implies)$\n$E=\\overline{E}=E \\cup E^{\\prime}$であるため、$E$の全ての集積点は$E$の要素である。これは閉集合の定義であるため、$E$は閉じている。または、閉包と閉じることの定義から直ちに成立することがわかる。\n$(\\impliedby)$\n閉集合の定義により、$E$の全ての集積点は$E$に含まれる。したがって、$\\overline{E}=E\\cup E^{\\prime}=E$である。\n■\n(3c) $F$を$E\\subset F \\subset X$である閉集合とする。すると**(3b)** によって$F^{\\prime} \\subset \\overline{F}=F$である。また、(2a) によって$E^{\\prime} \\subset F^{\\prime} \\subset F$である。したがって、以下が成立する。\n$$ E \\subset F \\quad \\text{and} \\quad E^{\\prime}\\subset F $$\nしたがって\n$$ E\\cup E^{\\prime} =\\overline{E} \\subset F $$\n■\n定理4 $E$を空集合ではない実数集合であり、上に有界とする。そして$y=\\sup E$とする。すると$y \\in \\overline{E}$である。また、$E$が閉じていれば、$y \\in E$である。\n証明 $y \\in \\overline{E}$であることが成立すれば、その後の命題は (3a) により自明であるので、$y \\in \\overline{E}$のみ証明することにする。2つの場合に分けて証明する。\nケース 1. $y \\in E$\n$$ y \\in E \\subset \\overline{E} $$\nであるため、成立する。\nケース 2. $y \\notin E$\nすると全ての正数$h\u0026gt;0$に対して、$y-h\u0026lt;x\u0026lt;y$を満たす$x\\in E$が存在する。これは$y$の全ての近傍である$N_{h}(y)$内に$E$の要素が必ず含まれることを意味する。したがって、定義により$y$は$E$の集積点である。したがって$y\\in E\\cup E^{\\prime}=\\overline{E}$である。\n■\n定理2 参照\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1701,"permalink":"https://freshrimpsushi.github.io/jp/posts/1701/","tags":null,"title":"計量空間における閉包と派生集合"},{"categories":"확률분포론","contents":"定義 1 自由度 $r_{1}, r_{2} \u0026gt; 0$ に対して以下の確率密度関数を持つ連続確率分布 $F \\left( r_{1} , r_{2} \\right)$ をF分布という。 $$ f(x) = {{ 1 } \\over { B \\left( r_{1}/2 , r_{2} / 2 \\right) }} \\left( {{ r_{1} } \\over { r_{2} }} \\right)^{r_{1} / 2} x^{r_{1} / 2 - 1} \\left( 1 + {{ r_{1} } \\over { r_{2} }} x \\right)^{-(r_{1} + r_{2}) / 2} \\qquad , x \\in (0, \\infty) $$\n$B(r_{1} / 2, r_{2}/2)$ はベータ関数を意味する。 基本性質 モーメント生成関数 平均と分散 [2]: $X \\sim F ( r_{1} , r_{2})$ の場合 $$ \\begin{align*} E(X) =\u0026amp; {{ r_{2} } \\over { r_{2} - 2 }} \u0026amp; \\qquad , r_{2} \u0026gt; 2 \\\\ \\text{Var}(X) =\u0026amp; {{ 2 r_{2}^{2} (r_{1} + r_{2} - 2) } \\over { r_{1} (r_{2} -2)^{2} (r_{2} - 4) }} \u0026amp; \\qquad , r_{2} \u0026gt; 4 \\end{align*} $$ 定理 二つの確率変数 $U,V$ が独立で、$U \\sim \\chi^{2} ( r_{1})$、$V \\sim \\chi^{2} ( r_{2})$ だとする。\n$k$次のモーメント [a]: $d_{2} \u0026gt; 2k$ の場合 $\\displaystyle F := {{ U / r_{1} } \\over { V / r_{2} }}$ は$k$次のモーメントが存在し、 $$ E F^{k} = \\left( {{ r_{2} } \\over { r_{1} }} \\right)^{k} E U^{k} E V^{-k} $$ カイ二乗分布から導出 [b]: $${{ U / r_{1} } \\over { V / r_{2} }} \\sim F \\left( r_{1} , r_{2} \\right)$$ ベータ分布から導出 [c]: 自由度 $r_{1} , r_{2}$ のF分布に従う確率変数 $X \\sim F \\left( r_{1}, r_{2} \\right)$ に対して次のように定義された$Y$ は、ベータ分布 $\\text{Best} \\left( {{ r_{1} } \\over { 2 }} , {{ r_{2} } \\over { 2 }} \\right)$ に従う。 $$ Y := {{ \\left( r_{1} / r_{2} \\right) X } \\over { 1 + \\left( r_{1} / r_{2} \\right) X }} \\sim \\text{Beta} \\left( {{ r_{1} } \\over { 2 }} , {{ r_{2} } \\over { 2 }} \\right) $$ t分布から導出 [d]: 自由度 $\\nu \u0026gt; 0$ のt分布に従う確率変数 $X \\sim t(\\nu)$ に対して次のように定義された$Y$ は、F分布 $F (1,\\nu)$ に従う。 $$ Y := X^{2} \\sim F (1,\\nu) $$ 相互性Reciprocality [e]: $X \\sim F \\left( r_{1}, r_{2} \\right)$ の場合、その逆数の分布は次のようになる。 $$ {{ 1 } \\over { X }} \\sim F \\left( r_{2}, r_{1} \\right) $$ $\\chi^{2} \\left( r \\right)$ は自由度 $r$ のカイ二乗分布だ。 説明 t分布がスチューデントStudent t分布と呼ばれるように、F分布も統計学者ジョージ・スネデコーの名前を取ってスネデコーSnedecor F分布と呼ばれることがある。2\nF分布の確率密度関数は一見複雑に見えるが、実際には式を操作する必要はほとんどなく、カイ二乗分布との関係をよく理解することが最優先だ。カイ二乗分布が適合度検定に使用されたように、F分布は二つの母集団の分散を比較する際に使用できる。定理[b]で直接確認できるように、F分布はカイ二乗分布に従うデータの比として表されるため、この統計量が$1$から遠く離れている場合は、二つの分布の分散が異なると推測できるのだ。\n証明 1 確率変数のモーメント生成関数が存在するとは、すべての$k \\in \\mathbb{N}$ に対して$k$次のモーメントが存在することを意味する。しかし、定理[a]でのF分布の$k$次のモーメントは$k \u0026lt; d_{2} / 2$ のときに存在するため、モーメント生成関数は存在しえない。\n■\n[2] 定理[a]に記載されたモーメント公式を使用する。\n■\n[a] $t = {{ r_{1} } \\over { r_{2} }} x$ と置換すると$dt = {{ r_{1} } \\over { r_{2} }} dx$ となるので、 $$ \\begin{align*} E F^{k} =\u0026amp; \\int_{0}^{\\infty} x^{k} {{ 1 } \\over { B \\left( r_{1}/2 , r_{2} / 2 \\right) }} \\left( {{ r_{1} } \\over { r_{2} }} \\right)^{r_{1} / 2} x^{r_{1} / 2 - 1} \\left( 1 + {{ r_{1} } \\over { r_{2} }} x \\right)^{-(r_{1} + r_{2}) / 2} dx \\\\ =\u0026amp; {{ 1 } \\over { B \\left( r_{1}/2 , r_{2} / 2 \\right) }} \\left( {{ r_{1} } \\over { r_{2} }} \\right)^{r_{1} / 2} \\int_{0}^{\\infty} x^{k + r_{1} / 2 - 1} \\left( 1 + {{ r_{1} } \\over { r_{2} }} x \\right)^{-(r_{1} + r_{2}) / 2} dx \\\\ =\u0026amp; {{ 1 } \\over { B \\left( r_{1}/2 , r_{2} / 2 \\right) }} \\left( {{ r_{1} } \\over { r_{2} }} \\right)^{r_{1} / 2} \\int_{0}^{\\infty} \\left( {{ r_{2} } \\over { r_{1} }} t \\right)^{k + r_{1} / 2 - 1} \\left( 1 + t \\right)^{-(r_{1} + r_{2}) / 2} {{ r_{2} } \\over { r_{1} }} dt \\\\ =\u0026amp; {{ 1 } \\over { B \\left( r_{1}/2 , r_{2} / 2 \\right) }} \\left( {{ r_{1} } \\over { r_{2} }} \\right)^{r_{1} / 2} \\left( {{ r_{2} } \\over { r_{1} }} \\right)^{k + r_{1} / 2}\\int_{0}^{\\infty} t^{k + r_{1} / 2 } \\left( 1 + t \\right)^{-r_{1}/2 - r_{2}/ 2} dt \\\\ =\u0026amp; {{ 1 } \\over { B \\left( r_{1}/2 , r_{2} / 2 \\right) }} \\left( {{ r_{2} } \\over { r_{1} }} \\right)^{k }\\int_{0}^{\\infty} t^{k + r_{1} / 2 } \\left( 1 + t \\right)^{-(r_{1}/2+k) - (r_{2}/ 2-k)} dt \\end{align*} $$\nベータ関数の定積分形式の表示: $$ B(p,q)=\\int_{0}^{\\infty}\\frac{ t^{p-1} }{ (1+t)^{p+q}}dt $$\nベータ関数とガンマ関数の関係: $$ B(p,q) = {{\\Gamma (p) \\Gamma (q)} \\over {\\Gamma (p+q) }} $$\n$$ \\begin{align*} EF^{k} =\u0026amp; {{ 1 } \\over { B \\left( r_{1}/2 , r_{2} / 2 \\right) }} \\left( {{ r_{2} } \\over { r_{1} }} \\right)^{k } B \\left( {{ r_{1} } \\over { 2 }} + k, {{ r_{2} } \\over { 2 }} - k \\right) \\\\ =\u0026amp; \\left( {{ r_{2} } \\over { r_{1} }} \\right)^{k } {{ \\Gamma (r_{1}/2 + r_{2}/2) } \\over { \\Gamma (r_{1}/2 ) \\Gamma ( r_{2}/2) }} {{ \\Gamma (r_{1}/2 + k) \\Gamma ( r_{2}/2 - k) } \\over { \\Gamma (r_{1}/2 +k + r_{2}/2 - k) }} \\\\ =\u0026amp; \\left( {{ r_{2} } \\over { r_{1} }} \\right)^{k } {{ 1 } \\over { \\Gamma (r_{1}/2 ) \\Gamma ( r_{2}/2) }} {{ \\Gamma (r_{1}/2 + k) \\Gamma ( r_{2}/2 - k) } \\over { 1 }} \\\\ =\u0026amp; \\left( {{ r_{2} } \\over { r_{1} }} \\right)^{k } {{ \\Gamma (r_{1}/2 + k) 2^{k}} \\over { \\Gamma (r_{1}/2 ) }} {{ 2^{-k} \\Gamma ( r_{2}/2 - k) } \\over { \\Gamma ( r_{2}/2) }} \\end{align*} $$\nカイ二乗分布のモーメント: $X \\sim \\chi^{2} (r)$ とする。$k \u0026gt; - r/ 2$ の場合、$k$次のモーメントが存在する $$ E X^{k} = {{ 2^{k} \\Gamma (r/2 + k) } \\over { \\Gamma (r/2) }} $$\n$$ E F^{k} = \\left( {{ r_{2} } \\over { r_{1} }} \\right)^{k } E U^{k} E V^{-k} $$\n■\n[b] ジョイント密度関数から直接導く。\n■\n[c] 変数変換で直接導く。\n■\n[d] カイ二乗分布の比として遠回りする。\n■\n[e] 分子と分母が逆転しているので、定理[b]に従って自明だ。実用的な統計学者の視点からは、定理[b]に従ってF分布を定義し、それに基づいて確率密度関数を導出する方が自然だ。\n■\n参照 一般化: 非中心F分布 Hogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p194.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCasella. (2001). Statistical Inference(2nd Edition): p222.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1606,"permalink":"https://freshrimpsushi.github.io/jp/posts/1606/","tags":null,"title":"F分布"},{"categories":"확률분포론","contents":"定義 1 自由度 $r \u0026gt; 0$に対して、以下のような確率密度関数を持つ連続確率分布 $\\chi^{2} (r)$をカイ二乗分布chi-square Distributionと言う。 $$ f(x) = {{ 1 } \\over { \\Gamma (r/2) 2^{r/2} }} x^{r/2-1} e^{-x/2} \\qquad , x \\in (0, \\infty) $$\n$\\Gamma$はガンマ関数を示す。 基本性質 モーメント生成関数 [1]: $$m(t) = (1-2t)^{-r/2} \\qquad , t \u0026lt; {{ 1 } \\over { 2 }}$$ 平均と分散 [2] 平均と分散が$X \\sim \\chi^{2} (r)$である場合、 $$ \\begin{align*} E(X) =\u0026amp; r \\\\ \\text{Var} (X) =\u0026amp; 2r \\end{align*} $$ 十分統計量 [3]: カイ二乗分布に従うランダムサンプル $\\mathbf{X} := \\left( X_{1} , \\cdots , X_{n} \\right) \\sim \\chi^{2} (r)$が与えられたとする。$r$に対する十分統計量 $T$は次の通り。 $$ T = \\left( \\prod_{i} X_{i} \\right) $$ 定理 $k$次モーメント [a]: $X \\sim \\chi^{2} (r)$とする。$k \u0026gt; - r/ 2$の場合、$k$次モーメントが存在し、 $$ E X^{k} = {{ 2^{k} \\Gamma (r/2 + k) } \\over { \\Gamma (r/2) }} $$ ガンマ分布との関係 [b]: $$\\Gamma \\left( { r \\over 2 } , 2 \\right) \\iff \\chi ^2 (r)$$ F分布の導出 [c]: 2つの確率変数$U,V$が独立であり、$U \\sim \\chi^{2} ( r_{1})$、$V \\sim \\chi^{2} ( r_{2})$である場合、 $$ {{ U / r_{1} } \\over { V / r_{2} }} \\sim F \\left( r_{1} , r_{2} \\right) $$ 標準正規分布の二乗との関係 [d]: $X \\sim N(\\mu,\\sigma ^2)$の場合、 $$ V=\\left( { X - \\mu \\over \\sigma} \\right) ^2 \\sim \\chi ^2 (1) $$ 説明 カイ二乗分布は、統計学全般で広く使用される分布で、特に適合度検定や分散分析などで最初に遭遇することが多い。\n定理[d]は特に重要で、この定理の逆命題によって、標準化された残差Residualsの二乗がカイ二乗分布$\\chi^{2} (1)$に従わない場合、残差の正規性に問題があることを検出することができる。\n証明 戦略 [1], [a]: 置換積分を通じて定積分記号の中にある物を外へ出し、ガンマ関数に変えるトリックを使用する。\nガンマ関数の定義: $$ \\Gamma (x) := \\int_{0}^{\\infty} y^{x-1} e^{y} dy $$\n[1] $y=x(1/2-t)$のように置換すると、${{ 1 } \\over { 1/2 - t }}dy = dx$であるため、 $$ \\begin{align*} m(t) =\u0026amp; \\int_{0}^{\\infty} e^{tx} {{ 1 } \\over { \\Gamma (r/2) 2^{r/2} }} x^{r/2-1} e^{-x/2} dx \\\\ =\u0026amp; {{ 1 } \\over { \\Gamma (r/2) 2^{r/2} }} \\int_{0}^{\\infty} x^{r/2-1} e^{x(1/2-t)} dx \\\\ =\u0026amp; {{ 1 } \\over { \\Gamma (r/2) 2^{r/2} }} \\int_{0}^{\\infty} \\left( {{ y } \\over { 1/2 -t }} \\right)^{r/2-1} e^{y} {{ 1 } \\over { 1/2 - t }} dy \\\\ =\u0026amp; (1/2-t)^{-r/2}{{ 1 } \\over { \\Gamma (r/2) 2^{r/2} }} \\int_{0}^{\\infty} y^{r/2-1} e^{y} dy \\\\ =\u0026amp; (1-2t)^{-r/2}{{ 1 } \\over { \\Gamma (r/2) }} \\int_{0}^{\\infty} y^{r/2-1} e^{y} dy \\end{align*} $$ ガンマ関数の定義により、 $$ m(t) = (1-2t)^{-r/2} \\qquad , t \u0026lt; {{ 1 } \\over { 2 }} $$\n■\n[2] モーメント公式[a]に代入する。\n■\n[a] $y = x/2$のように置換すると、$2 dy = dx$であるため、 $$ \\begin{align*} EX^{k} =\u0026amp; \\int_{0}^{\\infty} x^{k} {{ 1 } \\over { \\Gamma (r/2) 2^{r/2} }} x^{r/2-1} e^{-x/2} dx \\\\ =\u0026amp; {{ 1 } \\over { \\Gamma (r/2) 2^{r/2} }} \\int_{0}^{\\infty} x^{r/2+k-1} e^{-x/2} dx \\\\ =\u0026amp; {{ 1 } \\over { \\Gamma (r/2) 2^{r/2} }} \\int_{0}^{\\infty} 2^{r/2+k-1} y^{r/2+k-1} e^{-y} 2dy \\\\ =\u0026amp; {{ 2^{k} } \\over { \\Gamma (r/2) }} \\int_{0}^{\\infty} y^{(r/2+k)-1} e^{-y} 2dy \\end{align*} $$ ガンマ関数の定義により、 $$ E X^{k} = {{ 2^{k} \\Gamma (r/2 + k) } \\over { \\Gamma (r/2) }} $$\n■\n[b] モーメント生成関数で示される。\n■\n[c] ジョイント密度関数で直接演繹する。\n■\n[d] 確率密度関数で直接演繹する。\n■\n参照 一般化: 非中心カイ二乗分布 Hogg et al. (2013). Introduction to Mathematical Statistics(7th Edition): p161.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1600,"permalink":"https://freshrimpsushi.github.io/jp/posts/1600/","tags":null,"title":"カイ二乗分布"},{"categories":"기하학","contents":"定義 平面上で二つの固定された点$F$、$F^{\\prime}$への距離の合計が一定である点の集合を楕円ellipseという。\n楕円の要素は以下の通り。\n$F$、$F^{\\prime}$を焦点という。\n$a$を長半軸、$b$を短半軸という。$b=\\sqrt{1-\\epsilon^{2}}a$が成立する。\n$\\epsilon$を楕円の離心率という。楕円がどれだけ圧縮されているかを示し、焦点は楕円の中心から$\\epsilon a$だけ離れている。$k$あるいは$e$として表記されることもある。\n$$ \\epsilon^{2}=k^{2}=e^{2} =\\begin{cases} \\frac{a^{2}-b^{2}}{a^{2}} ,\u0026amp;0\u0026lt;b\u0026lt;a \\\\ \\frac{b^{2}-a^{2}}{b^{2}}, \u0026amp;0\u0026lt;a\u0026lt;b \\end{cases} $$\n長半軸に垂直に引かれた線が楕円と交わる点から焦点までの距離$\\alpha$を通径という。$\\alpha = (1-\\epsilon^{2})a$が成立する。\n$r_{0}$は焦点から近点までの距離で、$r_{0}=(1-\\epsilon)a$が成立する。\n$r_{1}$は焦点から遠点までの距離であり、$apocenter까지의 거리이며 $、$가 성립한다.\n설명 두 초점이 같으면 원이기 때문에, 보통 타원이라고 하면 두 초점이 서로 다르다는 것을 의미한다.\n판별법 주어진 이차곡선 $、$에 대해서 $、$를 판별식discriminant이라 한다. 판별식이 음수인 이차곡선은 타원이다.\n타원의 방정식 타원의 중심이 $、$이고 장반경이 $、$, 단반경이 $、$인 타원의 방정식은 아래와 같다.\n$$ \\frac{(x-x_{0})^{2}}{a^{2}}+\\frac{(y-y_{0})^{2}}{b^{2}}=1 $$\n극 좌표에서 초점이 원점인 타원의 방정식 극 좌표계에서 타원의 방정식은 아래와 같다.\n$$ r = \\frac{\\alpha}{1+\\epsilon \\cos \\theta} $$\n혹은\n$$ r = \\frac{b^{2}/a}{1+\\frac{\\sqrt{a^{2}-b^{2}}}{a}\\cos\\theta} $$\n타원의 넓이 장반경이 $、$, 단반경이 $、$인 타원의 넓이 $、$는 아래와 같다.\n$$ A=ab\\pi $$\n타원의 둘레 위 그림과 같은 타원의 둘레는 아래와 같다.\n$$ 4b\\int _{0} ^{{\\textstyle \\frac{\\pi}{2}}} \\sqrt{ 1-k^{2}\\sin^{2} \\theta } d\\theta ,\\quad k^{2}=\\frac{b^{2}-a^{2} }{b^{2}} $$\n제2 종 타원 적분 아래의 적분을 각각 제2종 완전 타원 적분, 제2종 불완전 타원 적분 이라고 한다.\n$$ E(k)=\\int_{0}^{{\\textstyle \\frac{\\pi}{2}}}\\sqrt{1-k^{2} \\sin ^{2} \\theta} d\\theta $$\n$$ E(\\phi, k)=\\int_{0}^{\\phi}\\sqrt{1-k^{2} \\sin ^{2} \\theta}d\\theta $$\n타원의 일반화, 일립소이드 선형 변환 $、$에 대해 $、$차원 단위구 $、$ 의 이미지 $、$ 을 일립소이드ellipsoid라고 한다. $、$의 고유값 $、$와 그에 따른 단위 고유벡터 $が示されている。楕円体の軸は$u_{1} , \\cdots , u_{m}$が$에 대해 $を満たすよう表記される。\n","id":1685,"permalink":"https://freshrimpsushi.github.io/jp/posts/1685/","tags":null,"title":"楕円"},{"categories":"힐베르트공간","contents":"定義 関数空間 $\\mathbb{C}^{\\mathbb{R}}$ の関数 $f : \\mathbb{R} \\to \\mathbb{C}$ を考えてみよう。\n関数 $f$ のサポートsupportは、関数値が $0$ ではない点の集合にクロージャを取ったクローズセットとして次のように定義される。 $$ \\text{supp} f = \\overline{\\left\\{ x \\in \\mathbb{R} : f(x) \\ne 0 \\right\\}} $$\n$\\text{supp} f$が有界なら、$f$がコンパクトサポートを持つと言う。クロージャは閉集合であり、実数空間で閉じていて有界な集合はコンパクトだからである。\n$U\\Subset V$は$\\overline{U} \\subset V$であり$\\overline{U}$がコンパクトであることを意味する。つまり、$\\mathrm{supp}(f) \\Subset U$は$f$が$U$でコンパクトサポートを持つことを意味する。$\\subset \\subset$として書くこともある。\n連続関数の集合はベクトル空間になり、これを連続関数空間と呼び、次のように表記する。\n$$ C(\\mathbb{R}) := \\left\\{f \\text{ is continuous} \\right\\} $$\n$C^{1}$と混同する可能性がある場合は、$C^{0}$と書くこともある。\nコンパクトサポートを持つ連続関数のベクトル空間を次のように表記する。\n$$ C_{c} (\\mathbb{R}) := \\left\\{ f \\in C(\\mathbb{R}) : f \\text{ has compact support} \\right\\} $$\n$x \\to \\pm \\infty$の時、関数値が$0$に収束する連続関数のベクトル空間を次のように表記する。\n$$ C_{0} ( \\mathbb{R} ) := \\left\\{ f \\in C(\\mathbb{R}) : f(x) \\to 0 \\text{ as } x \\to \\pm \\infty \\right\\} $$\n$m$回まで微分可能であり、その導関数がすべて連続である連続関数のベクトル空間を次のように表記する。\n$$ C^{m}(\\mathbb{R}) :=\\left\\{ f \\in C(\\mathbb{R}) : f^{(n)} \\text{ is continuous } \\forall n \\le m \\right\\} $$\nこの場合$C^{0}(\\mathbb{R})$は$C(\\mathbb{R})$を意味する。この時の$C^{m}$の要素を$m$回 連続的に微分可能な関数continuously differentiable functionと呼ぶ。\n無限に微分可能で、その導関数がすべて連続である連続関数のベクトル空間を次のように表記する。 $$ C^{\\infty}(\\mathbb{R})=\\bigcap _{m=0}^{\\infty}C^{m}(\\mathbb{R}) $$ この時の$C^{\\infty}$の要素をスムース関数smooth functionと呼ぶ。\n※ 著者によっては$C_{0}$を$C_{c}$の意味で使う場合があるので、教科書で定義された表記をよく確認しよう。\n説明 ソボレフ空間、超関数論などでは$C_{c}^{\\infty}$を主に扱うことになる。\n当然ながら$C_{c} (\\mathbb{R})$は$C_{0} (\\mathbb{R})$の部分空間になる。二つとも単なる連続関数の空間$C (\\mathbb{R})$に比べて良い空間だが、作用素ノルム $\\left\\| \\cdot \\right\\|_{\\infty} $に対してバナッハ空間にならないことに注意する必要がある。例えば、次のような$\\left\\{ f_{k} \\right\\}_{k \\in \\mathbb{N}} \\subset C_{c} (\\mathbb{R})$を考えてみよう\n$$ f_{k} (x) := \\begin{cases} {{ \\sin x } \\over { x }} \\chi_{[ - k \\pi , k \\pi ]} (x) \u0026amp; , x \\ne 0 \\\\ 1 \u0026amp; , x = 0 \\end{cases} $$\n$f_{k}$はすべての$k \\in \\mathbb{N}$に対してコンパクトサポート$[-k \\pi , k \\pi]$を持つが、次のようなシンク関数 $\\text{sinc} \\in C_{0} (\\mathbb{R}) \\setminus C_{c} (\\mathbb{R})$に収束する。\n$$ \\text{sinc} x = \\begin{cases} {{ \\sin x } \\over { x }} \u0026amp; , x \\ne 0 \\\\ 1 \u0026amp; , x = 0 \\end{cases} $$\n距離空間として1 区間$[0, 1]$上で連続な実数値関数の集合を$X = C[0, 1]$としよう。そして、[距離] $d$を次のように定義しよう。\n$$ d(x, y) := \\int\\limits_{0}^{1} \\left| x(t) - y(t) \\right| dt \\qquad \\forall x, y \\in X $$\nすると、距離空間$(X, d)$は完備空間ではない。以下の図(a)に示すような関数$x_{m}$を考えよう。\n$n \\gt m$とすると、任意の$\\varepsilon \\gt 0$に対して$m \\gt 1/\\varepsilon$の時はいつでも$1 \\cdot \\frac{1}{m} \\lt \\varepsilon$が成立するので、$d(x_{m}, x_{n}) \\lt \\varepsilon$によって$\\left\\{ x_{m} \\right\\}$はコーシー数列である。\nしかし、$x_{m}(t) = 0$と$(t \\in [0, 1/2])$であり、$x_{m}(t) = 1$と$(t \\in [a_{m}, 1])$なので、次のようになる。\n$$ \\begin{align*} d(x_{m}, x) \u0026amp;= \\int\\limits_{0}^{1} \\left| x_{m(t)} - x(t) \\right| dt \\\\ \u0026amp;= \\int\\limits_{0}^{\\frac{1}{2}} \\left| 0 - x(t) \\right| dt + \\int\\limits_{\\frac{1}{2}}^{a_{m}} \\left| x_{m(t)} - x(t) \\right| dt + \\int\\limits_{a_{m}}^{1} \\left| 1 - x(t) \\right| dt \\\\ \u0026amp;= \\int\\limits_{0}^{\\frac{1}{2}} \\left| x(t) \\right| dt + \\int\\limits_{\\frac{1}{2}}^{a_{m}} \\left| x_{m(t)} - x(t) \\right| dt + \\int\\limits_{a_{m}}^{1} \\left| 1 - x(t) \\right| dt \\\\ \\end{align*} $$\n各被積分関数が$0$以上であるため、$d(x_{m}, x)$が$0$に収束するためには、各被積分関数が$0$でなければならない。つまり、$x$は$t\\in[0, \\frac{1}{2})$で$x(t) = 0$であり、$t\\in (\\frac{1}{2}, 1]$では$x(t) = 1$である。これは明らかに連続関数ではないため、$x \\notin X$であり、$\\left\\{ x_{m} \\right\\}$は$X$に収束しない。\nノルム空間として2 連続関数空間$C[0, 1]$は、積分ではなく、最大値をノルムとして与えると完備空間、つまり完備ノルム空間(バナッハ空間)となる。つまり、以下のように定義された$\\left\\| \\cdot \\right\\|$に対して$(C[0, 1], \\left\\| \\cdot \\right\\|)$はバナッハ空間である。\n$$ \\left\\| f \\right\\| := \\max\\limits_{t \\in [0, 1]} \\left| f(t) \\right|,\\qquad f \\in C[0, 1] $$\nErwin Kreyszig, Introductory Functional Analysis with Applications (1978), p38\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nErwin Kreyszig, Introductory Functional Analysis with Applications (1978), p61-62\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1594,"permalink":"https://freshrimpsushi.github.io/jp/posts/1594/","tags":null,"title":"関数のサポートと連続関数空間のクラス"},{"categories":"힐베르트공간","contents":"定義 ヒルベルト空間 $H$ の シャウダー基底 $\\left\\{ \\mathbf{e}_{k} \\right\\}_{k \\in \\mathbb{N}}$ が 正規直交系 である場合、$\\left\\{ \\mathbf{e}_{k} \\right\\}_{k \\in \\mathbb{N}}$ を $H$ の 正規直交基底Orthonormal Basis と呼ぶ。\n定理1 正規直交基底の同値条件 [1]: $H$ が ヒルベルト空間 だとする。$H$ の 正規直交系 $\\left\\{ \\mathbf{e}_{k} \\right\\}_{k \\in \\mathbb{N}} \\subset H$ に対して、以下はすべて 同値 である。 (i): $\\left\\{ \\mathbf{e}_{k} \\right\\}_{k \\in \\mathbb{N}} \\subset H$ は $H$ の 正規直交基底 である。 (ii): すべての $\\mathbf{x}\\in H$ に対して $$ \\mathbf{x}= \\sum_{k \\in \\mathbb{N}} \\langle \\mathbf{x}, \\mathbf{e}_{k} \\rangle \\mathbf{e}_{k} $$ (iii): すべての $\\mathbf{x}, \\mathbf{y} \\in H$ に対して $$ \\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\sum_{k \\in \\mathbb{N}} \\langle \\mathbf{x}, \\mathbf{e}_{k} \\rangle \\langle \\mathbf{e}_{k} , \\mathbf{y} \\rangle $$ (iv): すべての $\\mathbf{x}\\in H$ に対して $$ \\sum_{k \\in \\mathbb{N}} \\left| \\langle \\mathbf{x}, \\mathbf{e}_{k} \\rangle \\right|^{2} = \\left\\| \\mathbf{x}\\right\\|^{2} $$ (v): $\\overline{\\text{span}} \\left\\{ \\mathbf{e}_{k} \\right\\}_{k \\in \\mathbb{N}} = H$ (vi): $\\mathbf{x}\\in H$ で、すべての $k \\in \\mathbb{N}$ に対して $\\langle \\mathbf{x}, \\mathbf{e}_{k} \\rangle = 0$ ならば $\\mathbf{x}= \\mathbf{0}$ ユニタリ作用素と正規直交基底 [2]: $\\left\\{ \\mathbf{e}_{k} \\right\\}_{k \\in \\mathbb{N}}$ を $H$ の正規直交基底とする。すると、$H$ の正規直交基底は、ユニタリ作用素 $U : H \\to H$ に関して、正確に $\\left\\{ U \\mathbf{e}_{k} \\right\\}_{k \\in \\mathbb{N}}$ として表される。 説明 特に、定理 [2] のような結果を持って、$H$ のすべての正規直交基底がユニタリ作用素 $U$ によって 特徴付けられる と言う。\n証明 [1] の証明は参考文献を参照。\n[2] $\\left\\{ \\mathbf{v}_{k} \\right\\}_{k \\in \\mathbb{N}}$ もまた、$H$ の正規直交基底とする。作用素 $U : H \\to H$ を次のように定義する: $$ U \\left( \\sum_{k \\in \\mathbb{N}} c_{k} \\mathbf{e}_{k} \\right) := \\sum_{k \\in \\mathbb{N}} c_{k} \\mathbf{v}_{k} \\qquad , \\forall {c_{k}}_{k \\in \\mathbb{N}} \\in l^{2} $$ この場合、$U$ は有界で全単射であり、$\\mathbf{v}_{k} = U \\mathbf{e}_{k}$ である。\n$\\left\\{ \\mathbf{e}_{k} \\right\\}_{k \\in \\mathbb{N}}$ が $H$ の正規直交基底であるため、(i) $\\implies$ (ii) に従て、$\\mathbf{v} ,\\mathbf{w} \\in H$ を次のように表せる：\n$$ \\mathbf{v} = \\sum_{k \\in \\mathbb{N}} \\left\\langle \\mathbf{v} , \\mathbf{e}_{k} \\right\\rangle \\mathbf{e}_{k} \\\\ \\mathbf{w} = \\sum_{k \\in \\mathbb{N}} \\left\\langle \\mathbf{w} , \\mathbf{e}_{k} \\right\\rangle \\mathbf{e}_{k} $$\nすると、$U$ の定義と (i) $\\implies$ (iii) により、\n$$ \\begin{align*} \\left\\langle U^{ \\ast } U \\mathbf{v} , \\mathbf{w} \\right\\rangle =\u0026amp; \\left\\langle U \\mathbf{v} , U \\mathbf{w} \\right\\rangle \\\\ =\u0026amp; \\left\\langle \\sum_{k \\in \\mathbb{N}} \\left\\langle \\mathbf{v} , \\mathbf{e}_{k} \\right\\rangle \\mathbf{e}_{k} , \\sum_{k \\in \\mathbb{N}} \\left\\langle \\mathbf{w} , \\mathbf{e}_{k} \\right\\rangle \\mathbf{e}_{k} \\right\\rangle \\\\ =\u0026amp; \\sum_{k \\in \\mathbb{N}} \\left\\langle \\mathbf{v} , \\mathbf{e}_{k} \\right\\rangle \\overline{\\left\\langle \\mathbf{w} , \\mathbf{e}_{k} \\right\\rangle} \\\\ =\u0026amp; \\left\\langle \\mathbf{v} , \\mathbf{w} \\right\\rangle \\end{align*} $$\nつまり、$U^{ \\ast } U = I$ ので、$U$ はユニタリ作用素であり、逆作用素 $U^{-1} = U^{ \\ast }$ を持つ全単射である。一方、$U$ がユニタリである仮定から、\n$$ \\left\\langle U \\mathbf{e}_{i} , U \\mathbf{e}_{j} \\right\\rangle = \\left\\langle U^{ \\ast } U \\mathbf{e}_{i} , \\mathbf{e}_{j} \\right\\rangle = \\left\\langle \\mathbf{e}_{i} , \\mathbf{e}_{j} \\right\\rangle = \\delta_{ij} $$\nすなわち、$\\left\\{ U \\mathbf{e}_{k} \\right\\}_{k \\in \\mathbb{N}}$ は正規直交集合である。これが $H$ の基底になるためには、すべての $k \\in \\mathbb{N}$ に対して $\\left\\langle \\mathbf{v} , U \\mathbf{e}_{k} \\right\\rangle = 0$ と仮定する。すると、すべての $k \\in \\mathbb{N}$ に対して $\\left\\langle U^{ \\ast } \\mathbf{v} , \\mathbf{e}_{k} \\right\\rangle = 0$ よって、$U^{ \\ast } \\mathbf{v} = \\mathbf{0}$ でなければならない。先に $U^{ \\ast } = U^{-1}$ を示したので、両辺に $U$ を適用すれば、$\\mathbf{v} = \\mathbf{0}$ を得る。結果的に、(vi) $\\implies$ (i) に従って、$\\left\\{ U \\mathbf{e}_{k} \\right\\}_{k \\in \\mathbb{N}} \\subset H$ が $H$ の正規直交基底になることが確認できる。\n■\nOle Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010), p80-83\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1593,"permalink":"https://freshrimpsushi.github.io/jp/posts/1593/","tags":null,"title":"ヒルベルト空間の正規直交基底とユニタリ作用素"},{"categories":"고전역학","contents":"万有引力の法則1 万有引力の法則law of universal gravityは、ニュートンが1687年に『プリンキピア』を通じて発表した物理法則である。「すべての物体は互いに引き合う」と単純に言える。詳しく説明すると、以下の通りである。\n質量を持つすべての粒子は、他の粒子に引きつける力を作用する。この力の大きさは、相互作用する二つの粒子の質量の積に比例し、二つの粒子の中心間の距離の二乗に反比例する。\n上記の文章を数式で表すと、以下のようになる。\n$$ \\mathbf{F}_{ij} = G\\frac{m_{i}m_{j}}{r_{ij}^{2}}\\frac{\\mathbf{r}_{ij}}{|\\mathbf{r}_{ij}|} $$\n$\\mathbf{F}_{ij}$は、質量が$m_{i}$の粒子$i$が、質量が$m_{j}$の粒子$j$から受ける力を表す。作用・反作用の法則により、$\\mathbf{F}_{ij}=-\\mathbf{F}_{ji}$である。比例定数の$G$は、重力定数と呼ばれ、その値はSI単位で以下の通りである。\n$$ G=(6.67259\\pm0.00085)\\times 10^{-11}\\mathrm{Nm^{2}kg^{-2}} $$\nこの力$\\mathbf{F}$を重力gravityと呼ぶ。\nGrant R. Fowles and George L. Cassiday, Analytical Mechanics (7th Edition, 2005), p219-220\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1678,"permalink":"https://freshrimpsushi.github.io/jp/posts/1678/","tags":null,"title":"万有引力の法則：重力"},{"categories":"고전역학","contents":"角運動量1 運動量は、物体が直線運動する際の運動の状態を表す物理量である。質量が大きければ大きいほど、速度が速ければ速いほど、運動量は大きくなる。物理学では、物体の運動がどのように変化するかに関心がある。そのため、物体の運動状態を変える原因である力を、運動量の変化として表現する。\n$$ \\mathbf{F}=\\frac{d \\mathbf{p}}{dt} $$\nさて、回転運動に対しても似たような物理量を定義しようとしている。回転運動では、直線運動と異なり、回転半径があり、これが物体の運動に影響を与える。したがって、回転する物体の運動状態を示すものを角運動量angular momentumと呼び、以下のように定義する。\n$$ \\mathbf{L}=\\mathbf{r}\\times \\mathbf{p} $$\n線運動量は質量と速度の積であるため、原点によって値は変化しないが、角運動量には位置ベクトル$\\mathbf{r}$が含まれるため、原点をどこに設定するかによって値が異なる場合があるので、注意が必要である。幾つかの事実を確認すれば、このように定義することが妥当で自然であると納得できるだろう。\nトルク 直線運動では物体の運動状態を示すものを運動量とし、運動量の変化を力とした。同様に、回転運動では運動状態を示すものを角運動量としたので、角運動量の変化は回転運動の状態を変化させる何かと言える。その物理量をトルクTorqueと呼び、$\\mathbf{N}$で表現する。\n$$ \\mathbf{N}=\\frac{ d \\mathbf{L}}{ dt } $$\n上記の式の右辺を展開すると、次のようになる。\n$$ \\begin{align*} \\frac{ d \\mathbf{L}}{ dt }\u0026amp;=\\frac{ d (\\mathbf{r}\\times \\mathbf{p})}{ dt } \\\\ \u0026amp;=\\frac{d \\mathbf{r}}{dt}\\times \\mathbf{p}+\\mathbf{r}\\times \\frac{ d \\mathbf{p}}{ dt } \\\\ \u0026amp;= \\mathbf{v}\\times \\mathbf{p} + \\mathbf{r}\\times\\mathbf{F} \\end{align*} $$\n$\\mathbf{p} = m\\mathbf{v}$であるため、$\\mathbf{v}\\times \\mathbf{p}=\\mathbf{v}\\times(m\\mathbf{v})=\\mathbf{0}$となる。したがって、以下の式を得る。\n$$ \\frac{ d \\mathbf{L}}{ dt }=\\mathbf{r} \\times \\mathbf{F} $$\n上記の式によれば、合力が$\\mathbf{0}$であれば、角運動量の変化はなく、これは回転運動の状態が変わらないことを意味する。角運動量の定義が実際の物理現象をよく表していることが確認できる。さらに、上記の式からトルクに関する公式を以下のように得る。\n$$ \\mathbf{N}=\\mathbf{r} \\times \\mathbf{F} $$\n直線運動する物体を考えてみよう。その物体が運動する方向と同じ方向に外力が与えられたと仮定しよう。すると、方向が$\\mathbf{r}$と$\\mathbf{F}$の方向と同じであるため、トルクは$\\mathbf{0}$である。トルクは角運動量を変化させるものであるが、それが$\\mathbf{0}$であるということは、物体の回転運動の状態が変わらないという意味である。実際に、直線運動する物体の回転運動の状態は「回転していない」のままである。したがって、この式が実際の物理現象をよく表していることが分かる。さらに、中心力を受けながら回転運動する物体の場合、位置ベクトル$\\mathbf{r}$と中心力$\\mathbf{F}$の方向が同じであるため、トルクが$\\mathbf{0}$となり、角運動量が保存されることが分かる。この事実から面積速度一定の法則が導かれる。 トルクは回転運動の変化を与えるもので、その変化をどのように与えるかは、右手の法則で決まる。上のGIF2にあるように、トルクの方向が上方向であれば、右手の法則によって物体が反時計回りに動くように状態が変わる。\nGrant R. Fowles and George L. Cassiday, Analytical Mechanics (7th Edition, 2005), p226-227\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Torque\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1674,"permalink":"https://freshrimpsushi.github.io/jp/posts/1674/","tags":null,"title":"角運動量とトルク"},{"categories":"고전역학","contents":"ニュートンの運動法則 1 イギリスの数学者であり物理学者であるアイザック・ニュートンIssac Newtonは1687年、プリンキピアPrincipia, 自然哲学の数学的原理において、以下の運動に関する3つの法則を提案した。\n外力を受けていない物体は、その運動状態を変えない。\n運動の変化は、物体に作用する力に比例する。\n物体1が物体2に力を加えるとき、物体2は物体1に対して大きさは同じで方向は反対の力を同時に加える。\nこれら3つの法則をまとめてニュートンの運動法則と呼び、それぞれ1法則、2法則、3法則と呼ばれる。また、これらの法則を基に物体の運動を記述する学問をニュートン力学Newton mechanicsまたは古典力学classical mechanicsと呼ぶ。\n第1法則 通常は慣性の法則と呼ばれる。慣性interiaとは、すべての物体が持つ特性であり、運動の変化に抵抗しようとする性質を指す。つまり、止まっている物体は動こうとせず、動いている物体は止まろうとしない性質が慣性である。静止した物体が動くためには外力が必要であり、逆に動いている物体が止まろうとするときも外力が必要である。このような法則がよく成り立つ空間を慣性基準系inertia frame of referenceと呼ぶ。この定義により、加速する基準系は慣性基準系ではないとわかる。例えば、加速する車内で静止している（座っている）対象を別の基準系から見た場合、その対象が加速運動をしているように見えるため、外力なしで運動状態が変わっているようである。\n第2法則 第2法則は主に\n$$ \\mathbf{F}=m\\mathbf{a} $$\nこの式で表される。ただし、より詳細な表現は以下のとおりである。\n$$ \\mathbf{F}=\\frac{ d \\mathbf{p}}{ d t} $$\nニュートンは、物体の運動を表す物理量を運動量と定義した。また、力を物体の運動状態を変化させるものと定義した。したがって、物体の運動状態が変化するということは、運動量が変化するということである。これにより、物体に加えられた力$\\mathbf{F}$が運動量の変化率に比例するという式を立てることができ、これがまさに上記の式である。\n第3法則 主に作用・反作用の法則endと呼ばれる。法則で説明される2つの力のうち、一方を作用と呼び、もう一方を反作用と呼ぶ。\nGrant R. Fowles and George L. Cassiday, Analytical Mechanics (7th Edition, 2005), p47-58\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1671,"permalink":"https://freshrimpsushi.github.io/jp/posts/1671/","tags":null,"title":"ニュートンの運動の法則"},{"categories":"고전역학","contents":"質量1 ニュートンの運動の法則では、慣性とは運動の変化に抵抗する性質と説明されている。つまり、慣性が大きいほど動かしづらく、慣性が小さいほど動かしやすいということだ。これは、軽い物体を押して動かすよりも重い物体を押して動かす方がより困難であるという経験とピッタリ合っている。つまり、慣性の大きさは質量の大きさで言えるということだ。質量とは、物体が重いか軽いかの度合いを言うものだ。これが質量の意味を定義したものだ。質量の値を定義する方法は以下の通りだ。\n二つの物体があるとしよう。それぞれの物体の質量を$m_{1}$、$m_{2}$としよう。そして、これらが同じ力で反対方向に動かされるとしよう。簡単に言えば、二つの物体の間にバネを挟んで、両側から押して離した状態を想像してみよう。二つの物体は、それぞれ$\\mathbf{v}_{1}$、$\\mathbf{v}_{2}$の速度で弾き出される。この時、二つの物体の質量の比を以下のように定義する。\n$$ \\frac{m_{2}}{m_{1}}=\\left|\\frac{\\mathbf{v}_{1}}{\\mathbf{v}_{2}} \\right| $$\nこの時、物体1の質量$m_{1}$を基準にして、他の物質の質量を決定できる。\n運動量と力 物体の質量と速度の積を運動量momentumといい、$\\mathbf{p}$と書く。角運動量と区別するため、直線運動量linear momentumとも言う。\n$$ \\mathbf{p}=m\\mathbf{v} $$\n運動量はその名の通り、運動する物体が持っている物理量だ。したがって、物体の運動に変化が生じたことは、物体の運動量が増加したり減少したりしたことと同じだ。すると、ニュートンの第二法則で述べられている運動の変化は、時間による運動量の変化と言える。また、力を物体の運動を変化させるものとしたからには、運動量の定義からニュートンの第二法則を以下の数式で表すことができる。\n$$ \\begin{equation} \\mathbf{F}=k\\frac{ d \\mathbf{p}}{ d t } \\end{equation} $$\nこれを解説すると、「物体に加えられた力$\\mathbf{F}$は、物体の運動量の変化量に比例する」ということだ。ここで、$k$は比例定数だ。物体の質量$m$が時間が経過しても変わらないと仮定すると（高校の物理から大学の物理まで多くの状況でそうである）、以上の式を以下のように書くことができる。\n$$ \\mathbf{F}=k\\frac{d(m\\mathbf{v})}{dt}=km\\frac{d \\mathbf{v}}{dt}=km\\mathbf{a} $$\nここで、$\\mathbf{a}$は質量が$m$の物体に力$\\mathbf{F}$が作用した時に、その物体が持つ加速度だ。比例定数を$k=1$とすると、あの有名なその式になる。\n$$ \\mathbf{F}=m\\mathbf{a} $$\nニュートンの運動の法則と上述の定義から自然に運動量保存の法則が導かれる。$(1)$の左辺は物体系（または粒子系）に作用する合力であり、右辺は物体系の運動量の変化率だ。外力が存在しない場合、運動量の変化率が$0$であるため、運動量が保存されていることが分かる。\nGrant R. Fowles and George L. Cassiday, Analytical Mechanics (第7版, 2005), p\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1673,"permalink":"https://freshrimpsushi.github.io/jp/posts/1673/","tags":null,"title":"物理学における質量、力、運動量の定義"},{"categories":"고전역학","contents":"定義 粒子の集まりを粒子系system of particlesと呼ぶ。\n説明 1 質量が$m_{1}$、$m_2$、$\\cdots$、$m_{n}$の粒子の位置ベクトルがそれぞれ$\\mathbf{r}_{1}$、$\\mathbf{r}_{2}$、$\\cdots$、$\\mathbf{r}_{n}$の時、この粒子系の質量中心center of massを以下のように定義する。\n$$ \\mathbf{r}_{cm}=\\frac{m_{1}\\mathbf{r}_{1}+m_{2}\\mathbf{r}_{2}+\\cdots + m_{n}\\mathbf{r}_{n}}{m_{1}+ m_{2}+ \\cdots+ m_{n}}=\\frac{\\sum m_{i}\\mathbf{r}_{i}}{m} $$\nここで、$m=\\sum \\limits_{i}m_{i}$は粒子系の全質量を示している。下付き文字$cm$はcenter of massの略である。それにより、質量中心の速度は自然に以下のように定義される。\n$$ \\begin{equation} \\mathbf{v}_{cm}=\\frac{m_{1}\\mathbf{v}_{1}+m_{2}\\mathbf{v}_{2}+\\cdots + m_{n}\\mathbf{v}_{n}}{m_{1}+ m_{2}+ \\cdots+ m_{n}}=\\frac{\\sum m_{i}\\mathbf{v}_{i}}{m} \\label{velocity-of-cm} \\end{equation} $$\n3次元直交座標系で$\\mathbf{r}_{i}=x_{i}\\hat{\\mathbf{x}}+y_{i}\\hat{\\mathbf{y}}+z_{i}\\hat{\\mathbf{z}}$とすると、各座標における質量中心は以下のようになる。\n$$ x_{cm}=\\frac{\\sum m_{i}x_{i} }{m},\\quad y_{cm}=\\frac{\\sum m_{i}y_{i} }{m},\\quad z_{cm}=\\frac{\\sum m_{i}z_{i} }{m} $$\nこの粒子系の線形運動量は自然に各粒子の運動量の合計として定義される。\n$$ \\mathbf{p}=\\sum \\mathbf{p}_{i}=\\sum m_{i}\\mathbf{v}_{i} $$\nそれにより、$(1)$によって粒子系の線形運動量は粒子系全体の質量と質量中心の速度の積として表されることが分かる。\n$$ \\mathbf{p}=\\sum m_{i}\\mathbf{v}_{i}=m\\mathbf{v}_{cm} $$\nこれでそれぞれの粒子が外部から受ける力が$\\mathbf{F}_{1}$、$\\mathbf{F}_{2}$、$\\cdots$、$\\mathbf{F}_{n}$とする。さらに、粒子$i$が粒子$j$から受ける力を$\\mathbf{F}_{ij}$とする。そうすると、粒子$i$の運動方程式は以下のようになる。\n$$ \\mathbf{F}_{i} + \\sum \\limits_{j=1}^{n}\\mathbf{F}_{ij}=m_{i}\\ddot{\\mathbf{r}_{i}}=\\dot{\\mathbf{p}_{i}} $$\nしたがって、粒子系全体に対する力をすべて足すと以下のようになる。\n$$ \\sum \\limits_{i=1}^{n}\\mathbf{F}_{i}+\\sum \\limits _{i=1}^{n}\\sum \\limits_{j=1}^{n}\\mathbf{F}_{ij}=\\sum \\limits_{i=1}^{n}\\dot{\\mathbf{p}_{i}} $$\nここで、粒子は自分自身に対しては何の力も及ぼさないので$\\mathbf{F}_{ii}=\\mathbf{0}$は0である。また、方程式の2番目の項で、$\\mathbf{F}_{ij}$と$\\mathbf{F}_{ji}$はそれぞれ粒子$i$が粒子$j$に、粒子$j$が粒子$i$に与える力を表しているため、作用・反作用の法則により、これらの大きさは同じだが方向が反対であり、足し合わせると$\\mathbf{0}$になる。よって、2番目の項は$\\mathbf{0}$である。したがって、粒子系全体の運動方程式は次のようになる。\n$$ \\sum \\limits _{i=1} ^{n} \\mathbf{F}_{i}=\\dot{\\mathbf{p}_{i}}=m\\mathbf{a}_{cm} $$\nつまり、粒子系では質量中心の加速度は、系全体の質量を持つ一つの粒子が全体の外力を受けた時の加速度と同じである。\nGrant R. Fowles and George L. Cassiday, Analytical Mechanics (7th Edition, 2005), p275-277\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1670,"permalink":"https://freshrimpsushi.github.io/jp/posts/1670/","tags":null,"title":"粒子系の質量中心と線運動量"},{"categories":"바나흐공간","contents":"定義1 $(X, \\left\\| \\cdot \\right\\|)$をノルム空間と呼ぶことにする。$X$のすべての元$\\mathbf{x}\\in X$に対して、以下を満たすスカラーの系列$\\left\\{ a_{k} \\right\\}_{k \\in \\mathbb{N}}$が一意に存在するならば、$\\left\\{ \\mathbf{e}_{k} \\right\\}_{k \\in \\mathbb{N}} \\subset X$を$X$のシャウダー基底Schauder basisという。\n$$ \\mathbf{x}= \\sum_{k \\in \\mathbb{N}} a_{k} \\mathbf{e}_{k} $$\n説明 ベクトル空間の基底は、特に「無限」の線形結合について議論するとき、シャウダー基底と呼ばれる。無限について語るだけあって、バナッハ空間に関する性質が多く関連しており、特にヒルベルト空間については、以下の有用な定理が知られている。\n正規直交基底の同値条件：$H$がヒルベルト空間とする。$H$の正規直交系$\\left\\{ \\mathbf{e}_{k} \\right\\}_{k \\in \\mathbb{N}} \\subset H$に対して、以下はすべて同値である。\n(i): $\\left\\{ \\mathbf{e}_{k} \\right\\}_{k \\in \\mathbb{N}} \\subset H$は$H$の正規直交基底である。 (ii): すべての$\\mathbf{x}\\in H$に対して、 $$ \\mathbf{x}= \\sum_{k \\in \\mathbb{N}} \\langle \\mathbf{x}, \\mathbf{e}_{k} \\rangle \\mathbf{e}_{k} $$ (iii): すべての$\\mathbf{x}, \\mathbf{y} \\in H$に対して、 $$ \\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\sum_{k \\in \\mathbb{N}} \\langle \\mathbf{x}, \\mathbf{e}_{k} \\rangle \\langle \\mathbf{e}_{k} , \\mathbf{y} \\rangle $$ (iv): すべての$\\mathbf{x}\\in H$に対して、 $$ \\sum_{k \\in \\mathbb{N}} \\left| \\langle \\mathbf{x}, \\mathbf{e}_{k} \\rangle \\right|^{2} = \\left\\| \\mathbf{x}\\right\\|^{2} $$ (v): $\\overline{\\text{span}} \\left\\{ \\mathbf{e}_{k} \\right\\}_{k \\in \\mathbb{N}} = H$ (vi): $\\mathbf{x}\\in H$であり、すべての$k \\in \\mathbb{N}$に対して、$\\langle \\mathbf{x}, \\mathbf{e}_{k} \\rangle = 0$ならば$\\mathbf{x}= \\mathbf{0}$ 参照 有限次元ベクトル空間の基底: ハメル基底 Ole Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010), p42\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1583,"permalink":"https://freshrimpsushi.github.io/jp/posts/1583/","tags":null,"title":"無限次元ベクトル空間とシャウダー基底"},{"categories":"해석개론","contents":"정리1 この記事はリーマン・スティルチェス積分を基準に書かれています。$\\alpha=\\alpha (x)=x$とすると、リーマン積分と同じです。 $f$が$[a,b]$でリーマン（-スティルチェス）積分可能だとしましょう。すると、定数$c\\in \\mathbb{R}$に対して$cf$も$[a,b]$で積分可能であり、その値は以下の通りです。 $$ \\int_{a}^{b}cf d\\alpha = c\\int_{a}^{b}f d\\alpha $$\n二つの関数$f_{1}$、$f_{2}$が$[a,b]$でリーマン（-スティルチェス）積分可能であるとしましょう。すると、$f_{1}+f_{2}$も積分可能であり、その値は以下の通りです。 $$ \\int _{a} ^{b}(f_{1}+f_{2})d\\alpha = \\int _{a} ^{b} f_{1}d\\alpha + \\int_{a}^{b} f_{2} d\\alpha $$\n積分は線形であるということです。\n$$ \\int _{a} ^{b}(f_{1}+cf_{2})d\\alpha = \\int _{a} ^{b} f_{1}d\\alpha + c\\int_{a}^{b} f_{2} d\\alpha $$\nわざわざ加算と定数倍を別々に書いた理由は、証明を別々にするためです。\n補助定理 $[a,b]$でリーマン（-スティルチェス）積分可能な関数$f$と任意の正数$\\varepsilon\u0026gt; 0$に対して、以下の式を満たす$[a,b]$の分割$P$が存在します。\n$$ \\begin{align} U(P,f,\\alpha) \\lt \\int_{a}^{b}f d\\alpha +\\varepsilon \\tag{L1} \\\\ \\int_{a}^{b}f d\\alpha -\\varepsilon \\lt L(P,f,\\alpha) \\tag{L2} \\end{align} $$\n$U$、$L$はそれぞれリーマン（-スティルチェス）上積分、下積分です。\n証明 $\\eqref{L1}$ 任意の正数$\\varepsilon \\gt 0$が与えられたとします。すると、積分可能の必要十分条件により、以下の式を満たす分割$P$が存在します。\n$$ U(P,f,\\alpha)-L(P,f,\\alpha) \\lt \\varepsilon $$\nこの時$L(P,f,\\alpha) \\le \\displaystyle \\int_{a}^{b}fd\\alpha$なので、次が成立します。\n$$ U(P,f,\\alpha)-\\int_{a}^{b}f d\\alpha\\le U(P,f,\\alpha)-L(P,f,\\alpha) \\lt \\varepsilon $$\n従って、要約すると次のようになります。\n$$ U(P,f,\\alpha ) \\lt \\int_{a}^{b}f d\\alpha +\\varepsilon $$\n■\n$\\eqref{L2}$ 証明$\\eqref{L1}$でと同様に、次を満たす分割$P$が存在します。\n$$ U(P,f,\\alpha)-L(P,f,\\alpha) \\lt \\varepsilon $$\n$\\displaystyle \\int_{a}^{b}fd\\alpha \\le U(P,f,\\alpha)$なので、次が成立します。\n$$ \\int_{a}^{b}f d\\alpha-L(P,f,\\alpha)\\le U(P,f,\\alpha)-L(P,f,\\alpha) \\lt \\varepsilon $$\n従って、要約すると次のようになります。\n$$ \\int_{a}^{b}f d\\alpha -\\varepsilon \\lt L(P,f,\\alpha) $$\n■\n証明 $f_{1}, f_{2}, f$が積分可能の時、$f_{1}+f_{2}, cf$も積分可能であり、その値が実際に$\\displaystyle \\int f_{1} + \\int f_{2}, c\\int f$と同じであることを示します。\n1. Case 1. $c=0$\n$cf=0$が積分可能であることは自明です。また、次の等式が成立することも自明です。\n$$ \\int_{a}^{b}0fd\\alpha=0=0\\int_{a}^{b}fd\\alpha $$\nCase 2. $c\u0026gt;0$\n任意の正数$\\varepsilon \u0026gt;0$が与えられたとします。すると、積分可能の必要十分条件によって、次を満たす分割$P=\\left\\{ a=x_{0} \\lt \\cdots \\lt x_{i} \\lt \\cdots \\lt x_{n}=b\\right\\}$が存在します。\n$$ \\begin{equation} U(P,f,\\alpha) - L(P,f,\\alpha)\u0026lt;\\frac{\\varepsilon}{c} \\end{equation} $$\nそして、次のようにしましょう。\n$$ \\begin{align*} M_{i} \u0026amp;= \\sup _{[x_{i-1}, x_{i}]} f(x) \\\\ m_{i} \u0026amp;= \\inf _{[x_{i-1}, x_{i}]} f(x) \\\\ M_{i}^{c} \u0026amp;= \\sup _{[x_{i-1}, x_{i}]} cf(x) \\\\ m_{i}^{c} \u0026amp;= \\inf _{[x_{i-1}, x_{i}]} cf(x) \\end{align*} $$\nすると$c\u0026gt;0$なので$cM_{i} = M_{i}^{c}$であり、$cm_{i} = m_{i}^{c}$です。すると、リーマン（-スティルチェス）和の定義と$(1)$によって、次が成立します。\n$$ \\begin{align} U(P,cf,\\alpha)- L(P,cf,\\alpha) \u0026amp;= \\sum \\limits_{i=1}^{n}M_{i}^{c}\\Delta \\alpha_{i}-\\sum \\limits_{i=1}^{n}m_{i}^{c}\\Delta \\alpha_{i} \\nonumber\\\\ \u0026amp;= \\sum \\limits_{i=1}^{n}cM_{i}\\Delta \\alpha_{i}-\\sum \\limits_{i=1}^{n}cm_{i}\\Delta \\alpha_{i} \\nonumber\\\\ \u0026amp;= c\\left( \\sum \\limits_{i=1}^{n}M_{i}\\Delta \\alpha_{i}-\\sum \\limits_{i=1}^{n}m_{i}\\Delta \\alpha_{i} \\right) \\nonumber\\\\ \u0026amp;= c\\Big[ U(P,f,\\alpha)-L(P,f,\\alpha)\\Big] \\nonumber\\\\ \u0026amp;\\lt \\varepsilon \\end{align} $$\n従って、積分可能の必要十分条件により、$cf$は積分可能です。 積分は上積分より小さいので、次が成立します。\n$$ c \\int_{a}^{b}fd \\alpha \\le cU(P,f,\\alpha) = U(P,cf,\\alpha) $$\nこれは、$(2)$と補助定理によって、次が成立します。\n$$ c\\int _{a}^{b}f d\\alpha \\le U(P,cf,\\alpha) lt \\int _{a}^{b} cf d\\alpha +\\varepsilon $$\nこの時、$\\varepsilon$は任意の正数と仮定したので、次が成立します。\n$$ \\begin{equation} c\\int_{a}^{b}fd\\alpha \\le \\int_{a}^{b}cfd\\alpha \\end{equation} $$\n反対方向の不等号を示す過程も似ています。$(1)$と補助定理によって、次が成立します。\n$$ cU(P,f,\\alpha) \\le c\\int_{a}^{b}fd\\alpha +\\varepsilon $$\nまた、次の式が成立します。\n$$ \\int_{a}^{b} cfd\\alpha \\le U(P,cf,\\alpha)=cU(P,f,\\alpha) $$\n上の二つの式から、下の式を得ます。\n$$ \\int_{a}^{b} cfd \\alpha \\le cU(P,f,\\alpha)\u0026lt; c\\int_{a}^{b}fd\\alpha +\\varepsilon $$\nこの時、$\\varepsilon$は任意の正数なので、次が成立します。\n$$ \\begin{equation} \\int_{a}^{b} cf d\\alpha \\le c\\int_{a}^{b}fd\\alpha \\end{equation} $$\n$(3)$と$(4)$によって、次が成立します。\n$$ \\int_{a}^{b}cfd\\alpha = c\\int_{a}^{b}fd\\alpha $$\nCase 3. $c=-1$\n証明の過程はCase 2. と似ています。まず、任意の正数$\\varepsilon$が与えられたとします。$f$は積分可能なので、積分可能の必要十分条件により、与えられた$\\varepsilon$に対して、次を満たす分割$P$が存在します。\n$$ U(P,f,\\alpha) - L(P,f,\\alpha) \u0026lt;\\varepsilon $$\n今、次のようにしましょう。\n$$ \\begin{align*} M_{i} \u0026amp;= \\sup _{[x_{i-1},x_{i}]}f \\\\ m_{i} \u0026amp;= \\inf_{[x_{i-1},x_{i}]}f \\\\ M_{i}^{\\ast} \u0026amp;= \\sup _{[x_{i-1},x_{i}]}(-f) \\\\ m_{i}^{\\ast} \u0026amp;= \\inf_{[x_{i-1},x_{i}]}(-f) \\end{align*} $$\nすると$M_{i}=-m_{i}^{\\ast}$であり、$m_{i}=-M_{i}^{\\ast}$です。従って$M_{i}-m_{i}=M_{i}^{\\ast}-m_{i}^{\\ast}$です。それゆえ、次が成立します。\n$$ \\begin{align*} U(P,-f,\\alpha)-L(P,-f,\\alpha) \u0026amp;= \\sum\\limits_{i=1}^{n}M_{i}^{\\ast}\\Delta \\alpha_{i}-\\sum\\limits_{i=1}^{n}m_{i}^{\\ast}\\Delta \\alpha_{i} \\\\ \u0026amp;= \\sum\\limits_{i=1}^{n}M_{i}\\Delta \\alpha_{i} - \\sum\\limits_{i=1}^{n}m_{i}\\Delta\\alpha_{i} \\\\ \u0026amp;= U(P,f,\\alpha) -L(P,f,\\alpha) \\\\ \u0026amp;\\lt \\varepsilon \\end{align*} $$\n従って、$-f$は積分可能です。\nCase 2. の証明と同様に、補助定理によって、次が成立します。\n$$ U(P,-f,\\alpha) \\lt \\int_{a}^{b}(-f)d\\alpha +\\varepsilon $$\nまた、次の式が成立します。\n$$ -\\int_{a}^{b}fd\\alpha\\le -L(P,f,\\alpha)=U(P,-f,\\alpha) \\lt \\int_{a}^{b}(-f)d\\alpha + \\varepsilon $$\nこの時、$\\varepsilon$は任意の正数なので、次が成立します。 $$ -\\int_{a}^{b}fd\\alpha \\le \\int_{a}^{b}(-f)d\\alpha $$\nそれから、補助定理によって、次の式が成立します。\n$$ \\int_{a}^{b}(-f)d\\alpha -\\varepsilon \\lt L(P,-f,\\alpha)=-U(P,f,\\alpha)\\le-\\int_{a}^{b}fd\\alpha $$\n$\\varepsilon$は任意の正数なので、次が成立します。\n$$ \\int_{a}^{b}(-f)d\\alpha \\le -\\int_{a}^{b}fd\\alpha $$\n従って、次を得ます。\n$$ \\int_{a}^{b}(-f)d\\alpha =-\\int_{a}^{b}fd\\alpha $$\nCase 4. $c \\lt 0 \\quad \\text{and} \\quad c\\ne -1$\nCase 2. と Case 3. によって成立します。\n■\n2. $f=f_{1}+f_{2}$としましょう。$P$を$[a,b]$の任意の分割とします。すると、リーマン（-スティルチェス）上積分、下積分の定義によって、次が成立します。\n$$ \\begin{equation} \\begin{aligned} L(P,f_{1},\\alpha) + L(P,f_{2},\\alpha)\u0026amp; \\le L(P,f,\\alpha) \\\\ \u0026amp;\\le U(P,f,\\alpha) \\\\ \u0026amp;\\le U(P,f_{1},\\alpha) +U(P,f_{2},\\alpha) \\end{aligned} \\end{equation} $$\n任意の正数$\\varepsilon \u0026gt; 0$が与えられたとします。すると、積分可能の必要十分条件によって、次を満たす分割$P_{j}$が存在します。\n$$ U(P_{j},f_{j},\\alpha)-L(P_{j},f_{j},\\alpha)\u0026lt;\\varepsilon,\\quad (j=1,2) $$\n今、$P$を再び$P_{1}$と$P_{2}$の共通細分としましょう。すると、$(5)$によって、次が成立します。\n$$ \\begin{align*} U(P,f,\\alpha)-L(P,f,\\alpha) \u0026amp;\\le \\left[ U(P,f_{1},\\alpha)-L(P,f_{1},\\alpha) \\right] + \\left[ U(P,f_{2},\\alpha)-L(P,f_{2},\\alpha) \\right] \\\\ \u0026amp;\u0026lt; \\varepsilon \\end{align*} $$\n従って、積分可能の必要十分条件により、$f$は積分可能です。 それから、補助定理によって、下の式が成立します。\n$$ U(P,f_{j},\\alpha)\u0026lt;\\int _{a}^{b}f_{j}d\\alpha+\\varepsilon,\\quad (j=1,2) $$\nまた、定義によって積分より上積分が大きいため、次が成立します。\n$$ \\int_{a}^{b}fd\\alpha \\le U(P,f,\\alpha) $$\n上の式と$(5)$の三番目の不等式によって、次が成立します。\n$$ \\begin{align*} \\int_{a}^{b}fd\\alpha \u0026amp;\\le U(P,f,\\alpha) \\\\ \u0026amp;\\le U(P,f_{1},\\alpha)+U(P,f_{2},\\alpha) \\\\ \u0026amp;\u0026lt; \\int_{a}^{b}f_{1}d\\alpha +\\int_{a}^{b}f_{2}d\\alpha + 2\\varepsilon \\end{align*} $$\nこの時、$\\varepsilon$は任意の正数なので、次が成立します。\n$$ \\begin{equation} \\int_{a}^{b} fd\\alpha \\le \\int_{a}^{b}f_{1}d\\alpha + \\int_{a}^{b} f_{2}d\\alpha \\label{6} \\end{equation} $$\n反対方向の不等式が成立することを示せば、証明は完了です。積分可能な関数の定数倍も積分可能であることを上で示したので、$-f_{1}, -f_{2}$も積分可能であることがわかります。従って、これら二つの関数に対して上の過程を繰り返せば、下の式を得ます\n$$ \\int_{a}^{b}(-f)d\\alpha \\le \\int_{a}^{b}(-f_{1})d\\alpha + \\int_{a}^{b} (-f_{2})d\\alpha $$\nまた、$\\displaystyle \\int (-f)d\\alpha=-\\int fd\\alpha$なので、両辺に$-1$を掛けると、次を得ます。\n$$ \\begin{equation} \\int_{a}^{b}fd\\alpha \\ge \\int_{a}^{b}f_{1}d\\alpha + \\int_{a}^{b} f_{2}d\\alpha \\label{7} \\end{equation} $$\n従って、$(6)$と$(7)$によって、次を得ます。\n$$ \\int_{a}^{b}fd\\alpha = \\int_{a}^{b}f_{1}d\\alpha + \\int_{a}^{b} f_{2}d\\alpha $$\n■\nウォルター・ルーディン, 数学解析の原理 (第3版, 1976), p128-129\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1666,"permalink":"https://freshrimpsushi.github.io/jp/posts/1666/","tags":null,"title":"リーマン(-シュティールス)積分の線形性"},{"categories":"그래프이론","contents":"定義 平面グラフ グラフを平面に描いた時、エッジが重ならずに描けるなら、そのグラフを平面グラフと言う。\n説明 平面グラフが描かれると、平面上で区切られる領域をフェースFaceと呼ぶ。次の平面グラフ $K_{4}$ は四つのフェース $f_{1}, f_{2}, f_{3}, f_{4}$ を持ち、その中でも特にバウンドされていない $f_{4}$ を無限フェースInfinite Faceと呼ぶ。\n平面グラフはその名前の通り、適合する部分なしに平面に描けるグラフのことを言う。理解を深めるためには、平面グラフではないグラフが何かを見る方がいい。例えば、完全グラフ $K_{5}$ と二部グラフ $K_{3,3}$ は平面グラフではないが、どのようにしても重なるエッジがあるためだ。\n数学者なら当然、グラフが平面グラフになる条件が何かが気になるだろう。これについては、上の例で一般化された定理が知られている。\n定理 クラトフスキーの定理 1 グラフが平面グラフであることと、そのグラフのサブグラフが $K_{5}$ や $K_{3,3}$ とホメオモーフィックなものがないことは同値である。\n意義 簡単に言えば、$K_{5}$ に似ているものや$K_{3,3}$ に似ているものが含まれていれば平面グラフではなく、なければ平面グラフであることを保証してくれる定理だ。一見すると、この定理で平面グラフについて全てが解決されるように見えるが、グラフ理論は平面グラフを根に数多くの概念や問題を生み出してきた。最も簡単な一般化として、$k \\in \\mathbb{N}$ 個のエッジが重なることを許すことから始め、数学全般でよく見られるデュアル、点と線と面などを扱う証明幾何学の領域はもちろん、空間の構造自体に関心を持つ組み合わせ位相幾何学にも続く大旅行の始まりである。\nだからと言って、グラフを専攻していない人が平面グラフに関する様々な定理を覚えて証明に慣れる必要はないが、もっと広い数学の世界を見るために必ず知っておくべき概念だと言えるだろう。定義を見ればわかるが、そもそも平面グラフを理解するために広い数学的基盤が必要なわけではない。だから、少なくとも知っておこう。\n自閉症テスト 古いインターネットのミームの一つで、上のような問題を自閉症テストAutism Testと言う。問題では、水道と電力、ガスを三つの家に供給しなければならないが、その線が交差してはならないという制限をかけ、「難しいが可能だ」と言う。これを純粋な数学問題として考えれば、クラトフスキーの定理によって不可能だと反論できる。これらの切り抜きが\u0026rsquo;自閉症テスト\u0026rsquo;と呼ばれる理由は、\u0026lsquo;不可能なのに解答にこだわり、全ての可能性を検討する様子が自閉症に似ている\u0026rsquo;という種類の軽蔑的、侮辱的なジョークであるためだろう。\n数学問題でない形でアプローチすると、上のような解決策もある。実際、自閉症テストの健全な楽しみは、このように問題の条件を破る強引または斬新な反則にある。\nWilson. (1970). Introduction to Graph Theory: p2.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1565,"permalink":"https://freshrimpsushi.github.io/jp/posts/1565/","tags":null,"title":"平面グラフとクラトフスキーの定理"},{"categories":"상미분방정식","contents":"ビルドアップ1 質量が$m$、減衰係数が$\\gamma$、バネ定数が$k$の時、スプリングに吊るされた物体の振動を示す運動方程式は以下の通りだ。\n$$ m x^{\\prime \\prime} + \\gamma x^{\\prime} + kx = F $$\n$x_{1}=x$、$x_{2}=x_{1}^{\\prime}$とすると、上の運動方程式は以下のようなシステムとして表現できる。\n$$ \\begin{align*} x_{1}^{\\prime}(t) =\u0026amp;\\ x_{2}(t) \\\\ x_{2}^{\\prime} (t) =\u0026amp;\\ x_{1}^{\\prime \\prime}(t) = -\\dfrac{\\gamma}{m}x_{2}(t)-\\dfrac{k}{m}x_{1}(t)-\\dfrac{1}{m}F(t) \\end{align*} $$\nこれを以下のように行列で表せる。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\begin{pmatrix}x_{1}^{\\prime} \\\\ x_{2}^{\\prime}\\end{pmatrix} =\u0026amp;\\ \\begin{pmatrix} 0 \u0026amp; 1 \\\\ -\\dfrac{k}{m} \u0026amp; -\\dfrac{\\gamma}{m} \\end{pmatrix} \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ -\\dfrac{1}{m}F \\end{pmatrix} \\\\ \\implies\u0026amp;\u0026amp; \\mathbf{x}^{\\prime}(t) =\u0026amp;\\ A\\mathbf{x}(t)+g(t) \\end{align*} $$\n$g(t)=0$が同次方程式の場合、2階微分方程式の解は$\\mathbf{x}^{\\prime}=A\\mathbf{x}$の行列乗算を解く問題に単純化されることが分かる。\n一般化 $x_{1}$、$x_{2}$、$\\cdots$、$x_{n}$を$t$に対する関数としよう。$F_{1}$、$F_{2}$、$\\cdots$、$F_{n}$を$x_{1}$、$x_{2}$、$\\cdots$、$x_{n}$に対する関数としよう。すると$x_{i}(t),$ $1\\le i \\le n$に対する1階微分方程式のシステムは以下の通りだ。\n$$ \\begin{align*} x_{1}^{\\prime}(t) =\u0026amp;\\ F_{1}(t,x_{1},x_{2},\\cdots,x_{n}) \\\\ x_{2}^{\\prime}(t) =\u0026amp;\\ F_{2}(t,x_{1},x_{2},\\cdots,x_{n}) \\\\ \\vdots \u0026amp; \\\\ x_{n}^{\\prime}(t) =\u0026amp;\\ F_{n}(t,x_{1},x_{2},\\cdots,x_{n}) \\end{align*} \\tag{1} $$\nこの時、各$F_{i}$が線形ならば線形システムと呼び、そうでなければ非線形システムと呼ぶ。1階線形微分方程式システムのより一般的な形は以下の通りだ。\n$$ \\begin{align*} x_{1}^{\\prime}(t) =\u0026amp;\\ p_{11}(t)x_{1}(t)+\\cdots p_{1n}(t)x_{n}(t) + g_{1}(t) \\\\ x_{2}^{\\prime}(t) =\u0026amp;\\ p_{21}(t)x_{1}(t)+\\cdots p_{2n}(t)x_{n}(t) + g_{2}(t) \\\\ \\vdots \u0026amp; \\\\ x_{n}^{\\prime}(t) =\u0026amp;\\ p_{n1}(t)x_{1}(t)+\\cdots p_{nn}(t)x_{n}(t) + g_{n}(t) \\end{align*} $$\n$$ \\mathbf{x}^{\\prime}(t) = \\mathbf{P}(t)\\mathbf{x}(t) + \\mathbf{g}(t) $$\nこの時、$\\mathbf{x}$、$\\mathbf{g}$はベクトル値関数、$\\mathbf{P}$は行列関数だ。 各$g_{i}(t)$が$0$であれば同次システム、そうでなければ非同次システムと呼ぶ。\nソリューション 区間$I : \\alpha \\lt t \\lt \\beta$上のODEシステム$(1)$のソリューションは、区間$I$上の各点で微分可能な$n$個の関数で構成される。\n$$ x_{1} = \\phi_{1}(t),\\quad x_{2} = \\phi_{2}(t),\\quad \\dots,\\quad x_{n} = \\phi_{n}(t) $$\n初期条件 固定された$t_{0} \\in I$と$x_{i}^{0}$に対して、次の$n$個の条件を初期条件と呼ぶ。\n$$ x_{1}(t_{0}) = x_{1}^{0},\\quad x_{2}(t_{0}) = x_{2}^{0},\\quad \\cdots,\\quad x_{n}(t_{0}) = x_{n}^{0} \\tag{2} $$\nODEシステム$(1)$と初期条件$(2)$を組み合わせたものを初期値問題と呼び、通常はIVPと略される。「初期値問題のソリューションを見つけること」を「初期値問題を解く」と言う。初期値問題のソリューションは、ピカールの定理によってその存在と一意性が保証されている。\nWilliam E. Boyce, Boyce\u0026rsquo;s Elementary Differential Equations and Boundary Value Problems (11th Edition, 2017), p281-283\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1659,"permalink":"https://freshrimpsushi.github.io/jp/posts/1659/","tags":null,"title":"一次線形微分方程式システム"},{"categories":"힐베르트공간","contents":"ビルドアップ1 ヒルベルト空間 $\\left( H, \\left\\langle \\cdot , \\cdot \\right\\rangle_{H} \\right)$ と $\\left( K, \\left\\langle \\cdot , \\cdot \\right\\rangle_{K} \\right)$ において有界線形作用素 $T : K \\to H$ が与えられているとしよう。すると、任意の固定された元 $\\mathbf{w} \\in H$ に対して、以下のように定義された $\\Phi : K \\to \\mathbb{C}$ は線形汎関数 $\\Phi \\in K^{ \\ast }$ になる。\n$$ \\Phi \\mathbf{v} := \\left\\langle T \\mathbf{v} , \\mathbf{w} \\right\\rangle_{H} $$\nリースの表現定理によれば、ヒルベルト空間 $K$ は $\\Phi \\in K^{ \\ast }$ とすべての $\\mathbf{v} \\in K$ に対して以下を満たす唯一の元 $T^{ \\ast } \\mathbf{w} \\in K$ が存在しなければならない。\n$$ \\Phi \\mathbf{v} = \\left\\langle \\mathbf{v} , T^{ \\ast } \\mathbf{w} \\right\\rangle_{K} $$\n先に固定された元 $\\mathbf{w} \\in H$ に対して $T^{ \\ast } \\mathbf{w} \\in K$ が具体的に何であるかはわからないが、$T^{ \\ast }$ は $\\mathbf{w}$ を $T^{ \\ast } \\mathbf{w}$ にマッピングする作用素 $T^{ \\ast } : H \\to K$ と見なすことができる。このような議論から、次のような概念を思い浮かべることができる。\n定義 $H,K$ がヒルベルト空間であるとする。有界線形作用素 $T : K \\to H$ に対して、以下を満たす $T^{ \\ast } : H \\to K$ を $T$ の随伴作用素adjoint operatorと呼ぶ。\n$$ \\left\\langle T \\mathbf{v} , \\mathbf{w} \\right\\rangle_{H} = \\left\\langle \\mathbf{v} , T^{ \\ast } \\mathbf{w} \\right\\rangle_{K} ,\\quad \\forall \\mathbf{v} \\in K $$\n説明 双対作用素dual operatorとも呼ばれる。$T^{\\#}$とも表示される。\n随伴作用素は以下のような性質を持つ。\n$T^{ \\ast }$ は線形であり、有界である。 $\\left( T^{ \\ast } \\right)^{ \\ast } = T$ $\\left\\| T^{ \\ast } \\right\\| = \\left\\| T \\right\\|$ 一方、$H = K$ の時、次のような良い性質を持つ随伴作用素はもっと特別な名前で呼ばれる。$H$ がヒルベルト空間であり、$T : H \\to H$ が線形で有界であるとする1。\n$T = T^{ \\ast }$ の場合、$T$ は自己随伴self-adjointと呼ばれる。 $TT^{ \\ast } = T^{ \\ast }T = I$ の場合、$T$ はユニタリunitaryと呼ばれる。 $T$ が自己随伴である場合、全ての $\\mathbf{v} , \\mathbf{w} \\in H$ に対して\n$$ \\left\\langle T \\mathbf{v} , \\mathbf{w} \\right\\rangle = \\left\\langle \\mathbf{v} , T \\mathbf{w} \\right\\rangle $$\n$T$ がユニタリである場合、全ての $\\mathbf{v} , \\mathbf{w} \\in H$ に対して\n$$ \\left\\langle T \\mathbf{v} , T \\mathbf{w} \\right\\rangle = \\left\\langle \\mathbf{v} , \\mathbf{w} \\right\\rangle $$\n定義から、ユニタリ $T$ は可逆であり、\n$$ T^{-1} = T^{ \\ast } $$\n特に、ユニタリ作用素は正規直交基底と関連した非常に重要な性質を持つ。\nOle Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010), p71-72\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1562,"permalink":"https://freshrimpsushi.github.io/jp/posts/1562/","tags":null,"title":"ヒルベルト空間の共役作用素"},{"categories":"수리물리","contents":"説明 微分方程式を解く方法の一つに、微分演算子を使って解く方法がある。微分演算子$D$を以下のように定義しよう。\n$$ D:= \\frac{d}{dx} $$\n微分される変数を明確に表示する時は、$D_{x}$のように記される。偏微分においては、以下のように表される。\n$$ \\partial _{x}:=\\frac{ \\partial }{ \\partial x},\\quad \\partial_{y}=\\frac{ \\partial }{ \\partial y} $$\n微分演算子を使えば、微分方程式は以下のように表される。\n$$ \\begin{align*} y^{\\prime \\prime}+4y^{\\prime}-y=0 \u0026amp;\u0026amp; \\implies\u0026amp;\u0026amp; D^{2}y+4Dy-y=0 \\\\ \u0026amp;\u0026amp; \u0026amp;\u0026amp; (D^{2}+4D-1)y=0 \\end{align*} $$ ここで、$y=0$の解が物理的に意味がない。したがって、微分方程式の解は$Dy=ry$を満たす定数$r$に関する二次方程式 $$ r^{2}+4r-1=0 $$ を解くことに変わる。$Dy=ry$を解くことは固有値問題であるため、実質的に固有値問題を解けば微分方程式を解いたことになる。微分演算子は微分が含まれているので、操作の順序に特に注意が必要だ。例えば、$D$と$x$は交換できず、$Dx\\ne xD$となる。$y$を$x$に関する関数とすると、 $$ Dxy=D(xy)=\\frac{ d }{ d x }(xy)=y+xy^{\\prime}=y+xDy=(xD+1)y $$ であるため、 $$ Dx=xD+1 $$ である。微分演算子について、以下のような有用な性質がある。\n性質 $$ \\begin{align*} D(D+x) \u0026amp;= D^{2}+xD+1 \\tag{a} \\\\ (D-a)(D-b)=(D-b)(D-a) \u0026amp;= D^{2}-(a+b)D+ab \\tag{b} \\\\ (D+1)(D^{2}-D+1) \u0026amp;= D^{3}+1 \\tag{c} \\\\ Dx \u0026amp;= xD+1 \\tag{d} \\\\ (D-x)(D+x) \u0026amp;=D^{2}-x^{2}+1 \\tag{e} \\\\ (D+x)(D-x) \u0026amp;= D^{2}-x^{2}-1 \\tag{f} \\end{align*} $$\n証明 証明方法が同じであるため、いくつかの証明過程を省略する。\n$(a)$ $$ \\begin{align*} D(D+x)y \u0026amp;= D(y^{\\prime}+xy) \\\\ \u0026amp;= y^{\\prime \\prime}+y+xy^{\\prime} \\\\ \u0026amp;= D^{2}y+xDy+y \\\\ \u0026amp;= (D^{2}+xD+1)y \\end{align*} $$ したがって、 $$ D(D+x) = D^{2}+xD+1 $$\n■\n$(b)$ $$ \\begin{align*} (D-a)(D-b)y \u0026amp;=(D-a)(y^{\\prime}-by) \\\\ \u0026amp;= y^{\\prime \\prime}-ay^{\\prime}-by^{\\prime}+aby \\\\ \u0026amp;= D^{2}y-(a+b)Dy+aby \\\\ \u0026amp;=[D^{2}-(a+b)D+ab]y \\\\ \u0026amp;=[D^{2}-(b+a)D+ba]y \\\\ \u0026amp;=(D-b)(D-a)y \\end{align*} $$ したがって、 $$ (D-a)(D-b)=(D-b)(D-a) = D^{2}-(a+b)D+ab $$ ■\n$(e)$ $$ \\begin{align*} (D-x)(D+x)y \u0026amp;= (D-x)(y^{\\prime}+xy) \\\\ \u0026amp;= y^{\\prime \\prime} -xy^{\\prime} +y+xy^{\\prime}-x^{2}y \\\\ \u0026amp;= D^{2}y+(1-x^{2})y \\\\ \u0026amp;= (D^{2}-x^{2}+1)y \\end{align*} $$ したがって、 $$ (D-x)(D+x)=D^{2}-x^{2}+1 $$\n■\n","id":1638,"permalink":"https://freshrimpsushi.github.io/jp/posts/1638/","tags":null,"title":"物理学における微分作用素とは?"},{"categories":"함수","contents":"ビルドアップ 以下の微分方程式は、変形ベッセル方程式と呼ばれる。\n$$ x^2 y^{\\prime \\prime} + xy^{\\prime}-(x^2-\\nu^2)y=0 $$\nベッセル方程式で$y$項の符号が$+ \\rightarrow -$に変わった形式だ。この微分方程式の解は、ベッセル方程式である微分方程式の公式によって以下のようになる。\n$$ y=Z_{\\nu}(ix)=AJ_{\\nu}(ix)+BN_{\\nu}(ix) $$\n一般的に使われる二つの解の形は、変形ベッセル関数と呼ばれており、特に$I_{\\nu}$を第一種変形ベッセル関数、$K_{\\nu}$を第二種変形ベッセル関数と呼ぶ。\n定義 第一種変形ベッセル関数$I_{\\nu}$と第二種変形ベッセル関数$K_{\\nu}$はそれぞれ次のように定義される。\n$$ \\begin{align*} I_{\\nu}(x)\u0026amp;=i^{-\\nu}J_{\\nu}(ix) \\\\ \\\\ K_{\\nu}(x) \u0026amp;= \\frac{\\pi}{2}i^{\\nu+1}\\left[ J_{\\nu}(ix)+iN_{\\nu}(ix) \\right] \\\\ \u0026amp;= \\frac{\\pi}{2}i^{\\nu+1}H_{p}^{(1)}(ix) \\\\ \u0026amp;=\\frac{\\pi}{2}\\frac{I_{-\\nu}(x)-I_{\\nu}(x)}{\\sin (\\nu\\pi )} \\end{align*} $$\nここで、$J_{\\nu}$ $H_{\\nu}^{(1)}(x)$はハンケル関数である。\n説明 $i$が前に掛けられる理由は、実数$x$に対して$I_{\\nu}(x)$、$K_{\\nu}(x)$の値が実数になるようにするためである。この状況は、$y^{\\prime \\prime}+y=0$の解が$\\cos x$、$\\sin x$であり、$y^{\\prime \\prime}-y=0$の解が$\\cosh x=\\cos (ix)$、$\\sinh (x)=\\sin (ix)$であるものと似ている。方程式のこのような特性のために、$I_{\\nu}$と$K_{\\nu}$は双曲線ベッセル関数とも呼ばれる。\n積分形 2010年にOlverらによって、次のような積分形が知られている1。\n$$ I_{\\nu} (z) = {{ \\left( {{ z } \\over { 2 }} \\right)^{\\nu} } \\over { \\sqrt{\\pi} \\Gamma \\left( \\nu + {{ 1 } \\over { 2 }} \\right) }} \\int_{-1}^{1} e^{zt} \\left( 1 - t^{2} \\right)^{\\nu - {{ 1 } \\over { 2 }}} dt $$\nこのような変形ベッセル関数は、数理物理学だけでなく、方向統計学などでも非常に重要であり、空間統計分析でセミバリオグラムの無難な選択肢の一つであるマテーン関数にも登場する。\nSungkyu Jung. \u0026ldquo;Geodesic projection of the von Mises–Fisher distribution for projection pursuit of directional data.\u0026rdquo; Electron. J. Statist. 15 (1) 984 - 1033, 2021. https://doi.org/10.1214/21-EJS1807\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1624,"permalink":"https://freshrimpsushi.github.io/jp/posts/1624/","tags":null,"title":"変形ベッセル方程式と変形ベッセル関数"},{"categories":"상미분방정식","contents":"定義[^1] ベッセル方程式の二番目の解をノイマン関数と呼び、$N_{\\nu}(x)$または$Y_{\\nu}(x)$で表される。非整数の$\\nu$に対して\n$$ N_{\\nu}(x)=Y_{\\nu}(x)=\\frac{\\cos (\\nu \\pi)J_{\\nu}(x)-J_{-\\nu}(x)}{\\sin (\\nu\\pi)} $$\n$\\nu$が整数の場合は極限で定義する。$n\\in \\mathbb{Z}$、$\\nu \\in \\mathbb{R}\\setminus\\mathbb{Z}$に対して\n$$ N_{n}(x)=\\lim \\limits_{\\nu \\rightarrow n}N_{\\nu}(x) $$\nこの時、$J_{\\pm \\nu}(x)$は第一種ベッセル関数である。従って、ベッセル方程式の一般解は次のようになる。\n$$ y(x)=AJ_{\\nu}(x)+BN_{\\nu}(x) $$\nここで$A$、$B$は任意の定数。\n説明 $$ x^{2}y^{\\prime \\prime} + xy^{\\prime} +(x^{2}-\\nu^{2})y=0 $$\n上記ベッセル方程式の級数解を$J_{\\pm\\nu}(x)$と表し、$\\nu$次の第一種ベッセル関数と言う。\n$$ J_{\\nu}(x)=\\sum \\limits_{n=0}^{\\infty} \\frac{(-1)^{n} }{\\Gamma (n+1) \\Gamma (n+\\nu+1)} \\left(\\frac{x}{2} \\right)^{2n+\\nu} $$\n$$ J_{-\\nu}(x)=\\sum \\limits_{n=0}^{\\infty}\\frac{(-1)^{n}}{\\Gamma (n+1)\\Gamma (n-\\nu+1)} \\left( \\frac{x}{2} \\right)^{2n-\\nu} $$\n見ての通り、二つの解は独立であるから、一般解は以下のようになる。\n$$ y(x)=AJ_{\\nu}(x)+BJ_{-\\nu}(x) $$\nしかし、$\\nu$が整数の場合、二つの解は線形独立ではない。だから、$\\nu$が整数の時でも、$J_{\\nu}(x)$と独立する二番目の解を見つける必要がある。\nちょっと$\\sin x$と$\\cos x$を考えてみよう。二つの関数は線形独立である。しかし、$\\sin x$と$\\cos x$の何らかの線形結合である$2\\sin x -5\\cos x$も$\\sin x$と線形独立である。このようなアイデアで、$J_{\\nu}(x)$と$J_{-\\nu}(x)$の任意の線形結合をベッセル方程式の二番目の解とする。\n$$ \\begin{equation} N_{\\nu}(x)=\\frac{\\cos (\\nu \\pi)J_{\\nu}(x)-J_{-\\nu}(x)}{\\sin (\\nu\\pi)} \\label{eq1} \\end{equation} $$\n$N_{\\nu}(x)$は$\\nu$の条件と関係なく$J_{\\nu}(x)$と独立であることが分かる。しかし、ここで再び問題が発生するが、$\\nu$が整数なら\n$$ N_{\\nu}(x)=\\frac{\\cos (\\nu \\pi)J_{\\nu}(x)-J_{-\\nu}(x)}{\\sin (\\nu\\pi)}=\\frac{(-1)^{\\nu}J_{\\nu}(x)-(-1)^{\\nu}J_{\\nu}(x)}{0}=\\frac{0}{0} $$\n定義できない。従って、$\\nu$が整数の時は以下のように極限を使って定義する。\n$$ N_{n}(x)=\\lim \\limits_{\\nu \\rightarrow n}N_{\\nu}(x)\\quad \\text{for }n\\in \\mathbb{Z},\\ \\nu \\in \\mathbb{R}\\setminus \\mathbb{Z} $$\nこの時、任意の$x \\ne 0$に対して上記の極限が存在する。\nまとめ 整数の$\\nu$に対して、ベッセル関数$J_{\\pm \\nu}(x)$は以下の式を満たす。つまり、独立ではない。\n$$ J_{-\\nu}(x)=(-1)^{\\nu}J_{\\nu}(x) $$\n証明 $$ J_{-\\nu}(x)=\\sum \\limits_{n=0}^{\\infty}\\frac{(-1)^{n}}{\\Gamma (n+1)\\Gamma (n-\\nu+1)} \\left( \\frac{x}{2} \\right)^{2n-\\nu} $$\n$n=k+\\nu$に置き換えると、\n$$ J_{-\\nu}(x)=\\sum \\limits_{k=-\\nu}^{\\infty}\\frac{(-1)^{k+\\nu}}{\\Gamma (k+\\nu+1)\\Gamma (k+1)} \\left( \\frac{x}{2} \\right)^{2k+\\nu} $$\nガンマ関数は$0$と負の整数で発散するので、$k=-\\nu,-\\nu+1,\\cdots,-1$の時、分母の$\\Gamma (k+1)$は発散し、$J_{-\\nu}(x)=0$となる。従って、\n$$ \\begin{align*} J_{-\\nu}(x)\u0026amp;=\\sum \\limits_{k=-\\nu}^{\\infty}\\frac{(-1)^{k+\\nu}}{\\Gamma (k+\\nu+1)\\Gamma (k+1)} \\left( \\frac{x}{2} \\right)^{2k+\\nu} \\\\ \u0026amp;=\\sum \\limits_{k=0}^{\\infty}\\frac{(-1)^{k+\\nu}}{\\Gamma (k+\\nu+1)\\Gamma (k+1)} \\left( \\frac{x}{2} \\right)^{2k+\\nu} \\\\ \u0026amp;=(-1)^{\\nu}\\sum \\limits_{k=0}^{\\infty}\\frac{(-1)^{k}}{\\Gamma (k+\\nu+1)\\Gamma (k+1)} \\left( \\frac{x}{2} \\right)^{2k+\\nu} \\\\ \u0026amp;=(-1)^{\\nu}J_{\\nu}(x) \\end{align*} $$\n■\n","id":1618,"permalink":"https://freshrimpsushi.github.io/jp/posts/1618/","tags":null,"title":"ベッセル方程式の第二の級数解：第二種ベッセル関数、ノイマン関数、ウェーバー関数"},{"categories":"정수론","contents":"定義 1 次のように定義された算術関数 $u$ をユニット関数という。 $$ u(n) := 1 $$\n基本性質 [1] ユニット級数：約数の数に等しい $\\sigma_{0}$。つまり、 $$ \\sum_{d \\mid n} u(d) = \\sigma_{0} (n) $$ [2] 完全乗法性：全ての $m,n \\in \\mathbb{N}$ に対して $u(mn) = u(m) u(n)$ 説明 $$ \\begin{matrix} n \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; 4 \u0026amp; 5 \u0026amp; 6 \u0026amp; 7 \u0026amp; 8 \u0026amp; 9 \u0026amp; 10 \\\\ u (n) \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \\\\ \\sum_{d \\mid n} u(d) \u0026amp; 1 \u0026amp; 2 \u0026amp; 2 \u0026amp; 3 \u0026amp; 2 \u0026amp; 4 \u0026amp; 2 \u0026amp; 4 \u0026amp; 3 \u0026amp; 4 \\end{matrix} $$ ユニット関数という名前からわかるように、とても重要な関数である。畳み込みを考えると、任意の算術関数 $f$ の級数 $F$ は実際には次のように表される。 $$ f \\ast\\ u = F $$\n証明 [1] $$ \\sum_{d \\mid n} u(d) = \\sum_{d \\mid n} 1 = \\sigma_{0} (n) $$ 約数関数の定義により自明である。\n■\n[2] $$ u(mn) = 1 = 1 \\cdot 1 = u(m) u(n) $$\n■\nApostol. (1976). Introduction to Analytic Number Theory: p31.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1547,"permalink":"https://freshrimpsushi.github.io/jp/posts/1547/","tags":null,"title":"解析的数論におけるユニット関数"},{"categories":"함수","contents":"定義 ルジャンドル多項式Legendre polynomialは、いくつかの方法で定義される。\n微分方程式の解として 次のルジャンドル微分方程式の解をルジャンドル多項式という。\n$$ (1-x^{2}) \\dfrac{d^{2} y}{dx^{2}} -2x\\dfrac{dy}{dx} + l(l+1) y = 0 $$\nロドリゲスの公式 次の関数$P_{l}$をルジャンドル多項式という。\n$$ P_{l}(x) = \\dfrac{1}{2^{l} l!} \\dfrac{d^{l}}{dx^{l}}(x^{2}-1)^{l} $$\nこれをロドリゲスの公式という。\n直交性を利用して 説明 定義によると$P_{n}$は確かに多項\u0026lsquo;関数\u0026rsquo;であるが、慣例的にルジャンドル\u0026lsquo;多項式\u0026rsquo;と呼ばれる。これは韓国語だけではなく、英語圏でもLegendre polynomialと呼ばれる。\nルジャンドル多項式は、数学、物理学、工学など様々な分野で使用される。直交性を含む数学的に多くの良い性質を持ち、球座標系でラプラス方程式の解として現れるからである。\n性質 ","id":1611,"permalink":"https://freshrimpsushi.github.io/jp/posts/1611/","tags":null,"title":"ルジャンドル多項式"},{"categories":"확률분포론","contents":"定義 1 $\\alpha , \\beta \u0026gt; 0$について、次のような確率密度関数をもつ連続確率分布$\\text{Beta}(\\alpha,\\beta)$をベータ分布beta Distributionと言う。 $$ f(x) = {{ 1 } \\over { B(\\alpha,\\beta) }} x^{\\alpha - 1} (1-x)^{\\beta - 1} \\qquad , x \\in [0,1] $$\n$B$はベータ関数を示している。 基本性質 モーメント生成関数 [1]: $$m(t) = 1 + \\sum_{k=1}^{\\infty} \\left( \\prod_{r=0}^{k-1} {{ \\alpha + r } \\over { \\alpha + \\beta + r }} {{ t^{k} } \\over { k! }} \\right) \\qquad , t \\in \\mathbb{R}$$ 平均と分散 [2]: $X \\sim \\text{Beta}(\\alpha,\\beta)$ならば $$ \\begin{align*} E(X) =\u0026amp; {\\alpha \\over {\\alpha + \\beta} } \\\\ \\text{Var} (X) =\u0026amp; { { \\alpha \\beta } \\over {(\\alpha + \\beta + 1) { ( \\alpha + \\beta ) }^2 } } \\end{align*} $$ 十分統計量 [3]: ベータ分布に従うランダムサンプル$\\mathbf{X} := \\left( X_{1} , \\cdots , X_{n} \\right) \\sim \\text{Beta} \\left( \\alpha, \\beta \\right)$が与えられているとする。 $\\left( \\alpha, \\beta \\right)$に対する十分統計量$T$は以下のようである。 $$ T = \\left( \\prod_{i} X_{i}, \\prod_{i} \\left( 1 - X_{i} \\right) \\right) $$\n定理 ガンマ分布からの導出 [a]: 二つの確率変数$X_{1},X_{2}$が独立であり、$X_{1} \\sim \\Gamma ( \\alpha_{1} , 1)$、$X_{2} \\sim \\Gamma ( \\alpha_{2} , 1)$であるとすると $$ {{ X_{1} } \\over { X_{1} + X_{2} }} \\sim \\text{beta} \\left( \\alpha_{1} , \\alpha_{2} \\right) $$ F分布からの導出 [b]: 自由度$r_{1} , r_{2}$のF分布に従う確率変数$X \\sim F \\left( r_{1}, r_{2} \\right)$について、定義された$Y$はベータ分布に従う。 $$ Y := {{ \\left( r_{1} / r_{2} \\right) X } \\over { 1 + \\left( r_{1} / r_{2} \\right) X }} \\sim \\text{Beta} \\left( {{ r_{1} } \\over { 2 }} , {{ r_{2} } \\over { 2 }} \\right) $$ 説明 ガンマ分布がガンマ関数から来たように、ベータ分布もベータ関数から名前がついた分布だ。ベータ関数は次のようにガンマ関数との関係を持っており、ガンマ関数で表すことができる。 $$ B(p,q) = {{\\Gamma (p) \\Gamma (q)} \\over {\\Gamma (p+q) }} $$ 実際に、ガンマ分布はベータ分布を導出することができる。\nベータ関数が二項係数の一般化と考えられるように、ベータ分布の確率密度関数をよく観察すると、その形状が二項分布の確率質量関数$P(k) = { _n {C} _k }{ p ^ k }{ (1-p) ^ { n - k } }$に似ていることがわかる。正確にベータ分布の定義に一致するわけではないが、$\\alpha$を成功回数、$\\beta$を失敗回数と考えると、 $$ n = \\alpha + \\beta \\\\ \\displaystyle p = {{\\alpha } \\over {\\alpha + \\beta}} \\\\ \\displaystyle q = {{\\beta } \\over {\\alpha + \\beta}} $$ 似た感じがする。実際に、ベイズでは二項分布の共役事前分布としても使われる。\n証明 [1] 式が複雑だが、論理的に難しい部分はない。\n指数関数の級数展開: $$ { { e ^ x } }=\\sum _{ n=0 }^{ \\infty }{ \\frac { { x } ^{ n } }{ n! } } $$\nオイラー積分: $$ B(p,q)=\\int_0^1 t^{p-1}(1-t)^{q-1}dt $$\n$$ \\begin{align*} m(t) =\u0026amp; \\int_{0}^{1} e^{tx} {{ 1 } \\over { B(\\alpha,\\beta) }} x^{\\alpha - 1} (1-x)^{\\beta - 1} dx \\\\ =\u0026amp; {{ 1 } \\over { B(\\alpha,\\beta) }} \\int_{0}^{1} \\left( \\sum_{k=0}^{\\infty} {{ (tx)^{k} } \\over { k! }} \\right) x^{\\alpha - 1} (1-x)^{\\beta - 1} dx \\\\ =\u0026amp; {{ 1 } \\over { B(\\alpha,\\beta) }} \\sum_{k=0}^{\\infty} {{ t^{k} } \\over { k! }} \\int_{0}^{1} x^{\\alpha + k - 1} (1-x)^{\\beta - 1} dx \\\\ =\u0026amp; {{ 1 } \\over { B(\\alpha,\\beta) }} \\sum_{k=0}^{\\infty} {{ t^{k} } \\over { k! }} B \\left( \\alpha + k , \\beta \\right) \\\\ =\u0026amp; \\sum_{k=0}^{\\infty} {{ t^{k} } \\over { k! }} {{ B \\left( \\alpha + k , \\beta \\right) } \\over { B(\\alpha,\\beta) }} \\\\ =\u0026amp; {{ t^{0} } \\over { 0! }} {{ B \\left( \\alpha + 0 , \\beta \\right) } \\over { B(\\alpha,\\beta) }} + \\sum_{k=1}^{\\infty} {{ t^{k} } \\over { k! }} {{ B \\left( \\alpha + k , \\beta \\right) } \\over { B(\\alpha,\\beta) }} \\end{align*} $$\nベータ関数とガンマ関数の関係: $$B(p,q) = {{\\Gamma (p) \\Gamma (q)} \\over {\\Gamma (p+q) }}$$\nベータ関数をガンマ関数で展開すると\n$$ \\begin{align*} m(t) =\u0026amp; 1 + \\sum_{k=1}^{\\infty} {{ t^{k} } \\over { k! }} {{ B \\left( \\alpha + k , \\beta \\right) } \\over { B(\\alpha,\\beta) }} \\\\ =\u0026amp; 1 + \\sum_{k=1}^{\\infty} {{ t^{k} } \\over { k! }} {{ \\Gamma ( \\alpha + k ) \\Gamma ( \\beta ) } \\over { \\Gamma \\left( \\alpha + \\beta + k \\right) }} {{ \\Gamma ( \\alpha + \\beta ) } \\over { \\Gamma \\left( \\alpha \\right) \\Gamma \\left( \\beta \\right) }} \\\\ =\u0026amp; 1 + \\sum_{k=1}^{\\infty} {{ t^{k} } \\over { k! }} {{ \\Gamma ( \\alpha + k ) } \\over { \\Gamma \\left( \\alpha + \\beta + k \\right) }} {{ \\Gamma ( \\alpha + \\beta ) } \\over { \\Gamma \\left( \\alpha \\right) }} \\\\ =\u0026amp; 1 + \\sum_{k=1}^{\\infty} {{ t^{k} } \\over { k! }} {{ \\Gamma ( \\alpha + k ) } \\over { \\Gamma \\left( \\alpha \\right) }} {{ \\Gamma ( \\alpha + \\beta ) } \\over { \\Gamma \\left( \\alpha + \\beta + k \\right) }} \\\\ =\u0026amp; 1 + \\sum_{k=1}^{\\infty} {{ t^{k} } \\over { k! }} {{ \\Gamma ( \\alpha ) \\prod_{r=0}^{k-1} ( \\alpha + r) } \\over { \\Gamma \\left( \\alpha \\right) }} {{ \\Gamma ( \\alpha + \\beta ) } \\over { \\Gamma \\left( \\alpha + \\beta \\right) \\prod_{r=0}^{k-1} ( \\alpha + \\beta + r) }} \\\\ =\u0026amp; 1 + \\sum_{k=1}^{\\infty} {{ t^{k} } \\over { k! }} \\prod_{r=0}^{k-1} {{ \\alpha + r } \\over { \\alpha + \\beta + r }} \\end{align*} $$\n■\n[2] 直接導く。\n■\n[3] $(1 - x)$で何か気持ち悪さがあるかもしれないが、ただ直接導く。\n[a] 確率密度関数を使って直接導く。\n■\n[b] 確率密度関数を使って直接導く。\n■\nコード 以下は、ベータ分布の確率密度関数をGIFで示すJuliaコードである。\n@time using LaTeXStrings @time using Distributions @time using Plots cd(@__DIR__) x = 0:0.01:1 B = collect(0.1:0.1:10.0); append!(B, reverse(B)) animation = @animate for β ∈ B plot(x, pdf.(Beta(0.5, β), x), color = :black, label = \u0026#34;α = 0.5, β = $(rpad(β, 3, \u0026#39;0\u0026#39;))\u0026#34;, size = (400,300)) xlims!(0,1); ylims!(0,5); title!(L\u0026#34;\\mathrm{pmf\\,of\\,Beta} (0.5, \\beta)\u0026#34;) end gif(animation, \u0026#34;pdf0.gif\u0026#34;) animation = @animate for β ∈ B plot(x, pdf.(Beta(1, β), x), color = :black, label = \u0026#34;α = 1, β = $(rpad(β, 3, \u0026#39;0\u0026#39;))\u0026#34;, size = (400,300)) xlims!(0,1); ylims!(0,5); title!(L\u0026#34;\\mathrm{pmf\\,of\\,Beta} (1, \\beta)\u0026#34;) end gif(animation, \u0026#34;pdf1.gif\u0026#34;) animation = @animate for β ∈ B plot(x, pdf.(Beta(2, β), x), color = :black, label = \u0026#34;α = 2, β = $(rpad(β, 3, \u0026#39;0\u0026#39;))\u0026#34;, size = (400,300)) xlims!(0,1); ylims!(0,5); title!(L\u0026#34;\\mathrm{pmf\\,of\\,Beta} (2, \\beta)\u0026#34;) end gif(animation, \u0026#34;pdf2.gif\u0026#34;) Hogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p165.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1540,"permalink":"https://freshrimpsushi.github.io/jp/posts/1540/","tags":null,"title":"ベータ分布"},{"categories":"상미분방정식","contents":"定義1 下の微分方程式を、関連ルジャンドル微分方程式という。\n$$ \\begin{equation} \\begin{aligned} \u0026amp;\u0026amp;(1-x^{2})\\frac{ d^{2}y }{ dx^{2} }-2x \\frac{dy}{dx}+\\left[ +l(l+1)-\\frac{m^{2}}{1-x^{2}} \\right]y =\u0026amp;\\ 0 \\\\ \\mathrm{or} \u0026amp;\u0026amp; \\frac{ d }{ dx } \\left[ (1-x^{2})y^{\\prime} \\right] +\\left[ l(l+1)-\\frac{m^{2}}{1-x^{2}} \\right]y =\u0026amp;\\ 0 \\end{aligned} \\label{1} \\end{equation} $$\n関連ルジャンドル微分方程式の解を$P_{l}^{m}(x)$として表記し、これを関連ルジャンドル多項式や一般化されたルジャンドル多項式という。\n$$ \\begin{align*} P_{l}^{m}(x)\u0026amp;= (1-x ^{2})^{\\frac{|m|}{2}} \\frac{ d^{|m|} }{ dx^{|m|} } P_{l}(x) \\\\ \u0026amp;=(1-x ^{2})^{\\frac{|m|}{2}} \\frac{ d^{|m|} }{ dx^{|m|} }\\left[ \\dfrac{1}{2^{l} l!} \\dfrac{d^{l}}{dx^{l}}(x^2-1)^{l}\\right] \\end{align*} $$\nここで、$P_{l}(x)$はルジャンドル多項式だ。 $m$の符号によって区分される場合、\n$$ P_{l}^{m}(x) = (1-x ^{2})^{\\frac{m}{2}} \\dfrac{1}{2^{l} l!} \\dfrac{d^{l+m}}{dx^{l+m}}(x^2-1)^{l} $$\n$$ P_{l}^{-m}=(-1)^{m}\\frac{(l-m)!}{(l+m)!}P_{l}^{m}(x) $$\n関連ルジャンドル多項式は、球座標系のラプラス方程式を解く際に出現する。ここで、定数$l$、$m$は、量子力学で量子数と関連している。\n解 $m=0$の場合は、ルジャンドル微分方程式だ。この場合の解を基に、$m\\ne 0$の場合の解も見つけることができる。まず、関連ルジャンドル微分方程式の解は定数$l$、$m$によって決まるので、以下のように表記しよう。\n$$ y=P_{l}^{m}(x) $$\nこれを$\\eqref{1}$に代入して整理すると以下のようになる。\n$$ \\begin{equation} \\frac{ d }{ dx }\\left[ (1-x^{2})\\frac{ d P_{l}^{m}(x)}{ dx } \\right]+\\left[ l(l+1)-\\frac{m^{2}}{1-x^{2}} \\right]P_{l}^{m}(x)=0 \\label{2} \\end{equation} $$\nそして、解が以下の形であると仮定しよう。\n$$ P_{l}^{m}(x)=(1-x^{2})^{\\frac{|m|}{2}}u(x) $$\n$x$を一度微分すると\n$$ \\frac{ d P_{l}^{m}(x)}{ d x }=-|m|x(1-x^{2})^{\\frac{|m|}{2}-1}u(x)+(1-x^{2})^{\\frac{|m|}{2}}u^{\\prime}(x) $$\nこれを$\\eqref{2}$の最初の項に代入して整理すると以下のようになる。\n$$ \\begin{align*} \\frac{ d }{ dx }\\left[ (1-x^{2})\\frac{ d P_{l}^{m}(x)}{ dx } \\right] =\u0026amp;\\ \\frac{ d }{ dx }\\left[ -|m|x(1-x^{2})^{\\frac{|m|}{2}}u(x)+(1-x^{2})^{\\frac{|m|}{2}+1}u^{\\prime}(x) \\right] \\\\ =\u0026amp;\\ -|m|(1-x^{2})^{\\frac{|m|}{2}}u(x)+|m|^{2}x^{2}(1-x^{2})^{\\frac{|m|}{2}-1}u(x) \\\\ \u0026amp; -|m|x(1-x^{2})^{\\frac{|m|}{2}}u^{\\prime}(x)-(|m|+2)x(1-x^{2})^{\\frac{|m|}{2}}u^{\\prime}(x) \\\\ \u0026amp; +(1-x^{2})^{\\frac{|m|}{2}+1}u^{\\prime \\prime}(x) \\\\ =\u0026amp;\\ (1-x^{2})^{\\frac{|m|}{2}+1}u^{\\prime \\prime}(x)-2(|m|+1)(1-x^{2})^{\\frac{|m|}{2}}u^{\\prime}(x) \\\\ \u0026amp; -[|m|(|m|+1)x^{2}-|m|] (1-x^{2})^{\\frac{|m|}{2}-1}u(x) \\end{align*} $$\n両辺に$\\dfrac{1}{(1-x^{2})^{|m|/2}}$を掛けると次のようになる。\n$$ \\begin{align*} \u0026amp;\\frac{1}{(1-x^{2})^{|m|/2}}\\frac{ d }{ dx }\\left[ (1-x^{2})\\frac{ d P_{l}^{m}(x)}{ dx } \\right] \\\\ =\u0026amp;\\ (1-x^{2})u^{\\prime \\prime}(x)-2(|m|+1)xu^{\\prime}(x) -[|m|(|m|+1)x^{2}-|m|] (1-x^{2})^{-1}u(x) \\end{align*} $$\nしたがって、$\\eqref{2}$の両辺に$\\dfrac{1}{(1-x^{2})^{|m|/2}}$を掛けると\n$$ \\begin{equation} \\begin{aligned} \u0026amp;(1-x^{2})u^{\\prime \\prime}(x)-2(|m|+1)xu^{\\prime}(x) \\\\ \u0026amp;-\\left( \\frac{|m|(|m|+1)x^{2}-|m|}{1-x^{2}}+l(l+1)-\\frac{m^{2}}{1-x^{2}}\\right)u(x)=0 \\end{aligned} \\label{1} \\end{equation} $$\n$u(x)$の係数を整理すると以下のようになる。\n$$ \\begin{align*} \u0026amp;\\frac{|m|(|m|+1)x^{2}-|m|}{1-x^{2}}+l(l+1)-\\frac{m^{2}}{1-x^{2}} \\\\ =\u0026amp;\\ \\frac{|m|(|m|+1)x^{2}-|m|+l(l+1)(1-x^{2})-m^{2}}{1-x^{2}} \\\\ =\u0026amp;\\ \\frac{-m^{2}(1-x^{2})-|m|(1-x^{2})+l(l+1)(1-x^{2})}{1-x^{2}} \\\\ =\u0026amp;\\ l(l+1)-m^{2}-|m| \\\\ =\u0026amp;\\ l(l+1)-|m|(|m|+1) \\end{align*} $$\n従って、$\\eqref{3}$は以下のような形で整理される。\n$$ \\begin{equation} (1-x^{2})\\frac{ d^{2} u }{ d x^{2} }-2(|m|+1)x\\frac{ d u}{ dx }+[l(l+1)-|m|(|m|+1)]u=0 \\label{4} \\end{equation} $$\n$m=0$であれば、実際にルジャンドル微分方程式になる。従って、$|m|=0$の場合の解は$P_{l}^{0}(x)=P_{l}(x)$である。今一度、$(4)$を$x$に関して微分してみよう。係数を整理すると以下の式を得る。\n$$ \\begin{equation} (1-x^{2}) \\frac{ d^{3} u }{ d x^{3} } -2[(|m|+1)+1]x\\frac{ d^{2} u}{ dx^{2} }+[l(l+1)-(|m|+1)(|m|+2)]\\frac{ d u}{ d x}=0 \\label{5} \\end{equation} $$\n再び、$\\eqref{5}$を$x$に関して微分して係数を整理すると以下の式を得る。\n$$ \\begin{equation} (1-x^{2}) \\frac{ d^{4} u }{ d x^{4} } -2[(|m|+2)+1]x\\frac{ d^{3} u}{ dx^{3} }+[l(l+1)-(|m|+2)(|m|+3)]\\frac{ d^{2} u}{ d x^{2}}=0 \\label{6} \\end{equation} $$\nこれらの式をよく見ると、$\\eqref{4}$を$u$に、$|m|$を$|m|+1$に置き換えたときに$\\eqref{5}$を得ることができることがわかる。$\\eqref{5}$で同様の方法で置換すると、$\\eqref{6}$を得る。これにより、$|m|=0$の場合の解は$P_{l}(x)$、$|m|=1$の場合の解は$\\dfrac{ d }{ d x }P_{l}(x)$、$|m|=2$の場合の解は$\\dfrac{d^{2}}{dx^{2}}P_{l}(x)$であることがわかる。したがって、これを一般化すると次のようになる。\n$$ u(x)=\\frac{d^{|m|}}{dx^{{|m|}}}P_{l}(x) $$\nしたがって、関連ルジャンドル多項式は以下のようになる。\n$$ \\begin{align*} P_{l}^{m}(x)\u0026amp;= (1-x ^{2})^{\\frac{|m|}{2}} \\frac{ d^{|m|} }{ dx^{|m|} } P_{l}(x) \\\\ \u0026amp;=(1-x ^{2})^{\\frac{|m|}{2}} \\frac{ d^{|m|} }{ dx^{|m|} }\\left[ \\dfrac{1}{2^{l} l!} \\dfrac{d^{l}}{dx^{l}}(x^2-1)^{l}\\right] \\end{align*} $$\n■\nMary L. Boas, 数理物理学(Mathematical Methods in the Physical Sciences, 最中峻輔 訳) (3rd Edition, 2008), p597-598\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1605,"permalink":"https://freshrimpsushi.github.io/jp/posts/1605/","tags":null,"title":"関連するルジャンドル微分方程式と多項式"},{"categories":"양자역학","contents":"概要 時間に依存しないシュレーディンガー方程式time independent Schrodinger equation $$ H\\psi=\\left(-\\frac{\\hbar^{2}}{2m}\\frac{ d ^{2} }{ d x^{2} }+V\\right)\\psi=E\\psi \\\\ H\\psi=\\left(-\\frac{\\hbar^{2}}{2m}\\nabla^{2}+V\\right)\\psi=E\\psi $$\n時間に依存するシュレーディンガー方程式time dependent Schrodinger equation $$ i\\hbar\\frac{ \\partial \\psi}{ \\partial t}=\\left(-\\frac{\\hbar^{2}}{2m}\\frac{ \\partial ^{2} }{\\partial x^{2} }+V\\right)\\psi \\\\ i\\hbar\\frac{ \\partial \\psi}{ \\partial t}=\\left(-\\frac{\\hbar^{2}}{2m}\\nabla^{2}+V\\right)\\psi $$\nシュレーディンガー方程式とは、複素波動関数のエネルギー、位置、時間に関連する偏微分方程式を言う。簡単に言うと、古典力学では\n$$ F=ma $$\nのようなものである。これを使って、様々なポテンシャル状況での波動関数と波動関数のエネルギーを計算することができる。まず、波数が$k$で角振動数が$\\omega$の時間と位置に関する1次元波動関数は下のようである。\n$$ \\psi (x,t)=e^{i(kx-\\omega t)} \\tag{1} $$\n方程式を簡単にするために、前の定数は省略された。ド・ブロイ関係式は下のようである。\n$$ \\lambda=\\frac{h}{p} $$\n$$ k=\\frac{p}{\\hbar} \\tag{2} $$\nプランクの黒体放射とアインシュタインの光電効果から、下の関係式を得る。\n$$ E=h\\nu=\\hbar \\omega \\tag{3} $$\n$\\nu=\\frac{\\omega}{2\\pi}$は粒子の振動数である。量子力学は波動関数と演算子、固有値方程式を通じて記述されるので、これを使ってシュレーディンガー方程式を導出する。\n時間に依存しないシュレーディンガー方程式 固有関数を波動関数 $\\psi$とし、固有値を$\\psi$のエネルギー$E$とするエネルギー演算子$E_{op}$を得るのが目的である。粒子のエネルギーは運動エネルギー+ポテンシャルエネルギーなので\n$$ E=\\frac{p^{2}}{2m}+V $$\nド・ブロイ関係式$(2)$により$p=k\\hbar$であるため\n$$ E=\\frac{\\hbar^{2}k^{2}}{2m}+V $$\n両辺に波動関数$\\psi$を掛けると\n$$ \\frac{\\hbar^{2}k^{2}}{2m}\\psi+V\\psi=E\\psi \\tag{4} $$\nこの時、波動関数が$(1)$なので\n$$ \\frac{d^{2}\\psi }{dx^{2} }=-k^{2}\\psi\\quad \\implies\\quad -\\frac{\\hbar^{2}}{2m}\\frac{d^{2}\\psi }{dx^{2} }=\\frac{\\hbar^{2}k^{2}}{2m}\\psi $$\nしたがって$(4)$は\n$$ \\begin{align*} \u0026amp;\u0026amp;-\\frac{\\hbar^{2}}{2m}\\frac{ d ^{2}\\psi}{ dx^{2} }+V\\psi=E\\psi \\\\ \\implies \u0026amp;\u0026amp;\\left(-\\frac{\\hbar^{2}}{2m}\\frac{ d ^{2}}{ dx^{2} }+V\\right)\\psi=E\\psi \\end{align*} $$\nこの方程式を時間に依存しないシュレーディンガー方程式と呼ぶ。また、エネルギーを得るエネルギー演算子\n$$ -\\frac{\\hbar^{2}}{2m}\\frac{ d ^{2}}{ dx^{2} }+V=H $$\nを簡単に$H$と表示し、ハミルトニアンと言う。3次元場合、ハミルトニアンとシュレーディンガー方程式は以下のようである。\n$$ H=-\\frac{\\hbar^{2}}{2m}\\nabla^{2}+V $$\n$$ \\left(-\\frac{\\hbar^{2}}{2m}\\nabla^{2}+V\\right)\\psi=E\\psi \\tag{5} $$\n$H$を使用して、時間に依存しないシュレーディンガー方程式を簡単に表現すると\n$$ H\\psi=E\\psi $$\n時間に依存するシュレーディンガー方程式 $(3)$によれば、粒子のエネルギーは角振動数$\\omega$とプランク定数$\\hbar$で表される。角振動数は、波動関数$(1)$を時間に対して微分した時に得られる。 $$ \\frac{ \\partial \\psi}{ \\partial t }=-i\\omega\\psi $$ したがって、 $$ E\\psi=\\hbar \\omega \\psi=i\\hbar\\frac{ \\partial \\psi}{ \\partial t } $$ これを$(5)$に代入すると、時間に依存するシュレーディンガー方程式が得られる。 $$ i\\hbar\\frac{ \\partial \\psi}{ \\partial t}=\\left(-\\frac{\\hbar^{2}}{2m}\\frac{ \\partial ^{2} }{ \\partial x^{2} }+V\\right)\\psi \\\\ i\\hbar\\frac{ \\partial \\psi}{ \\partial t}=\\left(-\\frac{\\hbar^{2}}{2m}\\nabla^{2}+V\\right)\\psi $$\n","id":1598,"permalink":"https://freshrimpsushi.github.io/jp/posts/1598/","tags":null,"title":"シュレーディンガー方程式の導出"},{"categories":"수리물리","contents":"説明 ラプラス方程式を満たす関数を調和関数という。球面調和関数は球座標系でのラプラス方程式を満たす関数を言い、正確には径成分を除いた極角$\\theta$と方位角$\\phi$に対する一般解を意味する。\nProof The Laplace equation in the spherical coordinate system is as follows.\n$$ \\begin{equation} \\nabla ^2 f = \\frac{1}{r^2}\\frac{\\partial}{\\partial r} \\left( r^2\\frac{\\partial f}{\\partial r} \\right) + \\frac{1}{r^2\\sin\\theta}\\frac{\\partial}{\\partial\\theta}\\left( \\sin\\theta \\frac{\\partial f}{\\partial \\theta} \\right) + \\frac{1}{r^2\\sin^2\\theta}\\frac{\\partial^2 f}{\\partial^2 \\phi}=0 \\label{eq1} \\end{equation} $$\nAssuming that $f$ is separable in variables,\n$$ f=R(r)\\Theta (\\theta)\\Phi (\\phi) $$\nSubstituting $f$ into the Laplace equation $(1)$ and multiplying both sides by $\\dfrac{r^{2} }{R\\Theta \\Phi}$ yields the following.\n$$ {\\color{blue}\\frac{1}{R}\\frac{d}{d r} \\left( r^2\\frac{d R}{d r} \\right)} + {\\color{green}\\frac{1}{\\Theta\\sin\\theta}\\frac{d}{d\\theta}\\left( \\sin\\theta \\frac{d \\Theta}{d \\theta} \\right) + \\frac{1}{\\Phi\\sin^2\\theta}\\frac{d^2 \\Phi}{d \\phi^2}} =0 $$\nNow, the first term affects only $r$, and the rest affects only $\\theta$, $\\phi$. Therefore, each part is constant because if $r$ changes, affecting the first term, the rest remains unchanged, invalidating the equation. That\u0026rsquo;s why each term highlighted in colors is constant.\nLet\u0026rsquo;s consider the term regarding $\\theta$, $\\phi$ as a constant $-l(l+1)$. Initially, it is assumed that $l$ is any complex number, but eventually, it must be an integer.\n$$ \\frac{1}{\\Theta\\sin\\theta}\\frac{d}{d\\theta}\\left( \\sin\\theta \\frac{d \\Theta}{d \\theta} \\right) + \\frac{1}{\\Phi\\sin^2\\theta}\\frac{d^2 \\Phi}{d \\phi^{2}}=-l(l+1) $$\nMultiplying both sides by $\\sin ^{2} \\theta$ and rearranging gives the following.\n$$ \\begin{equation} \\frac{\\sin \\theta}{\\Theta}\\frac{d}{d\\theta}\\left( \\sin\\theta \\frac{d \\Theta}{d \\theta} \\right)+l(l+1)\\sin^{2}\\theta =- \\frac{1}{\\Phi}\\frac{d^2 \\Phi}{d \\phi^{2}} \\label{eq2} \\end{equation} $$\nSince the variables $\\theta$, $\\phi$ are separated on both sides, as before, each side is constant. Let\u0026rsquo;s set the right side as $m^{2}$. Then, we obtain the following.\n$$ \\frac{d^2 \\Phi}{d \\phi^{2}}=-m^{2}\\Phi $$\nThis is a simple second-order differential equation. In brief, a function that differentiates into itself is an exponential function. However, since the constant must become negative when squared, it must include $i$ in the exponent.\n$$ \\begin{equation} \\Phi (\\phi)=e^{im\\phi} \\label{eq3} \\end{equation} $$\nThe reason why $e^{-im\\phi}$ is not considered is that solutions exist for both positive and negative values of the same magnitude for $m$. For example, when there\u0026rsquo;s a solution for $m=-1, 0,1 $, both $e^{i\\phi}$ and $e^{-i\\phi}$ can be obtained, so it\u0026rsquo;s sufficient to express the solution with $(3)$ only. Since $\\phi$ is the azimuthal angle, it must satisfy $\\Phi (\\phi)=\\Phi (\\phi+2\\pi)$. Therefore, we obtain the following.\n$$ e^{im\\phi}=e^{im(\\phi+2\\pi)}=e^{im\\phi}e^{i2m\\pi} \\\\ \\implies e^{i2m\\pi}=1 $$\nUsing Euler\u0026rsquo;s formula, we obtain the following.\n$$ e^{i2m\\pi}=\\cos(2m\\pi)+i\\sin(2m\\pi)=1 $$\nFor this equation to hold, $m$ must be an integer.\n$$ m=0,\\pm1,\\pm2,\\cdots $$\nThis result is related to the fact in quantum mechanics that the magnetic quantum number exists only as an integer value between $-l$ and $l$. Now, let\u0026rsquo;s solve the differential equation for $\\theta$. From $(2)$, we obtain the following equation.\n$$ \\sin \\theta\\frac{d}{d\\theta}\\left( \\sin\\theta \\frac{d \\Theta}{d \\theta} \\right)+[l(l+1)\\sin^{2}\\theta -m^2]\\Theta=0 $$\nThis is an associated Legendre differential equation, and its solution is as follows.\n$$ \\begin{align} \\Theta (\\theta) \u0026amp;= P_{l}^{m}(\\cos \\theta) \\nonumber \\\\ \u0026amp;= (1-\\cos ^{2}\\theta)^{\\frac{|m|}{2}} \\frac{ d^{|m|} }{ dx^{|m|} } P_{l}(x) \\nonumber \\\\ \u0026amp;=(1-\\cos ^{2}\\theta)^{\\frac{|m|}{2}} \\frac{ d^{|m|} }{ dx^{|m|} }\\left[ \\dfrac{1}{2^l l!} \\dfrac{d^l}{dx^l}(x^2-1)^l \\right] \\label{eq4} \\end{align} $$\nHere, $P_{l}(x)$ is a Legendre polynomial.\n$$ P_{l}(x)=\\dfrac{1}{2^l l!} \\dfrac{d^l}{dx^l}(x^2-1)^l $$\nFrom the condition of the Legendre polynomial, $l$ is a non-negative integer.\n$$ l=0,1,2,\\cdots $$\nWhere $\\eqref{eq4}$, the polynomial of order $(x^{2}-1)^{l}$, is differentiated $l+|m|$ times, leading to $|m|\u0026gt;l$, rendering it meaningless. Therefore, $m$ must satisfy the condition of $-l\\le m \\le l$.\n$$ m=-l,-l+1,\\cdots,-2,-1,0,1,2,\\cdots,l-1,l $$\nFinally, the spherical harmonic function $Y_{l}^{m}(\\theta,\\phi)$ is as follows.\n$$ Y_{l}^{m}(\\theta,\\phi)=e^{im\\phi}P_{l}^{m}(\\cos \\theta) $$\nHere, $l$ is $l=0,1,2\\cdots$ and $m$ is an integer that satisfies $ -l\\le m \\le l$.\n■\n","id":1580,"permalink":"https://freshrimpsushi.github.io/jp/posts/1580/","tags":null,"title":"球面調和関数：球面座標ラプラス方程式の極角、方位角に対する一般解"},{"categories":"정수론","contents":"定義 1 素数 $p_{1} , \\cdots , p_{k}$ に対して、自然数 $n$ を以下のように表すとしよう。このように定義された算術関数 $\\mu$ をメビウス関数という。 $$ \\mu (n) := \\begin{cases} 1 \u0026amp;, n=1 \\\\ (-1)^{k} \u0026amp;, a_{1} = \\cdots = a_{k} = 1 \\\\ 0 \u0026amp; , \\text{otherwise} \\end{cases} $$\n基礎性質 [1] メビウス級数: アイデンティティ $I$ だ。言い換えると、 $$ \\sum_{d \\mid n } \\mu (d) = I(n) $$ [2] 乗法性: $\\gcd (m,n) = 1$ を満たす全ての $m, n \\in \\mathbb{N}$ に対して、$\\mu (mn) = \\mu (m) \\mu (n)$ 説明 $$ \\begin{matrix} n \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; 4 \u0026amp; 5 \u0026amp; 6 \u0026amp; 7 \u0026amp; 8 \u0026amp; 9 \u0026amp; 10 \\\\ \\mu (n) \u0026amp; 1 \u0026amp; -1 \u0026amp; -1 \u0026amp; 0 \u0026amp; -1 \u0026amp; 1 \u0026amp; -1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ \\sum_{d \\mid n} \\mu (d) \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\end{matrix} $$ メビウス関数は簡単に言えば、同じ素数が$2$回以上かけられていない数だけを気にする関数だ。素数が$2$回以上かけられていなかった場合、素数が偶数個使われたのか奇数個使われたのかによって正負が変わるだけだ。しかし、これは単に定義についての説明に過ぎない。メビウス関数はそれ自体では直感的な意味が分からないかもしれないが、整数論全般に渡って終わりなく登場する重要な関数だ。特に解析的整数論ではなおさらである。\n証明 [1] $n = p_{1}^{a_{1}} \\cdots p_{k}^{a_{k}} \u0026gt; 1$ とすると、二項定理により、 $$ \\begin{align*} \u0026amp; \\sum_{d \\mid n } \\mu (d) \\\\ =\u0026amp; \\mu (1) \\\\ \u0026amp; + \\mu (p_{1}) + \\cdots + \\mu (p_{k}) \\\\ \u0026amp; + \\mu (p_{1}p_{2}) + \\cdots + \\mu (p_{k-1}p_{k}) \\\\ \u0026amp; \\vdots \\\\ \u0026amp; + \\mu (p_{1}p_{2} \\cdots p_{k-1}p_{k}) \\\\ =\u0026amp; 1 \\\\ \u0026amp; + \\underbrace{(-1) + \\cdots + (-1)}_{k} \\\\ \u0026amp; + \\underbrace{1 + \\cdots + 1}_{k(k-1)/2} \\\\ \u0026amp; \\vdots \\\\ \u0026amp; + (-1)^{k} \\\\ =\u0026amp; 1 + \\binom{k}{1} (-1) + \\binom{k}{2}(-1)^{2} + \\cdots + \\binom{k}{k} (-1)^{k} \\\\ =\u0026amp; [1 + (-1)]^{k} \\end{align*} $$\n■\n[2] $m$ または $n$ のいずれかが素数の平方を約数に持つ場合、$\\mu$ の定義から $\\mu (mn) = 0$ であり、$\\mu (m) \\mu (n) = 0$ であるため、 $$ \\mu (mn) = \\mu (m) \\mu (n) $$ したがって、$m$ と $n$ がそれぞれの素数が一度だけかけられた形であると仮定しよう。 $$ m = p_{1} \\cdots p_{s} \\\\ n = q_{1} \\cdots q_{t} $$ すると、 $$ \\mu (mn) = (-1)^{s + t} = (-1)^{s} (-1)^{t} = \\mu (m) \\mu (n) $$\n■\nApostol. (1976). イントロダクション トゥ アナリティック ナンバー セオリー: p24.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1531,"permalink":"https://freshrimpsushi.github.io/jp/posts/1531/","tags":null,"title":"解析数論におけるメビウス関数"},{"categories":"그래프이론","contents":"定義 グラフ $G$ で、始点が$v \\in V(G)$ で終点が$w \\in V(G)$ のパスの集まりを$P(v,w)$ とし、$v \\in V(G)$ を含むサイクルの集まりを$C(v)$ としよう。そして、ウォーク $x$ の長さを$l(x)$と示そう。\n二つの頂点$v,w \\in V(G)$ 間の距離$d$ は、$v$ が始点で$w$ が終点のパスの長さの中で最も小さい値で定義される。つまり、 $$ d(v,w) := \\min_{v,w \\in V(G)} \\left\\{ l(x) : x \\in P(v,w) \\right\\} $$ 頂点$v \\in V(G)$ に対し、$v$ との距離が正確に$i$ の頂点の集まり$N^{i} (v) \\subset V(G)$ を**$i$-ネイバーフッド**と言う。 グラフ$G$ の直径$\\text{diam}$ は、すべての頂点間の距離の中で最大の値として定義される。つまり、 $$ \\text{diam} (G) := \\sup_{v,w \\in V(G)} \\left\\{ d(v,w) : v \\ne w \\right\\} $$ グラフ$G$ の周長$\\text{girth}$ は、すべてのサイクルの長さの中で最小の値として定義される。つまり、 $$ \\text{girth}(G) := \\min_{v \\in V(G)} \\left\\{ l(x) : x \\in C(v) \\right\\} $$ 説明 グラフ内の頂点間の距離は、ジオデシック距離とも呼ばれる。ユークリッド距離と異なり、一つの頂点から別の頂点に到達するまでの実際の歩数を表しているため、この名前は妥当と言えるだろう。例えば、以下のグラフでは、$d(v,w)$ はどんな場合でも$2$ より小さくなることは無く、$d(v,w) = 2$ になる。\n直径の定義は初めは奇妙に感じられるかもしれない。比喩として、円 $x^2 + y^2 = r^2$を考えてみよう。円上に任意の二点を選んだ場合、二点間の距離が最も遠くなるのは、二点が円の中心に対して対称的に位置するときだ。もちろん、その距離は$2r$で、これが我々が円の直径と呼ぶものだ。例えば、上記のグラフの直径は$2$である。\n周長は概念的に、グラフにおける最小のサイクルのサイズを言う。明示的には述べていないが、グラフにサイクルが存在しない場合は、その周長を無限大とする。また、任意のグラフにおいて周長の最小値が$3$ より低くなることはないと自明である。例えば、下の図を見れば、いくつかのサイクルが存在するものの、最小のサイクルの長さは$3$ であるため、周長は$3$ になる。\n","id":1530,"permalink":"https://freshrimpsushi.github.io/jp/posts/1530/","tags":null,"title":"グラフ内の距離、近傍、直径、周囲"},{"categories":"수리물리","contents":"説明 物理学では、演算子とは、関数を別の関数に対応させる関数を言う。その中でもデル演算子del operatorは、ある関数が与えられたとき、その関数の導関数を関数値として持つ関数である。演算子という言葉が馴染みがない場合は、対象を計算する規則として理解すればいい。例えば、$\\dfrac{d}{dx}$という関数に$f$を入れると、$f^{\\prime}$という関数値が出る。\n$$ \\dfrac{d }{dx} \\left( f \\right) = f^{\\prime} $$\nデル演算子は通常、以下のように紹介される。\n$$ \\nabla = \\frac{ \\partial }{ \\partial x }\\hat{\\mathbf{x}}+\\frac{ \\partial }{ \\partial y }\\hat{\\mathbf{y}}+\\frac{ \\partial }{ \\partial z }\\hat{\\mathbf{z}} $$\n上の式で見られるように、まるでベクトルのように扱われるため、$\\vec{\\nabla}$のように表記されることもある。これを利用して、スカラー関数$f$とベクター関数$\\mathbf{A}=A_{x}\\hat{\\mathbf{x}}+A_{y}\\hat{\\mathbf{y}}+A_{z}\\hat{\\mathbf{z}}$に対する3つの演算を学ぶ。下の3つの演算は、上から順に$f$のグラディエント、$\\mathbf{A}$のダイバージェンス、$\\mathbf{A}$のカールと呼ばれる。\n$$ \\begin{align*} \\nabla f\u0026amp;=\\frac{ \\partial f }{ \\partial x }\\hat{\\mathbf{x}}+\\frac{ \\partial f }{ \\partial y }\\hat{\\mathbf{y}}+\\frac{ \\partial f }{ \\partial z }\\hat{\\mathbf{z}} \\\\ \\nabla \\cdot \\mathbf{A}\u0026amp;= \\frac{ \\partial A_{x} }{ \\partial x }+\\frac{ \\partial A_{y} }{ \\partial y }+\\frac{ \\partial A_{z} }{ \\partial z } \\\\ \\nabla\\times \\mathbf{A}\u0026amp;= \\left( \\frac{\\partial A_{z}}{\\partial y} - \\frac{\\partial A_{y}}{\\partial z} \\right)\\hat{\\mathbf{x}} + \\left( \\frac{\\partial A_{x}}{\\partial z} - \\frac{ \\partial A_{z}}{\\partial x} \\right)\\hat{\\mathbf{y}} + \\left( \\frac{\\partial A_{y}}{\\partial x}-\\frac{\\partial A_{x}}{\\partial y} \\right)\\hat{\\mathbf{z}} \\end{align*} $$\n見ればわかるが、$\\nabla$をベクトルのように理解すれば、上の計算を自然と受け入れることができる。**しかし、このように理解するのは間違っている。**複雑な式が現れた場合、多くの部分で誤った計算をしてしまう。特にデル演算子が多く入っている式では、計算の過程と結果が理解できず、時間を無駄にすることがある。\n右辺の値またはベクトルを左辺と同じように記述する理由は、単に直感的によく合うためであり、実際には$\\nabla$と$\\mathbf{A}$の内積や外積ではない。各文書に入って導出過程を見れば理解できるだろう。だから、デル演算子というものを忘れて$\\nabla f$、$\\nabla \\cdot \\mathbf{A}$、$\\nabla \\times \\mathbf{A}$を丸ごと1つの値またはベクトルとして理解すべきだ。\n(X) $\\nabla f$ = デル演算子とスカラー関数の積\n(O) $\\nabla f$ = 与えられたスカラー関数を3つの空間座標で微分したものを各成分として持つベクター関数であり、$f$がどの方向に、どれだけ増加するかに関する情報を持っている。\nもしくは\nベクター関数$\\mathbf{A} = (A_{x}, A_{y}, A_{z})$に対して、 $$ \\frac{ d A_{x} }{ d x }+\\frac{ d A_{y} }{ d y }+\\frac{ dA_{z} }{ d z } $$ といった形の式は物理学でよく登場するので、いつも長々と書く必要はなく、簡単に $$ \\nabla \\cdot \\mathbf{A} $$ と表現することにしようという約束だ。便宜上$\\nabla = (\\frac{ \\partial }{ \\partial x }, \\frac{ \\partial }{ \\partial y }, \\frac{ \\partial }{ \\partial z })$と定義すると直感的でぴったり合う表現法であるというわけだ。\nと理解するのが正しい。\n二つのベクトルの内積は交換可能なので、$\\nabla$をベクトルと思い込むと、$\\nabla\\cdot \\mathbf{A}$と$\\mathbf{A}\\cdot \\nabla$を同じものと思い込むかもしれない。二つの式は全く異なる。そもそも$\\nabla$は微分に関する演算なので、順序が非常に重要である。\n$x\\left(\\dfrac{df}{dx}\\right)$と$\\dfrac{d(xf)}{dx}$の結果が同じではないと考えると理解しやすいだろう。だから、$\\mathbf{A}\\cdot \\nabla$を二つのベクトルの内積と理解するのではなく、$A_{x}\\frac{ \\partial }{ \\partial x }+A_{y}\\frac{ \\partial }{ \\partial y }+A_{z}\\frac{ \\partial }{ \\partial z }$が長すぎるために簡単に表現するために作られた記号と理解すべきだ。別の例でいうと\n$$ \\nabla \\times (\\mathbf{A} \\times \\mathbf{B}) = (\\mathbf{B} \\cdot \\nabla)\\mathbf{A} - (\\mathbf{A} \\cdot \\nabla)\\mathbf{B} + \\mathbf{A} (\\nabla \\cdot \\mathbf{B}) - \\mathbf{B} (\\nabla \\cdot \\mathbf{A}) $$\nが成り立つために\n$$ \\nabla \\times (\\nabla \\times \\mathbf{A})=(\\mathbf{A} \\cdot \\nabla)\\nabla - (\\nabla \\cdot \\nabla)\\mathbf{A} + \\nabla (\\nabla \\cdot \\mathbf{A}) - \\mathbf{A} (\\nabla \\cdot \\nabla) $$\nも成り立つと誤解するかもしれないが、実際には\n$$ \\nabla \\times (\\nabla \\times \\mathbf{A})=\\nabla(\\nabla \\cdot \\mathbf{A})-\\nabla ^{2} \\mathbf{A} $$\nが正しい式である。前述の例と同様に$\\dfrac{d}{dx} (fg)=\\dfrac{df}{dx}g+f\\dfrac{dg}{dx}$と$\\dfrac{d}{dx} \\left( \\dfrac{df}{dx} \\right) =\\dfrac{d^2 f}{dx^2}$の結果が全く異なることと同じ文脈である。\n参考 デル演算子 $\\nabla$ グラディエント $\\nabla f$ ダイバージェンス $\\nabla \\cdot \\mathbf{F}$ カール $\\nabla \\times \\mathbf{F}$ ラプラシアン $\\nabla^{2} f$ ","id":1575,"permalink":"https://freshrimpsushi.github.io/jp/posts/1575/","tags":null,"title":"物理学におけるデル演算子"},{"categories":"양자역학","contents":"定義 二つの演算子 $A, B$に対して、$AB - BA$を$A, B$の交換子として定義し、次のように表す。\n$$ [A,B]=AB-BA $$\n説明 最初に交換子の定義に接したときに、$AB - BA = 0$ではないかと疑問に思うかもしれない。しかし、演算子は行列として表され、二つの行列の掛け算は交換法則が成立しないので、掛ける順番によって異なる結果が出ることがある。\n量子力学を勉強するためには、ベクトルと行列、内積の一般化が必要だ。演算子もまたベクトル(行列)であるため、これを行列として表すことができる。交換子が$0$の二つの演算子は互いに交換可能commuteであると言う。交換子を使用する理由は、計算を速くするためである。例えば、$p$を運動量演算子、$x$を位置とする。波動関数$\\psi$に対して以下の式が与えられたとする。\n$$ p x \\psi - xp\\psi = [p, x]\\psi $$\n$[p,x]$の値がわからない場合は、左側に示されているように解く必要がある。すなわち、$\\psi$に$x$を適用した後、それに$p$を適用すること（第一項）、そして$\\psi$に$p$を適用した後、それに$x$を適用すること（第二項）を引く必要があるため、計算が長くなる。しかし、$[p,x]$の値を知っていれば、右側に示されているように、煩わしい計算プロセスが減少する。この二つの交換子は$[p,x]=-i\\hbar$であるため、直ちに答えが$-i\\hbar \\psi$であることがわかる。\n反交換子 一方で、反交換子は以下のように定義される。\n$$ \\left\\{A,B\\right\\}=AB+BA $$\n","id":1574,"permalink":"https://freshrimpsushi.github.io/jp/posts/1574/","tags":null,"title":"量子力学における交換子とは"},{"categories":"그래프이론","contents":"定義 1 グラフ $G$ が与えられたとする。\nエッジの有限 シーケンスを ウォークと呼び、以下のように表す。 $$ v_{0} v_{1} , v_{1} v_{2} , \\cdots , v_{m-1} v_{m} \\\\ v_{0} \\rightarrow v_{1} \\rightarrow v_{2} \\rightarrow \\cdots \\rightarrow v_{m-1} \\rightarrow v_{m} $$ ここで、$v_{0}$ を 始点、$v_{m}$ を 終点、$m$ を 長さと呼ぶ。 ウォークのエッジが全て異なる場合、トレイルと呼ぶ。 ウォークの頂点が全て異なる場合、パスと呼ぶ。 ウォークの始点と終点が同じ場合、閉じていると呼ぶ。 閉じたパスを サイクルと呼ぶ。 これらの概念は、有向グラフに対しても同様に定義でき、以下のように無限グラフに対しても定義できる。\n$G$が無限グラフだとする。以下の定義ではウォークはトレイル、パス、サイクルに置き換えられる。 6. (有限)ウォークは一般的なグラフのウォークと全く同じである。 7. 一方向無限ウォークは次のように定義される。 $$ v_{0} \\to v_{1} \\to v_{2} \\to \\cdots $$ 8. 双方向無限ウォークは次のように定義される。 $$ \\cdots \\to v_{-2} \\to v_{-1} \\to v_{0} \\to v_{1} \\to v_{2} \\to \\cdots $$\n説明 パスの定義において、始点と終点は例外である。つまり、$v_{0} = v_{m}$である場合にはパスとなり、したがって「閉じたパス」が存在することになる。また、パスとトレイルの定義においては、頂点が全て異なればエッジも全て異なるため、パスはトレイルでもある。\nサイクルは、簡単に言うと、グラフで見つけることができる「輪の形」で、グラフ理論全体で広く使用される概念である。例として、次の図を見ると、グラフで合計三つのサイクルを見つけることができる：\nまた、サイクルに関して次の定理が知られている。\n定理 有限グラフ $G$ の全ての頂点の 次数が $2$ 以上であれば、$G$ にはサイクルが含まれる。\n証明 $G$ がループやマルチプルエッジを持てば当然サイクルが存在するため、$G$は単純グラフと仮定する。\n任意の頂点 $v_{0} \\in V(G)$ 一つを選び、以下のようなパスを考える。 $$ v_{0} \\to v_{1} \\to v_{2} \\to \\cdots $$ 仮定により、全ての頂点の次数は $2$ 以上であるため、少なくとも二つの頂点と隣接している。したがって、$v_{i+1}$ は直前の頂点 $v_{i}$ に隣接している頂点の中から $v_{i-1}$ を除いた任意の頂点を選び続けることができる。しかし、$G$ は有限グラフであるため、いずれはすでにパスに含まれている頂点 $v_{k}$ を選ばなければならなくなる。そうすると、始点と終点が $v_{k}$ であるパス $v_{k} \\to \\cdots \\to v_{k}$ は、$G$においてサイクルとして存在することがわかる。\n■\nWilson. (1970). Introduction to Graph Theory: p27.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1528,"permalink":"https://freshrimpsushi.github.io/jp/posts/1528/","tags":null,"title":"グラフ理論における歩行、道、経路、サイクル"},{"categories":"정수론","contents":"定義 1 $\\alpha \\in \\mathbb{C}$ に対して以下の $\\sigma_{\\alpha} : \\mathbb{N} \\to \\mathbb{C}$ を約数関数と呼ぶ。 $$ \\sigma_{\\alpha} (n) := \\sum_{d \\mid n} d^{\\alpha} $$\n基礎性質 [1] 乗法性: $\\gcd (m,n) = 1$ を満たすすべての $m, n \\in \\mathbb{N}$ に対して、$\\sigma_{\\alpha} (mn) = \\sigma_{\\alpha} (m) \\sigma_{\\alpha} (n)$ [2]: 素数 $p$ と 自然数 $a$ に対して、 $$ \\sigma_{\\alpha} \\left( p^{a} \\right) = \\begin{cases} a +1 \u0026amp; , \\alpha = 0 \\\\ {{ p^{\\alpha (a+1)} - 1 } \\over { p^{\\alpha} - 1 }} \u0026amp;,\\alpha \\ne 0 \\end{cases} $$ 説明 特に\n$\\alpha = 0$ の場合、約数の数を示す関数 $d := \\sigma_{0}$ として表すこともある。 $\\alpha = 1$ の場合、初等整数論のシグマ関数 $\\sigma := \\sigma_{1}$ となる。 証明 [1] ディリクレ積と乗法的性質: $f$ と $g$ が乗法的関数ならば、$f \\ast\\ g$ も乗法的関数である。\n単位関数 $u$ とべき乗関数 $N^{\\alpha}$ を次のように定義する。 $$ u(n) := 1 \\\\ N^{\\alpha} (n) := n^{\\alpha} $$ $u$ と $N^{\\alpha}$ は乗法的関数であるので、その畳み込みも $$ \\left( N^{\\alpha} \\ast\\ u \\right)(n) = \\sum_{d \\mid n} N^{\\alpha} (d) u \\left( {{ d } \\over { n }} \\right) = \\sum_{d \\mid n} d^{\\alpha} = \\sigma_{\\alpha} (n) $$ 乗法的関数でなければならない。\n■\n[2] $p^{a}$ の約数は $1 , p , \\cdots ,p^{a}$ であるから、 $$ \\sigma_{\\alpha} ( n) = 1 + p^{\\alpha} + \\cdots + p^{a\\alpha} $$ $\\alpha = 0$ の場合、 $$ \\sigma_{\\alpha} ( n) = \\underbrace{1 + 1 + \\cdots + 1}_{a+1} = a + 1 $$ $\\alpha \\ne 0$ の場合、等比級数の公式 により、 $$ \\sigma_{\\alpha} ( n) = {{ p^{\\alpha (a+1)} - 1 } \\over { p^{\\alpha} - 1 }} $$\n■\nApostol. (1976). Introduction to Analytic Number Theory: p38.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1527,"permalink":"https://freshrimpsushi.github.io/jp/posts/1527/","tags":null,"title":"解析的整数論における約数関数"},{"categories":"확률분포론","contents":"要約 $$ \\Gamma \\left( { r \\over 2 } , 2 \\right) \\iff \\chi ^2 (r) $$\n説明 ガンマ分布とカイ二乗分布は、上記のような性質を持っている。\n証明 戦略：二つの分布のモーメント生成関数が同じ形で表現できることを示す。\nカイ二乗分布の$\\chi ^2 (r)$のモーメント生成関数は$\\displaystyle m_{1}(t) = (1- 2t)^{- {r \\over 2} }$であり、ガンマ分布の$\\Gamma (k, \\theta)$のモーメント生成関数は$m_{2}(t) = (1-\\theta t)^{-k}$である。ガンマ分布のモーメント生成関数に$\\displaystyle k = {r \\over 2}$と$\\theta = 2$を代入すると、 $$ m_{2}(t) = (1-\\theta t)^{-k} = (1- 2t)^{- {r \\over 2} } =m_{1}(t) $$\n■\n","id":135,"permalink":"https://freshrimpsushi.github.io/jp/posts/135/","tags":null,"title":"ガンマ分布とカイ二乗分布の関係"},{"categories":"그래프이론","contents":"定義 1 全ての頂点の次数が同じであるグラフをレギュラーグラフRegular Graphと言う。特に、全ての頂点の次数が$r$の場合、$r$-レギュラーグラフと言う。言い換えると、次を満たすグラフ$G$を$r$-レギュラーグラフと言う。 $$ \\deg (v) = r \\qquad , \\forall v \\in V(G) $$ $2$-レギュラーな連結グラフをサイクルと言う。 例 レギュラーグラフ ペーターセングラフ プラトニックグラフ：正多面体をグラフで表したもの。ペーターセングラフと同様に$3$-レギュラーグラフであり、以下の五つだけが存在する。\n完全グラフ：グラフの頂点数が$n$であれば、$(n-1)$-レギュラーグラフは完全グラフとなる。 サイクル サイクルは、最も単純な形のグラフであり、純粋なグラフ理論では大きな関心を集めている。もちろん、サイクルグラフ自体のことではなく、グラフ内でサイクルの形をした部分についての話である。[ NOTE: サイクルからたった一つのエッジを取り除いたグラフもパスと呼ばれる。] サイクルの形を直接見れば、なぜ$2$-レギュラーがサイクルと呼ばれるのかすぐに理解できる。\n一方、なぜサイクルが連結である必要があるのかについての例は次の通り。二つのコンポーネントで構成された次のグラフ$G = A \\cup B$は確かに$2$-レギュラーだが、二つのサイクルのユニオンとして表されるため、真のサイクルと呼ぶにはふさわしくないことがわかる。もちろん、$A$と$B$はそれぞれがサイクルである。\nWilson. (1970). Introduction to Graph Theory: p17.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1522,"permalink":"https://freshrimpsushi.github.io/jp/posts/1522/","tags":null,"title":"レギュラーグラフ"},{"categories":"확률분포론","contents":"要約 $$ \\Gamma \\left(1, { 1 \\over \\lambda } \\right) \\iff \\text{exp} (\\lambda) $$\n説明 指数分布の直感的な定義を考えると、あるイベントが起きるまでの時間に関心があるということだ。離散確率分布で考えるならば、幾何分布がこれに該当する。\nこの時、イベントの「発生回数」について一般化したものが負の二項分布だ。このセンスから、指数分布の一般化はガンマ分布と言えるだろう。この時の「発生回数」はガンマ分布の$\\displaystyle \\Gamma \\left( k, { 1 \\over \\lambda } \\right)$から$k$に該当するけど、ガンマ分布のパラメーター$k$は特に整数である必要はないから、完全に同等と考えるのは難しい。\nガンマ分布では、指数分布のパラメーター$\\lambda$ではなく$\\displaystyle { 1 \\over \\lambda }$を取ることにも注目しよう。難しく考える必要はない、ただこのように考えることができると知っていれば十分だ。\n証明 戦略：二つの分布のモーメント生成関数が同じ形で表現できることを示す。\n指数分布$\\text{exp} (\\lambda)$のモーメント生成関数は$\\displaystyle m_{1}(t) := (1- {t \\over \\lambda})^{-1}$で、ガンマ分布$\\Gamma (k, \\theta)$のモーメント生成関数は$\\displaystyle m_{2}(t) := (1-\\theta t)^{-k}$だ。ガンマ分布のモーメント生成関数に$ k = 1$と$\\displaystyle \\theta = { 1 \\over \\lambda }$を代入すると $$ m_{2}(t) = (1 - \\theta t)^{-k} = (1- {t \\over \\lambda})^{-1} =m_{1}(t) $$\n■\n","id":133,"permalink":"https://freshrimpsushi.github.io/jp/posts/133/","tags":null,"title":"ガンマ分布と指数分布の関係"},{"categories":"정수론","contents":"定義 1 $\\forall n \\in \\mathbb{N}$について、$f(n) = 0$でない算術関数$f$が次を満たす場合、乗法的関数という。 $$ f(mn) = f(m) f(n) \\qquad,\\gcd(m,n)=1 $$ 乗法的関数が次の条件を満たす場合、完全乗法的関数という。 $$ f(mn) = f(m) f(n) \\qquad,m,n \\in \\mathbb{N} $$ 基本性質 [1]: $f$が乗法的なら、$f(1) = 1$である。 [2]: $f$が乗法的関数であることと、全ての素数$p_{1} , \\cdots , p_{r}$と全ての$a_{1} , \\cdots, a_{r} \\in \\mathbb{N}$に対して$f \\left( p_{1}^{a_{1}} \\cdots p_{r}^{a_{r}} \\right) = f \\left( p_{1}^{a_{1}} \\right) \\cdots f \\left( p_{r}^{a_{r}} \\right)$であることは同値である。 [3]: $f$が乗法的なら、$p \\mid n$を満たす素数$p$について $$ \\sum_{d \\mid n} \\mu (d) f(d) = \\prod_{p \\mid n} \\left( 1 - f(p) \\right) $$ [4]: $f$が乗法的なら、$f$が完全乗法的関数であることと、全ての素数$p$と全ての$a \\in \\mathbb{N}$に対して$f \\left( p^{a} \\right) = \\left[ f(p) \\right]^{a}$であることは同値である。 [5]: $f$が乗法的なら、$f$が完全乗法的関数であることと、 $f$のディリクレ積に対する逆数$f^{-1}$が次のように表されることは同値である。 $$ f^{-1} (n) = \\mu (n) f (n) $$ [6]: $f$が完全乗法的であれば、$F(n) := \\sum_{d \\mid n} f(d)$も乗法的である。 $\\mu$はメビウス関数である。 説明 当然ながら、完全乗法的であれば乗法的である。\n算術関数が乗法的であるということは、数学全般で言われる「独立」に似た条件を満たすもので、関数値を細かく考えることができるため、自然に豊かな数学的性質を持つことになる。\n定理[1]はとても簡単に導出されるが、メビウス関数の集合が群になるための重要な条件となる$f(1) \\ne 0$を含意する。\n証明 [1] $f(1) = f( 1 \\cdot 1) = f(1) f(1)$であるから、$f(1) = 1$でなければならない。\n■\n[2] $(\\Rightarrow)$\n$f$が乗法的であり、異なる素数$p \\ne q$に対して常に$\\gcd \\left( p^{a} , q^{b} \\right) = 1$であるから $$ \\begin{align*} f \\left( p_{1}^{a_{1}} \\cdots p_{r}^{a_{r}} \\right) =\u0026amp; f \\left( p_{1}^{a_{1}} \\right) f \\left( p_{2}^{a_{2}} \\cdots p_{r}^{a_{r}} \\right) \\\\ =\u0026amp; f \\left( p_{1}^{a_{1}} \\right) f \\left( p_{2}^{a_{2}} \\right) f \\left( p_{3}^{a_{3}} \\cdots p_{r}^{a_{r}} \\right) \\\\ =\u0026amp; f \\left( p_{1}^{a_{1}} \\right) \\cdots f \\left( p_{r}^{a_{r}} \\right) \\end{align*} $$\n$(\\Leftarrow)$\n$m = p_{1}^{a_{1}} \\cdots p_{r}^{a_{r}}$、$n = q_{1}^{b_{1}} \\cdots q_{s}^{b_{s}}$そして$\\gcd(m,n) =1$とすると $$ \\begin{align*} f(mn) =\u0026amp; f \\left( p_{1}^{a_{1}} \\cdots p_{r}^{a_{r}} q_{1}^{b_{1}} \\cdots q_{s}^{b_{s}} \\right) \\\\ =\u0026amp; f \\left( p_{1}^{a_{1}} \\right) \\cdots f \\left( p_{r}^{a_{r}} \\right) f \\left( q_{1}^{b_{1}} \\right) \\cdots f \\left( q_{s}^{b_{s}} \\right) \\\\ =\u0026amp; f \\left( p_{1}^{a_{1}} \\cdots p_{r}^{a_{r}} \\right) f \\left( q_{1}^{b_{1}} \\cdots q_{s}^{b_{s}} \\right) \\\\ =\u0026amp; f(m)f(n) \\end{align*} $$\n■\n[3] 戦略：メビウス関数の性質を使わなければならない。\nメビウス関数の定義：素数$p_{1} , \\cdots , p_{k}$に対して自然数$n$が$n = p_{1}^{a_{1}} \\cdots p_{k}^{a_{k}}$と表されるとする。次のように定義された算術関数$\\mu$をメビウス関数という。 $$ \\mu (n) := \\begin{cases} 1 \u0026amp;, n=1 \\\\ (-1)^{k} \u0026amp;, a_{1} = \\cdots = a_{k} = 1 \\\\ 0 \u0026amp; , \\text{otherwise} \\end{cases} $$ 乗法性：$\\gcd (m,n) = 1$を満たす全ての$m, n \\in \\mathbb{N}$に対して$\\mu (mn) = \\mu (m) \\mu (n)$\n$$ g(n) := \\sum_{d \\mid n} \\mu (d) f(d) $$ そして、$m$が$g(m,n) = 1$を満たすとする。すると、$a \\mid m$と$b \\mid n$を満たす$d = ab$については$\\gcd(a,b) = 1$も成り立つため、シグマを別々に考えることができる。また、$\\mu$が乗法的であり、前提で$f$も乗法的であるため、 $$ \\begin{align*} g(mn) =\u0026amp; \\sum_{ab \\mid mn} \\mu (ab) f(ab) \\\\ =\u0026amp; \\sum_{a \\mid m \\\\ b \\mid n} \\mu (ab) f(ab) \\\\ =\u0026amp; \\sum_{a \\mid m \\\\ b \\mid n} \\mu (a) \\mu (b) f(a) f(b) \\\\ =\u0026amp; \\sum_{a \\mid m} \\mu (a) f(a) \\sum_{b \\mid n} \\mu (b) f(b) \\\\ =\u0026amp; g(m)g(n) \\end{align*} $$\nしたがって、$g$も乗法的である。そうすると定理[2]により、$g \\left( p_{1}^{a_{1}} \\cdots p_{r}^{a_{r}} \\right) = g \\left( p_{1}^{a_{1}} \\right) \\cdots g \\left( p_{r}^{a_{r}} \\right)$であるため、固定された素数$p$について$g \\left( p^{a} \\right)$の計算だけで良い。定理[1]により、$f(1)=1$そして$\\mu (1) = 1$であり、$\\mu (p) = -1$、そしてそれ以上の次数$k \\ge 2$に対しては全て$\\mu (p^{k}) = 0$であるから $$ \\begin{align*} g \\left( p^{a} \\right) =\u0026amp; \\sum_{d \\mid p^{a}} \\mu (d) f(d) \\\\ =\u0026amp; \\mu (1) f(1) + \\mu (p) f(p) \\\\ =\u0026amp; 1 \\cdot 1 - 1 \\cdot f(p) \\\\ =\u0026amp; 1 - f(p) \\end{align*} $$ 自然数$n = p_{1}^{a_{1}} \\cdots p_{r}^{a_{r}}$に対してまとめると $$ g(n) = \\prod_{p \\mid n} g \\left( p^{a} \\right) = \\prod_{p \\mid n} \\left( 1 - f(p) \\right) $$\n■\n[4] $(\\Rightarrow)$\n$f$は完全乗法的であるから、 $$ f \\left( p^{a} \\right) = f(p) \\left( p^{a-1} \\right) = \\left[ f(p) \\right]^{2} \\left( p^{a-2} \\right) = \\cdots = \\left[ f \\left( p \\right) \\right]^{a} $$\n$(\\Leftarrow)$\n素数$p_{1} , \\cdots , p_{s}$と整数$a_{1} , \\cdots, a_{s} , b_{1} , \\cdots , b_{s} \\in \\mathbb{N}_{0}$そして自然数$c_{1}, \\cdots , c_{t} \\in \\mathbb{N}$に対して任意の$m$と$n$が次のように表されるとしよう。 $$ m = p_{1}^{a_{1}} \\cdots p_{t}^{a_{t}} \\\\ n = p_{1}^{b_{1}} \\cdots p_{t}^{b_{t}} \\\\ mn = p_{1}^{c_{1}} \\cdots p_{t}^{c_{t}} $$ $m$と$n$が素数$p_{i}$を約数として持たない場合、それぞれ$a_{i} = 0$、$b_{i} = 0$である。$f$は乗法的であるから、 $$ \\begin{align*} f(mn) =\u0026amp; f \\left( p_{1}^{c_{1}} \\cdots p_{t}^{c_{t}} \\right) \\\\ =\u0026amp; f \\left( p_{1}^{c_{1}}) \\cdots f( p_{t}^{c_{t}} \\right) \\\\ =\u0026amp; \\left[ f ( p_{1}) \\right]^{c_{1}} \\cdots \\left[ f ( p_{t}) \\right]^{c_{t}} \\\\ =\u0026amp; \\left[ f ( p_{1}) \\right]^{a_{1}} \\left[ f ( p_{1}) \\right]^{b_{1}} \\cdots \\left[ f ( p_{t}) \\right]^{a_{t}} \\left[ f ( p_{t}) \\right]^{b_{t}} \\\\ =\u0026amp; \\left[ f ( p_{1}) \\right]^{a_{1}} \\cdots \\left[ f ( p_{t}) \\right]^{a_{t}} \\left[ f ( p_{1}) \\right]^{b_{1}} \\cdots \\left[ f ( p_{t}) \\right]^{b_{t}} \\\\ =\u0026amp; \\left[ f ( p_{1}^{a_{1}}) \\right] \\cdots \\left[ f ( p_{t}^{a_{t}}) \\right] \\left[ f ( p_{1}^{b_{1}}) \\right] \\cdots \\left[ f ( p_{t}^{b_{t}}) \\right] \\\\ =\u0026amp; f \\left( p_{1}^{a_{1}} \\cdots p_{t}^{a_{t}} \\right) f \\left( p_{1}^{b_{1}} \\cdots p_{t}^{b_{t}} \\right) \\\\ =\u0026amp; f(m)f(n) \\end{align*} $$\n■\n[5] $(\\Rightarrow)$\n$g(n) := \\mu (n) f(n)$としよう。\n$f$は完全乗法的であり、$\\displaystyle \\sum_{d \\mid n } \\mu (d) = I(n)$であるから、 $$ \\begin{align*} (gf)(n) =\u0026amp; \\sum_{d \\mid n} \\mu (d) f (d) f \\left( {{ n } \\over { d }} \\right) \\\\ =\u0026amp; f(n) \\sum_{d \\mid n} \\mu (d) \\\\ =\u0026amp; f(n) I(n) \\end{align} $$ 定理[1]で、$f(1) = 1$であり、アイデンティティ関数$I$は$n \u0026gt; 1$で$I(n) = 0$であるから、$f(n) I(n) = I(n)$である。まとめると、$(g \\ast\\ f) (n) = I (n)$であるから、$g = f^{-1}$である。\n$(\\Leftarrow)$\n任意の素数$p$と自然数$a \\in \\mathbb{N}$に対して$n = p^{a}$としよう。\n$f^{-1} (n) := \\mu (n) f (n)$なら $$ \\begin{align*} \\sum_{d \\mid n} \\mu (d) f (d) f \\left( {{ n } \\over { d }} \\right) =\u0026amp; \\sum_{d \\mid n} f^{-1} (d) f \\left( {{ n } \\over { d }} \\right) \\\\ =\u0026amp; (f^{-1} \\ast\\ f ) (n) \\\\ =\u0026amp; I(n) \\\\ =\u0026amp; 0 \\end{align*} $$\nメビウス関数の定義：素数$p_{1} , \\cdots , p_{k}$に対して自然数$n$が$n = p_{1}^{a_{1}} \\cdots p_{k}^{a_{k}}$と表されるとする。次のように定義された算術関数$\\mu$をメビウス関数という。 $$ \\mu (n) := \\begin{cases} 1 \u0026amp;, n=1 \\\\ (-1)^{k} \u0026amp;, a_{1} = \\cdots = a_{k} = 1 \\\\ 0 \u0026amp; , \\text{otherwise} \\end{cases} $$\nメビウス関数の定義に従って、$k \\ge 2$なら$\\mu (p^{k} ) = 0$であるから、 $$ \\mu (1) f(1) f \\left( p^{a} \\right) + \\mu (p) f(p) f \\left( p^{a-1} \\right) + 0 + \\cdots + 0 = 0 $$ 前提で$f$は乗法的関数であったので、定理[1]により$f(1) = 1$である。一方、メビウス関数は$\\mu (1) = 1$であり、$\\mu (p) = -1$であるから、 $$ 1 \\cdot 1 \\cdot f \\left( p^{a} \\right) + (-1) \\cdot f(p) f \\left( p^{a-1} \\right) = 0 $$ まとめると、 $$ f \\left( p^{a} \\right) = f(p) f \\left( p^{a-1} \\right) $$ 再帰的に解くと$f \\left( p^{a} \\right) = f(p)^{a}$であるので、**定理[4]**により、$f$は完全乗法的である。\n■\n[6] $\\gcd (m,n) = 1$と仮定すると、$m$の約数と$n$の約数を区別できる。便宜上、$a_{1} , \\cdots, a_{M}$が$m$の約数、$b_{1} , \\cdots , b_{N}$が$n$の約数だとしよう。 $$ \\begin{align*} F(mn) =\u0026amp; \\sum_{d \\mid mn} f(d) \\\\ =\u0026amp; f(1) + f(a_{1}) + \\cdots + f (a_{M}) + f(b_{1}) + \\cdots + f(b_{N}) \\\\ \u0026amp; + f(a_{1}b_{N}) + \\cdots + f(a_{M} b_{1}) + \\cdots + f(a_{1} a_{2} b_{N}) + \\cdots + f(mn) \\\\ =\u0026amp; \\sum_{a \\mid m} f(a) \\sum_{b \\mid n} f(b) \\\\ =\u0026amp; F(m) F(n) \\end{align*} $$\n■\nApostol. (1976). Introduction to Analytic Number Theory: p33.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1521,"permalink":"https://freshrimpsushi.github.io/jp/posts/1521/","tags":null,"title":"算術関数の乗法的性質"},{"categories":"그래프이론","contents":"定義 1 単純グラフ $G$ が与えられたとする。\n$E(G) = \\emptyset$ ならば、$G$ をヌルグラフという。 $E \\left( \\overline{G} \\right) = \\emptyset$ ならば、$G$ を完全グラフという。 説明 ヌルグラフとは、文字通り空のグラフを意味する。EmptyではなくNullという表現を使ったのは、実際に$G \\ne \\emptyset$ であっても、グラフとしての意味がないためである。例えば、次の図のように頂点だけが存在しているなら、それをグラフと呼ぶ理由はほとんどない。しかし、数学で$0$ という数が非常に重要であるように、ヌルグラフはグラフ理論全般で非常に頻繁に言及される。\n完全グラフは元のグラフ$G$ の補グラフ $\\overline{G} $ として定義される。$\\overline{G}$ がヌルグラフであることは、元のグラフ$G$ の全ての頂点が例外なく接続されていることを意味する。例えば、上のグラフの補グラフは以下の通りである。\n追加定義 特に、頂点の数が$n$ の完全グラフを$K_{n}$ と表記する場合もある。\n何らかのグラフのサブグラフとしての完全グラフは、クリークと呼ばれる。 グラフ$G$ のクリーク$K_{n}$ の中で最大の$n$ を、$G$ のクリーク数 $\\omega (G)$ と言い、 $G$ の補グラフ$\\overline{G}$ のクリーク数を、$G$ の独立数 $\\beta (G) := \\omega \\left( \\overline{G} \\right)$ と言う。 一方、向きがある完全グラフをトーナメントと呼ぶ。 Wilson. (1970). Graph TheoryのIntroduction: p17.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1520,"permalink":"https://freshrimpsushi.github.io/jp/posts/1520/","tags":null,"title":"ヌルグラフと完全グラフ"},{"categories":"확률분포론","contents":"定義 1 $k, \\theta \u0026gt; 0$に対して、以下の確率密度関数を持つ連続確率分布$\\Gamma ( k , \\theta )$をガンマ分布Gamma Distributionと呼ぶ。 $$ f(x) = {{ 1 } \\over { \\Gamma ( k ) \\theta^{k} }} x^{k - 1} e^{ - x / \\theta} \\qquad , x \u0026gt; 0 $$\n$\\Gamma$はガンマ関数を示す。 ガンマ分布の確率密度関数は$\\alpha , \\beta \u0026gt; 0$に対して、以下のようにも定義される。本質的には$\\theta = {{ 1 } \\over { \\beta }}$かの違いだけだ。 $$ f(x) = {{ \\beta^{\\alpha } } \\over { \\Gamma ( \\alpha ) }} x^{\\alpha - 1} e^{ - \\beta x} \\qquad , x \u0026gt; 0 $$ 基本性質 モーメント生成関数 [1]: $$m(t) = \\left( 1 - \\theta t\\right)^{-k} \\qquad , t \u0026lt; {{ 1 } \\over { \\theta }}$$ 平均と分散 [2]: $X \\sim \\Gamma ( \\alpha , \\beta )$ならば $$ \\begin{align*} E(X) =\u0026amp; k \\theta \\\\ \\text{Var} (X) =\u0026amp; k \\theta^{2} \\end{align*} $$ 十分統計量 [3]: ガンマ分布に従うランダムサンプル$\\mathbf{X} := \\left( X_{1} , \\cdots , X_{n} \\right) \\sim \\Gamma \\left( k, \\theta \\right)$が与えられているとする。 $\\left( k, \\theta \\right)$に対する十分統計量$T$は次の通り。 $$ T = \\left( \\prod_{i} X_{i}, \\sum_{i} X_{i} \\right) $$\n定理 スケーリング [a]: $X \\sim \\Gamma ( k , \\theta )$ならばスカラー$c \u0026gt; 0$に対して$c X \\sim \\Gamma ( k , c \\theta )$ ポアソン分布との関係 $$ \\int_{\\mu}^{\\infty} { { z^{k-1} e^{-z} } \\over { \\Gamma (k) } } dz = \\sum_{x=0}^{k-1} { { {\\mu}^{x} e^{-\\mu} } \\over {x!} } $$ 指数分布との関係 [c]: $$\\Gamma \\left(1, { 1 \\over \\lambda } \\right) \\iff \\text{exp} (\\lambda)$$ カイ二乗分布との関係 [d]: $$\\Gamma \\left( { r \\over 2 } , 2 \\right) \\iff \\chi ^2 (r)$$ ベータ分布導出 [e]: 2つの確率変数$X_{1},X_{2}$が独立であり、$X_{1} \\sim \\Gamma ( \\alpha_{1} , 1)$、$X_{2} \\sim \\Gamma ( \\alpha_{2} , 1)$とすると $$ {{ X_{1} } \\over { X_{1} + X_{2} }} \\sim \\text{beta} \\left( \\alpha_{1} , \\alpha_{2} \\right) $$ 説明 ガンマ分布は、ガンマ関数にちなんで名付けられた関数であり、その確率密度関数の積分が$1$になることはオイラー積分に由来する。直感的な意味を持つというよりは、統計学的に有用な特性が多いため人為的に導入された分布である。このような分布をサンプリング分布Sampling Distributionとも呼ばれるが、ガンマ分布は特有の形状のおかげで様々な分布へと姿を変え、多くの便利な特性を提供してくれる。\nベイズ理論 ベイジアンでは、ポアソン分布の共役事前分布として使用されることもある。\n証明 [1] $\\displaystyle t \u0026lt; {{ 1 } \\over { \\theta }}$の時、$\\displaystyle y := x {{ ( 1 - \\theta t ) } \\over { \\theta }}$と置くと$\\displaystyle dy = {{ ( 1 - \\theta t ) } \\over { \\theta }} dx$であるから $$ \\begin{align*} m(t) =\u0026amp; \\int_{0}^{\\infty} e^{tx} {{ 1 } \\over { \\Gamma ( k ) \\theta^{k} }} x^{k - 1} e^{ - x / \\theta} dx \\\\ =\u0026amp; \\int_{0}^{\\infty} {{ 1 } \\over { \\Gamma ( k ) \\theta^{k} }} x^{k - 1} e^{ x (t - 1 / \\theta) } dx \\\\ =\u0026amp; \\int_{0}^{\\infty} {{ 1 } \\over { \\Gamma ( k ) \\theta^{k} }} x^{k - 1} e^{ - x {{( 1 - \\theta t)} \\over {\\theta}} } dx \\\\ =\u0026amp; \\int_{0}^{\\infty} {{ 1 } \\over { \\Gamma ( k ) \\theta^{k} }} \\left( {{ y \\theta } \\over { 1 - \\theta t }} \\right)^{k - 1} e^{ - y } {{ \\theta } \\over { 1 - \\theta t }}dy \\\\ =\u0026amp; \\left( {{ 1 } \\over { 1 - \\theta t }} \\right)^{k } \\int_{0}^{\\infty} {{ \\theta^{k} } \\over { \\Gamma ( k ) \\theta^{k} }} y^{k-1} e^{ - y } dy \\end{align*} $$ オイラー積分により、$\\displaystyle \\int_{0}^{\\infty} {{ 1 } \\over { \\Gamma ( k ) }} y^{k-1} e^{ - y } dy = 1$である。 $$ m(t) = \\left( 1 - \\theta t\\right)^{-k} \\qquad , t \u0026lt; {{ 1 } \\over { \\theta }} $$\n■\n[2] 直接演繹する。\n■\n[3] 直接演繹する。\n[a] $X \\sim \\Gamma ( k , \\theta )$と$c \u0026gt;0$に対して$Y = c X$とすると $$ \\begin{align*} m_{X}(t) =\u0026amp; \\int_{0}^{\\infty} e^{tx} {{ 1 } \\over { \\Gamma ( k ) \\theta^{k} }} x^{k - 1} e^{ - x / \\theta} dx \\\\ =\u0026amp; \\int_{0}^{\\infty} e^{tx} {{ c^{k} } \\over { \\Gamma ( k ) (c\\theta)^{k} }} x^{k - 1} e^{ - cx / c\\theta} dx \\\\ =\u0026amp; \\int_{0}^{\\infty} e^{{{ t } \\over { c }} cx} {{ 1 } \\over { \\Gamma ( k ) (c\\theta)^{k} }} (cx)^{k - 1} e^{ - cx / c\\theta} c dx \\\\ =\u0026amp; \\int_{0}^{\\infty} e^{{{ t } \\over { c }} y} {{ 1 } \\over { \\Gamma ( k ) (c\\theta)^{k} }} y^{k - 1} e^{ - y / c\\theta} dy \\end{align*} $$ [1]モーメント生成関数により $$ \\begin{align*} m_{Y}(t) =\u0026amp; E \\left( e^{tY} \\right) \\\\ =\u0026amp; E \\left( e^{tcX} \\right) \\\\ =\u0026amp; \\int_{0}^{\\infty} e^{{{ tc } \\over { c }} y} {{ 1 } \\over { \\Gamma ( k ) (c\\theta)^{k} }} y^{k - 1} e^{ - y / c\\theta} dy \\\\ =\u0026amp; \\int_{0}^{\\infty} e^{tz} {{ 1 } \\over { \\Gamma ( k ) (c\\theta)^{k} }} z^{k - 1} e^{ - z / c\\theta} dz \\\\ =\u0026amp; (1 - c \\theta)^{-k} \\end{align*} $$ それ故に$Y \\sim \\Gamma ( k , c \\theta)$である。\n■\nb 数学的帰納法で示す。\n■\n[c] モーメント生成関数で示す。\n■\n[d] モーメント生成関数で示す。\n■\nコード こちらは、ガンマ分布の確率密度関数をGIFアニメで表示するJuliaのコードです。\n@time using LaTeXStrings\r@time using Distributions\r@time using Plots\rcd(@__DIR__)\rx = 0:0.1:20\rΘ = collect(0.1:0.1:10.0); append!(Θ, reverse(Θ))\ranimation = @animate for θ ∈ Θ\rplot(x, pdf.(Gamma(1, θ), x),\rcolor = :black,\rlabel = \u0026#34;r = 1, θ = $(rpad(θ, 4, \u0026#39;0\u0026#39;))\u0026#34;, size = (400,300))\rxlims!(0,20); ylims!(0,0.5); title!(L\u0026#34;\\mathrm{pmf\\,of\\,} \\Gamma (1, \\theta)\u0026#34;)\rend\rgif(animation, \u0026#34;pdf1.gif\u0026#34;)\ranimation = @animate for θ ∈ Θ\rplot(x, pdf.(Gamma(2, θ), x),\rcolor = :black,\rlabel = \u0026#34;r = 2, θ = $(rpad(θ, 4, \u0026#39;0\u0026#39;))\u0026#34;, size = (400,300))\rxlims!(0,20); ylims!(0,0.5); title!(L\u0026#34;\\mathrm{pmf\\,of\\,} \\Gamma (2, \\theta)\u0026#34;)\rend\rgif(animation, \u0026#34;pdf2.gif\u0026#34;)\ranimation = @animate for θ ∈ Θ\rplot(x, pdf.(Gamma(4, θ), x),\rcolor = :black,\rlabel = \u0026#34;r = 4, θ = $(rpad(θ, 4, \u0026#39;0\u0026#39;))\u0026#34;, size = (400,300))\rxlims!(0,20); ylims!(0,0.5); title!(L\u0026#34;\\mathrm{pmf\\,of\\,} \\Gamma (4, \\theta)\u0026#34;)\rend\rgif(animation, \u0026#34;pdf4.gif\u0026#34;) Hogg et al. (2013). Introduction to Mathematical Statistics(7th Edition): p158.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1517,"permalink":"https://freshrimpsushi.github.io/jp/posts/1517/","tags":null,"title":"ガンマ分布"},{"categories":"그래프이론","contents":"定義 1 グラフ$G$について、グラフ$H$が$V(H) \\subset V(G)$と$ E(H) \\subset E(G)$を満たす場合、$H$を$G$のサブグラフと言う。\n説明 注意すべき点は、$H$が$G$のサブグラフであると$H \\subset G$のように示してはいけないことである。サブグラフの概念はグラフ理論自体の興味の対象としてよりは、むしろ自然で常識的な用語としての意味がある。\n例 サブグラフを定義することで、次のような例を考えることができる：\nコンポーネント 切断されたグラフ$D$のコンポーネントは全て$D$のサブグラフである。\nグラフの減算 グラフ$G$が与えられたとする：\nエッジについて：$e \\in E(G)$に対して、$E(G) - e$は$G$のエッジ$e$を取り除いたグラフとして定義され、自然に$G$のサブグラフとなる。このようなグラフの減算操作$-$は、エッジの集合に対しても拡張可能である。$E(G) - E$は、$G$のエッジの部分集合$E$のエッジを取り除いたグラフとして定義され、同様に$G$のサブグラフとなる。 バーテックスについて：$v \\in V(G)$に対して、$E(G) - v$は$G$のバーテックス$v$と$v$に付随する全てのエッジを取り除いたグラフとして定義される。バーテックスについても、バーテックスの集合$V \\subset V(G)$に対して拡張可能である。$E(G) - V$は、$G$のバーテックスの部分集合$V$のバーテックスとそれに付随するエッジを取り除いたグラフとして定義され、当然、$G$のサブグラフとなる。 グラフの収縮：上記の二つの減算操作で記号$-$に注目する。集合の減算記号$\\setminus$は、グラフでのエッジのContractionを意味するために使われる。エッジが収縮されるということは、エッジ$e$が消えて、$e$に接続されていた二つの端点$v$と$w$を同一視することである。 このようなグラフの減算は、グラフを構築するアルゴリズムなどを話すときに非常に便利なツールとなる。\nスパニング $G$のサブグラフ$H$が$V\\left( H \\right) = V(G)$を満たす場合、$H$を$G$のスパニングと言う。簡単に言えば、バーテックスはそのままで、エッジのみが取り除かれた可能性があるサブグラフである。グラフの減算でエッジが取り除かれたケースに当たる。\n誘導グラフ $G$のサブグラフ$H$が $$ E \\left( H \\right) = \\left\\{ uv \\in E(G) : u,v \\in V \\left( H \\right) \\right\\} $$ を満たす場合、誘導グラフと呼ばれる。誘導グラフは、特定のルールなしにエッジが取り除かれ、バーテックスのみが保持されるスパニングとは異なり、バーテックスが保持されている部分では接続するエッジも保持されているため、元のグラフ$G$の性質をある程度受け継いでいると推測できる。\nWilson. (1970). グラフ理論への招待: p12.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1513,"permalink":"https://freshrimpsushi.github.io/jp/posts/1513/","tags":null,"title":"サブグラフ"},{"categories":"그래프이론","contents":"定義 1 二つのグラフ $G_{1}$ と $G_{2}$ に対して $V(G_{1}) \\cap V(G_{2}) = \\emptyset$ としよう。\n二つのグラフの ユニオンUnion $G = G_{1} \\cup G_{2}$ は、頂点セット $V(G_{1}) \\cup V(G_{2})$ とエッジセット $E (G_{1}) \\cup E ( G_{2} )$ を持つグラフだ。 グラフ $H$ が他のグラフのユニオンで表されない場合、$H$ を 接続されているConnectedといい、それ以外の場合は 切断されているDisconnectedという。 切断されたグラフを構成する各 接続グラフConnected Graphを コンポーネントComponentと呼ぶ。 特にエッジ $b \\in G$ の削除によりグラフが切断される場合、$b$ を ブリッジBridgeと呼ぶ。 説明 これらの定義は位相数学で接続性を定義する方法とよく似ている。\n純粋なグラフの定義からの 接続性Connectednessは重要な話のように見えるが、皮肉なことに、切断されたコンポーネントは完全に個別に扱うことができるので、接続されたグラフだけを考えれば十分だ。接続性が重要でないわけではなく、通常は研究のために切断されたケースを考える必要はないということだ。\n接続性が注目されるのは、実際のデータを反映した分析やランダムネットワークを扱う適用ネットワーク理論でよくある。ランダムネットワークが確かに接続グラフになるかどうかは、様々なシミュレーションなどでかなり重要な問題だ。ネットワークの接続性が保証されていない場合を考えよう。孤立ノードIsolated Nodeは、ほとんどの数学モデルでは影響力がなく、孤立ノードがなくても、ネットワークの一部だけを考慮して残りを捨てなければならない大惨事が起こる可能性がある。\n要約 2 シンプルグラフ $G$ が $n$ 個の頂点を持っているとする。$G$ が $k$ 個のコンポーネントを持っている場合、$G$ のエッジの数 $m$ は次を満たす。 $$ n-k \\le m \\le (n-k)(n-k+1)/2 $$\n証明 Part 1. $n-k \\le m$\n$G$ がヌルグラフの場合、$n=k$ であり、$m=0$ であるため、$ n-k = 0 \\le 0 = m$ が成立する。\n$G$ のコンポーネントが $k$ 個ある場合に $n - k \\ge m$ が成立すると仮定しよう。$G$ が $k$ 個のコンポーネントを持つための最少のエッジ数を $m_{0}$ とする。ここで一つのエッジを削除すると、コンポーネントの数は $k+1$ 、エッジの数は $m_{0}-1$ となる。したがって、$n - (k + 1) \\le m_{0} - 1$ であり、整理して $n - k \\le m_{0}$ を得る。\nこれら二つの事実から、数学的帰納法により $n-k \\le m$ が一般的に成立するという結論になる。\nPart 2. $m \\le (n-k)(n-k+1)/2$\n$G$ のすべてのコンポーネントが完全グラフであるとしよう。すると、$1 \\le j \\le i \\le n$ である二つの完全グラフ $K_{i}$、$K_{j}$ が存在するだろう。これら二つのグラフをそれぞれ $K_{i+1}$、$K_{j-1}$ に変えると、頂点の総数は変わらないが、エッジの総数は次のように変わる。 $$ \\left[ (i+1)i - i(i-1) \\right]/2 - \\left[ j(j-1) - (j-1)(j-2) \\right]/2 = i - j + 1 $$ これは正の数で、したがって、$m$ が最大化されるためには、$G$ は頂点の数が $(n-k+1)$ 個の完全グラフと、孤立頂点 $k-1$ 個を持たなければならない。この場合、$G$ のエッジの数は $(n-k)(n-(k-1))/2$ であり、求めていた結果を得る。\n■\nWilson. (1970). グラフ理論序論: p10.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWilson. (1970). グラフ理論序論: p27.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1512,"permalink":"https://freshrimpsushi.github.io/jp/posts/1512/","tags":null,"title":"グラフの集合表記"},{"categories":"확률분포론","contents":"説明 指数分布とポアソン分布の直感的な定義について考えてみよう。指数分布は、あるイベントが発生するまでにかかる時間に関心があり、ポアソン分布は単位時間内にあるイベントが何回発生するかに関心がある。イベントが発生する時間と、イベントが発生する回数について、二つの分布は一方を固定して他方に関心を持つ。例えば、$\\exp (\\lambda)$と$\\text{Poi}(\\lambda)$のパラメータを$\\lambda = 1$とすることを考えよう。指数分布を見た場合、イベントが発生するまでの単位時間であり、ポアソン分布を見た場合、単位時間あたりにイベントが一回発生すると見ることができる。\nここで、ポアソン分布の$\\lambda$が大きくなれば、単位時間あたりのイベントの発生回数が大きくなり、その分イベントが一回発生する時間は短くなるはずだ。この意味で、指数分布の平均$\\displaystyle {{1} \\over {\\lambda}}$とポアソン分布の平均$\\lambda$は、パラメータを表す記号$\\lambda$を共有するのが妥当だと考えられる。多くの教科書で、二つの分布のパラメータを$\\lambda$と表記するが、このように考えると受け入れやすくなるだろう。\n数学的には、ガンマ分布と指数分布が関連しており、ガンマ分布がポアソン分布と関連しているので、指数分布とポアソン分布も何らかの関係があることを容易に推測できるだろう。\nProof According to the relationship between the gamma distribution and the exponential distribution, $$ X_{i} \\sim \\exp (\\lambda) \\iff X_{i} \\sim \\Gamma (1, {{1} \\over {\\lambda}} ) $$ If you add all $k$ random variables following the gamma distribution, $$ Y_{k} = \\sum_{i=1}^{k} X_{i} \\sim \\Gamma (n, {{1} \\over {\\lambda}} ) $$ Since the exponential distribution is memoryless, $Y_{i}$ and $Y_{j}$ are independent, and $Y_{k}$ simply represents the time at which the $k$th event occurs. Meanwhile, if the cumulative distribution function of $\\displaystyle Y_{k}$ is called $F_{k}$, $$ F_{k}(1) = 1 - \\int_{1}^{\\infty} { {1} \\over {\\Gamma (k) {{1} \\over {\\lambda ^ k}} }} x^{k-1} e^{-\\lambda x} dx $$ To summarize, $$ F_{k}(1) = 1 - \\int_{1}^{\\infty} { { \\lambda^{k} } \\over {\\Gamma (k) }} x^{k-1} e^{-\\lambda x} dx $$ If we substitute $\\lambda x = z$ with $\\lambda dx = dz$, $$ F_{k}(1) = 1 - \\int_{\\lambda}^{\\infty} { { z^{k-1} e^{- z } } \\over {\\Gamma (k) }} dz $$ According to the relationship between gamma distribution and Poisson distribution, $$ F_{k}(1) = 1 - \\int_{\\lambda}^{\\infty} { { z^{k-1} e^{-z} } \\over { \\Gamma (k) } } dz = 1 - \\sum_{y=0}^{k-1} { { {\\lambda}^{y} e^{-\\lambda} } \\over {y!} } $$ Since $Y_{k}$ represents the time at which the $k$th event occurs, the probability that exactly $n$ events occur within unit time $1$ is the same as the probability that $Y_{n}$ is less than or equal to $1$ and $Y_{n+1}$ is greater than $1$. $$ \\begin{align*} P(N = n) =\u0026amp; P( Y_{n} \\le 1 \\land Y_{n+1}\u0026gt;1 ) \\\\ =\u0026amp; P( Y_{n} \\le 1 ) P ( Y_{n+1}\u0026gt;1 ) \\\\ =\u0026amp; P( Y_{n} \\le 1 ) \\left( 1 - P ( Y_{n+1} \\le 1 ) \\right) \\\\ =\u0026amp; P( Y_{n} \\le 1 ) - P( Y_{n} \\le 1 ) P ( Y_{n+1} \\le 1 ) \\\\ =\u0026amp; P( Y_{n} \\le 1 ) - P( Y_{n} \\le 1 \\land Y_{n+1} \\le 1 ) \\\\ =\u0026amp; P( Y_{n} \\le 1 ) - P( Y_{n+1} \\le 1 ) \\\\ =\u0026amp; F_{n}(1) - F_{n+1}(1) \\\\ =\u0026amp; \\left( 1 - \\sum_{y=0}^{n-1} { { {\\lambda}^{y} e^{-\\lambda} } \\over {y!} } \\right) - \\left( 1 - \\sum_{y=0}^{n} { { {\\lambda}^{y} e^{-\\lambda} } \\over {y!} } \\right) \\\\ =\u0026amp; { { {\\lambda}^{n} e^{-\\lambda} } \\over {n!} } \\end{align*} $$ This refers to the probability mass function of the Poisson distribution with parameter $\\lambda$, thus $N \\sim \\text{Poi} (\\lambda)$\n■\nSee Also Relationship between Exponential Distribution and Poisson Distribution Defining Poisson Processes through Exponential Distribution Defining Poisson Processes through Differential Matrix ","id":296,"permalink":"https://freshrimpsushi.github.io/jp/posts/296/","tags":null,"title":"指数分布とポアソン分布の関係"},{"categories":"확률분포론","contents":"定義 1 $\\lambda \u0026gt; 0$に対して、以下の確率密度関数を持つ連続確率分布$\\exp ( \\lambda)$を指数分布Exponential Distributionと呼ぶ。 $$ f(x) = \\lambda e^{-\\lambda x} \\qquad , x \\ge 0 $$\n本によっては、パラメーターがその逆数 $\\displaystyle \\theta = {{ 1 } \\over { \\lambda }}$ を使うこともある。 基本性質 モーメント生成関数 [1]: $$m(t) = {{ \\lambda } \\over { \\lambda - t }} \\qquad , t \u0026lt; \\lambda$$ 平均と分散 [2]: $X \\sim \\exp ( \\lambda)$の場合 $$ \\begin{align*} E(X) =\u0026amp; {{ 1 } \\over { \\lambda }} \\\\ \\text{Var} (X) =\u0026amp; {{ 1 } \\over { \\lambda^{2} }} \\end{align*} $$ 十分統計量と最尤推定量 [3]: ランダムサンプル $\\mathbf{X} := \\left( X_{1} , \\cdots , X_{n} \\right) \\sim \\exp \\left( \\lambda \\right)$が与えられたとする。 $\\lambda$に対する十分統計量$T$と最尤推定量$\\hat{\\lambda}$は以下のとおりである。 $$ \\begin{align*} T =\u0026amp; \\sum_{k=1}^{n} X_{k} \\\\ \\hat{\\lambda} =\u0026amp; {{ n } \\over { \\sum_{k=1}^{n} X_{k} }} \\end{align*} $$\n定理 無記憶性 [a]: $X \\sim \\exp ( \\lambda ) $の場合 $$ P ( X \\ge s + t \\mid X \\ge s ) = P (X \\ge t) $$ ガンマ分布との関係 [b]: $$\\Gamma \\left(1, { 1 \\over \\lambda } \\right) \\iff \\text{exp} (\\lambda)$$ ワイブル分布への一般化 $$ f(x) = {{ k } \\over { \\theta }} \\left( {{ x } \\over { \\theta }} \\right)^{k-1} e^{-(x/\\theta)^{k}} \\qquad , x \\ge 0 $$ 説明 幾何分布との関係 指数分布は、注目する事象が発生するまでの時間が従う分布で、幾何分布の連続化とも見なせる。幾何分布の発生回数に対する一般化として負の二項分布を考えることができるが、指数分布の発生回数に対する一般化はガンマ分布とも言えるだろう。\nポアソン分布との関係 一方、ポアソン分布と指数分布は似た現象に注目しているが、それぞれ単位時間あたりの事象の発生回数、事象が発生するまでの時間に関心があるという違いがある。この二つの分布の関係は、本の中にはこれら二つの分布で同じギリシャ文字$\\lambda$を使うこともある理由である。特に、ポアソン分布の平均が$\\lambda$、指数分布の平均が$\\displaystyle {{ 1 } \\over { \\lambda }}$であることを考えると、二つの分布の関係はある種の「逆」のように受け取ることができるだろう。\n証明 [1] $t \u0026lt; \\lambda$の時のみ $$ \\begin{align*} m(t) =\u0026amp; \\int_{0}^{\\infty} e^{tx} f(x) dx \\\\ =\u0026amp; \\int_{0}^{\\infty} e^{tx} \\lambda e^{-\\lambda x} dx \\\\ =\u0026amp; \\lambda \\int_{0}^{\\infty} e^{(t - \\lambda ) x} dx \\\\ =\u0026amp; \\lambda {{ 1 } \\over { t - \\lambda }} [ 0 - 1 ] \\\\ =\u0026amp; {{ \\lambda } \\over { \\lambda - t }} \\end{align*} $$\n■\n[2] 直接導出する。\n■\n[3] 直接導出する。\n■\n[a] 条件付き確率で導出する。\n■\n[b] モーメント生成関数で示す。\n■\nc 確率密度関数から明らかである。\n■\n可視化 以下は、指数分布の確率密度関数をアニメーションGIFで示すJuliaのコードです。\n@time using LaTeXStrings\r@time using Distributions\r@time using Plots\rcd(@__DIR__)\rx = 0:0.1:10\rΛ = collect(0.1:0.1:5.0); append!(Λ, reverse(Λ))\ranimation = @animate for λ ∈ Λ\rplot(x, pdf.(Exponential(λ), x),\rcolor = :black,\rlabel = \u0026#34;λ = $(round(λ, digits = 2))\u0026#34;, size = (400,300))\rxlims!(0,10); ylims!(0,0.5); title!(L\u0026#34;\\mathrm{pdf\\,of\\,} \\exp(\\lambda)\u0026#34;)\rend\rgif(animation, \u0026#34;pdf.gif\u0026#34;) Hogg et al. (2013). Introduction to Mathematical Statistics(7th Edition): p159.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1510,"permalink":"https://freshrimpsushi.github.io/jp/posts/1510/","tags":null,"title":"指数分布"},{"categories":"동역학","contents":"定義 1 フロー 空間$X$と関数$f : X \\to X$について、次のようなベクター場が微分方程式として与えられているとしよう。 $$ \\dot{x} = f(x) $$ 時間変数$t$と初期値$x_{0}$に対する自律微分方程式の解をフローと呼び、$F(t, x_{0})$のように表す。固定された単位時間$t = T$に対して、$F_{T}(x) := F(T,x)$をタイム-$T$マップと呼ぶ。\nタイムエボリューション 通常、一つの座標のみを残すプロジェクション$P : X \\to \\mathbb{R}^{1}$について、$P \\left( F \\left( t, x_{0} \\right) \\right)$を時間$t$の関数として見た場合、これをタイムエボリューションとも呼ぶ。\n説明 フローは軌跡または相空間とも呼ばれる。[ 注: 数学全般で言及される相空間とは同音異義語で、概念的には大きな関連性はない。 ]\nその定義から、フロー$F$は初期値$x_{0}$を固定して$t$に従った変化を描写することが分かる。タイム-$T$マップはもともと微分方程式で表され、連続的な動力系をマップで扱うために導入された。これにより、多次元マップでの議論を微分方程式に拡張することができるようになる。\n例 例として$\\dot{x} = x$という単純な自律システムを考えてみよう：このシステムの解は単純に$x = x_{0} e^{t}$であるため、このシステムのフローは初期値$x_{0}$に対して$F(t,x_{0}) = x_{0} e^{t}$となるだろう。一方、初期値を固定せずに、$x$から始まるシステムが時間$T$が経過したとき、タイム-$T$マップによって確認される。タイム-$T$マップは以下のように$x$を時間$T$が経過した後の$x e^{T}$にマッピングする。 $$ F_{T} : x \\mapsto x e^{T} $$ 動力学で広く使われる表現ではないかもしれないが、一般的な多次元マップのように表現したい場合、次のような式を立てることができる。 $$ F_{T+1} (x) = F_{1} \\left( F_{T}(x) \\right) $$\nYorke. (1996). CHAOS: An Introduction to Dynamical Systems: p277.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1507,"permalink":"https://freshrimpsushi.github.io/jp/posts/1507/","tags":null,"title":"自律システムのフローとタイム-Tマップ"},{"categories":"동역학","contents":"定義 1 空間 $V$ と関数 $f : V \\to V$ に対し、次のようなベクトル場が微分方程式として与えられているとしよう。 $$ \\dot{v} = f(v) $$\n変数 $t$ を含む微分方程式で、$t$ が明示的に示されていない場合、自律微分方程式Automonous Differential Equationと言う。 定数関数 $f_{0} (v)$ が自律微分方程式 $\\dot{v} = f(v)$ の解である場合、$f_{0}$ を平衡点Equilbriumと言う。 説明 自律システム 自律微分方程式で表される力学系を自律システムAutonomous Systemと言う。幾何学的にはほとんどがベクトル場で表されるため、適切な文脈では単にベクトル場とも呼ばれる。通常、変数 $t$ は時間を意味し、方程式が変数 $t$ を含みながら明示的に示されていないことは、例えば、以下のような式を指す。 $$ \\dot{y} = y $$ 上記の微分方程式の自明でない解は$y = e^t$である。なぜ自律という言葉が付くかは、非自律微分方程式を考えれば理解できる。非自律微分方程式は、名前の通り、変数 $t$ が微分方程式に明示的に示される微分方程式を言う。例えば、以下のように項 $\\sin t$ が追加されたものなどだ。 $$ \\dot{y} = y + \\sin t $$ このような微分方程式で表されるシステムは、$y$ そのものではなく、時間 $t$ に従って外部から何らかの干渉を受けると見なされる。この意味で、非自律微分方程式でない方程式を自律微分方程式と呼ぶのは適切だと思える。\n固定点 平衡点は物理学のセンスが強く、数学では単に固定点Fixed Pointという表現が好まれる。システムで固定点とは、その名の通り動かない点である。点が動かないということは、位置の変化量を示す微分係数がすべて$0$であり、固定点である限り定数関数である。厳密な表現では、関数が定義された定義域$X$ではなく、微分方程式の解が構成する関数空間$C^{1} (X)$の一元、つまり関数としての固定点であるが、教科書によっては、ゆるく$X$の一元が固定点と呼ばれることもある。\n微分方程式の記法 微分幾何学での$s$に対する微分と$t$に対する微分の記法: $$ {{ df } \\over { ds }} = f^{\\prime} \\quad \\text{and} \\quad {{ df } \\over { dt }} = \\dot{f} $$ ドット$\\dot{}$でもプライム$'$でも、微分は微分だが、微分幾何学の文脈では以上のように記号を区別できる。通常、$s$は単位速度曲線のパラメータであり、$t = t(s)$は線の長さの再パラメータ化を経た曲線のパラメータを表す。\n必ずそうである必要はないが、ダイナミクスでは、時間$t$に対する変化としてベクトル場を扱うため、プライム$y '$の代わりにドット$\\dot{y}$を多用することがある。\n例 例としてローレンツアトラクタを考えてみよう： $$ \\begin{cases} \\dot{x} = - \\sigma x + \\sigma y \\\\ \\dot{y} = - xz + \\rho x - y \\\\ \\dot{z} = xy - \\beta z \\end{cases} $$ 固定点は、ドメイン$\\mathbb{R}^3$上で動かない点を描写しているので、すべての左辺に$0$を代入することによって得られる。 $$ \\begin{cases} \\displaystyle 0 = - \\sigma x + \\sigma y \\\\ \\displaystyle 0 = - xz + \\rho x - y \\\\ \\displaystyle 0 = xy - \\beta z \\end{cases} $$ 簡単な計算を通じて、次の三つの固定点$F_{i}$を見つけ出すことができる。 $$ F_{1} = F_{1}(t) = (0,0,0) \\\\ F_{2} = F_{2}(t) = \\left( \\sqrt{\\beta (\\rho - 1)},\\sqrt{\\beta (\\rho - 1)}, (\\rho-1) \\right) \\\\ F_{3} = F_{3}(t) = \\left( -\\sqrt{\\beta (\\rho - 1)},-\\sqrt{\\beta (\\rho - 1)}, (\\rho-1) \\right) $$ ここで、$F_{i} = F_{i} (t)$のように関数として表されていることに注意せよ。一見、$F_{i}$は$\\mathbb{R}^{3}$上の点のように見えるが、定義によれば、時間$t$に従って値が変わらない定数関数であり、ローレンツ微分方程式の解として得られたものだ。概念的には、三次元空間の点と違いはない。\n参照 マップで表される力学系 微分方程式で表される力学系 力学系の厳密な定義 Yorke. (1996). CHAOS: An Introduction to Dynamical Systems: p271~277.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1505,"permalink":"https://freshrimpsushi.github.io/jp/posts/1505/","tags":null,"title":"微分方程式で表される動力学系と平衡点"},{"categories":"그래프이론","contents":"定義 1 有向グラフ $G$が与えられているとしよう。\nエッジ $vw$が存在する場合、エッジは$v$から出て$w$に入ると言われる。\n頂点 $v$に入るエッジの数を入力次数Indegreeと呼び、$\\deg^{-} (v)$として表される。 頂点 $v$から出るエッジの数を出力次数Outdegreeと呼び、$\\deg^{+}(v)$として表される。 $\\deg^{-} (v) = 0$である頂点をソースSource、$\\deg^{+} (v) = 0$である頂点をシンクSinkと言う。ソースでもシンクでもない点をインターナルInternalと呼ぶ。 エッジ $vw$が存在する場合、頂点 $v,w$はエッジ $vw$に接続されているincidentと言われる。\n頂点 $v$に接続されているエッジの数を次数Degreeと呼び、$\\deg (v)$として表される。 次数が$0$である頂点を孤立頂点Isolated Vertexと言う。 次数が$1$である頂点をエンド頂点End Vertexと言う。 グラフ$G$の最大次数を$\\Delta (G)$、最小次数を$\\delta (G)$として表現する。\n説明 ちなみに、接続されているという表現はそれほど好ましい言い回しではない。二つの頂点 $v,w$がエッジ$e$によって繋がれている場合、$v$と$w$は隣接しているAdjacentと表現され、接続されているという言葉はエッジ$e$に接続されている$v,w$を描写するために使われる。もともと形容詞のIncidentは、付随する、従うなどの意味で使われる英単語だ。\n次数の概念は、長い間多くの関心を集めてきたが、特に純粋数学ではシンプルなグラフがよく扱われるため、次数に関する研究が多い。\n有向グラフ 例えば、次のグラフでは、赤色が入力次数、青色が出力次数を示す。もちろん、入力次数と出力次数の合計は同じである。\nここで、入力次数が$0$の頂点をソースと言う。入るものがなく、出るだけである点から、ソースという名前は適切だと考えられる。これは動力学でのシンク、ソースと似ている。\n次数 例えば、次のグラフで、各頂点の次数を計算することができる。有向グラフを単純なグラフとして表した場合、入力次数と出力次数の合計と同じであることを確認しよう。[ 注: このように方向性をなくしたグラフは、元の有向グラフの基底グラフUnderlying Graphと言われる。]\n次のグラフでは、頂点$C$と$F$はエンド頂点、$H$は孤立頂点だ。\n応用数学では、$H$がなければ、つまりネットワークが完全に接続されている場合、$D$のようなノードをハブと呼ぶ。通常、ハブはネットワーク全体に大きな影響を与えるか、すべての通信を中継することができる要素を描写する。\nレギュラーグラフ 特に、すべての頂点の次数が同じであるグラフ、$\\delta (G) = \\Delta (G)$と表示される、をレギュラーグラフと呼ぶ。\nWilson. (1970). グラフ理論入門: p12.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1496,"permalink":"https://freshrimpsushi.github.io/jp/posts/1496/","tags":null,"title":"グラフ理論における次数"},{"categories":"정수론","contents":"定義 1 算術関数 $f$ に対して、以下を満たす算術関数 $f^{-1}$ が一意に存在する場合、$f^{-1}$ を $f$ の**(ディリクレ)逆**という。 $$ f \\ast\\ f^{-1} = f^{-1} \\ast\\ f = I $$\nここで、$I$ は畳み込みに対する恒等関数だ。 定理 [1]: 算術関数 $f$ が $f(1) \\ne 0$ の場合、その逆 $f^{-1}$ が一意に存在し、以下のような再帰関数で表される。 $$ f^{-1}(n) = \\begin{cases} \\displaystyle {{1} \\over {f(1)}} \u0026amp;,n=1 \\\\ \\displaystyle {{-1} \\over {f(1)}} \\sum_{d \\mid n , d \u0026lt; n } f \\left( {{ n } \\over { d }} \\right) f^{-1}(d) \u0026amp;, n \u0026gt; 1\\end{cases} $$ [2]: 二つの算術関数 $f$、$g$ に対して $f(1) \\ne 0$、$g(1) \\ne 0$ が満たされる場合、 $$ (f \\ast\\ g)^{-1} = g^{-1} \\ast\\ f^{-1} = f^{-1} \\ast\\ g^{-1} $$ 説明 ほとんどの数学で扱われる逆関数と違い、ディリクレ逆は写像の逆ではなく、代数的なセンスでの逆を指す。自然に代数的構造が思い浮かび、その存在性と一意性も気になるだろう。幸いなことに、逆の存在性は非常に単純な条件を満たすことで充分である。\n特に乗法的算術関数の場合、以下の理由から逆の存在性が確実に保証される。 $$ f(1) = f(1 \\cdot 1 ) = f(1) f(1) = 1 \\ne 0 $$ これらの事実から、算術関数の集合がアーベル群となる条件は $f(1) \\ne 0$ になる。\n証明 [1] $\\displaystyle I(n) = \\left[ {{ 1 } \\over { n }} \\right]$ なので、$n=1$ の時 $\\left( f \\ast\\ f^{-1} \\right)(1) = I(1) = 1$ であり、したがって $\\displaystyle f^{-1}(1) = {{ 1 } \\over { f(1) }}$ である。$f(1) \\ne 0$ なので、$f^{-1}(1)$ も一意である。$n \u0026gt;1$ ならば $\\left( f \\ast\\ f^{-1} \\right)(n) = I(n) = 0$ なので、 $$ \\sum_{d \\mid n} f \\left( {{ n } \\over { d }} \\right) f^{-1} (d) = 0 $$ $n=d$ である項を除けば、 $$ f(1) f^{-1}(n) + \\sum_{d \\mid n \\\\ d \u0026lt; n} f \\left( {{ n } \\over { d }} \\right) f^{-1} (d) = 0 $$ 整理すると、 $$ f^{-1}(n) = {{-1} \\over {f(1)}} \\sum_{d \\mid n \\\\ d \u0026lt; n } f \\left( {{ n } \\over { d }} \\right) f^{-1}(d) $$ 先に $n=1$ の場合に $f^{-1}(1) \\ne 0$ が一意に存在することを示したので、数学的帰納法により、$f^{-1}(n)$ も一意に存在する。\n■\n[2] 戦略: 存在性を証明すれば、畳み込みの性質により直接計算できる。\n畳み込みの性質:\n(1) 結合法則: $$ \\left( f \\ast g \\right) \\ast k = f \\ast (g \\ast k) $$ (2) 交換法則: $$ f \\ast\\ g = g \\ast\\ f $$ 上記の[1]により、$f(1) \\ne 0$ かつ $g(1) \\ne 0$ であるため、$f^{-1}$ と $g^{-1}$ は一意に存在する。同様に、 $$ (f \\ast\\ g) (1) = \\sum_{ d \\mid 1} f(d)g(1/d) = f(1) g(1) \\ne 0 $$ したがって、$\\left( f \\ast g \\right)^{-1}$ も一意に存在する。そうすると、畳み込みの結合法則により、 $$ \\begin{align*} (f \\ast\\ g) \\ast ( g^{-1} \\ast\\ f^{-1} ) =\u0026amp; f \\ast (g * g^{-1} ) \\ast\\ f^{-1} \\\\ =\u0026amp; f \\ast\\ I \\ast\\ f^{-1} \\\\ =\u0026amp; f * f^{-1} \\\\ =\u0026amp; I \\end{align*} $$ $\\left( f \\ast g \\right)^{-1}$ は一意であるため、$\\left( f \\ast g \\right)^{-1} = g^{-1} \\ast\\ f^{-1}$ でなければならない。また、畳み込みの交換法則により、 $$ (f \\ast\\ g)^{-1} = g^{-1} \\ast\\ f^{-1} = f^{-1} \\ast\\ g^{-1} $$\n■\nApostol. (1976). Introduction to Analytic Number Theory: p30.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1494,"permalink":"https://freshrimpsushi.github.io/jp/posts/1494/","tags":null,"title":"ディリクレ積の逆수"},{"categories":"확률분포론","contents":"数式 $X \\sim \\text{Poi}(\\lambda)$ 面 $$ E(X) = \\lambda \\\\ \\text{Var}(X) = \\lambda $$\n導出 戦略：ポアソン分布の定義から直接演繹する。階乗と級数を分けるトリックが重要だ。\nポアソン分布の定義: $\\lambda \u0026gt; 0$ に対して、次の確率質量関数を持つ離散確率分布 $\\text{Poi} ( \\lambda )$ をポアソン分布という。 $$ p(x) = {{ e^{-\\lambda} \\lambda^{x} } \\over { x! }} \\qquad , x = 0 , 1 , 2, \\cdots $$\n平均 $$ \\begin{align*} E(X) =\u0026amp; \\sum _{ x=0 }^{ \\infty }{ x\\frac { { \\lambda ^ x }{ e ^ { - \\lambda } } }{ x! } } \\\\ =\u0026amp; { e ^ { - \\lambda } }\\sum _{ x=0 }^{ \\infty }{ x\\frac { { \\lambda ^ x } }{ x! } } \\\\ =\u0026amp; { e ^ { - \\lambda } }\\sum _{ x=1 }^{ \\infty }{ \\frac { { \\lambda \\cdot \\lambda } ^{ x-1 } }{ (x-1)! } } \\\\ =\u0026amp; { \\lambda e } ^{ -\\lambda } \\sum _{ x=1 }^{ \\infty }{ \\frac { { \\lambda } ^{ x-1 } }{ (x-1)! } } \\\\ =\u0026amp; \\lambda { e ^ { - \\lambda } }{ e^ \\lambda } \\\\ =\u0026amp; \\lambda \\end{align*} $$\n■\n分散 $$ \\begin{align*} E({ X }^{ 2 }) =\u0026amp; \\sum _{ x=0 }^{ \\infty }{ { x } ^{ 2 } \\frac { { \\lambda ^ x }{ e ^ { - \\lambda } } }{ x! } } \\\\ =\u0026amp; { e ^ { - \\lambda } }\\sum _{ x=1 }^{ \\infty }{ x\\frac { { \\lambda ^ x } }{ (x-1)! } } \\\\ =\u0026amp; { e ^ { - \\lambda } }\\sum _{ x=1 }^{ \\infty }{ \\frac { { (x-1+1)\\lambda } ^{ x } }{ (x-1)! } } \\\\ =\u0026amp; { e ^ { - \\lambda } }\\sum _{ x=1 }^{ \\infty }{ \\frac { { (x-1)\\lambda } ^{ x }+{ \\lambda ^ x } }{ (x-1)! } } \\\\ =\u0026amp; { e ^ { - \\lambda } }\\sum _{ x=1 }^{ \\infty }{ \\left\\{ \\frac { { (x-1)\\lambda } ^{ x } }{ (x-1)! }+\\frac { { \\lambda ^ x } }{ (x-1)! } \\right\\} } \\\\ =\u0026amp; { e ^ { - \\lambda } }\\left\\{ \\sum _{ x=2 }^{ \\infty }{ \\frac { { \\lambda ^ x } }{ (x-2)! } }+\\sum _{ x=1 }^{ \\infty }{ \\frac { { \\lambda ^ x } }{ (x-1)! } } \\right\\} \\\\ =\u0026amp; { e ^ { - \\lambda } }\\left\\{ \\sum _{ x=2 }^{ \\infty }{ \\frac { { { \\lambda ^ 2 }\\cdot \\lambda } ^{ x-2 } }{ (x-2)! } }+\\sum _{ x=1 }^{ \\infty }{ \\frac { { \\lambda \\cdot \\lambda } ^{ x-1 } }{ (x-1)! } } \\right\\} \\\\ =\u0026amp; { e ^ { - \\lambda } }({ \\lambda ^ 2 }{ e^ \\lambda }+\\lambda { e^ \\lambda }) \\\\ =\u0026amp; { \\lambda ^ 2 }+\\lambda \\end{align*} $$ 従って $$ \\text{Var} (X)=E({ X }^{ 2 })-{ \\left\\{ E(X) \\right\\} } ^{ 2 } ={ (\\lambda }^{ 2 }+\\lambda )-{ \\lambda }^{ 2 }=\\lambda $$\n■\n","id":61,"permalink":"https://freshrimpsushi.github.io/jp/posts/61/","tags":null,"title":"ポアソン分布の平均と分散"},{"categories":"확률분포론","contents":"定義 1 $\\lambda \u0026gt; 0$に基づき、以下の確率質量関数を持つ離散確率分布$\\text{Poi} ( \\lambda )$をポアソン分布Poisson Distributionという。 $$ p(x) = {{ e^{-\\lambda} \\lambda^{x} } \\over { x! }} \\qquad , x = 0 , 1 , 2, \\cdots $$\n基本性質 モーメント生成関数 [1]: $$m(t) = \\exp \\left[ \\lambda \\left( e^{t} - 1 \\right) \\right] \\qquad , t \\in \\mathbb{R}$$ 平均と分散 [2]: $X \\sim \\text{Poi}(\\lambda)$ならば $$ \\begin{align*} E(X) =\u0026amp; \\lambda \\\\ \\text{Var}(X) =\u0026amp; \\lambda \\end{align*} $$ 十分統計量と最尤推定量 [3]: ランダムサンプル$\\mathbf{X} := \\left( X_{1} , \\cdots , X_{n} \\right) \\sim \\text{Poi} \\left( p \\right)$が与えられているとする。 $\\lambda$に対する十分統計量$T$と最尤推定量$\\hat{\\lambda}$は以下の通りである。 $$ \\begin{align*} T =\u0026amp; \\sum_{k=1}^{n} X_{k} \\\\ \\hat{\\lambda} =\u0026amp; {{ 1 } \\over { n }} \\sum_{k=1}^{n} X_{k} \\end{align*} $$\n定理 二項分布の極限分布としてのポアソン分布の導出 [a]: $X_{n} \\sim B(n,p)$としよう。 $\\mu \\approx np$ならば $$ X_{n} \\overset{D}{\\to} \\text{Poi} (\\mu) $$\nポアソン分布の極限分布としての標準正規分布の導出 [b]: $X_{n} \\sim \\text{Poi} \\left( n \\right)$であり$\\displaystyle Y_{n} := {{ X_{n} - n } \\over { \\sqrt{n} }}$ならば $$ Y_{n} \\overset{D}{\\to} N(0,1) $$ 解説 命名 ポアソン分布の確率質量関数は初見には複雑に見えるが、実際には私たちに馴染み深い指数関数の級数展開から来ている。 $$ e^{x} = 1 + {{ x } \\over { 1 ! }} + {{ x^{2} } \\over { 2! }} + {{ x^{3} } \\over { 3! }} + \\cdots $$ パラメーター$x = \\lambda$は通常固定されていると仮定されるため、両辺を定数$e^{\\lambda}$で割ることにより $$ 1 = {{ e^{-\\lambda} \\lambda^{0} } \\over { 0! }} + {{ e^{-\\lambda} \\lambda^{1} } \\over { 1! }} + {{ e^{-\\lambda} \\lambda^{2} } \\over { 2! }} + {{ e^{-\\lambda} \\lambda^{3} } \\over { 3! }} + \\cdots $$ 従って、（当たり前だが）ポアソン分布の確率質量関数の合計は$1$となる。このようにポアソン分布は二項分布、幾何分布、負の二項分布と異なり、その名称が数式から来ているわけではない。\n偉大な物理学者で数学者でもあるポアソンは、1837年に発表した論文刑法と民法判例における判断の確率についての研究Recherches sur la probabilite des jugements en matiere criminelle et en matiere civileで、単位時間内に特定の事件が発生する確率が特定の分布に従うと述べた。この分布はポアソンの名を取ってポアソン分布と呼ばれるようになり、今でも多数の確率理論や統計技術にポアソンの名が付いている。\n平均と分散が同じ分布 様々な応用に先立ち、ポアソン分布自体も興味深い研究対象である。ポアソン分布の最も注目すべき基本的性質の一つは、平均と分散がパラメータ$\\lambda$と等しいことである。\n指数分布との関係 一方、ポアソン分布と指数分布は類似した現象に関心を持っているが、前者は単位時間あたりに発生する事象の回数に、後者は事象が発生するまでにかかる時間に関心があるという差がある。これら二つの分布の関係により、いくつかの書籍では両方の分布に同じギリシャ文字$\\lambda$を使用していることもある。特に、ポアソン分布の平均が$\\lambda$であり、指数分布の平均が$\\displaystyle {{ 1 } \\over { \\lambda }}$であることを考えると、二つの分布の関係をある種の「逆」のように受け取ることができる。\n証明 [1] $$ \\begin{align*} m(t) =\u0026amp; \\sum_{x=0}^{n} e^{tx} p(x) \\\\ =\u0026amp; \\sum_{x=0}^{n} e^{tx} {{ \\lambda^{x} e^{-\\lambda} } \\over { x! }} \\\\ =\u0026amp; e^{-\\lambda} \\sum_{x=0}^{n} {{ \\left( e^{t}\\lambda \\right)^{x} } \\over { x! }} \\\\ =\u0026amp; e^{-\\lambda} e^{\\lambda e^{t}} \\\\ =\u0026amp; \\exp \\left[ -\\lambda + \\lambda e^{t} \\right] \\\\ =\u0026amp; \\exp \\left[ \\lambda ( e^{t} - 1) \\right] \\end{align*} $$\n■\n[2] 直接導ける。\n■\n[3] 直接導ける。\n■\n[a] モーメント生成関数で近似する。\n■\n[b] テイラー展開で項を省略して近似する。\n■\nコード 以下は、ポアソン分布の確率質量関数をGIFアニメーションで示すJuliaコードである。\n@time using LaTeXStrings\r@time using Distributions\r@time using Plots\rcd(@__DIR__)\rx = 0:20\rΛ = collect(1:0.1:10); append!(Λ, reverse(Λ))\ranimation = @animate for λ ∈ Λ\rscatter(x, pdf.(Poisson(λ), x),\rcolor = :black,\rlabel = \u0026#34;λ = $(round(λ, digits = 2))\u0026#34;, size = (400,300))\rxlims!(0,10); ylims!(0,0.5); title!(L\u0026#34;\\mathrm{pmf\\,of\\,Poi}(\\lambda)\u0026#34;)\rend\rgif(animation, \u0026#34;pmf.gif\u0026#34;) Hogg et al. (2013). Introduction to Mathematical Statistics(7th Edition): p152.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1491,"permalink":"https://freshrimpsushi.github.io/jp/posts/1491/","tags":null,"title":"ポアソン分布"},{"categories":"정수론","contents":"定義 1 以下のように定義された算術関数 $I$ をアイデンティティ関数と言う。 $$ I(n) := \\left[ {{ 1 } \\over { n }} \\right] $$\n[1] アイデンティティ級数：単位関数 $u$ である。つまり、 $$ \\sum_{d \\mid n}I(d) = u(n) = 1 $$ [2] 完全乗算的：すべての $n , m \\in \\mathbb{N}$ に対して $I (mn) = I(m) I(n)$ [a] 畳み込みにおける単位元：すべての算術関数 $f$ に対して $$ I \\ast\\ f = f \\ast\\ I = f $$ $\\left[ x \\right] = \\lceil x \\rceil$ は床関数Floor function と呼ばれ、$x$ より小さくまたは等しい値の中で最大の整数を表す。 説明 $$ \\begin{matrix} n \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; 4 \u0026amp; 5 \u0026amp; 6 \u0026amp; 7 \u0026amp; 8 \u0026amp; 9 \u0026amp; 10 \\\\ I(n) \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ \\sum_{d \\mid n} I(d) \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \\end{matrix} $$ ほとんどの数学では、アイデンティティ関数の名前は$i(x) = x$ のように定義域の要素が自分自身にマッピングされる関数に付けられるが、少なくとも解析的整数論ではノルム $N (n) = n$ と呼ばれる。$I$ は見ての通り、畳み込み $\\ast$ に対して常に存在する単位元の役割を果たすため、アイデンティティという名前が付けられた。\n証明 [1] $\\displaystyle I(n) = \\left[ {{ 1 } \\over { n }} \\right] = \\begin{cases} 1 \u0026amp; , n=1 \\\\ 0 \u0026amp;, n\u0026gt;1 \\end{cases}$ が成り立つ。したがって、 $$ \\sum_{d \\mid n}I(d) = 1 + 0 + \\cdots = 1 $$\n■\n[2] ケース 1. $m = n = 1$ $$ I ( mn ) = I(1) = 1 = 1 \\cdot 1 = I(1) I(1) = I(m) I(n) $$ ケース 2. $m = 1 \\land n \u0026gt; 1$ $$ I(mn) = I (n) = 1 \\cdot I (n) = I(m) I(n) $$ ケース 3. $m \u0026gt; 1 \\land n = 1$ $$ I(mn) = I (m) = I(m) \\cdot 1 = I(m) I(n) $$ ケース 4. $m \u0026gt; 1 \\land n \u0026gt; 1$ $$ I(mn) = 0 = 0 \\cdot 0 = I(m) I(n) $$ ■\n[a] $d$ は $n$ の約数であるため、$d \\ne n$ の場合には$\\displaystyle \\left[ {{ d } \\over { n }} \\right] = 0$ であり、 $$ (f \\ast\\ I)(n) = \\sum_{d \\mid n} f(d) I \\left( {{ n } \\over { d }} \\right) = \\sum_{d \\mid n} f(d) \\left[ {{ d } \\over { n }} \\right] = f(n) $$ 算術関数の畳み込みの交換法則により、$f \\ast\\ I = I \\ast\\ f = f$\n■\nApostol. (1976). Introduction to Analytic Number Theory: p30.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1490,"permalink":"https://freshrimpsushi.github.io/jp/posts/1490/","tags":null,"title":"ディリクレ積に関する恒等式"},{"categories":"수리물리","contents":"微分方程式 学部レベルの物理学を勉強する人向けに、できるだけ直感的に説明した。\n微分方程式とは、簡単に言うと微分を含んだ方程式のことだ。難しく考える必要はなくて、加速度は位置の二回微分なので、もっとも有名な物理学の式$F=ma$も微分方程式である。\n多項式$x^{3}+3x+1=0$は最高次数が3なので3次方程式と呼ばれる。同様に、微分方程式で最も多く微分された回数が$n$の時、その微分方程式を$n$階微分方程式と呼ぶ。n次degree方程式と呼ぶことも多いが、正確にはn階order方程式が正しい。$f^{\\prime \\prime}$を$f$の2階微分ではなく、2階微分と呼ぶことを思い出そう。\n今、以下の事実を受け入れて進もう。\n$n$次方程式の解が$n$個あるように、$n$階微分方程式の解も$n$個ある。 微分方程式の解を線形結合しても微分方程式の解となる。 線形結合とは、与えられた対象にそれぞれ定数を掛けて足し合わせたもので、例えば$x$と$y$の線形結合は定数$a, b$に対して$ax+by$という式になる。\n一般解 一般解とは、微分方程式のすべての解を表すことができる一般的な形をいう。最も身近な一般解の例は根の公式だ。2次方程式の根の公式\n$$ x = \\dfrac{-b \\pm \\sqrt{b^{2}-4ac}}{2a} $$\nはすべての$ax^{2} + bx + c = 0$形の2次方程式の解を表す最も一般的な形だ。「グラフが$(0,3)$を通る直線の関数を見つけなさい」という問題の答えは\n$$ y=x+3,\\quad y=3x+3,\\quad y=5x+3 $$\nなどがある。ここで、可能なすべての答えを一度に簡潔に示すと次のようになる。\n$$ y=ax+3 $$\nしたがって、$y=ax+3$がその問題の一般解である。\n解法 微分方程式を解くということは、一般解を求めることと同じだ。以下の4つの微分方程式は物理学を勉強する時に頻繁に接触する。だから、理解できたら覚えておくといい。\n$X=X(x)$を1変数関数、$\\alpha$を定数とする。\n1階微分方程式 $$ \\frac{ d X}{ dx }=\\alpha X $$\n微分して自分自身が出てくる関数を見つけることだ。高校から学んでいる通り、これは$e^{x}$だ。定数$\\alpha$に対する条件を満たす答えは次のようになる。\n$$ X(x)=e^{\\alpha x} $$\nここで前にどんな係数が掛かっていても成り立つので、一般解を求めると\n$$ X(x)=Ae^{\\alpha x} $$\nこのとき、$A$は任意の定数。\n係数が正の2階微分方程式 $$ \\begin{equation} \\frac{ d^{2}X }{ dx^{2} }=\\alpha ^{2} X \\end{equation} $$\n2回微分しても自分自身であり、符号を保持する関数はやはり指数関数$e^{x}$だ。定数$\\alpha$に対する条件を満たす答えは次のようになる。\n$$ X(x)=e^{\\alpha x} $$\n$(1)$で定数を$\\alpha$ではなく$\\alpha ^{2}$と記述する理由は解を綺麗に示すためである。$\\alpha$で表すと解が$X(x)=e^{\\sqrt{\\alpha}x}$になるので、上の場合より綺麗ではない。そして$(-1)\\times (-1)=1$なので、\n$$ X(x)=e^{-\\alpha x} $$\nまた、$(1)$に適応する解だとわかる。したがって、一般解を求めると次のようになる。\n$$ X(x)=Ae^{\\alpha x}+Be^{-\\alpha x} $$\nこのとき、$A$、$B$は定数。\n係数が負の2階微分方程式 $$ \\frac{ d ^{2}X}{ dx^{2} }=-\\alpha^{2}X \\tag{2} $$\n2回微分したときに自分自身でありながら、符号が変わる関数は、よく知っているように$\\cos x$と$\\sin x$だ。定数に対する条件を満たす答えは\n$$ X(x)=\\cos (\\alpha x),\\quad X(x)=\\sin ( \\alpha x) $$\nしたがって、一般解は\n$$ X(x)=A\\cos (\\alpha x) +B\\sin (\\alpha x) \\tag{3} $$\nこのとき、$A$、$B$は定数。しかし、指数関数の指数に複素数$i$が含まれていると、同様に$(2)$を満たすため、$X(x)=e^{i\\alpha x}$と$X(x)=e^{-i\\alpha x}$も解であることがわかる。したがって、一般解は\n$$ X(x)=Ce^{i\\alpha x}+De^{-i\\alpha x} \\tag{4} $$\nこのとき、$C$、$D$は定数。オイラーの公式$e^{i\\alpha x}=\\cos (\\alpha x) + i \\sin (\\alpha x)$により、正弦関数と余弦関数は指数関数と入れ替えて書くことができるため、$(3)$と$(4)$は表現が異なるだけで、実質同じ式である。量子力学では解が複素関数(波動関数)の場合、$i$が含まれる指数関数の形で書き、力学などで解が明らかに実関数の場合、よく$\\cos$で書かれる。\n定数項が含まれる2階微分方程式 $$ \\begin{equation} \\frac{ d ^{2} X}{ d x^{2}}=\\pm\\alpha^{2}X+\\beta \\end{equation} $$\n$(1)$と$(2)$の式を簡潔に表現するために係数を省略して次のように書ける。\n$$ \\frac{ d ^{2}X}{ d x^{2} }\\pm X=0 $$\nこれは$X$と$X$を2回微分すると符号の差を除いて同じになるという意味だ。しかし、$(5)$のように微分方程式に定数項が含まれていると、符号だけでなく他の違いもあるということだ。$X$と$X^{\\prime \\prime}$を定数項だけで異なるようにするには、簡単だ。$X$に定数項があると考えてみる。一度微分するだけでなくなる定数項は、2回微分すると当然なくなる。だから、微分方程式に現れた定数項に適切な他の定数を掛けて$X$に含めてしまえば解になる。$X^{\\prime \\prime}=\\alpha^{2} X + \\beta$の解は\n$$ X(x)=Ae^{\\alpha x} + B e^{-\\alpha x} - \\dfrac{\\beta}{\\alpha^{2}} $$\nで、$X^{\\prime \\prime}=-\\alpha ^{2}X+C$の解は\n$$ X(x)=Ce^{i\\alpha x}+De^{-i\\alpha x} - \\dfrac{\\beta}{\\alpha^{2}} $$\n$(5)$に代入してみると実際に成立することが確認できる。\n","id":1538,"permalink":"https://freshrimpsushi.github.io/jp/posts/1538/","tags":null,"title":"物理学のための微分方程式の基礎：よく遭遇する微分方程式の解法"},{"categories":"확률분포론","contents":"定義 1 $r \\in \\mathbb{N}$ と $p \\in (0,1]$ に基づいて、次の確率質量関数を持つ離散確率分布 $\\text{NB}(r,p)$ を負の二項分布Negative Binomial Distributionっていう。 $$ p(x) = \\binom{r+x-1}{x-1} p^{r}(1-p)^{x} \\qquad, x = 0,1,2,\\cdots $$\n基本的な性質 モーメント生成関数 [1]: $$m(t) = \\left[ {{ p } \\over { 1 - (1-p) e^{t} }} \\right]^{r} \\qquad , t \u0026lt; -\\log (1-P)$$ 平均と分散 [2]: $X \\sim \\text{NB}(r, p)$ の場合 $$ \\begin{align*} E(X) =\u0026amp; {{ r (1-p) } \\over { p }} \\\\ \\text{Var}(X) =\u0026amp; {{ r (1-p) } \\over { p^{2} }}\\end{align*} $$ 説明 負の二項分布は、確率 $p$ のある事象が $r$ 回発生するまでの試行回数に関心がある。例えば、コインを投げて表が２回出るまでに何回投げなければならないかを考えてみよう。表が出る確率が $50%$ なので、表が１回出るには２回投げればいいし、それがもう１回繰り返されなければならないので、その期待値は $4$ だと推測できる。\n直感的に、負の二項分布は試行回数 $r$ を一般化した幾何分布だと見ることができる。実際、事象が発生する回数が１回、つまり $r = 1$ の場合は、正確に幾何分布と同じになる。\n命名の理由 確率質量関数の形が負の二項係数と関連があるため、負の二項分布と呼ばれる。\n定理 幾何分布の一般化 [b]: $Y = X_{1} + \\cdots + X_{r}$ で、$X_{i} \\overset{\\text{iid}}{\\sim} \\text{Geo}(p)$ の場合、$Y \\sim \\text{NB}(r,p)$ 証明 [1] 負の二項係数: $$ (-1)^{k} \\binom{-r}{k} = \\binom{r + k - 1}{ k } $$\n$$ \\begin{align*} m(t) =\u0026amp; \\sum_{x=0}^{\\infty} e^{tx} p(x) \\\\ =\u0026amp; \\sum_{x=0}^{\\infty} e^{tx} \\binom{r+x-1}{x} p^{r} (1-p)^{x} \\\\ =\u0026amp; p^{r}\\sum_{x=0}^{\\infty} \\binom{-r}{x} (-1)^{x} \\left[ (1-p) e^{t} \\right]^{x} \\\\ =\u0026amp; p^{r}\\sum_{x=0}^{\\infty} \\binom{-r}{x} \\left[ - (1-p) e^{t} \\right]^{x} \\end{align*} $$\n二項級数: $|x| \u0026lt; 1$ のとき、$\\alpha \\in \\mathbb{C}$ に対して $\\displaystyle (1 + x )^{\\alpha} = \\sum_{k=0}^{\\infty} \\binom{\\alpha}{k} x^{k}$\n二項級数によると、$\\displaystyle \\sum_{x=0}^{\\infty} \\binom{-r}{x} \\left[ - (1-p) e^{t} \\right]^{x} = \\left[ 1 - (1-p) e^{t} \\right]^{-r}$ なので $$ m(t) = \\left[ {{ p } \\over { 1 - (1-p) e^{t} }} \\right]^{r} \\qquad , t \u0026lt; -\\log (1-P) $$\n■\n[2] 幾何分布の一般化という点を利用。\n■\n[b] 幾何分布の確率質量関数が $p(x) = p (1-p)^{x} \\qquad,x=0,1,2,\\cdots$ で定義されるとき、そのモーメント生成関数は以下の通りです。 $$ m(t) = p \\left( 1 - (1-p) e^{t} \\right)^{-1} $$ 互いに独立な確率変数 $X_1, X_2, \\cdots , X_r$ が $\\text{Geo} (p)$ に従うので、$Y$ のモーメント生成関数は $$ \\begin{align*} M_Y(t) =\u0026amp; E(e^{Yt}) \\\\ =\u0026amp; E(e^{(X_1+X_2+\\cdots+X_r)t}) \\\\ =\u0026amp; E(e^{X_1 t}) E(e^{X_2 t}) \\cdots E(e^{X_r t}) \\\\ =\u0026amp; \\prod_{i=1}^r p { (1 - (1-p) e^t ) }^{-1} \\\\ =\u0026amp; p^r \\left\\{ (1 - (1-p) e^t ) \\right\\}^{-r} \\end{align*} $$ これは負の二項分布 $\\text{NB}(r,p)$ のモーメント生成関数と同じなので、$Y \\sim \\text{NB}(r,p)$\n■\nコード 以下は、負の二項分布の確率質量関数をGIFで示すJuliaのコードです。\n@time using LaTeXStrings\r@time using Distributions\r@time using Plots\rcd(@__DIR__)\rx = 0:20\rP = collect(0.2:0.01:0.8); append!(P, reverse(P))\ranimation = @animate for p ∈ P\rscatter(x, pdf.(NegativeBinomial(5, p), x),\rcolor = :black, markerstrokecolor = :black,\rlabel = \u0026#34;r = 5, p = $(rpad(p, 4, \u0026#39;0\u0026#39;))\u0026#34;, size = (400,300))\rxlims!(0,20); ylims!(0,0.5); title!(L\u0026#34;\\mathrm{pmf\\,of\\,NB}(5, p)\u0026#34;)\rend\rgif(animation, \u0026#34;pmf5.gif\u0026#34;)\ranimation = @animate for p ∈ P\rscatter(x, pdf.(NegativeBinomial(10, p), x),\rcolor = :black, markerstrokecolor = :black,\rlabel = \u0026#34;r = 10, p = $(rpad(p, 4, \u0026#39;0\u0026#39;))\u0026#34;, size = (400,300))\rxlims!(0,20); ylims!(0,0.5); title!(L\u0026#34;\\mathrm{pmf\\,of\\,NB}(10, p)\u0026#34;)\rend\rgif(animation, \u0026#34;pmf10.gif\u0026#34;) Hogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p145.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1489,"permalink":"https://freshrimpsushi.github.io/jp/posts/1489/","tags":null,"title":"負の二項分布"},{"categories":"정수론","contents":"定義 1 二つの算術関数$f$、$g$に対し、以下を満たす算術関数$h$を$f$と$g$のディリクレ積と呼ぶ。 $$ h(n) = \\sum_{d \\mid n} f(d) g \\left( {{ n } \\over { d }} \\right) $$ ディリクレ積は$h (n) = \\left( f \\ast g \\right) (n) $や$h = f \\ast g$として表現される。\n説明 ディリクレ積は、その形から推測できるように、畳み込みとも呼ばれる。この定義で算術関数を単に$a_{d}$、$b_{n/d}$と記述することは非常に不便であると想像できる。\n畳み込み$\\ast$について、算術関数の集合は、次のような基本的な代数的性質を持つ。解析的整数論に興味があるならば、２項演算$\\ast$を算術関数の集合$A$に適用して得られるアーベル群$(A,*)$を思い浮かべることが自然であろう。残念ながら、正確な答えは「いいえ」だが、もっと適切な条件を提供することで、アーベル群を形成することができる。\n一方で畳み込みは、掛け合わされる二つの関数のうち一方が算術関数でなくてもよいように一般化される。\n基本性質 [1] 結合法則: $$ \\left( f \\ast g \\right) \\ast k = f \\ast (g \\ast k) $$ [2] 交換法則: $$ f \\ast g = g \\ast f $$ 証明 [1] $A = f \\ast g$、$B := (g \\ast k)$とすると $$ \\begin{align*} \\left( f \\ast g \\right) \\ast k =\u0026amp; A \\ast k \\\\ =\u0026amp; \\sum_{cm = n} A(m) k(c) \\\\ =\u0026amp; \\sum_{cm=n} \\left[ \\sum_{ab=m} f(a) g(b) \\right] k(c) \\\\ =\u0026amp; \\sum_{abc=n} f(a) g(b) k(c) \\\\ =\u0026amp; \\sum_{am=n} f(a) \\left[ \\sum_{bc=m} g(b) k(c) \\right] \\\\ =\u0026amp; \\sum_{am=n} f(a) B(m) \\\\ =\u0026amp; f \\ast B \\\\ =\u0026amp; f \\ast (g \\ast k) \\end{align*} $$\n■\n[2] $$ \\begin{align*} \\left( f \\ast g \\right)(n) =\u0026amp; \\sum_{d \\mid n} f(d) g \\left( {{ n } \\over { d }} \\right) \\\\ =\u0026amp; \\sum_{ab=n} f(a) g(b) \\\\ =\u0026amp; \\sum_{ab=n} g(b) f(a) \\\\ =\u0026amp; \\sum_{d \\mid n} g(d) f\\left( {{ n } \\over { d }} \\right) \\\\ =\u0026amp; (g \\ast f)(n) \\end{align*} $$\n■\n一般化 一般化されたディリクレ積 アポストル. (1976). 解析的整数論入門: p29.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1488,"permalink":"https://freshrimpsushi.github.io/jp/posts/1488/","tags":null,"title":"算術関数のディリクレ積"},{"categories":"정수론","contents":"定義 1 定義域が自然数の集合$\\mathbb{N}$であり、値域が実数の集合$\\mathbb{R}$または複素数の集合$\\mathbb{C}$である関数を算術関数という。\n説明 解析的整数論では、様々な算術関数の性質や関係に関心を持ち、以下のような例がある：\n恒等関数 $I$ 約数関数 $\\sigma_{\\alpha}$ ノルム $N$ 約数関数 $\\sigma_{\\alpha}$ メビウス関数 $\\mu$ オイラーのトーティエント関数 $\\varphi$ 単位関数 $u$ マンゴルト関数 $\\Lambda$ リュービル関数 $\\lambda$ 算術関数の定義に新しさはなく、実質的には数列そのものだ。実際、数列はもともと関数であるが、数学の多くの分野ではその用語自体が関数と区別されて使われることが普通である。しかし、（解析的）整数論では扱うものが自然数であるため、定義域として$\\mathbb{N}$または$\\mathbb{Z}$があれば十分であり、数列と関数を区別する理由がほとんどなくなる。ただし、より関数に近い形で扱われるため、算術関数という用語が使われる。形式的には、定義域がベクトル場ではないが、値域が$\\mathbb{R}$または$\\mathbb{C}$である点が汎関数と似ている。\nまた、算術関数の級数にも関心がある。例えば、与えられた算術関数$f$に対して$\\displaystyle F(n) = \\sum_{d \\mid n} f(d)$を求めることだ。$\\displaystyle \\sum_{d \\mid n}$は$n$の全ての約数$d$に対して計算を行うもので、一般的な解析学で$\\displaystyle \\sum_{k=1}^{\\infty}$を計算することと似た感覚で理解できる。\nApostol. (1976). Introduction to Analytic Number Theory: p24.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1487,"permalink":"https://freshrimpsushi.github.io/jp/posts/1487/","tags":null,"title":"解析的整数論における算術関数"},{"categories":"줄리아","contents":"このポストの時点でのJuliaの最新バージョンはv1.3.1です。\nガイド ステップ1. Juliaのダウンロード Generic Linux Binaries for x86から自分のCPUのビットに合ったファイルをダウンロードする。\nステップ2. 圧縮を解除して移動 圧縮を解除する。\nJuliaが保存される場所へフォルダを移動する。どこでも好きな場所で構わないが、このポストでは/home/[ユーザー名]/julia-1.3.1へ移動させた。\nステップ3. シンボリックリンク 次のコマンドを使ってシンボリックリンクを作成する。\nsudo ln -s /home/[유저이름]/julia-1.3.1/bin/julia /usr/bin/julia Juliaコマンドを使って実行すれば、1.3.1バージョンが正常にインストールされたことを確認できる。\n最新バージョンが必要でない場合 sudo apt-get install julia 上のコマンドを使って、シンボリックリンクを設定することなく素早くインストールすることもできる。ただし、この方法では最新の安定版がインストールされない。\n環境 OS: Ubuntu 18.04 ","id":1511,"permalink":"https://freshrimpsushi.github.io/jp/posts/1511/","tags":null,"title":"LinuxでJuliaの最新バージョンをインストールする方法"},{"categories":"확률분포론","contents":"定義 1 $p \\in (0,1]$に対して、次のような確率質量関数を持つ離散確率分布 $\\text{Geo}(p)$を幾何分布Geometric Distributionと呼ぶ。 $$ p(x) = p (1 - p)^{x-1} \\qquad , x = 1 , 2, 3, \\cdots $$\n二つの定義が使用されているので、公式と定義域に特に注意が必要である。 基本性質 モーメント生成関数 [1]: $$m(t) = {{ p e^{t} } \\over { 1 - (1-p) e^{t} }} \\qquad , t \u0026lt; -\\log (1-p)$$ 平均と分散 [2]: $X \\sim \\text{Geo} (p)$なら $$ \\begin{align*} E(X) =\u0026amp; {{ 1 } \\over { p }} \\\\ \\text{Var}(X) =\u0026amp; {{ 1-p } \\over { p^{2} }} \\end{align*} $$ 十分統計量と最尤推定量 [3]: ランダムサンプル $\\mathbf{X} := \\left( X_{1} , \\cdots , X_{n} \\right) \\sim \\text{Geo} \\left( p \\right)$が与えられたとする。$p$に対する十分統計量 $T$と最尤推定量 $\\hat{p}$は以下の通りである。 $$ \\begin{align*} T =\u0026amp; \\sum_{k=1}^{n} X_{k} \\\\ \\hat{p} =\u0026amp; {{ n } \\over { \\sum_{k=1}^{n} X_{k} }} \\end{align*} $$ 定理 無記憶性 [a]: $X \\sim \\text{Geo} (p)$なら $$ P(X \\ge s+ t ,|, X \\ge s) = P(X \\ge t) $$ 幾何分布への一般化 [b]: $Y = X_{1} + \\cdots + X_{r}$であり$X_{i} \\overset{\\text{iid}}{\\sim} \\text{Geo}(p)$なら$Y \\sim \\text{NB}(r,p)$ 解説 指数分布との関係 幾何分布は、確率$0 \u0026lt; p \\le 1$で成功するまでの試行回数に関心を持っている。その確率質量関数は、確率$(1-p)$で$x-1$回失敗した後、最後に確率$p$で成功する確率を表している。この特性により、指数分布の離散化と見ることができる。\n名称 確率質量関数が幾何級数の形をしているため、この分布が幾何分布と呼ばれる。$a := p$、$r := (1-p)$と置くと、$p(x) = a r ^{x-1}$の馴染みのある式を得る。実際にモーメント生成関数を求めるときも、幾何級数の公式が登場する。\n証明 [1] $$ \\begin{align*} M(t) =\u0026amp; \\sum_{x=1}^{\\infty} e^{tx} p(x) \\\\ =\u0026amp; \\sum_{x=1}^{\\infty} e^{tx} p (1-p)^{x-1} \\\\ =\u0026amp; p e^{t} \\sum_{x=1}^{\\infty} \\left[ e^{t}(1-p) \\right]^{x-1} \\end{align*} $$ $ t \u0026lt; -\\log (1-p)$のとき、幾何級数の公式により $$ p e^{t} \\sum_{x=1}^{\\infty} \\left[ e^{t}(1-p) \\right]^{x-1} = {{ p e^{t} } \\over { 1 - (1-p) e^{t} }} $$\n■\n[2] 二つの方法がある。\n■\n[3] 直接演繹する。\n■\n[a] 条件付き確率で演繹する。\n■\n[b] モーメント生成関数で演繹する。\n■\nコード 次は、幾何分布の確率質量関数をGIFで表示するJuliaのコードだ。\n@time using LaTeXStrings @time using Distributions @time using Plots cd(@__DIR__) x = 0:20 P = collect(0.01:0.01:0.5); append!(P, reverse(P)) animation = @animate for p ∈ P scatter(x, pdf.(Geometric(p), x), color = :black, markerstrokecolor = :black, label = \u0026#34;p = $(rpad(p, 4, \u0026#39;0\u0026#39;))\u0026#34;, size = (400,300)) xlims!(0,20); ylims!(0,0.3); title!(L\u0026#34;\\mathrm{pmf\\,of\\,Geo}(p)\u0026#34;) end gif(animation, \u0026#34;pmf.gif\u0026#34;) Hogg et al. (2013). Introduction to Mathematical Statistics(7th Edition): p145.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1486,"permalink":"https://freshrimpsushi.github.io/jp/posts/1486/","tags":null,"title":"幾何分布"},{"categories":"확률분포론","contents":"定義 1 $n \\in \\mathbb{N}$ と $p \\in [0,1]$ に対して以下の確率質量関数を有する離散確率分布 $\\text{Bin}(n,p)$ を 二項分布Binomial Distributionと呼ぶ。 $$ p(x) = \\binom{n}{x} p^{x} (1-p)^{n-x} \\qquad , x = 0 , 1, \\cdots n $$\n基本性質 積率母関数 [1]: $$m(t) = \\left[ (1-p) + pe^{t} \\right]^{n} \\qquad , t \\in \\mathbb{R}$$ 平均と分散 [2]: もし $X \\sim \\text{Bin}(n,p)$ ならば $$ \\begin{align*} E(X) =\u0026amp; np \\\\ \\text{Var}(X) =\u0026amp; np(1-p) \\end{align*} $$ 定理 二項分布の極限分布としてのポアソン分布導出 [a]: $X_{n} \\sim B(n,p)$ とする。もし $\\mu \\approx np$ ならば $$ X_{n} \\overset{D}{\\to} \\text{Poi} (\\mu) $$ 二項分布の極限分布としての標準正規分布導出 [b]: もし $X_i \\sim B(1,p)$ で $Y_n = X_1 + X_2 + \\cdots + X_n$ ならば $Y_n \\sim B(n,p)$ で $$ { { Y_n - np } \\over {\\sqrt{ np(1-p) } } }\\overset{D}{\\to} N(0,1) $$ 説明 ベルヌーイ分布 二項分布は、人が最も簡単に考えられるベルヌーイ試行Bernoulli Experimentから始まる。ベルヌーイ試行は、確率 $0 \\le p \\le 1$ で成功するか失敗するかの2つの結果しかなく、これを $n$ 回で一般化したものが二項分布である。逆に、ベルヌーイ分布は二項分布が $n=1$ の時の特別なケースである。\n多項分布 さらに、成功か失敗かの2つのケースではなく $k$ の場合に一般化することで、多変量分布 $M (n; p_{1} , \\cdots , p_{k})$ を多項分布Multinomial Distributionと呼ぶ。その確率質量関数は次のように与えられる。 $$ p(x_{1} , \\cdots , x_{k}) = {{ n! } \\over { x_{1} ! \\cdots x_{k}! }} p_{1}^{x_{1}} \\cdots p_{k}^{x_{k}} $$\n証明 [1] $$ \\begin{align*} M(t) =\u0026amp; \\sum_{x=0}^{n} e^{tx} p(x) \\\\ =\u0026amp; \\sum_{x=0}^{n} e^{tx} \\binom{n}{x} p^{x} (1-p)^{n-x} \\\\ =\u0026amp; \\sum_{x=0}^{n} \\binom{n}{x} \\left( pe^{t} \\right)^{x} (1-p)^{n-x} \\end{align*} $$ 二項定理によると $$ \\sum_{x=0}^{n} \\binom{n}{x} \\left( pe^{t} \\right)^{x} (1-p)^{n-x} = \\left[ pe^{t} + (1-p) \\right]^{n} $$\n■\n[2] 戦略: 教科課程のように数式的トリックを使って導出することもできるが、積率母関数も求めてあるので数理統計学の理論を使って簡単に導出してみよう。\n$M$ の導関数は $$ M ' (t) = n \\left[ (1-p) + pe^{t} \\right]^{n-1} \\left( pe^{t} \\right) $$ 積率母関数の定義から $ E(X) = M ' (0):$ であるため $$ \\mu := E(X) = M ' (0) = np $$ $M$ の二階導関数は $$ M '' (t) = n \\left[ (1-p) + pe^{t} \\right]^{n-1} \\left( pe^{t} \\right) + n(n-1) \\left[ (1-p) + pe^{t} \\right]^{n-2} \\left( pe^{t} \\right)^{2} $$ $M '' (0) = np + n(n-1)p^{2}$ であるため $$ \\begin{align*} \\text{Var}(X) =\u0026amp; E \\left( X^{2} \\right) - \\mu^{2} \\\\ =\u0026amp; M '' (0) - (np)^{2} \\\\ =\u0026amp; np + n(n-1)p^{2} - n^{2}p^{2} \\\\ =\u0026amp; np(1-p) \\end{align*} $$\n■\n[a] 積率生成関数で近似する。\n■\n[b] 中心極限定理のように近似する。\n■\nコード 次はJuliaのコードで、二項分布の確率質量関数をGIFで表示するものである。\n@time using LaTeXStrings @time using Distributions @time using Plots cd(@__DIR__) x = 0:20 P = collect(0.0:0.01:1.0); append!(P, reverse(P)) animation = @animate for p ∈ P scatter(x, pdf.(Binomial(10, p), x), color = :black, markerstrokecolor = :black, label = \u0026#34;n = 10, p = $(rpad(p, 4, \u0026#39;0\u0026#39;))\u0026#34;, size = (400,300)) xlims!(0,20); ylims!(0,0.5); title!(L\u0026#34;\\mathrm{pmf\\,of\\,Bin}(10, p)\u0026#34;) end gif(animation, \u0026#34;pmf10.gif\u0026#34;) animation = @animate for p ∈ P scatter(x, pdf.(Binomial(20, p), x), color = :black, markerstrokecolor = :black, label = \u0026#34;n = 20, p = $(rpad(p, 4, \u0026#39;0\u0026#39;))\u0026#34;, size = (400,300)) xlims!(0,20); ylims!(0,0.5); title!(L\u0026#34;\\mathrm{pmf\\,of\\,Bin}(20, p)\u0026#34;) end gif(animation, \u0026#34;pmf20.gif\u0026#34;) Hogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p142.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1480,"permalink":"https://freshrimpsushi.github.io/jp/posts/1480/","tags":["줄리아"],"title":"二項分布"},{"categories":"정수론","contents":"素数 1万番目までの素数のリストである。\nダウンロード\r2 3 5 7 11 13 17 19 23 29 31 37 41 43 47 53 59 61 67 71 73 79 83 89 97 101 103 107 109 113 127 131 137 139 149 151 157 163 167 173 179 181 191 193 197 199 211 223 227 229 233 239 241 251 257 263 269 271 277 281 283 293 307 311 313 317 331 337 347 349 353 359 367 373 379 383 389 397 401 409 419 421 431 433 439 443 449 457 461 463 467 479 487 491 499 503 509 521 523 541 547 557 563 569 571 577 587 593 599 601 607 613 617 619 631 641 643 647 653 659 661 673 677 683 691 701 709 719 727 733 739 743 751 757 761 769 773 787 797 809 811 821 823 827 829 839 853 857 859 863 877 881 883 887 907 911 919 929 937 941 947 953 967 971 977 983 991 997 1009 1013 1019 1021 1031 1033 1039 1049 1051 1061 1063 1069 1087 1091 1093 1097 1103 1109 1117 1123 1129 1151 1153 1163 1171 1181 1187 1193 1201 1213 1217 1223 1229 1231 1237 1249 1259 1277 1279 1283 1289 1291 1297 1301 1303 1307 1319 1321 1327 1361 1367 1373 1381 1399 1409 1423 1427 1429 1433 1439 1447 1451 1453 1459 1471 1481 1483 1487 1489 1493 1499 1511 1523 1531 1543 1549 1553 1559 1567 1571 1579 1583 1597 1601 1607 1609 1613 1619 1621 1627 1637 1657 1663 1667 1669 1693 1697 1699 1709 1721 1723 1733 1741 1747 1753 1759 1777 1783 1787 1789 1801 1811 1823 1831 1847 1861 1867 1871 1873 1877 1879 1889 1901 1907 1913 1931 1933 1949 1951 1973 1979 1987 1993 1997 1999 2003 2011 2017 2027 2029 2039 2053 2063 2069 2081 2083 2087 2089 2099 2111 2113 2129 2131 2137 2141 2143 2153 2161 2179 2203 2207 2213 2221 2237 2239 2243 2251 2267 2269 2273 2281 2287 2293 2297 2309 2311 2333 2339 2341 2347 2351 2357 2371 2377 2381 2383 2389 2393 2399 2411 2417 2423 2437 2441 2447 2459 2467 2473 2477 2503 2521 2531 2539 2543 2549 2551 2557 2579 2591 2593 2609 2617 2621 2633 2647 2657 2659 2663 2671 2677 2683 2687 2689 2693 2699 2707 2711 2713 2719 2729 2731 2741 2749 2753 2767 2777 2789 2791 2797 2801 2803 2819 2833 2837 2843 2851 2857 2861 2879 2887 2897 2903 2909 2917 2927 2939 2953 2957 2963 2969 2971 2999 3001 3011 3019 3023 3037 3041 3049 3061 3067 3079 3083 3089 3109 3119 3121 3137 3163 3167 3169 3181 3187 3191 3203 3209 3217 3221 3229 3251 3253 3257 3259 3271 3299 3301 3307 3313 3319 3323 3329 3331 3343 3347 3359 3361 3371 3373 3389 3391 3407 3413 3433 3449 3457 3461 3463 3467 3469 3491 3499 3511 3517 3527 3529 3533 3539 3541 3547 3557 3559 3571 3581 3583 3593 3607 3613 3617 3623 3631 3637 3643 3659 3671 3673 3677 3691 3697 3701 3709 3719 3727 3733 3739 3761 3767 3769 3779 3793 3797 3803 3821 3823 3833 3847 3851 3853 3863 3877 3881 3889 3907 3911 3917 3919 3923 3929 3931 3943 3947 3967 3989 4001 4003 4007 4013 4019 4021 4027 4049 4051 4057 4073 4079 4091 4093 4099 4111 4127 4129 4133 4139 4153 4157 4159 4177 4201 4211 4217 4219 4229 4231 4241 4243 4253 4259 4261 4271 4273 4283 4289 4297 4327 4337 4339 4349 4357 4363 4373 4391 4397 4409 4421 4423 4441 4447 4451 4457 4463 4481 4483 4493 4507 4513 4517 4519 4523 4547 4549 4561 4567 4583 4591 4597 4603 4621 4637 4639 4643 4649 4651 4657 4663 4673 4679 4691 4703 4721 4723 4729 4733 4751 4759 4783 4787 4789 4793 4799 4801 4813 4817 4831 4861 4871 4877 4889 4903 4909 4919 4931 4933 4937 4943 4951 4957 4967 4969 4973 4987 4993 4999 5003 5009 5011 5021 5023 5039 5051 5059 5077 5081 5087 5099 5101 5107 5113 5119 5147 5153 5167 5171 5179 5189 5197 5209 5227 5231 5233 5237 5261 5273 5279 5281 5297 5303 5309 5323 5333 5347 5351 5381 5387 5393 5399 5407 5413 5417 5419 5431 5437 5441 5443 5449 5471 5477 5479 5483 5501 5503 5507 5519 5521 5527 5531 5557 5563 5569 5573 5581 5591 5623 5639 5641 5647 5651 5653 5657 5659 5669 5683 5689 5693 5701 5711 5717 5737 5741 5743 5749 5779 5783 5791 5801 5807 5813 5821 5827 5839 5843 5849 5851 5857 5861 5867 5869 5879 5881 5897 5903 5923 5927 5939 5953 5981 5987 6007 6011 6029 6037 6043 6047 6053 6067 6073 6079 6089 6091 6101 6113 6121 6131 6133 6143 6151 6163 6173 6197 6199 6203 6211 6217 6221 6229 6247 6257 6263 6269 6271 6277 6287 6299 6301 6311 6317 6323 6329 6337 6343 6353 6359 6361 6367 6373 6379 6389 6397 6421 6427 6449 6451 6469 6473 6481 6491 6521 6529 6547 6551 6553 6563 6569 6571 6577 6581 6599 6607 6619 6637 6653 6659 6661 6673 6679 6689 6691 6701 6703 6709 6719 6733 6737 6761 6763 6779 6781 6791 6793 6803 6823 6827 6829 6833 6841 6857 6863 6869 6871 6883 6899 6907 6911 6917 6947 6949 6959 6961 6967 6971 6977 6983 6991 6997 7001 7013 7019 7027 7039 7043 7057 7069 7079 7103 7109 7121 7127 7129 7151 7159 7177 7187 7193 7207 7211 7213 7219 7229 7237 7243 7247 7253 7283 7297 7307 7309 7321 7331 7333 7349 7351 7369 7393 7411 7417 7433 7451 7457 7459 7477 7481 7487 7489 7499 7507 7517 7523 7529 7537 7541 7547 7549 7559 7561 7573 7577 7583 7589 7591 7603 7607 7621 7639 7643 7649 7669 7673 7681 7687 7691 7699 7703 7717 7723 7727 7741 7753 7757 7759 7789 7793 7817 7823 7829 7841 7853 7867 7873 7877 7879 7883 7901 7907 7919 7927 7933 7937 7949 7951 7963 7993 8009 8011 8017 8039 8053 8059 8069 8081 8087 8089 8093 8101 8111 8117 8123 8147 8161 8167 8171 8179 8191 8209 8219 8221 8231 8233 8237 8243 8263 8269 8273 8287 8291 8293 8297 8311 8317 8329 8353 8363 8369 8377 8387 8389 8419 8423 8429 8431 8443 8447 8461 8467 8501 8513 8521 8527 8537 8539 8543 8563 8573 8581 8597 8599 8609 8623 8627 8629 8641 8647 8663 8669 8677 8681 8689 8693 8699 8707 8713 8719 8731 8737 8741 8747 8753 8761 8779 8783 8803 8807 8819 8821 8831 8837 8839 8849 8861 8863 8867 8887 8893 8923 8929 8933 8941 8951 8963 8969 8971 8999 9001 9007 9011 9013 9029 9041 9043 9049 9059 9067 9091 9103 9109 9127 9133 9137 9151 9157 9161 9173 9181 9187 9199 9203 9209 9221 9227 9239 9241 9257 9277 9281 9283 9293 9311 9319 9323 9337 9341 9343 9349 9371 9377 9391 9397 9403 9413 9419 9421 9431 9433 9437 9439 9461 9463 9467 9473 9479 9491 9497 9511 9521 9533 9539 9547 9551 9587 9601 9613 9619 9623 9629 9631 9643 9649 9661 9677 9679 9689 9697 9719 9721 9733 9739 9743 9749 9767 9769 9781 9787 9791 9803 9811 9817 9829 9833 9839 9851 9857 9859 9871 9883 9887 9901 9907 9923 9929 9931 9941 9949 9967 9973 10007 10009 10037 10039 10061 10067 10069 10079 10091 10093 10099 10103 10111 10133 10139 10141 10151 10159 10163 10169 10177 10181 10193 10211 10223 10243 10247 10253 10259 10267 10271 10273 10289 10301 10303 10313 10321 10331 10333 10337 10343 10357 10369 10391 10399 10427 10429 10433 10453 10457 10459 10463 10477 10487 10499 10501 10513 10529 10531 10559 10567 10589 10597 10601 10607 10613 10627 10631 10639 10651 10657 10663 10667 10687 10691 10709 10711 10723 10729 10733 10739 10753 10771 10781 10789 10799 10831 10837 10847 10853 10859 10861 10867 10883 10889 10891 10903 10909 10937 10939 10949 10957 10973 10979 10987 10993 11003 11027 11047 11057 11059 11069 11071 11083 11087 11093 11113 11117 11119 11131 11149 11159 11161 11171 11173 11177 11197 11213 11239 11243 11251 11257 11261 11273 11279 11287 11299 11311 11317 11321 11329 11351 11353 11369 11383 11393 11399 11411 11423 11437 11443 11447 11467 11471 11483 11489 11491 11497 11503 11519 11527 11549 11551 11579 11587 11593 11597 11617 11621 11633 11657 11677 11681 11689 11699 11701 11717 11719 11731 11743 11777 11779 11783 11789 11801 11807 11813 11821 11827 11831 11833 11839 11863 11867 11887 11897 11903 11909 11923 11927 11933 11939 11941 11953 11959 11969 11971 11981 11987 12007 12011 12037 12041 12043 12049 12071 12073 12097 12101 12107 12109 12113 12119 12143 12149 12157 12161 12163 12197 12203 12211 12227 12239 12241 12251 12253 12263 12269 12277 12281 12289 12301 12323 12329 12343 12347 12373 12377 12379 12391 12401 12409 12413 12421 12433 12437 12451 12457 12473 12479 12487 12491 12497 12503 12511 12517 12527 12539 12541 12547 12553 12569 12577 12583 12589 12601 12611 12613 12619 12637 12641 12647 12653 12659 12671 12689 12697 12703 12713 12721 12739 12743 12757 12763 12781 12791 12799 12809 12821 12823 12829 12841 12853 12889 12893 12899 12907 12911 12917 12919 12923 12941 12953 12959 12967 12973 12979 12983 13001 13003 13007 13009 13033 13037 13043 13049 13063 13093 13099 13103 13109 13121 13127 13147 13151 13159 13163 13171 13177 13183 13187 13217 13219 13229 13241 13249 13259 13267 13291 13297 13309 13313 13327 13331 13337 13339 13367 13381 13397 13399 13411 13417 13421 13441 13451 13457 13463 13469 13477 13487 13499 13513 13523 13537 13553 13567 13577 13591 13597 13613 13619 13627 13633 13649 13669 13679 13681 13687 13691 13693 13697 13709 13711 13721 13723 13729 13751 13757 13759 13763 13781 13789 13799 13807 13829 13831 13841 13859 13873 13877 13879 13883 13901 13903 13907 13913 13921 13931 13933 13963 13967 13997 13999 14009 14011 14029 14033 14051 14057 14071 14081 14083 14087 14107 14143 14149 14153 14159 14173 14177 14197 14207 14221 14243 14249 14251 14281 14293 14303 14321 14323 14327 14341 14347 14369 14387 14389 14401 14407 14411 14419 14423 14431 14437 14447 14449 14461 14479 14489 14503 14519 14533 14537 14543 14549 14551 14557 14561 14563 14591 14593 14621 14627 14629 14633 14639 14653 14657 14669 14683 14699 14713 14717 14723 14731 14737 14741 14747 14753 14759 14767 14771 14779 14783 14797 14813 14821 14827 14831 14843 14851 14867 14869 14879 14887 14891 14897 14923 14929 14939 14947 14951 14957 14969 14983 15013 15017 15031 15053 15061 15073 15077 15083 15091 15101 15107 15121 15131 15137 15139 15149 15161 15173 15187 15193 15199 15217 15227 15233 15241 15259 15263 15269 15271 15277 15287 15289 15299 15307 15313 15319 15329 15331 15349 15359 15361 15373 15377 15383 15391 15401 15413 15427 15439 15443 15451 15461 15467 15473 15493 15497 15511 15527 15541 15551 15559 15569 15581 15583 15601 15607 15619 15629 15641 15643 15647 15649 15661 15667 15671 15679 15683 15727 15731 15733 15737 15739 15749 15761 15767 15773 15787 15791 15797 15803 15809 15817 15823 15859 15877 15881 15887 15889 15901 15907 15913 15919 15923 15937 15959 15971 15973 15991 16001 16007 16033 16057 16061 16063 16067 16069 16073 16087 16091 16097 16103 16111 16127 16139 16141 16183 16187 16189 16193 16217 16223 16229 16231 16249 16253 16267 16273 16301 16319 16333 16339 16349 16361 16363 16369 16381 16411 16417 16421 16427 16433 16447 16451 16453 16477 16481 16487 16493 16519 16529 16547 16553 16561 16567 16573 16603 16607 16619 16631 16633 16649 16651 16657 16661 16673 16691 16693 16699 16703 16729 16741 16747 16759 16763 16787 16811 16823 16829 16831 16843 16871 16879 16883 16889 16901 16903 16921 16927 16931 16937 16943 16963 16979 16981 16987 16993 17011 17021 17027 17029 17033 17041 17047 17053 17077 17093 17099 17107 17117 17123 17137 17159 17167 17183 17189 17191 17203 17207 17209 17231 17239 17257 17291 17293 17299 17317 17321 17327 17333 17341 17351 17359 17377 17383 17387 17389 17393 17401 17417 17419 17431 17443 17449 17467 17471 17477 17483 17489 17491 17497 17509 17519 17539 17551 17569 17573 17579 17581 17597 17599 17609 17623 17627 17657 17659 17669 17681 17683 17707 17713 17729 17737 17747 17749 17761 17783 17789 17791 17807 17827 17837 17839 17851 17863 17881 17891 17903 17909 17911 17921 17923 17929 17939 17957 17959 17971 17977 17981 17987 17989 18013 18041 18043 18047 18049 18059 18061 18077 18089 18097 18119 18121 18127 18131 18133 18143 18149 18169 18181 18191 18199 18211 18217 18223 18229 18233 18251 18253 18257 18269 18287 18289 18301 18307 18311 18313 18329 18341 18353 18367 18371 18379 18397 18401 18413 18427 18433 18439 18443 18451 18457 18461 18481 18493 18503 18517 18521 18523 18539 18541 18553 18583 18587 18593 18617 18637 18661 18671 18679 18691 18701 18713 18719 18731 18743 18749 18757 18773 18787 18793 18797 18803 18839 18859 18869 18899 18911 18913 18917 18919 18947 18959 18973 18979 19001 19009 19013 19031 19037 19051 19069 19073 19079 19081 19087 19121 19139 19141 19157 19163 19181 19183 19207 19211 19213 19219 19231 19237 19249 19259 19267 19273 19289 19301 19309 19319 19333 19373 19379 19381 19387 19391 19403 19417 19421 19423 19427 19429 19433 19441 19447 19457 19463 19469 19471 19477 19483 19489 19501 19507 19531 19541 19543 19553 19559 19571 19577 19583 19597 19603 19609 19661 19681 19687 19697 19699 19709 19717 19727 19739 19751 19753 19759 19763 19777 19793 19801 19813 19819 19841 19843 19853 19861 19867 19889 19891 19913 19919 19927 19937 19949 19961 19963 19973 19979 19991 19993 19997 20011 20021 20023 20029 20047 20051 20063 20071 20089 20101 20107 20113 20117 20123 20129 20143 20147 20149 20161 20173 20177 20183 20201 20219 20231 20233 20249 20261 20269 20287 20297 20323 20327 20333 20341 20347 20353 20357 20359 20369 20389 20393 20399 20407 20411 20431 20441 20443 20477 20479 20483 20507 20509 20521 20533 20543 20549 20551 20563 20593 20599 20611 20627 20639 20641 20663 20681 20693 20707 20717 20719 20731 20743 20747 20749 20753 20759 20771 20773 20789 20807 20809 20849 20857 20873 20879 20887 20897 20899 20903 20921 20929 20939 20947 20959 20963 20981 20983 21001 21011 21013 21017 21019 21023 21031 21059 21061 21067 21089 21101 21107 21121 21139 21143 21149 21157 21163 21169 21179 21187 21191 21193 21211 21221 21227 21247 21269 21277 21283 21313 21317 21319 21323 21341 21347 21377 21379 21383 21391 21397 21401 21407 21419 21433 21467 21481 21487 21491 21493 21499 21503 21517 21521 21523 21529 21557 21559 21563 21569 21577 21587 21589 21599 21601 21611 21613 21617 21647 21649 21661 21673 21683 21701 21713 21727 21737 21739 21751 21757 21767 21773 21787 21799 21803 21817 21821 21839 21841 21851 21859 21863 21871 21881 21893 21911 21929 21937 21943 21961 21977 21991 21997 22003 22013 22027 22031 22037 22039 22051 22063 22067 22073 22079 22091 22093 22109 22111 22123 22129 22133 22147 22153 22157 22159 22171 22189 22193 22229 22247 22259 22271 22273 22277 22279 22283 22291 22303 22307 22343 22349 22367 22369 22381 22391 22397 22409 22433 22441 22447 22453 22469 22481 22483 22501 22511 22531 22541 22543 22549 22567 22571 22573 22613 22619 22621 22637 22639 22643 22651 22669 22679 22691 22697 22699 22709 22717 22721 22727 22739 22741 22751 22769 22777 22783 22787 22807 22811 22817 22853 22859 22861 22871 22877 22901 22907 22921 22937 22943 22961 22963 22973 22993 23003 23011 23017 23021 23027 23029 23039 23041 23053 23057 23059 23063 23071 23081 23087 23099 23117 23131 23143 23159 23167 23173 23189 23197 23201 23203 23209 23227 23251 23269 23279 23291 23293 23297 23311 23321 23327 23333 23339 23357 23369 23371 23399 23417 23431 23447 23459 23473 23497 23509 23531 23537 23539 23549 23557 23561 23563 23567 23581 23593 23599 23603 23609 23623 23627 23629 23633 23663 23669 23671 23677 23687 23689 23719 23741 23743 23747 23753 23761 23767 23773 23789 23801 23813 23819 23827 23831 23833 23857 23869 23873 23879 23887 23893 23899 23909 23911 23917 23929 23957 23971 23977 23981 23993 24001 24007 24019 24023 24029 24043 24049 24061 24071 24077 24083 24091 24097 24103 24107 24109 24113 24121 24133 24137 24151 24169 24179 24181 24197 24203 24223 24229 24239 24247 24251 24281 24317 24329 24337 24359 24371 24373 24379 24391 24407 24413 24419 24421 24439 24443 24469 24473 24481 24499 24509 24517 24527 24533 24547 24551 24571 24593 24611 24623 24631 24659 24671 24677 24683 24691 24697 24709 24733 24749 24763 24767 24781 24793 24799 24809 24821 24841 24847 24851 24859 24877 24889 24907 24917 24919 24923 24943 24953 24967 24971 24977 24979 24989 25013 25031 25033 25037 25057 25073 25087 25097 25111 25117 25121 25127 25147 25153 25163 25169 25171 25183 25189 25219 25229 25237 25243 25247 25253 25261 25301 25303 25307 25309 25321 25339 25343 25349 25357 25367 25373 25391 25409 25411 25423 25439 25447 25453 25457 25463 25469 25471 25523 25537 25541 25561 25577 25579 25583 25589 25601 25603 25609 25621 25633 25639 25643 25657 25667 25673 25679 25693 25703 25717 25733 25741 25747 25759 25763 25771 25793 25799 25801 25819 25841 25847 25849 25867 25873 25889 25903 25913 25919 25931 25933 25939 25943 25951 25969 25981 25997 25999 26003 26017 26021 26029 26041 26053 26083 26099 26107 26111 26113 26119 26141 26153 26161 26171 26177 26183 26189 26203 26209 26227 26237 26249 26251 26261 26263 26267 26293 26297 26309 26317 26321 26339 26347 26357 26371 26387 26393 26399 26407 26417 26423 26431 26437 26449 26459 26479 26489 26497 26501 26513 26539 26557 26561 26573 26591 26597 26627 26633 26641 26647 26669 26681 26683 26687 26693 26699 26701 26711 26713 26717 26723 26729 26731 26737 26759 26777 26783 26801 26813 26821 26833 26839 26849 26861 26863 26879 26881 26891 26893 26903 26921 26927 26947 26951 26953 26959 26981 26987 26993 27011 27017 27031 27043 27059 27061 27067 27073 27077 27091 27103 27107 27109 27127 27143 27179 27191 27197 27211 27239 27241 27253 27259 27271 27277 27281 27283 27299 27329 27337 27361 27367 27397 27407 27409 27427 27431 27437 27449 27457 27479 27481 27487 27509 27527 27529 27539 27541 27551 27581 27583 27611 27617 27631 27647 27653 27673 27689 27691 27697 27701 27733 27737 27739 27743 27749 27751 27763 27767 27773 27779 27791 27793 27799 27803 27809 27817 27823 27827 27847 27851 27883 27893 27901 27917 27919 27941 27943 27947 27953 27961 27967 27983 27997 28001 28019 28027 28031 28051 28057 28069 28081 28087 28097 28099 28109 28111 28123 28151 28163 28181 28183 28201 28211 28219 28229 28277 28279 28283 28289 28297 28307 28309 28319 28349 28351 28387 28393 28403 28409 28411 28429 28433 28439 28447 28463 28477 28493 28499 28513 28517 28537 28541 28547 28549 28559 28571 28573 28579 28591 28597 28603 28607 28619 28621 28627 28631 28643 28649 28657 28661 28663 28669 28687 28697 28703 28711 28723 28729 28751 28753 28759 28771 28789 28793 28807 28813 28817 28837 28843 28859 28867 28871 28879 28901 28909 28921 28927 28933 28949 28961 28979 29009 29017 29021 29023 29027 29033 29059 29063 29077 29101 29123 29129 29131 29137 29147 29153 29167 29173 29179 29191 29201 29207 29209 29221 29231 29243 29251 29269 29287 29297 29303 29311 29327 29333 29339 29347 29363 29383 29387 29389 29399 29401 29411 29423 29429 29437 29443 29453 29473 29483 29501 29527 29531 29537 29567 29569 29573 29581 29587 29599 29611 29629 29633 29641 29663 29669 29671 29683 29717 29723 29741 29753 29759 29761 29789 29803 29819 29833 29837 29851 29863 29867 29873 29879 29881 29917 29921 29927 29947 29959 29983 29989 30011 30013 30029 30047 30059 30071 30089 30091 30097 30103 30109 30113 30119 30133 30137 30139 30161 30169 30181 30187 30197 30203 30211 30223 30241 30253 30259 30269 30271 30293 30307 30313 30319 30323 30341 30347 30367 30389 30391 30403 30427 30431 30449 30467 30469 30491 30493 30497 30509 30517 30529 30539 30553 30557 30559 30577 30593 30631 30637 30643 30649 30661 30671 30677 30689 30697 30703 30707 30713 30727 30757 30763 30773 30781 30803 30809 30817 30829 30839 30841 30851 30853 30859 30869 30871 30881 30893 30911 30931 30937 30941 30949 30971 30977 30983 31013 31019 31033 31039 31051 31063 31069 31079 31081 31091 31121 31123 31139 31147 31151 31153 31159 31177 31181 31183 31189 31193 31219 31223 31231 31237 31247 31249 31253 31259 31267 31271 31277 31307 31319 31321 31327 31333 31337 31357 31379 31387 31391 31393 31397 31469 31477 31481 31489 31511 31513 31517 31531 31541 31543 31547 31567 31573 31583 31601 31607 31627 31643 31649 31657 31663 31667 31687 31699 31721 31723 31727 31729 31741 31751 31769 31771 31793 31799 31817 31847 31849 31859 31873 31883 31891 31907 31957 31963 31973 31981 31991 32003 32009 32027 32029 32051 32057 32059 32063 32069 32077 32083 32089 32099 32117 32119 32141 32143 32159 32173 32183 32189 32191 32203 32213 32233 32237 32251 32257 32261 32297 32299 32303 32309 32321 32323 32327 32341 32353 32359 32363 32369 32371 32377 32381 32401 32411 32413 32423 32429 32441 32443 32467 32479 32491 32497 32503 32507 32531 32533 32537 32561 32563 32569 32573 32579 32587 32603 32609 32611 32621 32633 32647 32653 32687 32693 32707 32713 32717 32719 32749 32771 32779 32783 32789 32797 32801 32803 32831 32833 32839 32843 32869 32887 32909 32911 32917 32933 32939 32941 32957 32969 32971 32983 32987 32993 32999 33013 33023 33029 33037 33049 33053 33071 33073 33083 33091 33107 33113 33119 33149 33151 33161 33179 33181 33191 33199 33203 33211 33223 33247 33287 33289 33301 33311 33317 33329 33331 33343 33347 33349 33353 33359 33377 33391 33403 33409 33413 33427 33457 33461 33469 33479 33487 33493 33503 33521 33529 33533 33547 33563 33569 33577 33581 33587 33589 33599 33601 33613 33617 33619 33623 33629 33637 33641 33647 33679 33703 33713 33721 33739 33749 33751 33757 33767 33769 33773 33791 33797 33809 33811 33827 33829 33851 33857 33863 33871 33889 33893 33911 33923 33931 33937 33941 33961 33967 33997 34019 34031 34033 34039 34057 34061 34123 34127 34129 34141 34147 34157 34159 34171 34183 34211 34213 34217 34231 34253 34259 34261 34267 34273 34283 34297 34301 34303 34313 34319 34327 34337 34351 34361 34367 34369 34381 34403 34421 34429 34439 34457 34469 34471 34483 34487 34499 34501 34511 34513 34519 34537 34543 34549 34583 34589 34591 34603 34607 34613 34631 34649 34651 34667 34673 34679 34687 34693 34703 34721 34729 34739 34747 34757 34759 34763 34781 34807 34819 34841 34843 34847 34849 34871 34877 34883 34897 34913 34919 34939 34949 34961 34963 34981 35023 35027 35051 35053 35059 35069 35081 35083 35089 35099 35107 35111 35117 35129 35141 35149 35153 35159 35171 35201 35221 35227 35251 35257 35267 35279 35281 35291 35311 35317 35323 35327 35339 35353 35363 35381 35393 35401 35407 35419 35423 35437 35447 35449 35461 35491 35507 35509 35521 35527 35531 35533 35537 35543 35569 35573 35591 35593 35597 35603 35617 35671 35677 35729 35731 35747 35753 35759 35771 35797 35801 35803 35809 35831 35837 35839 35851 35863 35869 35879 35897 35899 35911 35923 35933 35951 35963 35969 35977 35983 35993 35999 36007 36011 36013 36017 36037 36061 36067 36073 36083 36097 36107 36109 36131 36137 36151 36161 36187 36191 36209 36217 36229 36241 36251 36263 36269 36277 36293 36299 36307 36313 36319 36341 36343 36353 36373 36383 36389 36433 36451 36457 36467 36469 36473 36479 36493 36497 36523 36527 36529 36541 36551 36559 36563 36571 36583 36587 36599 36607 36629 36637 36643 36653 36671 36677 36683 36691 36697 36709 36713 36721 36739 36749 36761 36767 36779 36781 36787 36791 36793 36809 36821 36833 36847 36857 36871 36877 36887 36899 36901 36913 36919 36923 36929 36931 36943 36947 36973 36979 36997 37003 37013 37019 37021 37039 37049 37057 37061 37087 37097 37117 37123 37139 37159 37171 37181 37189 37199 37201 37217 37223 37243 37253 37273 37277 37307 37309 37313 37321 37337 37339 37357 37361 37363 37369 37379 37397 37409 37423 37441 37447 37463 37483 37489 37493 37501 37507 37511 37517 37529 37537 37547 37549 37561 37567 37571 37573 37579 37589 37591 37607 37619 37633 37643 37649 37657 37663 37691 37693 37699 37717 37747 37781 37783 37799 37811 37813 37831 37847 37853 37861 37871 37879 37889 37897 37907 37951 37957 37963 37967 37987 37991 37993 37997 38011 38039 38047 38053 38069 38083 38113 38119 38149 38153 38167 38177 38183 38189 38197 38201 38219 38231 38237 38239 38261 38273 38281 38287 38299 38303 38317 38321 38327 38329 38333 38351 38371 38377 38393 38431 38447 38449 38453 38459 38461 38501 38543 38557 38561 38567 38569 38593 38603 38609 38611 38629 38639 38651 38653 38669 38671 38677 38693 38699 38707 38711 38713 38723 38729 38737 38747 38749 38767 38783 38791 38803 38821 38833 38839 38851 38861 38867 38873 38891 38903 38917 38921 38923 38933 38953 38959 38971 38977 38993 39019 39023 39041 39043 39047 39079 39089 39097 39103 39107 39113 39119 39133 39139 39157 39161 39163 39181 39191 39199 39209 39217 39227 39229 39233 39239 39241 39251 39293 39301 39313 39317 39323 39341 39343 39359 39367 39371 39373 39383 39397 39409 39419 39439 39443 39451 39461 39499 39503 39509 39511 39521 39541 39551 39563 39569 39581 39607 39619 39623 39631 39659 39667 39671 39679 39703 39709 39719 39727 39733 39749 39761 39769 39779 39791 39799 39821 39827 39829 39839 39841 39847 39857 39863 39869 39877 39883 39887 39901 39929 39937 39953 39971 39979 39983 39989 40009 40013 40031 40037 40039 40063 40087 40093 40099 40111 40123 40127 40129 40151 40153 40163 40169 40177 40189 40193 40213 40231 40237 40241 40253 40277 40283 40289 40343 40351 40357 40361 40387 40423 40427 40429 40433 40459 40471 40483 40487 40493 40499 40507 40519 40529 40531 40543 40559 40577 40583 40591 40597 40609 40627 40637 40639 40693 40697 40699 40709 40739 40751 40759 40763 40771 40787 40801 40813 40819 40823 40829 40841 40847 40849 40853 40867 40879 40883 40897 40903 40927 40933 40939 40949 40961 40973 40993 41011 41017 41023 41039 41047 41051 41057 41077 41081 41113 41117 41131 41141 41143 41149 41161 41177 41179 41183 41189 41201 41203 41213 41221 41227 41231 41233 41243 41257 41263 41269 41281 41299 41333 41341 41351 41357 41381 41387 41389 41399 41411 41413 41443 41453 41467 41479 41491 41507 41513 41519 41521 41539 41543 41549 41579 41593 41597 41603 41609 41611 41617 41621 41627 41641 41647 41651 41659 41669 41681 41687 41719 41729 41737 41759 41761 41771 41777 41801 41809 41813 41843 41849 41851 41863 41879 41887 41893 41897 41903 41911 41927 41941 41947 41953 41957 41959 41969 41981 41983 41999 42013 42017 42019 42023 42043 42061 42071 42073 42083 42089 42101 42131 42139 42157 42169 42179 42181 42187 42193 42197 42209 42221 42223 42227 42239 42257 42281 42283 42293 42299 42307 42323 42331 42337 42349 42359 42373 42379 42391 42397 42403 42407 42409 42433 42437 42443 42451 42457 42461 42463 42467 42473 42487 42491 42499 42509 42533 42557 42569 42571 42577 42589 42611 42641 42643 42649 42667 42677 42683 42689 42697 42701 42703 42709 42719 42727 42737 42743 42751 42767 42773 42787 42793 42797 42821 42829 42839 42841 42853 42859 42863 42899 42901 42923 42929 42937 42943 42953 42961 42967 42979 42989 43003 43013 43019 43037 43049 43051 43063 43067 43093 43103 43117 43133 43151 43159 43177 43189 43201 43207 43223 43237 43261 43271 43283 43291 43313 43319 43321 43331 43391 43397 43399 43403 43411 43427 43441 43451 43457 43481 43487 43499 43517 43541 43543 43573 43577 43579 43591 43597 43607 43609 43613 43627 43633 43649 43651 43661 43669 43691 43711 43717 43721 43753 43759 43777 43781 43783 43787 43789 43793 43801 43853 43867 43889 43891 43913 43933 43943 43951 43961 43963 43969 43973 43987 43991 43997 44017 44021 44027 44029 44041 44053 44059 44071 44087 44089 44101 44111 44119 44123 44129 44131 44159 44171 44179 44189 44201 44203 44207 44221 44249 44257 44263 44267 44269 44273 44279 44281 44293 44351 44357 44371 44381 44383 44389 44417 44449 44453 44483 44491 44497 44501 44507 44519 44531 44533 44537 44543 44549 44563 44579 44587 44617 44621 44623 44633 44641 44647 44651 44657 44683 44687 44699 44701 44711 44729 44741 44753 44771 44773 44777 44789 44797 44809 44819 44839 44843 44851 44867 44879 44887 44893 44909 44917 44927 44939 44953 44959 44963 44971 44983 44987 45007 45013 45053 45061 45077 45083 45119 45121 45127 45131 45137 45139 45161 45179 45181 45191 45197 45233 45247 45259 45263 45281 45289 45293 45307 45317 45319 45329 45337 45341 45343 45361 45377 45389 45403 45413 45427 45433 45439 45481 45491 45497 45503 45523 45533 45541 45553 45557 45569 45587 45589 45599 45613 45631 45641 45659 45667 45673 45677 45691 45697 45707 45737 45751 45757 45763 45767 45779 45817 45821 45823 45827 45833 45841 45853 45863 45869 45887 45893 45943 45949 45953 45959 45971 45979 45989 46021 46027 46049 46051 46061 46073 46091 46093 46099 46103 46133 46141 46147 46153 46171 46181 46183 46187 46199 46219 46229 46237 46261 46271 46273 46279 46301 46307 46309 46327 46337 46349 46351 46381 46399 46411 46439 46441 46447 46451 46457 46471 46477 46489 46499 46507 46511 46523 46549 46559 46567 46573 46589 46591 46601 46619 46633 46639 46643 46649 46663 46679 46681 46687 46691 46703 46723 46727 46747 46751 46757 46769 46771 46807 46811 46817 46819 46829 46831 46853 46861 46867 46877 46889 46901 46919 46933 46957 46993 46997 47017 47041 47051 47057 47059 47087 47093 47111 47119 47123 47129 47137 47143 47147 47149 47161 47189 47207 47221 47237 47251 47269 47279 47287 47293 47297 47303 47309 47317 47339 47351 47353 47363 47381 47387 47389 47407 47417 47419 47431 47441 47459 47491 47497 47501 47507 47513 47521 47527 47533 47543 47563 47569 47581 47591 47599 47609 47623 47629 47639 47653 47657 47659 47681 47699 47701 47711 47713 47717 47737 47741 47743 47777 47779 47791 47797 47807 47809 47819 47837 47843 47857 47869 47881 47903 47911 47917 47933 47939 47947 47951 47963 47969 47977 47981 48017 48023 48029 48049 48073 48079 48091 48109 48119 48121 48131 48157 48163 48179 48187 48193 48197 48221 48239 48247 48259 48271 48281 48299 48311 48313 48337 48341 48353 48371 48383 48397 48407 48409 48413 48437 48449 48463 48473 48479 48481 48487 48491 48497 48523 48527 48533 48539 48541 48563 48571 48589 48593 48611 48619 48623 48647 48649 48661 48673 48677 48679 48731 48733 48751 48757 48761 48767 48779 48781 48787 48799 48809 48817 48821 48823 48847 48857 48859 48869 48871 48883 48889 48907 48947 48953 48973 48989 48991 49003 49009 49019 49031 49033 49037 49043 49057 49069 49081 49103 49109 49117 49121 49123 49139 49157 49169 49171 49177 49193 49199 49201 49207 49211 49223 49253 49261 49277 49279 49297 49307 49331 49333 49339 49363 49367 49369 49391 49393 49409 49411 49417 49429 49433 49451 49459 49463 49477 49481 49499 49523 49529 49531 49537 49547 49549 49559 49597 49603 49613 49627 49633 49639 49663 49667 49669 49681 49697 49711 49727 49739 49741 49747 49757 49783 49787 49789 49801 49807 49811 49823 49831 49843 49853 49871 49877 49891 49919 49921 49927 49937 49939 49943 49957 49991 49993 49999 50021 50023 50033 50047 50051 50053 50069 50077 50087 50093 50101 50111 50119 50123 50129 50131 50147 50153 50159 50177 50207 50221 50227 50231 50261 50263 50273 50287 50291 50311 50321 50329 50333 50341 50359 50363 50377 50383 50387 50411 50417 50423 50441 50459 50461 50497 50503 50513 50527 50539 50543 50549 50551 50581 50587 50591 50593 50599 50627 50647 50651 50671 50683 50707 50723 50741 50753 50767 50773 50777 50789 50821 50833 50839 50849 50857 50867 50873 50891 50893 50909 50923 50929 50951 50957 50969 50971 50989 50993 51001 51031 51043 51047 51059 51061 51071 51109 51131 51133 51137 51151 51157 51169 51193 51197 51199 51203 51217 51229 51239 51241 51257 51263 51283 51287 51307 51329 51341 51343 51347 51349 51361 51383 51407 51413 51419 51421 51427 51431 51437 51439 51449 51461 51473 51479 51481 51487 51503 51511 51517 51521 51539 51551 51563 51577 51581 51593 51599 51607 51613 51631 51637 51647 51659 51673 51679 51683 51691 51713 51719 51721 51749 51767 51769 51787 51797 51803 51817 51827 51829 51839 51853 51859 51869 51871 51893 51899 51907 51913 51929 51941 51949 51971 51973 51977 51991 52009 52021 52027 52051 52057 52067 52069 52081 52103 52121 52127 52147 52153 52163 52177 52181 52183 52189 52201 52223 52237 52249 52253 52259 52267 52289 52291 52301 52313 52321 52361 52363 52369 52379 52387 52391 52433 52453 52457 52489 52501 52511 52517 52529 52541 52543 52553 52561 52567 52571 52579 52583 52609 52627 52631 52639 52667 52673 52691 52697 52709 52711 52721 52727 52733 52747 52757 52769 52783 52807 52813 52817 52837 52859 52861 52879 52883 52889 52901 52903 52919 52937 52951 52957 52963 52967 52973 52981 52999 53003 53017 53047 53051 53069 53077 53087 53089 53093 53101 53113 53117 53129 53147 53149 53161 53171 53173 53189 53197 53201 53231 53233 53239 53267 53269 53279 53281 53299 53309 53323 53327 53353 53359 53377 53381 53401 53407 53411 53419 53437 53441 53453 53479 53503 53507 53527 53549 53551 53569 53591 53593 53597 53609 53611 53617 53623 53629 53633 53639 53653 53657 53681 53693 53699 53717 53719 53731 53759 53773 53777 53783 53791 53813 53819 53831 53849 53857 53861 53881 53887 53891 53897 53899 53917 53923 53927 53939 53951 53959 53987 53993 54001 54011 54013 54037 54049 54059 54083 54091 54101 54121 54133 54139 54151 54163 54167 54181 54193 54217 54251 54269 54277 54287 54293 54311 54319 54323 54331 54347 54361 54367 54371 54377 54401 54403 54409 54413 54419 54421 54437 54443 54449 54469 54493 54497 54499 54503 54517 54521 54539 54541 54547 54559 54563 54577 54581 54583 54601 54617 54623 54629 54631 54647 54667 54673 54679 54709 54713 54721 54727 54751 54767 54773 54779 54787 54799 54829 54833 54851 54869 54877 54881 54907 54917 54919 54941 54949 54959 54973 54979 54983 55001 55009 55021 55049 55051 55057 55061 55073 55079 55103 55109 55117 55127 55147 55163 55171 55201 55207 55213 55217 55219 55229 55243 55249 55259 55291 55313 55331 55333 55337 55339 55343 55351 55373 55381 55399 55411 55439 55441 55457 55469 55487 55501 55511 55529 55541 55547 55579 55589 55603 55609 55619 55621 55631 55633 55639 55661 55663 55667 55673 55681 55691 55697 55711 55717 55721 55733 55763 55787 55793 55799 55807 55813 55817 55819 55823 55829 55837 55843 55849 55871 55889 55897 55901 55903 55921 55927 55931 55933 55949 55967 55987 55997 56003 56009 56039 56041 56053 56081 56087 56093 56099 56101 56113 56123 56131 56149 56167 56171 56179 56197 56207 56209 56237 56239 56249 56263 56267 56269 56299 56311 56333 56359 56369 56377 56383 56393 56401 56417 56431 56437 56443 56453 56467 56473 56477 56479 56489 56501 56503 56509 56519 56527 56531 56533 56543 56569 56591 56597 56599 56611 56629 56633 56659 56663 56671 56681 56687 56701 56711 56713 56731 56737 56747 56767 56773 56779 56783 56807 56809 56813 56821 56827 56843 56857 56873 56891 56893 56897 56909 56911 56921 56923 56929 56941 56951 56957 56963 56983 56989 56993 56999 57037 57041 57047 57059 57073 57077 57089 57097 57107 57119 57131 57139 57143 57149 57163 57173 57179 57191 57193 57203 57221 57223 57241 57251 57259 57269 57271 57283 57287 57301 57329 57331 57347 57349 57367 57373 57383 57389 57397 57413 57427 57457 57467 57487 57493 57503 57527 57529 57557 57559 57571 57587 57593 57601 57637 57641 57649 57653 57667 57679 57689 57697 57709 57713 57719 57727 57731 57737 57751 57773 57781 57787 57791 57793 57803 57809 57829 57839 57847 57853 57859 57881 57899 57901 57917 57923 57943 57947 57973 57977 57991 58013 58027 58031 58043 58049 58057 58061 58067 58073 58099 58109 58111 58129 58147 58151 58153 58169 58171 58189 58193 58199 58207 58211 58217 58229 58231 58237 58243 58271 58309 58313 58321 58337 58363 58367 58369 58379 58391 58393 58403 58411 58417 58427 58439 58441 58451 58453 58477 58481 58511 58537 58543 58549 58567 58573 58579 58601 58603 58613 58631 58657 58661 58679 58687 58693 58699 58711 58727 58733 58741 58757 58763 58771 58787 58789 58831 58889 58897 58901 58907 58909 58913 58921 58937 58943 58963 58967 58979 58991 58997 59009 59011 59021 59023 59029 59051 59053 59063 59069 59077 59083 59093 59107 59113 59119 59123 59141 59149 59159 59167 59183 59197 59207 59209 59219 59221 59233 59239 59243 59263 59273 59281 59333 59341 59351 59357 59359 59369 59377 59387 59393 59399 59407 59417 59419 59441 59443 59447 59453 59467 59471 59473 59497 59509 59513 59539 59557 59561 59567 59581 59611 59617 59621 59627 59629 59651 59659 59663 59669 59671 59693 59699 59707 59723 59729 59743 59747 59753 59771 59779 59791 59797 59809 59833 59863 59879 59887 59921 59929 59951 59957 59971 59981 59999 60013 60017 60029 60037 60041 60077 60083 60089 60091 60101 60103 60107 60127 60133 60139 60149 60161 60167 60169 60209 60217 60223 60251 60257 60259 60271 60289 60293 60317 60331 60337 60343 60353 60373 60383 60397 60413 60427 60443 60449 60457 60493 60497 60509 60521 60527 60539 60589 60601 60607 60611 60617 60623 60631 60637 60647 60649 60659 60661 60679 60689 60703 60719 60727 60733 60737 60757 60761 60763 60773 60779 60793 60811 60821 60859 60869 60887 60889 60899 60901 60913 60917 60919 60923 60937 60943 60953 60961 61001 61007 61027 61031 61043 61051 61057 61091 61099 61121 61129 61141 61151 61153 61169 61211 61223 61231 61253 61261 61283 61291 61297 61331 61333 61339 61343 61357 61363 61379 61381 61403 61409 61417 61441 61463 61469 61471 61483 61487 61493 61507 61511 61519 61543 61547 61553 61559 61561 61583 61603 61609 61613 61627 61631 61637 61643 61651 61657 61667 61673 61681 61687 61703 61717 61723 61729 61751 61757 61781 61813 61819 61837 61843 61861 61871 61879 61909 61927 61933 61949 61961 61967 61979 61981 61987 61991 62003 62011 62017 62039 62047 62053 62057 62071 62081 62099 62119 62129 62131 62137 62141 62143 62171 62189 62191 62201 62207 62213 62219 62233 62273 62297 62299 62303 62311 62323 62327 62347 62351 62383 62401 62417 62423 62459 62467 62473 62477 62483 62497 62501 62507 62533 62539 62549 62563 62581 62591 62597 62603 62617 62627 62633 62639 62653 62659 62683 62687 62701 62723 62731 62743 62753 62761 62773 62791 62801 62819 62827 62851 62861 62869 62873 62897 62903 62921 62927 62929 62939 62969 62971 62981 62983 62987 62989 63029 63031 63059 63067 63073 63079 63097 63103 63113 63127 63131 63149 63179 63197 63199 63211 63241 63247 63277 63281 63299 63311 63313 63317 63331 63337 63347 63353 63361 63367 63377 63389 63391 63397 63409 63419 63421 63439 63443 63463 63467 63473 63487 63493 63499 63521 63527 63533 63541 63559 63577 63587 63589 63599 63601 63607 63611 63617 63629 63647 63649 63659 63667 63671 63689 63691 63697 63703 63709 63719 63727 63737 63743 63761 63773 63781 63793 63799 63803 63809 63823 63839 63841 63853 63857 63863 63901 63907 63913 63929 63949 63977 63997 64007 64013 64019 64033 64037 64063 64067 64081 64091 64109 64123 64151 64153 64157 64171 64187 64189 64217 64223 64231 64237 64271 64279 64283 64301 64303 64319 64327 64333 64373 64381 64399 64403 64433 64439 64451 64453 64483 64489 64499 64513 64553 64567 64577 64579 64591 64601 64609 64613 64621 64627 64633 64661 64663 64667 64679 64693 64709 64717 64747 64763 64781 64783 64793 64811 64817 64849 64853 64871 64877 64879 64891 64901 64919 64921 64927 64937 64951 64969 64997 65003 65011 65027 65029 65033 65053 65063 65071 65089 65099 65101 65111 65119 65123 65129 65141 65147 65167 65171 65173 65179 65183 65203 65213 65239 65257 65267 65269 65287 65293 65309 65323 65327 65353 65357 65371 65381 65393 65407 65413 65419 65423 65437 65447 65449 65479 65497 65519 65521 65537 65539 65543 65551 65557 65563 65579 65581 65587 65599 65609 65617 65629 65633 65647 65651 65657 65677 65687 65699 65701 65707 65713 65717 65719 65729 65731 65761 65777 65789 65809 65827 65831 65837 65839 65843 65851 65867 65881 65899 65921 65927 65929 65951 65957 65963 65981 65983 65993 66029 66037 66041 66047 66067 66071 66083 66089 66103 66107 66109 66137 66161 66169 66173 66179 66191 66221 66239 66271 66293 66301 66337 66343 66347 66359 66361 66373 66377 66383 66403 66413 66431 66449 66457 66463 66467 66491 66499 66509 66523 66529 66533 66541 66553 66569 66571 66587 66593 66601 66617 66629 66643 66653 66683 66697 66701 66713 66721 66733 66739 66749 66751 66763 66791 66797 66809 66821 66841 66851 66853 66863 66877 66883 66889 66919 66923 66931 66943 66947 66949 66959 66973 66977 67003 67021 67033 67043 67049 67057 67061 67073 67079 67103 67121 67129 67139 67141 67153 67157 67169 67181 67187 67189 67211 67213 67217 67219 67231 67247 67261 67271 67273 67289 67307 67339 67343 67349 67369 67391 67399 67409 67411 67421 67427 67429 67433 67447 67453 67477 67481 67489 67493 67499 67511 67523 67531 67537 67547 67559 67567 67577 67579 67589 67601 67607 67619 67631 67651 67679 67699 67709 67723 67733 67741 67751 67757 67759 67763 67777 67783 67789 67801 67807 67819 67829 67843 67853 67867 67883 67891 67901 67927 67931 67933 67939 67943 67957 67961 67967 67979 67987 67993 68023 68041 68053 68059 68071 68087 68099 68111 68113 68141 68147 68161 68171 68207 68209 68213 68219 68227 68239 68261 68279 68281 68311 68329 68351 68371 68389 68399 68437 68443 68447 68449 68473 68477 68483 68489 68491 68501 68507 68521 68531 68539 68543 68567 68581 68597 68611 68633 68639 68659 68669 68683 68687 68699 68711 68713 68729 68737 68743 68749 68767 68771 68777 68791 68813 68819 68821 68863 68879 68881 68891 68897 68899 68903 68909 68917 68927 68947 68963 68993 69001 69011 69019 69029 69031 69061 69067 69073 69109 69119 69127 69143 69149 69151 69163 69191 69193 69197 69203 69221 69233 69239 69247 69257 69259 69263 69313 69317 69337 69341 69371 69379 69383 69389 69401 69403 69427 69431 69439 69457 69463 69467 69473 69481 69491 69493 69497 69499 69539 69557 69593 69623 69653 69661 69677 69691 69697 69709 69737 69739 69761 69763 69767 69779 69809 69821 69827 69829 69833 69847 69857 69859 69877 69899 69911 69929 69931 69941 69959 69991 69997 70001 70003 70009 70019 70039 70051 70061 70067 70079 70099 70111 70117 70121 70123 70139 70141 70157 70163 70177 70181 70183 70199 70201 70207 70223 70229 70237 70241 70249 70271 70289 70297 70309 70313 70321 70327 70351 70373 70379 70381 70393 70423 70429 70439 70451 70457 70459 70481 70487 70489 70501 70507 70529 70537 70549 70571 70573 70583 70589 70607 70619 70621 70627 70639 70657 70663 70667 70687 70709 70717 70729 70753 70769 70783 70793 70823 70841 70843 70849 70853 70867 70877 70879 70891 70901 70913 70919 70921 70937 70949 70951 70957 70969 70979 70981 70991 70997 70999 71011 71023 71039 71059 71069 71081 71089 71119 71129 71143 71147 71153 71161 71167 71171 71191 71209 71233 71237 71249 71257 71261 71263 71287 71293 71317 71327 71329 71333 71339 71341 71347 71353 71359 71363 71387 71389 71399 71411 71413 71419 71429 71437 71443 71453 71471 71473 71479 71483 71503 71527 71537 71549 71551 71563 71569 71593 71597 71633 71647 71663 71671 71693 71699 71707 71711 71713 71719 71741 71761 71777 71789 71807 71809 71821 71837 71843 71849 71861 71867 71879 71881 71887 71899 71909 71917 71933 71941 71947 71963 71971 71983 71987 71993 71999 72019 72031 72043 72047 72053 72073 72077 72089 72091 72101 72103 72109 72139 72161 72167 72169 72173 72211 72221 72223 72227 72229 72251 72253 72269 72271 72277 72287 72307 72313 72337 72341 72353 72367 72379 72383 72421 72431 72461 72467 72469 72481 72493 72497 72503 72533 72547 72551 72559 72577 72613 72617 72623 72643 72647 72649 72661 72671 72673 72679 72689 72701 72707 72719 72727 72733 72739 72763 72767 72797 72817 72823 72859 72869 72871 72883 72889 72893 72901 72907 72911 72923 72931 72937 72949 72953 72959 72973 72977 72997 73009 73013 73019 73037 73039 73043 73061 73063 73079 73091 73121 73127 73133 73141 73181 73189 73237 73243 73259 73277 73291 73303 73309 73327 73331 73351 73361 73363 73369 73379 73387 73417 73421 73433 73453 73459 73471 73477 73483 73517 73523 73529 73547 73553 73561 73571 73583 73589 73597 73607 73609 73613 73637 73643 73651 73673 73679 73681 73693 73699 73709 73721 73727 73751 73757 73771 73783 73819 73823 73847 73849 73859 73867 73877 73883 73897 73907 73939 73943 73951 73961 73973 73999 74017 74021 74027 74047 74051 74071 74077 74093 74099 74101 74131 74143 74149 74159 74161 74167 74177 74189 74197 74201 74203 74209 74219 74231 74257 74279 74287 74293 74297 74311 74317 74323 74353 74357 74363 74377 74381 74383 74411 74413 74419 74441 74449 74453 74471 74489 74507 74509 74521 74527 74531 74551 74561 74567 74573 74587 74597 74609 74611 74623 74653 74687 74699 74707 74713 74717 74719 74729 74731 74747 74759 74761 74771 74779 74797 74821 74827 74831 74843 74857 74861 74869 74873 74887 74891 74897 74903 74923 74929 74933 74941 74959 75011 75013 75017 75029 75037 75041 75079 75083 75109 75133 75149 75161 75167 75169 75181 75193 75209 75211 75217 75223 75227 75239 75253 75269 75277 75289 75307 75323 75329 75337 75347 75353 75367 75377 75389 75391 75401 75403 75407 75431 75437 75479 75503 75511 75521 75527 75533 75539 75541 75553 75557 75571 75577 75583 75611 75617 75619 75629 75641 75653 75659 75679 75683 75689 75703 75707 75709 75721 75731 75743 75767 75773 75781 75787 75793 75797 75821 75833 75853 75869 75883 75913 75931 75937 75941 75967 75979 75983 75989 75991 75997 76001 76003 76031 76039 76079 76081 76091 76099 76103 76123 76129 76147 76157 76159 76163 76207 76213 76231 76243 76249 76253 76259 76261 76283 76289 76303 76333 76343 76367 76369 76379 76387 76403 76421 76423 76441 76463 76471 76481 76487 76493 76507 76511 76519 76537 76541 76543 76561 76579 76597 76603 76607 76631 76649 76651 76667 76673 76679 76697 76717 76733 76753 76757 76771 76777 76781 76801 76819 76829 76831 76837 76847 76871 76873 76883 76907 76913 76919 76943 76949 76961 76963 76991 77003 77017 77023 77029 77041 77047 77069 77081 77093 77101 77137 77141 77153 77167 77171 77191 77201 77213 77237 77239 77243 77249 77261 77263 77267 77269 77279 77291 77317 77323 77339 77347 77351 77359 77369 77377 77383 77417 77419 77431 77447 77471 77477 77479 77489 77491 77509 77513 77521 77527 77543 77549 77551 77557 77563 77569 77573 77587 77591 77611 77617 77621 77641 77647 77659 77681 77687 77689 77699 77711 77713 77719 77723 77731 77743 77747 77761 77773 77783 77797 77801 77813 77839 77849 77863 77867 77893 77899 77929 77933 77951 77969 77977 77983 77999 78007 78017 78031 78041 78049 78059 78079 78101 78121 78137 78139 78157 78163 78167 78173 78179 78191 78193 78203 78229 78233 78241 78259 78277 78283 78301 78307 78311 78317 78341 78347 78367 78401 78427 78437 78439 78467 78479 78487 78497 78509 78511 78517 78539 78541 78553 78569 78571 78577 78583 78593 78607 78623 78643 78649 78653 78691 78697 78707 78713 78721 78737 78779 78781 78787 78791 78797 78803 78809 78823 78839 78853 78857 78877 78887 78889 78893 78901 78919 78929 78941 78977 78979 78989 79031 79039 79043 79063 79087 79103 79111 79133 79139 79147 79151 79153 79159 79181 79187 79193 79201 79229 79231 79241 79259 79273 79279 79283 79301 79309 79319 79333 79337 79349 79357 79367 79379 79393 79397 79399 79411 79423 79427 79433 79451 79481 79493 79531 79537 79549 79559 79561 79579 79589 79601 79609 79613 79621 79627 79631 79633 79657 79669 79687 79691 79693 79697 79699 79757 79769 79777 79801 79811 79813 79817 79823 79829 79841 79843 79847 79861 79867 79873 79889 79901 79903 79907 79939 79943 79967 79973 79979 79987 79997 79999 80021 80039 80051 80071 80077 80107 80111 80141 80147 80149 80153 80167 80173 80177 80191 80207 80209 80221 80231 80233 80239 80251 80263 80273 80279 80287 80309 80317 80329 80341 80347 80363 80369 80387 80407 80429 80447 80449 80471 80473 80489 80491 80513 80527 80537 80557 80567 80599 80603 80611 80621 80627 80629 80651 80657 80669 80671 80677 80681 80683 80687 80701 80713 80737 80747 80749 80761 80777 80779 80783 80789 80803 80809 80819 80831 80833 80849 80863 80897 80909 80911 80917 80923 80929 80933 80953 80963 80989 81001 81013 81017 81019 81023 81031 81041 81043 81047 81049 81071 81077 81083 81097 81101 81119 81131 81157 81163 81173 81181 81197 81199 81203 81223 81233 81239 81281 81283 81293 81299 81307 81331 81343 81349 81353 81359 81371 81373 81401 81409 81421 81439 81457 81463 81509 81517 81527 81533 81547 81551 81553 81559 81563 81569 81611 81619 81629 81637 81647 81649 81667 81671 81677 81689 81701 81703 81707 81727 81737 81749 81761 81769 81773 81799 81817 81839 81847 81853 81869 81883 81899 81901 81919 81929 81931 81937 81943 81953 81967 81971 81973 82003 82007 82009 82013 82021 82031 82037 82039 82051 82067 82073 82129 82139 82141 82153 82163 82171 82183 82189 82193 82207 82217 82219 82223 82231 82237 82241 82261 82267 82279 82301 82307 82339 82349 82351 82361 82373 82387 82393 82421 82457 82463 82469 82471 82483 82487 82493 82499 82507 82529 82531 82549 82559 82561 82567 82571 82591 82601 82609 82613 82619 82633 82651 82657 82699 82721 82723 82727 82729 82757 82759 82763 82781 82787 82793 82799 82811 82813 82837 82847 82883 82889 82891 82903 82913 82939 82963 82981 82997 83003 83009 83023 83047 83059 83063 83071 83077 83089 83093 83101 83117 83137 83177 83203 83207 83219 83221 83227 83231 83233 83243 83257 83267 83269 83273 83299 83311 83339 83341 83357 83383 83389 83399 83401 83407 83417 83423 83431 83437 83443 83449 83459 83471 83477 83497 83537 83557 83561 83563 83579 83591 83597 83609 83617 83621 83639 83641 83653 83663 83689 83701 83717 83719 83737 83761 83773 83777 83791 83813 83833 83843 83857 83869 83873 83891 83903 83911 83921 83933 83939 83969 83983 83987 84011 84017 84047 84053 84059 84061 84067 84089 84121 84127 84131 84137 84143 84163 84179 84181 84191 84199 84211 84221 84223 84229 84239 84247 84263 84299 84307 84313 84317 84319 84347 84349 84377 84389 84391 84401 84407 84421 84431 84437 84443 84449 84457 84463 84467 84481 84499 84503 84509 84521 84523 84533 84551 84559 84589 84629 84631 84649 84653 84659 84673 84691 84697 84701 84713 84719 84731 84737 84751 84761 84787 84793 84809 84811 84827 84857 84859 84869 84871 84913 84919 84947 84961 84967 84977 84979 84991 85009 85021 85027 85037 85049 85061 85081 85087 85091 85093 85103 85109 85121 85133 85147 85159 85193 85199 85201 85213 85223 85229 85237 85243 85247 85259 85297 85303 85313 85331 85333 85361 85363 85369 85381 85411 85427 85429 85439 85447 85451 85453 85469 85487 85513 85517 85523 85531 85549 85571 85577 85597 85601 85607 85619 85621 85627 85639 85643 85661 85667 85669 85691 85703 85711 85717 85733 85751 85781 85793 85817 85819 85829 85831 85837 85843 85847 85853 85889 85903 85909 85931 85933 85991 85999 86011 86017 86027 86029 86069 86077 86083 86111 86113 86117 86131 86137 86143 86161 86171 86179 86183 86197 86201 86209 86239 86243 86249 86257 86263 86269 86287 86291 86293 86297 86311 86323 86341 86351 86353 86357 86369 86371 86381 86389 86399 86413 86423 86441 86453 86461 86467 86477 86491 86501 86509 86531 86533 86539 86561 86573 86579 86587 86599 86627 86629 86677 86689 86693 86711 86719 86729 86743 86753 86767 86771 86783 86813 86837 86843 86851 86857 86861 86869 86923 86927 86929 86939 86951 86959 86969 86981 86993 87011 87013 87037 87041 87049 87071 87083 87103 87107 87119 87121 87133 87149 87151 87179 87181 87187 87211 87221 87223 87251 87253 87257 87277 87281 87293 87299 87313 87317 87323 87337 87359 87383 87403 87407 87421 87427 87433 87443 87473 87481 87491 87509 87511 87517 87523 87539 87541 87547 87553 87557 87559 87583 87587 87589 87613 87623 87629 87631 87641 87643 87649 87671 87679 87683 87691 87697 87701 87719 87721 87739 87743 87751 87767 87793 87797 87803 87811 87833 87853 87869 87877 87881 87887 87911 87917 87931 87943 87959 87961 87973 87977 87991 88001 88003 88007 88019 88037 88069 88079 88093 88117 88129 88169 88177 88211 88223 88237 88241 88259 88261 88289 88301 88321 88327 88337 88339 88379 88397 88411 88423 88427 88463 88469 88471 88493 88499 88513 88523 88547 88589 88591 88607 88609 88643 88651 88657 88661 88663 88667 88681 88721 88729 88741 88747 88771 88789 88793 88799 88801 88807 88811 88813 88817 88819 88843 88853 88861 88867 88873 88883 88897 88903 88919 88937 88951 88969 88993 88997 89003 89009 89017 89021 89041 89051 89057 89069 89071 89083 89087 89101 89107 89113 89119 89123 89137 89153 89189 89203 89209 89213 89227 89231 89237 89261 89269 89273 89293 89303 89317 89329 89363 89371 89381 89387 89393 89399 89413 89417 89431 89443 89449 89459 89477 89491 89501 89513 89519 89521 89527 89533 89561 89563 89567 89591 89597 89599 89603 89611 89627 89633 89653 89657 89659 89669 89671 89681 89689 89753 89759 89767 89779 89783 89797 89809 89819 89821 89833 89839 89849 89867 89891 89897 89899 89909 89917 89923 89939 89959 89963 89977 89983 89989 90001 90007 90011 90017 90019 90023 90031 90053 90059 90067 90071 90073 90089 90107 90121 90127 90149 90163 90173 90187 90191 90197 90199 90203 90217 90227 90239 90247 90263 90271 90281 90289 90313 90353 90359 90371 90373 90379 90397 90401 90403 90407 90437 90439 90469 90473 90481 90499 90511 90523 90527 90529 90533 90547 90583 90599 90617 90619 90631 90641 90647 90659 90677 90679 90697 90703 90709 90731 90749 90787 90793 90803 90821 90823 90833 90841 90847 90863 90887 90901 90907 90911 90917 90931 90947 90971 90977 90989 90997 91009 91019 91033 91079 91081 91097 91099 91121 91127 91129 91139 91141 91151 91153 91159 91163 91183 91193 91199 91229 91237 91243 91249 91253 91283 91291 91297 91303 91309 91331 91367 91369 91373 91381 91387 91393 91397 91411 91423 91433 91453 91457 91459 91463 91493 91499 91513 91529 91541 91571 91573 91577 91583 91591 91621 91631 91639 91673 91691 91703 91711 91733 91753 91757 91771 91781 91801 91807 91811 91813 91823 91837 91841 91867 91873 91909 91921 91939 91943 91951 91957 91961 91967 91969 91997 92003 92009 92033 92041 92051 92077 92083 92107 92111 92119 92143 92153 92173 92177 92179 92189 92203 92219 92221 92227 92233 92237 92243 92251 92269 92297 92311 92317 92333 92347 92353 92357 92363 92369 92377 92381 92383 92387 92399 92401 92413 92419 92431 92459 92461 92467 92479 92489 92503 92507 92551 92557 92567 92569 92581 92593 92623 92627 92639 92641 92647 92657 92669 92671 92681 92683 92693 92699 92707 92717 92723 92737 92753 92761 92767 92779 92789 92791 92801 92809 92821 92831 92849 92857 92861 92863 92867 92893 92899 92921 92927 92941 92951 92957 92959 92987 92993 93001 93047 93053 93059 93077 93083 93089 93097 93103 93113 93131 93133 93139 93151 93169 93179 93187 93199 93229 93239 93241 93251 93253 93257 93263 93281 93283 93287 93307 93319 93323 93329 93337 93371 93377 93383 93407 93419 93427 93463 93479 93481 93487 93491 93493 93497 93503 93523 93529 93553 93557 93559 93563 93581 93601 93607 93629 93637 93683 93701 93703 93719 93739 93761 93763 93787 93809 93811 93827 93851 93871 93887 93889 93893 93901 93911 93913 93923 93937 93941 93949 93967 93971 93979 93983 93997 94007 94009 94033 94049 94057 94063 94079 94099 94109 94111 94117 94121 94151 94153 94169 94201 94207 94219 94229 94253 94261 94273 94291 94307 94309 94321 94327 94331 94343 94349 94351 94379 94397 94399 94421 94427 94433 94439 94441 94447 94463 94477 94483 94513 94529 94531 94541 94543 94547 94559 94561 94573 94583 94597 94603 94613 94621 94649 94651 94687 94693 94709 94723 94727 94747 94771 94777 94781 94789 94793 94811 94819 94823 94837 94841 94847 94849 94873 94889 94903 94907 94933 94949 94951 94961 94993 94999 95003 95009 95021 95027 95063 95071 95083 95087 95089 95093 95101 95107 95111 95131 95143 95153 95177 95189 95191 95203 95213 95219 95231 95233 95239 95257 95261 95267 95273 95279 95287 95311 95317 95327 95339 95369 95383 95393 95401 95413 95419 95429 95441 95443 95461 95467 95471 95479 95483 95507 95527 95531 95539 95549 95561 95569 95581 95597 95603 95617 95621 95629 95633 95651 95701 95707 95713 95717 95723 95731 95737 95747 95773 95783 95789 95791 95801 95803 95813 95819 95857 95869 95873 95881 95891 95911 95917 95923 95929 95947 95957 95959 95971 95987 95989 96001 96013 96017 96043 96053 96059 96079 96097 96137 96149 96157 96167 96179 96181 96199 96211 96221 96223 96233 96259 96263 96269 96281 96289 96293 96323 96329 96331 96337 96353 96377 96401 96419 96431 96443 96451 96457 96461 96469 96479 96487 96493 96497 96517 96527 96553 96557 96581 96587 96589 96601 96643 96661 96667 96671 96697 96703 96731 96737 96739 96749 96757 96763 96769 96779 96787 96797 96799 96821 96823 96827 96847 96851 96857 96893 96907 96911 96931 96953 96959 96973 96979 96989 96997 97001 97003 97007 97021 97039 97073 97081 97103 97117 97127 97151 97157 97159 97169 97171 97177 97187 97213 97231 97241 97259 97283 97301 97303 97327 97367 97369 97373 97379 97381 97387 97397 97423 97429 97441 97453 97459 97463 97499 97501 97511 97523 97547 97549 97553 97561 97571 97577 97579 97583 97607 97609 97613 97649 97651 97673 97687 97711 97729 97771 97777 97787 97789 97813 97829 97841 97843 97847 97849 97859 97861 97871 97879 97883 97919 97927 97931 97943 97961 97967 97973 97987 98009 98011 98017 98041 98047 98057 98081 98101 98123 98129 98143 98179 98207 98213 98221 98227 98251 98257 98269 98297 98299 98317 98321 98323 98327 98347 98369 98377 98387 98389 98407 98411 98419 98429 98443 98453 98459 98467 98473 98479 98491 98507 98519 98533 98543 98561 98563 98573 98597 98621 98627 98639 98641 98663 98669 98689 98711 98713 98717 98729 98731 98737 98773 98779 98801 98807 98809 98837 98849 98867 98869 98873 98887 98893 98897 98899 98909 98911 98927 98929 98939 98947 98953 98963 98981 98993 98999 99013 99017 99023 99041 99053 99079 99083 99089 99103 99109 99119 99131 99133 99137 99139 99149 99173 99181 99191 99223 99233 99241 99251 99257 99259 99277 99289 99317 99347 99349 99367 99371 99377 99391 99397 99401 99409 99431 99439 99469 99487 99497 99523 99527 99529 99551 99559 99563 99571 99577 99581 99607 99611 99623 99643 99661 99667 99679 99689 99707 99709 99713 99719 99721 99733 99761 99767 99787 99793 99809 99817 99823 99829 99833 99839 99859 99871 99877 99881 99901 99907 99923 99929 99961 99971 99989 99991 100003 100019 100043 100049 100057 100069 100103 100109 100129 100151 100153 100169 100183 100189 100193 100207 100213 100237 100267 100271 100279 100291 100297 100313 100333 100343 100357 100361 100363 100379 100391 100393 100403 100411 100417 100447 100459 100469 100483 100493 100501 100511 100517 100519 100523 100537 100547 100549 100559 100591 100609 100613 100621 100649 100669 100673 100693 100699 100703 100733 100741 100747 100769 100787 100799 100801 100811 100823 100829 100847 100853 100907 100913 100927 100931 100937 100943 100957 100981 100987 100999 101009 101021 101027 101051 101063 101081 101089 101107 101111 101113 101117 101119 101141 101149 101159 101161 101173 101183 101197 101203 101207 101209 101221 101267 101273 101279 101281 101287 101293 101323 101333 101341 101347 101359 101363 101377 101383 101399 101411 101419 101429 101449 101467 101477 101483 101489 101501 101503 101513 101527 101531 101533 101537 101561 101573 101581 101599 101603 101611 101627 101641 101653 101663 101681 101693 101701 101719 101723 101737 101741 101747 101749 101771 101789 101797 101807 101833 101837 101839 101863 101869 101873 101879 101891 101917 101921 101929 101939 101957 101963 101977 101987 101999 102001 102013 102019 102023 102031 102043 102059 102061 102071 102077 102079 102101 102103 102107 102121 102139 102149 102161 102181 102191 102197 102199 102203 102217 102229 102233 102241 102251 102253 102259 102293 102299 102301 102317 102329 102337 102359 102367 102397 102407 102409 102433 102437 102451 102461 102481 102497 102499 102503 102523 102533 102539 102547 102551 102559 102563 102587 102593 102607 102611 102643 102647 102653 102667 102673 102677 102679 102701 102761 102763 102769 102793 102797 102811 102829 102841 102859 102871 102877 102881 102911 102913 102929 102931 102953 102967 102983 103001 103007 103043 103049 103067 103069 103079 103087 103091 103093 103099 103123 103141 103171 103177 103183 103217 103231 103237 103289 103291 103307 103319 103333 103349 103357 103387 103391 103393 103399 103409 103421 103423 103451 103457 103471 103483 103511 103529 103549 103553 103561 103567 103573 103577 103583 103591 103613 103619 103643 103651 103657 103669 103681 103687 103699 103703 103723 103769 103787 103801 103811 103813 103837 103841 103843 103867 103889 103903 103913 103919 103951 103963 103967 103969 103979 103981 103991 103993 103997 104003 104009 104021 104033 104047 104053 104059 104087 104089 104107 104113 104119 104123 104147 104149 104161 104173 104179 104183 104207 104231 104233 104239 104243 104281 104287 104297 104309 104311 104323 104327 104347 104369 104381 104383 104393 104399 104417 104459 104471 104473 104479 104491 104513 104527 104537 104543 104549 104551 104561 104579 104593 104597 104623 104639 104651 104659 104677 104681 104683 104693 104701 104707 104711 104717 104723 104729\n","id":2339,"permalink":"https://freshrimpsushi.github.io/jp/posts/2339/","tags":null,"title":"1万番目までの素数点以下のリスト"},{"categories":"줄리아","contents":"コード 最初に、生えび寿司レストランには詳しい説明が含まれているが、ジュリアは並列処理をどれだけ容易にできるかを強調するために、わざと説明を省略したいと思っている。\nusing Base.Threads\rfor i in 1:10\rprintln(i^2)\rend 上のループを並列処理したい場合は、for文の前に@threadsをつけるだけでいい。\n@threads for i in 1:10\rprintln(i^2)\rend でも、一言だけアドバイスをすると、並列処理をしても全てが速くなるわけではないということだ。並列処理をうまく使えば非常に高いパフォーマンスを出すことができるが、コードの書きやすさが向上したからといって最適化も簡単になるわけではない。時間を測ってみるなどして、実行時間に注意しよう。\n環境 OS: Windows julia: v1.5.0 ","id":1474,"permalink":"https://freshrimpsushi.github.io/jp/posts/1474/","tags":null,"title":"ジュリアでの並列処理の方法"},{"categories":"양자역학","contents":"ベクトルの一般化 線形代数学を学んでいない理科生にとって、ベクトルは大きさと方向を持つ物理量であり、3次元空間の点を意味し、一般に $\\vec{x} = (x_{1}, x_{2}, x_{3})$ のように表される。この定義で古典力学や電磁気学を学ぶ上では大きな問題はないだろう。しかし、量子力学ではフーリエ解析、関数の内積などの概念が登場するため、ベクトルの一般化された定義を知らないと学習に大きな困難を経験する可能性がある。\n線形代数学において、ベクトルとは我々が直感的に考えるそのベクトルを抽象化したものである。3次元空間のベクトルと同じ性質を持つものを全てベクトルと呼び、ベクトルを集めた集合をベクトル空間と呼ぶ。その性質とは、我々が3次元空間の点を考えた時に当然満たされるべき性質のことである。例えば\nベクトルとベクトルを加えたものもベクトルである。 ベクトルに定数を乗じたものもベクトルである。 などがそれにあたる。その結果、3次元空間の点はベクトルになり、3次元空間はベクトル空間になる。以下には量子力学で最も重要な二つの例を紹介する。行列と関数もベクトルである。\n例 行列 サイズが $m \\times n$ の行列を集めた集合を考えてみよう。これらを加えても依然として $m \\times n$ 行列であり、何らかの定数を乗じても依然として $m \\times n$ 行列なので、この集合はベクトル空間になり、各行列はベクトルになる。\n実際、$\\mathbf{x} = (x_{1}, x_{2}, x_{3})$ のように組み合わせで表記することと $\\mathbf{x} = \\begin{bmatrix} x_{1} \u0026amp; x_{2} \u0026amp; x_{3} \\end{bmatrix}$ のように $1 \\times 3$ 行列で表記することに本質的な違いがないことを思い出してみると、行列がベクトルであるということがより理解しやすくなるだろう。\n関数 連続関数の集合を考えてみよう。$f$ と $g$ が連続関数であれば、これらを加えた $f+g$ も依然として連続関数である。また、任意の定数を乗じた $cf$ も依然として連続関数である。したがって、連続関数の集合はベクトル空間になり、各連続関数はベクトルになる。\n実際、関数値が3次元ベクトルであるベクトル関数の場合、以下のように記述されることを思い出してみよう。\n$$ f(x,y,z) = (xy, yz, z^{2}) $$\n内積の一般化 内積はベクトルを扱う際に非常に便利に使われる演算である。ベクトルという概念を一般化したように、内積の概念も一般化してみよう。まず一般化された内積の表記では、点 $\\cdot$ の代わりに二重山括弧 $\\left\\langle \\ ,\\ \\right\\rangle$ を使用する。$\\mathbf{x} = \\left( x_{1}, x_{2}, x_{3} \\right)$、$\\mathbf{y}=\\left( y_{1}, y_{2}, y_{3} \\right)$ とすると、以下のように表記される。\n$$ \\mathbf{x} \\cdot \\mathbf{y} = x_{1}y_{1} + x_{2}y_{2} + x_{3}y_{3} = \\left\\langle \\mathbf{x}, \\mathbf{y} \\right\\rangle $$\n量子力学では、中にコンマの代わりに線 $|$ を使用する。\n$$ \\mathbf{x} \\cdot \\mathbf{y} = \\braket{\\mathbf{x} \\vert \\mathbf{y} } $$\nこれをディラック記法という。ベクトルを一般化する際の核心は、「私たちがベクトルだと思うものが満たすべき性質」を満たすならば、それが何であれベクトルと呼ぶ点にある。内積の一般化でも同様に、「各成分を掛け合わせてすべて加算する」というコンセプトをそのまま保持する。どのようなベクトル空間を扱うかによって、内積の定義は以下のように異なる。\n例 行列 二つの行列 $A = \\begin{pmatrix} a_{11} \u0026amp; a_{12} \\\\ a_{21} \u0026amp; a_{22} \\end{pmatrix}, B = \\begin{pmatrix} b_{11} \u0026amp; b_{12} \\\\ b_{21} \u0026amp; b_{22} \\end{pmatrix}$ があるとする。これらの内積は、3次元ベクトルの内積と同じように「各成分の掛け算の和」として定義される。\n$$ \\braket{ A \\vert B } = a_{11}b_{11} + a_{12}b_{12} + a_{21}b_{21} + a_{22}b_{22} $$\n関数 上で述べたように、関数もベクトルであるので、二つの関数の内積も定義できる。関数の内積は以下のように定積分で定義される。\n$$ \\braket{\\psi \\vert \\phi} = \\int \\psi^{\\ast}(x) \\phi (x) dx $$\nここで、$\\psi^{\\ast}$ は $\\psi$ の複素共役を意味する。ただし、表記法にあいまいさがあるので注意しよう。関数の内積をなぜこのように定義するのかについては、\u0026lsquo;関数の内積を定積分で定義する理由\u0026rsquo;に詳しく説明されているので参照しよう。\n波動関数 量子力学において波動関数は、位置と時間に応じた粒子の状態を表現する関数であり、以下のように指数関数で表現される。\n$$ \\psi (x,t) = e^{i(kx + \\omega t)} $$\n主に $\\psi$ と $\\phi$ で表記され、それぞれ [プサイ]、[ファイ] と読む。$k$ は波数wave numberであり、運動量との関係 $p = \\hbar k$ を満たす。ここで $\\hbar$ は定数であるので、量子力学\nにおいては $k$ を運動量と同じと理解しても構わない。$\\omega$ は角振動数angular frequencyであり、エネルギーとの関係 $E = \\hbar \\omega$ を満たす。\nヒルベルト空間 ヒルベルト空間の厳密な定義は完備内積空間である。その数学的意味を理解しているともちろん良いが、物理学部の学生にとっては必須ではない。重要な点は、非常に良い性質を持つ集合にヒルベルト空間という名前を付けることと、波動関数を集めた集合がヒルベルト空間になるという点である。したがって、様々な良い数学的ツールを使って波動関数を扱うことができる。\n","id":1509,"permalink":"https://freshrimpsushi.github.io/jp/posts/1509/","tags":null,"title":"量子力学でベクトル、内積、波動関数, ヒルベルト空間"},{"categories":"행렬대수","contents":"定義 線形変換$A \\in \\mathbb{R}^{m \\times m}$について、$m$次元の単位球$N := \\left\\{ \\mathbb{x} \\in \\mathbb{R}^{m} : \\left\\| \\mathbb{x} \\right\\|_{2} = 1 \\right\\}$のイメージ$AN$を楕円体と言う。$A$の固有値$\\sigma_{1}^{2} \u0026gt; \\cdots \\ge \\sigma_{m}^{2} \\ge 0$とそれに対応する単位固有ベクトル$u_{1} , \\cdots , u_{m}$について、$\\sigma_{i} u_{i}$を楕円体の軸Axisと言う。\n説明 $m$次元の単位球は、中心が$\\mathbb{0} \\in \\mathbb{R}^{m}$で、半径が$1$の点の集まりで、$m=2$の時は、我々がよく知っている単位円になる。\n楕円体は、楕円形体あるいは超楕円Hyperellipseとも呼ばれる図形で、楕円面や楕円球面などの名前が間違っているというよりは、そういう緩和を意味に置かずに、読んでいる文脈に従ってその定義を把握する方がいい。どこかでは中までぎっしり詰まったものを楕円体と呼び、またどこかではその皮だけを楕円体と呼ぶこともある。\n幾何学 線形変換に十分慣れていれば、これがなぜ楕円を多次元に拡張したものと呼ばれるのか容易に理解できるだろう。直感的な例としては、単位円の全ての点に$A = \\begin{bmatrix} 2 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{bmatrix}$を取って横に長く伸ばした姿を想像するといい。これは円の方程式$N : x^{2} + y^{2} = 1$の解が線形変換$A$によって$\\displaystyle AN : {{ x^{2} } \\over { 2 }} + y^{2} = 1$の解に変わったものだ。この時、$A$の固有値は$\\sqrt{2}^{2} , \\sqrt{1}^{2}$であるから、楕円体$AN$の軸は当然$\\sqrt{2}(1,0)$と$\\sqrt{1}(0,1)$だ。\n線形代数 楕円体を語る時に、固有値を$\\sigma_{i}^{2}$と特に書くことは、楕円体が特異値分解と密接な関係があるためだ。特異値分解は数値的線形代数学において、$A \\in \\mathbb{R}^{m \\times n}$に対して\n$$ A v_{i} = \\sigma_{i} u_{i} $$\nを満たす何らかの$\\sigma_{i}\u0026gt;0$と$v_{i} \\in \\mathbb{R}^{n}$、$u_{i} \\in \\mathbb{R}^{m}$を見つける方法だ。特異値分解の存在性の証明において、$\\sigma_{i}^{2}$は$A^{T}A$の固有値であり、単位固有ベクトル$u_{1} , \\cdots , u_{m}$は相互独立だ。ここから$\\sigma_{i} u_{i}$を軸と呼ぶことは自然な定義になる。\n一般化 線形代数の説明から分かるように、本来楕円体は$A \\in \\mathbb{R}^{m \\times n}$に対しても一般化可能だ。しかし、読む立場からすると特異値と固有値の関係を理解するのも難しく、幾何学的な意味が弱くなってしまうので、仕方なく$A \\in \\mathbb{R}^{m \\times m}$についての定義を紹介した。この抽象的な定義を理解することに成功したら、$A$のランク$r = \\dim C (A)$について$\\sigma_{r+1} = \\cdots = \\sigma_{m} = 0$としてより一般的な楕円体の定義を受け入れることができるだろう。ただし、もはや$\\sigma_{i}^{2}$を$A$の固有値と言うことはできなくなり、特異値分解の話を出さない場合、「ある正の数$\\sigma_{i}\u0026gt;0$」と呼ぶしかないだろう。\n","id":1471,"permalink":"https://freshrimpsushi.github.io/jp/posts/1471/","tags":null,"title":"楕円の一般化：楕円体"},{"categories":"상미분방정식","contents":"定義1 $\\nu \\in \\mathbb{R}$に対して、以下の形の微分方程式を$\\nu$次のベッセル方程式という。\n$$ \\begin{align*} \u0026amp;\u0026amp; x^{2} y^{\\prime \\prime} +xy^{\\prime}+(x^{2}-\\nu^{2})y \u0026amp;= 0 \\\\ \\text{or} \u0026amp;\u0026amp; y^{\\prime \\prime}+\\frac{1}{x} y^{\\prime} + \\left( 1-\\frac{\\nu^{2}}{x^{2}} \\right)y \u0026amp;= 0 \\end{align*} $$\n説明 ベッセル方程式は、球座標系で波動方程式を解くときに出現する微分方程式だ。係数は定数ではなく、独立変数$x$に依存する。$x=0$の時、以下の式を満たすので、$x=0$は正則特異点である。\n$$ \\lim \\limits_{x\\rightarrow 0} x \\frac{x}{x^{2}}=1\u0026lt;\\infty,\\quad \\lim\\limits_{x\\rightarrow 0}x^{2}\\frac{x^{2}-\\nu^{2}}{x^{2}}=-\\nu^{2} \u0026lt; \\infty $$\nしたがって、フロベニウス法を用いて解を求めることができ、級数解は次のようになる。\n$$ \\begin{align*} J_{\\nu}(x) \u0026amp;= \\sum \\limits_{n=0}^{\\infty} \\frac{(-1)^{n} }{\\Gamma (n+1) \\Gamma (n+\\nu+1)} \\left(\\frac{x}{2} \\right)^{2n+\\nu} \\\\ J_{-\\nu}(x) \u0026amp; =\\sum \\limits_{n=0}^{\\infty}\\frac{(-1)^{n}}{\\Gamma (n+1)\\Gamma (n-\\nu+1)} \\left( \\frac{x}{2} \\right)^{2n-\\nu} \\end{align*} $$\nこれを**$\\nu$次第1種ベッセル関数**と呼ぶ。ここで、$\\Gamma (x)$はガンマ関数である。二つの級数の次数を見ると、互いに線型独立であることが分かる。したがって、$\\nu$次のベッセル方程式の一般解は次のようになる。\n$$ y(x)=AJ_{\\nu}(x)+BJ_{-\\nu}(x) $$\nただし、これは$\\nu$が整数でない場合にのみ成り立つ。$\\nu$が整数の場合、$J_{\\nu}$と$J_{-\\nu}$が独立でないため、ノイマン関数と呼ばれる第二の解を求める必要がある。\n解法 ベッセル方程式の解を次のような冪級数と仮定する。\n$$ \\begin{equation} y=\\sum \\limits_{n=0}^{\\infty}a_{n}x^{n+r}=x^{r}(a_{0}+a_{1}x+a_{2}x^{2}+\\cdots)=a_{0}x^{r}+a_{1}x^{r+1}+a_{2}x^{r+2}+\\cdots \\label{1} \\end{equation} $$\nまず、ベッセル方程式の形を少し変えてみよう。$x(xy^{\\prime})^{\\prime}=x^{2}y^{\\prime \\prime}+xy^{\\prime}$であるので、ベッセル方程式は次のように表せる。\n$$ x(xy^{\\prime})+(x^{2}-\\nu^{2})y=0 $$\nベッセル方程式に代入するため、$\\eqref{1}$から$x(xy^{\\prime})^{\\prime}, x^{2}y$を求めると次のようになる。\n$$ \\begin{align*} y^{\\prime} \u0026amp;=ra_{0}x^{r-1}+(r+1)a_{1}x^{r}+(r+2)a_{2}x^{r+1}+\\cdots \\\\ xy^{\\prime}\u0026amp;=ra_{0}x^{r}+(r+1)a_{1}x^{r+1}+(r+2)a_{2}x^{r+2}+\\cdots \\\\ (xy^{\\prime})^{\\prime}\u0026amp;=r^{2}a_{0}x^{r-1}+(r+1)^{2}a_{1}x^{r}+(r+2)^{2}a_{2}x^{r+1}+\\cdots \\\\ x(xy^{\\prime})^{\\prime}\u0026amp;=r^{2}a_{0}x^{r}+(r+1)^{2}a_{1}x^{r+1}+(r+2)^{2}a_{2}x^{r+2}+\\cdots \\\\ \u0026amp;= \\sum \\limits_{n=0}^{\\infty} a_{n}(r+n)^{2}x^{n+r} \\end{align*} $$\nこれをベッセル方程式に代入すると以下のようになる。\n$$ \\begin{align*} \u0026amp;\u0026amp;\u0026amp;\\left( r^{2}a_{0}x^{r}+(r+1)^{2}a_{1}x^{r+1}+(r+2)^{2}a_{2}x^{r+2}+\\cdots \\right) +(x^{2}-\\nu^{2})\\left( a_{0}x^{r}+a_{1}x^{r+1}+a_{2}x^{r+2}+\\cdots \\right) \\\\ \\implies\u0026amp;\u0026amp;\u0026amp; \\left( r^{2}a_{0}x^{r}+(r+1)^{2}a_{1}x^{r+1}+(r+2)^{2}a_{2}x^{r+2}+\\cdots \\right) +\\left( a_{0}x^{r+2}+a_{1}x^{r+3}+a_{2}x^{r+4}+\\cdots \\right) \\\\ \u0026amp;\u0026amp;\u0026amp;+ \\left( -\\nu^{2}a_{0}x^{r}-\\nu^{2}a_{1}x^{r+1}-\\nu^{2}a_{2}x^{r+2}+\\cdots \\right) \\end{align*} $$\nこれを$x$の次数に合わせて整理すると\n$$ \\begin{align*} a_{0}(r^{2}-\\nu^{2})x^{r}+a_{1}\\left( (r+1)^{2}-\\nu^{2} \\right)x^{r+1} +\\left(a_{2}(r+2)^{2}-a_{2}\\nu^{2} +a_{0} \\right)x^{r+2}\u0026amp; \\\\ +\\cdots + (a_{n}(r+n)^{2}-a_{n}\\nu^{2}+a_{n-2})x^{n+r}+\\cdots \u0026amp;= 0 \\end{align*} $$\n任意の$x$に対してこの方程式が常に成立するためには、すべての係数が$0$でなければならない。最初の項を見てみよう。\n$$ a_{0}(r^{2}-\\nu^{2})=0 $$\n$a_{0}\\ne 0$なので、$r=\\pm\\nu$である。次の項を見てみよう。\n$$ a_{1}\\left( (r+1)^{2}-\\nu^{2} \\right)=0 $$\n前に$r=\\pm \\nu$という条件を得たので、カッコの中の式は絶対に$0$になり得ない。したがって、$a_{1}=0$である。三番目の項の係数からは、一般的に次のように表される。\n$$ a_{n}(r+n)^{2}-a_{n}\\nu^{2}+a_{n-2}=0 $$\nこれを整理すると\n$$ \\begin{equation} a_{n}=\\frac{-a_{n-2}}{(r+n)^{2}-\\nu^{2}} \\label{2} \\end{equation} $$\n先に得た$a_{1}=0$と上記の条件を合わせると、すべての奇数である$n$に対して、$a_{n}=0$となることが分かる。したがって、$n$が偶数の時の$a_{n}$だけを求めれば良い。\nケース1. $r=\\nu$\nこの場合、$\\eqref{2}$は\n$$ a_{n}=\\frac{- a_{n-2}}{n^{2}+2n\\nu}=\\frac{-a_{n-2}}{n(n+2\\nu)} $$\n私たちは$n$が偶数である場合にのみ関心があるので、$n$を$2n$と表記しよう。すると\n$$ a_{2n}=\\frac{-a_{2n-2}}{2n(2n+2\\nu)}=\\frac{-a_{2n-2}}{2^{2}n(n+\\nu)} $$\nこれで$a_{2}$から順に求めてみると以下のようになる。\n$$ \\begin{align*} a_{2} \u0026amp; =\\frac{-a_{0}}{2^{2}\\cdot 1(\\nu+1)} \\\\ a_{4} \u0026amp;= \\frac{-a_{2}}{2^{2}\\cdot 2(\\nu+2)}=\\frac{a_{0}}{2^{4}\\cdot2\\cdot1(\\nu+1)(\\nu+2)} \\\\ a_{6} \u0026amp;=\\frac{-a_{4}}{2^{2}\\cdot3 (\\nu+3)}=\\frac{-a_{0}}{2^{6}\\cdot3\\cdot2\\cdot1(\\nu+1)(\\nu+2)(\\nu+3)} \\\\ \\vdots \\\\ a_{2n}\u0026amp;=\\frac{(-1)^{n}a_{0}}{2^{2n}n!(\\nu+1)(\\nu+2)\\cdots(\\nu+n)} \\end{align*} $$ ここで、ガンマ関数を利用すると、さらにシンプルに表せる。ガンマ関数には次のような性質がある。\n$$ \\Gamma (\\nu+1)=\\nu\\Gamma (\\nu) \\implies \\dfrac{1}{\\nu} = \\dfrac{\\Gamma (\\nu)}{\\Gamma (\\nu+1)} $$\nこれを利用すると、以下を得る。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\frac{1}{\\nu+1}\u0026amp;=\\frac{\\Gamma (\\nu+1)}{\\Gamma (\\nu+2)}\u0026amp; \\\\ \\implies \u0026amp;\u0026amp; \\frac{1}{(\\nu+1)(\\nu+2)}\u0026amp;=\\frac{\\Gamma (\\nu+1)}{(\\nu+2)\\Gamma (\\nu+2)}=\\frac{\\Gamma (\\nu+1)}{\\Gamma (\\nu+3)} \\\\ \\implies \u0026amp;\u0026amp; \\frac{1}{(\\nu+1)(\\nu+2)(\\nu+3)}\u0026amp;=\\frac{\\Gamma (\\nu+1)}{(\\nu+3)\\Gamma (\\nu+3)} =\\frac{\\Gamma (\\nu+1)}{\\Gamma (\\nu+4)} \\\\ \\implies \u0026amp;\u0026amp; \u0026amp;\\vdots \\\\ \\implies \u0026amp;\u0026amp; \\frac{1}{(\\nu+1)\\cdots(\\nu+n)}\u0026amp;=\\frac{\\Gamma (\\nu+1)}{\\Gamma (\\nu+n+1)} \\end{align*} $$\nこれを先に求めた係数に代入し、階乗もガンマ関数で表すと、各係数は以下のようになる。\n$$ \\begin{align*} a_{2} \u0026amp; =\\frac{-a_{0}\\Gamma (\\nu +1)}{2^{2} 1! \\Gamma (\\nu+2)} \\\\ a_{4} \u0026amp;= \\frac{a_{0} \\Gamma (\\nu+1)}{2^{4} 2!\\Gamma (\\nu+3)} \\\\ a_{6} \u0026amp;=\\frac{-a_{0}\\Gamma (\\nu+1)}{2^{6}3!\\Gamma (\\nu+4)} \\\\ \\vdots \\\\ a_{2n}\u0026amp;=\\frac{(-1)^{n}\\Gamma (\\nu+1)}{2^{2n}\\Gamma (n+1)\\Gamma (\\nu+n+1)}a_{0} \\end{align*} $$\nこれを$\\eqref{1}$に代入すると\n$$ \\begin{align*} y \u0026amp;= \\sum\\limits_{n=0}^{\\infty}a_{2n}x^{2n+\\nu} \\\\ \u0026amp;= \\sum \\limits_{n=0}^{\\infty}\\frac{(-1)^{n}a_{0}\\Gamma (\\nu+1)}{2^{2n}\\Gamma (n+1)\\Gamma (\\nu+n+1)}x^{2n+\\nu} \\end{align*} $$\n形を整理して\n$$ y=\\sum \\limits_{n=0}^{\\infty}\\frac{(-1)^{n}2^{\\nu}a_{0}\\Gamma (\\nu+1)}{\\Gamma (n+1)\\Gamma (n+\\nu+1)} \\left(\\frac{x}{2}\\right)^{2n+\\nu} $$\n$a_{0}=\\frac{1}{2^{\\nu} \\Gamma (\\nu+1)}$とすると\n$$ J_{\\nu}(x)=\\sum \\limits_{n=0}^{\\infty}\\frac{(-1)^{n}}{\\Gamma (n+1)\\Gamma (n+\\nu+1)} \\left(\\frac{x}{2}\\right)^{2n+\\nu} $$\nこれを$\\nu$次の第1種ベッセル関数と呼ぶ。\nケース2. $r=-\\nu$\nこの場合は単純に**ケース1.**の結果で$\\nu$を$-\\nu$に変えたものと同じだ。解法の過程は完全に同じなので、詳細な計算過程と説明なしに主要な結果のみ記載することにする。\n$$ a_{n}=\\frac{- a_{n-2}}{n(n-2\\nu)} $$\n$$ \\begin{align*} a_{2n}\u0026amp;=\\frac{ -a_{2n-2}}{ 2^{2}n(n-\\nu) } \\\\ \u0026amp;=\\frac{(1-)^{n}a_{0}}{2^{2n}n!(1-\\nu)(2-\\nu)\\cdots(n-\\nu)} \\end{align*} $$\n$$ \\frac{1}{(1-\\nu)(2-\\nu)\\cdots(n-\\nu)}=\\frac{\\Gamma (1-\\nu)}{\\Gamma (n-\\nu+1)} $$\n$$ a_{2n}=\\frac{(-1)^{n}\\Gamma (1-\\nu)}{2^{2n}\\Gamma (n+1)\\Gamma (n-\\nu+1)} $$\n$$ \\begin{align*} y \u0026amp;=\\sum \\limits _{n=0} ^{\\infty} a_{2n}x^{2n-\\nu} \\\\ \u0026amp;=\\sum \\limits_{n=0}^{\\infty}\\frac{(-1)^{n}a_{0}\\Gamma (1-\\nu)}{2^{2n}\\Gamma (n+1)\\Gamma (n-\\nu+1)}x^{2n-\\nu} \\\\ \u0026amp;=\\sum \\limits_{n=0}^{\\infty}\\frac{(-1)^{n}2^{-\\nu}a_{0}\\Gamma (1-\\nu)}{\\Gamma (n+1)\\Gamma (n-\\nu+1)} \\left( \\frac{x}{2} \\right)^{2n-\\nu} \\end{align*} $$\n$$ a_{0}=\\frac{2^{\\nu}}{\\Gamma (1-\\nu)} $$\n$$ J_{-\\nu}(x)=\\sum \\limits_{n=0}^{\\infty}\\frac{(-1)^{n}}{\\Gamma (n+1)\\Gamma (n-\\nu+1)} \\left( \\frac{x}{2} \\right)^{2n-\\nu} $$\n■\nメアリー・L・ボアス, 物理科学のための数学的方法(Mathematical Methods in the Physical Sciences, 最新版, 2008), p601-604\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1503,"permalink":"https://freshrimpsushi.github.io/jp/posts/1503/","tags":null,"title":"ベッセル方程式の級数解：第一種ベッセル関数"},{"categories":"수리통계학","contents":"定義1 確率変数$X_{1} , \\cdots , X_{n}$が次を満たすとき、$X_{1} , \\cdots , X_{n}$はペアワイズ独立と言われる。 $$ i \\ne j \\implies X_{i} \\perp X_{j} $$ 連続確率変数$X_{1} , \\cdots , X_{n}$の結合確率密度関数$f$が、それぞれの確率密度関数$f_{1} , \\cdots , f_{n}$に対して次を満たす場合、$X_{1} , \\cdots , X_{n}$は相互独立であると言う。 $$ f(x_{1} , \\cdots , x_{n} ) \\equiv f_{1} (x_{1}) \\cdots f_{n} (x_{n}) $$ 離散確率変数$X_{1} , \\cdots , X_{n}$の結合確率質量関数$p$が、それぞれの確率密度関数$p_{1} , \\cdots , p_{n}$に対して次を満たす場合、$X_{1} , \\cdots , X_{n}$は相互独立であると言う。 $$ p(x_{1} , \\cdots , x_{n} ) \\equiv p_{1} (x_{1}) \\cdots p_{n} (x_{n}) $$ 確率変数$X_{1} , \\cdots , X_{n}$が相互に独立であり、同じ分布を持つとき、iid（独立同分布）と呼ぶ。 説明 ペアワイズ独立の概念はそれ自体が重要であるというよりも、相互独立という望ましい条件を満たさない、より良くない条件というニュアンスを強く持つものである。自然と相互独立であればペアワイズにも独立であるが、その逆は成立しない。これをよく示す反例がベルンスタイン分布である。 iidは相互独立が数学的に扱いやすく、各々が同一という点で、数理統計学において重要な仮定として好まれる。例えば、その分布が$D$である場合、$X_{1} , \\cdots , X_{n}$を分布$D$に従うiid確率変数と言い、次のように表すこともできる。 $$ X_{1} , \\cdots , X_{n} \\overset{\\text{iid}}{\\sim} D $$ 定理 [1] 期待値: $X_{1} , \\cdots , X_{n}$が相互に独立である場合、それぞれに適用されるある関数$u_{1} , \\cdots , u_{n}$について $$ E \\left[ u_{1}(X_{1}) \\cdots u_{n}(X_{n}) \\right] = E \\left[ u_{1}(X_{1}) \\right] \\cdots E \\left[ u_{n}(X_{n}) \\right] $$ [2] モーメント生成関数: $X_{1} , \\cdots , X_{n}$が相互に独立であり、それぞれのモーメント生成関数が$M_{i}(t) \\qquad , -h_{i} \u0026lt; t \u0026lt; h_{i}$である場合、その線形組み合わせ$\\displaystyle T := \\sum_{i=1}^{n} a_{i} X_{i}$のモーメント生成関数は $$ M_{T} (t) = \\prod_{i=1}^{n} M_{i} \\left( a_{i} t \\right) \\qquad , -\\text{min}_{i=1, \\cdots, n} h_{i} \u0026lt; t \u0026lt; \\text{min}_{i=1, \\cdots, n} h_{i} $$ Hogg et al. (2013). Mathematical Statistics の導入 (第7版): p122~125.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1469,"permalink":"https://freshrimpsushi.github.io/jp/posts/1469/","tags":null,"title":"確率変数の独立性とiid"},{"categories":"수리통계학","contents":"定義 1 二つの確率変数 $X_{1}, X_{2}$ の結合確率密度関数 $f$ または確率質量関数 $p$ が、$X_{1}, X_{2}$ の確率密度関数 $f_{1}, f_{2}$ または確率質量関数 $p_{1}, p_{2}$ で以下を満たす場合、$X_{1}, X_{2}$ は独立であると言い、$X_{1} \\perp X_{2}$ と表記される。 $$ f(x_{1} , x_{2} ) \\equiv f_{1}(x_{1})f_{2}(x_{2}) \\\\ p(x_{1} , x_{2} ) \\equiv p_{1}(x_{1})p_{2}(x_{2}) $$\n定理 以下は、連続確率変数について言及されるが、便宜上離散確率変数についても同様である。\n以下は全て等価である。\n[1]: $X_{1} \\perp X_{2}$ [2] 確率密度関数： $$ f (x_{1} , x_{2}) \\equiv f_{1}(x_{1}) f_{2}(x_{2}) $$ [3] 累積分布関数：全ての $(x_{1} ,x_{2}) \\in \\mathbb{R}^{2}$ について $$ F (x_{1} , x_{2}) = F_{1}(x_{1}) F_{2}(x_{2}) $$ [4] 確率：全ての定数 $a\u0026lt;b$ および $c \u0026lt; d$ について $$ P(a \u0026lt; X_{1} \\le b, c \u0026lt; X_{2} \\le d) = P(a \u0026lt; X_{1} \\le b) P ( c \u0026lt; X_{2} \\le d) $$ [5] 期待値：$E \\left[ u (X_{1}) \\right]$ および $E \\left[ u (X_{2}) \\right]$ が存在する場合 $$ E \\left[ u(X_{1}) u(X_{2}) \\right] = E \\left[ u (X_{1}) \\right] E \\left[ u (X_{2}) \\right] $$ [6] モーメント生成関数：結合モーメント生成関数 $M(t_{1} , t_{2})$ が存在する場合 $$ M(t_{1} , t_{2}) = M (t_{1} , 0 ) M( 0, t_{2} ) $$ 説明 上記の等価条件の形から見て分かる通り、独立とは絡み合った（結合）関数を乗算形に分けることができる条件を言う。これは、確率を $$ P(A \\mid B) = {{ P(A B) } \\over { P(B) }} \\overset{\\text{ind}}{\\implies} P(AB) = P(A \\mid B) P(B) = P(A) P(B) $$ のように分けることができるイベントの独立を抽象化したものと見ることができる。独立を直感的に理解することも重要だが、数理統計学を学ぶ上でその数式的な形にもっと注意を払う必要がある。\n参照 測度論で定義される確率変数の独立 Hogg et al. (2013). Introduction to Mathematical Statistcs(第7版): p112.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1461,"permalink":"https://freshrimpsushi.github.io/jp/posts/1461/","tags":null,"title":"数理統計学における確率変数の独立"},{"categories":"최적화이론","contents":"定義 関数$f : \\mathbb{R}^{n} \\to \\mathbb{R}$の関数値を最小にする$x^{ \\ast } = \\argmin_{x} f(x)$を見つける問題を最適化問題Optimization Problemと呼び、その問題を解くアルゴリズムを最適化技法と呼ぶ。最適化問題で与えられた関数$f$は特に目的関数Objective Functionと言う。 全ての$x$に対して$f(x^{ \\ast }) \\le f(x)$を満たす$x^{ \\ast }$を全域最適解Global Optimizerと呼ぶ。 全ての$x \\in \\mathcal{N}$に対して$f(x^{ \\ast }) \\le f(x)$を満たす$x^{ \\ast }$の近傍$\\mathcal{N}$が存在する場合、その$x^{ \\ast }$を局所最適解Local Optimizerと呼ぶ。 これらの定義では、不等式の向きが逆でも、つまり最大化について説明されても、それらは総じて最適化と呼ばれる。\n説明 最適化技法を使用する人々は「利益を最大化」または「コストを最小化」と言うことができるが、数学者にとっては最大か最小かは重要な問題ではない。最小化が最適化とほぼ同義とされる理由は、最小化問題で使用されるアルゴリズムが、$-1$を単に乗じることで最大化問題にも適用できるからであり、数学的に非常に重要な関数であるメトリックやノルムが$0$以上の実数の集合を値域に持つから、つまり最小値が存在するという理由からである。\n近年、ディープラーニングが流行しているが、目的関数（またはコスト関数、損失関数）は通常、スムーズであると想定されているが、必ずそうであるとは限らない。したがって、それを克服するためのアルゴリズムとメソッドも研究されてきた。目的関数の定義域が必ずしも$\\mathbb{R}^{n}$でなければならないわけではない。\n全域最適解 最適解の存在性は、数々の条件によって示すことができるかもしれないが、局所最適解が全域最適解であることを示す定理はない。理想的には誰もが最適解を見つけたいと思うが、実際に見つけた解が最適解であることを心から期待することはほとんどない。最適化問題は多くあるが、すべての問題にピッタリ合う「最適化された」最適化技法はないため、数多くの改善アルゴリズムが開発されてきた。\n最適解の厳密さと孤立性 通常は以下の定義は無意味であるが、一応言及しておく。\n全ての$x \\in \\mathcal{N}$に対して$f(x^{ \\ast }) \u0026lt; f(x)$を満たす$x^{ \\ast }$の近傍$\\mathcal{N}$が存在する場合、その$x^{ \\ast }$を厳密な局所最適解Strict Local Optimizerと呼ぶ。 全ての$x \\in \\mathcal{N}$に対して$f(x^{ \\ast }) \u0026lt; f(x)$を満たす$x^{ \\ast }$の唯一の近傍$\\mathcal{N}$が存在する場合、その$x^{ \\ast }$を孤立した局所最適解Isolated Local Optimizerと呼ぶ。 ","id":1463,"permalink":"https://freshrimpsushi.github.io/jp/posts/1463/","tags":null,"title":"数学における最適化技術"},{"categories":"수리통계학","contents":"定義 離散確率変数$X_{1}, X_{2}, \\cdots , X_{n}$に対し、次の$p_{2, \\cdots , n \\mid 1}$を$X_{1} = x_{1}$が与えられた時の$ X_{2}, \\cdots , X_{n}$の結合条件付き確率質量関数という。 $$ p_{2, \\cdots , n \\mid 1} ( x_{2} , \\cdots ,x_{n} \\mid X_{1} = x_{1} ) = {{ p_{1, \\cdots , n}(x_{1} , x_{2} , \\cdots , x_{n}) } \\over { p_{1}( X_{1} = x_{1} ) }} $$ 連続確率変数$X_{1}, X_{2}, \\cdots , X_{n}$に対し、次の$f_{2, \\cdots , n \\mid 1}$を$X_{1} = x_{1}$が与えられた時の$ X_{2}, \\cdots , X_{n}$の結合条件付き確率密度関数という。 $$ f_{2, \\cdots , n \\mid 1} ( x_{2} , \\cdots ,x_{n} \\mid X_{1} = x_{1} ) = {{ f_{1, \\cdots , n}(x_{1} , x_{2} , \\cdots , x_{n}) } \\over { f_{1}( X_{1} = x_{1} ) }} $$ $X_{2} , \\cdots , X_{n}$に対する関数$u$が与えられた時、次を$X_{1} = x_{1}$が与えられた時の$u( X_{2}, \\cdots , X_{n} )$の条件付き期待値という。 $$ \\begin{align*} \u0026amp; E \\left[ u \\left( X_{2} , \\cdots , X_{n} \\right) \\mid X_{1} = x_{1} \\right] \\\\ =\u0026amp; \\int_{-\\infty}^{\\infty} \\cdots \\int_{-\\infty}^{\\infty} u (x_{2} , \\cdots , x_{n}) f_{2 , \\cdots , n \\mid 1} (x_{2} , \\cdots, x_{n} \\mid X_{1} = x_{1}) dx_{2} \\cdots , dx_{n} \\end{align*} $$ 定理 [1] 条件付き分散: $$ \\begin{align*} \\text{Var} (X_{2} | X_{1} = x_{1}) =\u0026amp; E \\left[ \\left( X_{2} - E (X_{2} \\mid X_{1} = x_{1}) \\right)^{2} \\mid X_{1} = x_{1} \\right] \\\\ =\u0026amp; E \\left( X_{2}^{2} \\mid X_{1} = x_{1} \\right) - \\left[ E(X_{2} \\mid X_{1} = x_{1}) \\right]^{2} \\end{align*} $$ [2]: $E \\left[ E (X_{2} | X_{1}) \\right] = E (X_{2} )$ [3]: $\\text{Var}(X_{2})$が存在すれば$\\text{Var} \\left[ E \\left( X_{2} \\mid X_{1} \\right) \\right] \\le \\text{Var} (X_{2})$ 説明 条件付き確率、条件付き期待値は、教科課程のレベルであったように、数理統計学でも最も計算が難しい部分に属している。他のことはさておき、多変量である以上、計算が多くなるのは避けられない。もちろん、条件付きという概念にはその価値がある。一方で、主として微積分学に依存している数理統計学と異なり、測度論に基づいた確率論へと発展すると、その計算ははるかに簡潔になる。要旨は、「無視することはないが、過度に執着することもない」ということだ。\n参照 測度論によって定義される条件付き確率分布 測度論によって定義される条件付き期待値 ","id":1458,"permalink":"https://freshrimpsushi.github.io/jp/posts/1458/","tags":null,"title":"数理統計学における条件付き確率分布"},{"categories":"함수","contents":"定義 オイラー積分Euler integral 以下の二つの積分をオイラー積分と呼ぶ。\n$(a)$ 第1種オイラー積分Euler integral of the first kind : ベータ関数 $$ B(p,q)=\\int_{0}^1 t^{p-1}(1-t)^{q-1}dt,\\quad p\u0026gt;0,\\quad q\u0026gt;0 $$ $(b)$ 第2種オイラー積分Euler integral of the second kind : ガンマ関数 $$ \\Gamma (p) = \\int_{0}^\\infty t^{p-1}e^{-t}dt,\\quad p\u0026gt;0 $$ 説明 第1種オイラー積分 1-1. ベータ関数: ガンマ関数を階乗の一般化とみなした場合、ベータ関数は二項係数の一般化と見ることができる。 $$ \\begin{pmatrix} n \\\\ k \\end{pmatrix}=\\frac{ 1 }{ (n+1)B(n-k+1,k+1) } $$ 1-2. ベータ関数の性質 $$ B(p,q)=B(q,p) $$ $$ B(p,q)=B(p+1,q)+B(p,q+1) $$ $$ B(p+1,q)=\\frac{ p }{p+q}B(p,q),\\quad B(p,q+1)=\\frac{ q }{p+q }B(p,q) $$ $$ B(p,p)=\\frac{ 1 }{ 2^{2p-1} }B(p,{\\textstyle \\frac{ 1 }{ 2 }}) $$ 1-3. ベータ関数の様々な表現\n$$ B(p,q)=\\int_{0}^{a}\\left( \\frac{ t }{ a }\\right)^{p-1} \\left( 1-\\frac{ t}{a}\\right)^{q-1}\\frac{ 1 }{a }dt=\\frac{ 1 }{ a^{p+q-1} }\\int_{0}^{a}t^{p-1}(a-t)^{q-1}dt $$ ベータ関数の定義に$t \\rightarrow \\dfrac{ t }{ a }$を置換すれば直ちに得られる。 $$ B(p,q)=2\\int_{0}^{\\pi/2}(\\sin\\theta)^{2p-1} (\\cos \\theta )^{2q-1}d\\theta $$ $$ B(p,q)=\\int_{0}^{\\infty} \\frac{ t^{p-1} }{(1+t)^{p+q}}dt $$ $$ B(p,q)=\\frac{ \\Gamma (p) \\Gamma (q) }{ \\Gamma (p+q) } $$ 第2種オイラー積分 2-1. ガンマ関数: ベータ関数を二項係数の一般化とみなした場合、ガンマ関数は階乗の一般化と見ることができる。 $$ \\Gamma (n)=(n-1)! $$ 2-2. ガンマ関数の性質 $$ \\Gamma (p+1) = p \\Gamma (p) $$ $$ \\Gamma (p) \\Gamma (1-p) = \\frac{ \\pi }{ \\sin (\\pi p) } $$ また、ガンマ関数を含んだ重要な公式がいくつか存在する。\n","id":1483,"permalink":"https://freshrimpsushi.github.io/jp/posts/1483/","tags":null,"title":"オイラー積分：ベータ関数とガンマ関数"},{"categories":"함수","contents":"まとめ $$ B(p,q) = {{\\Gamma (p) \\Gamma (q)} \\over {\\Gamma (p+q) }} $$\n説明 ベータ関数は $\\displaystyle B(p,q) := \\int_{0}^{1} t^{p-1} (1-t)^{q-1} dt $ で定義され、ガンマ関数と同様に多くの分野で応用されている重要な関数だ。ガンマ関数は再帰関係を使って簡単に計算できるので、上の関係式を利用してベータ関数も簡単に計算できる。直感的には、二項係数の一般化であり、階乗が現れるので自然にガンマ関数と関係が深い。\n証明 $\\displaystyle \\Gamma (p) = \\int_{0}^{\\infty} u^{p-1} e^{-u} du $ かつ $\\displaystyle \\Gamma (q) = \\int_{0}^{\\infty} v^{p-1} e^{-v} dv $ の場合、 $$ \\begin{align*} \\Gamma (p) \\Gamma (q) \u0026amp;= \\int_{0}^{\\infty} u^{p-1} e^{-u} du \\int_{0}^{\\infty} v^{p-1} e^{-v} dv \\\\ \u0026amp;= \\int_{0}^{\\infty} \\int_{0}^{\\infty} u^{p-1} v^{q-1} e^{-u} e^{-v} du dv \\end{align*} $$ $u + v = z$、$u = zt$、$v = z( 1 - t)$ に置き換えて $$ \\begin{align*} \\Gamma (p) \\Gamma (q) \u0026amp;= \\int_{0}^{\\infty} \\int_{0}^{1} (zt)^{p-1} (z(1-t))^{q-1} e^{-u-v} z dt dz \\\\ \u0026amp;= \\int_{0}^{\\infty} \\int_{0}^{1} z^{p+q+1} t^{p-1} (1-t)^{q-1} e^{-z} dt dz \\\\ \u0026amp;= \\int_{0}^{\\infty} z^{p+q+1} e^{-z} \\int_{0}^{1} t^{p-1} (1-t)^{q-1} dt dz \\\\ \u0026amp;= \\int_{0}^{\\infty} z^{p+q+1} e^{-z} dz \\int_{0}^{1} t^{p-1} (1-t)^{q-1} dt \\\\ \u0026amp;= \\Gamma (p + q) B(p, q) \\end{align*} $$ まとめると $$ {{\\Gamma (p) \\Gamma (q)} \\over {\\Gamma (p+q) }} = B(p,q) $$ ■\n系: ベータ関数の対称性 $$ B(p,q) = B(q,p) $$\n置き換えて証明することも馬鹿げているが、$\\displaystyle B(p,q) = {{\\Gamma (p) \\Gamma (q)} \\over {\\Gamma (p+q) }} = {{\\Gamma (q) \\Gamma (p)} \\over {\\Gamma (q+p) }} = B(q,p) $ が一行で十分だ。\n","id":1481,"permalink":"https://freshrimpsushi.github.io/jp/posts/1481/","tags":null,"title":"ベータ関数とガンマ関数の関係"},{"categories":"줄리아","contents":"概要 マクロは、Juliaでコーディングする時の便利機能であり、スコープの前に置いて実行される。例えば、自分のプログラムがどれくらいの時間を消費するか知りたい場合、次のように書くといい。\n@time for t in 1:10\rfoo()\rbar()\rend 例 多くの種類があるが、以下のマクロが特に広く使われている:\n@time：後に続く関数やスコープの実行時間を測定する。どんな状況でどう最適化すべきか分からない時、まず時間を測って、良い方を選ぶのが楽になる。言語によっては、時間を測るためのコードを書くのが面倒な場合があるが、Juliaの場合は、マクロ一つで実行時間だけでなく、メモリの使用量まで教えてくれる。 @.：その後に続く式の演算にドット(.)を追加する。 @threads：並列処理を簡単に実装できるマクロだ。 @animate：GIFを簡単に焼くマクロだ。 環境 OS: Windows julia: v1.5.0 ","id":1454,"permalink":"https://freshrimpsushi.github.io/jp/posts/1454/","tags":null,"title":"ジュリアの強力な便利機能、マクロ"},{"categories":"함수","contents":"ファクトリアル 自然数$n$に対して、$n!$が$n$階乗$factorial, 계승, 차례곱이라 읽고 아래와 같이 정의한다.\n$$ n!=n\\cdot(n-1)\\cdot(n-2)\\cdots 2\\cdot 1 =\\prod\\limits_{k=1}^n k $$\n설명 많은 곳에서 식을 깔끔하게 표현하기 위해 사용된다. $0$팩토리얼은 $0!:=1$으로 정의한다. 팩토리얼의 정의역을 일반화해서 감마함수라는 것을 정의할 수도 있다.\n더블 팩토리얼 자연수 $n$에 대해서 $n!!$을 $n$더블팩토리얼doble factorial, semifactorial, 겹계승 이라 읽고 아래와 같이 정의한다.\n$$ n!!=n\\cdot (n-2)\\cdot (n-4) \\cdot (n-6) \\cdots $$\n설명 팩토리얼이 $n$에서부터 $1$씩 뺀 값을 곱해나가는 것이었다면 더블 팩토리얼은 $n$에서부터 $2$씩 뺀 값을 곱해나가는 것이다. 따라서 $n$이 짝수이면 $2$에서 곱셈이 끝나고, 홀수이면 $1$에서 곱셈이 끝난다.$n$이 짝수이면,\n$$ n!!=\\prod \\limits_{k=1}^{\\frac{n}{2}}(2k)=n\\cdot(n-2)\\cdots 4\\cdot 2 $$\n$n$이 홀수이면,\n$$ n!!=\\prod \\limits_{k=1}^{\\frac{n+1}{2}}(2k-1)=n\\cdot(n-2)\\cdots 3\\cdot 1 $$\n예를들어 $7!!=7\\cdot 5\\cdot 3\\cdot 1=105$이고 $10!!=10\\cdot 8\\cdot 6\\cdot 4\\cdot 2=3840$\n$n!!$은 $(n!)!$과 표기법이 헷갈리기 때문에 자주 쓰이지는 않는다. 물론 실제로 쓸만한 곳이 많지도 않다. 다만 양자역학 등에서 복잡한 수식을 다룰 때 편의를 위해 사용하긴 한다. 팩토리얼과 마찬가지로 $0!!=1$으로 정의한다.\n멀티 팩토리얼 자연수 $n\u0026gt;k$에 대해서 $n!^{(k)}=n!_{k}$를 아래와 같이 정의하고 멀티 팩토리얼multifactorial, 다중계승이라 한다.\n$$ n!^{(k)}=n\\cdot(n-k)\\cdot (n-2k) \\cdot (n-3k)\\cdots $$\n설명 감마함수가 팩토리얼의 정의역을 확장한 것이라면 멀티 팩토리얼은 팩토리얼의 성질 그 자체를 확장한 것이라고 볼 수 있다. 더블 팩토리얼도 볼 일이 적기 때문에 당연히 멀티 팩토리얼은 더 만나기 어렵다. $n$에서부터 느낌표 갯수만큼 뺀 값을 음수가 나오기 전까지 곱해나가면 된다. 예를 들어\n$$ 9!!!!=9\\cdot 5\\cdot 1,\\quad 8!!!=8\\cdot 5\\cdot 2 $$\n$0\u0026lt;n \\le k$인 경우에는 $n!^{(k)}=1$으로 정의하고 $-k \u0026lt; n\\le 0$인 경우에는 $n!^{(k)}=1$と定義される。\nジョーク ","id":1477,"permalink":"https://freshrimpsushi.github.io/jp/posts/1477/","tags":null,"title":"階乗、二重階乗、多重階乗"},{"categories":"함수","contents":"非負整数のためのガンマ関数 $\\alpha \u0026gt;0$に対して $$ \\int_{0}^{\\infty} e^{-\\alpha x} dx=\\left[-\\frac{1}{\\alpha}e^{-\\alpha x}\\right]_{0}^{\\infty}=\\frac{1}{\\alpha} $$ 両辺を$\\alpha$に関して微分すると、ライプニッツの積分法則によって左辺の微分が積分記号の中に入ることができるので、 $$ \\begin{align*} \u0026amp;\u0026amp;\\int_{0}^\\infty -xe^{-\\alpha x}dx\u0026amp;=-\\frac{1}{\\alpha^2} \\\\ \\implies \u0026amp;\u0026amp; \\int_{0}^\\infty xe^{-\\alpha x}dx \u0026amp;= \\frac{1}{\\alpha ^2} \\end{align*} $$ 続けて微分すると、 $$ \\begin{align*} \\int_{0}^\\infty x^2e^{-\\alpha x}dx\u0026amp;=\\frac{2}{\\alpha^3} \\\\ \\int_{0}^\\infty x^3e^{-\\alpha x}dx\u0026amp;=\\frac{3\\cdot 2}{\\alpha^4} \\\\ \\int_{0}^\\infty x^4e^{-\\alpha x}dx \u0026amp;=\\frac{4\\cdot 3\\cdot 2}{\\alpha^5} \\\\ \u0026amp;\\vdots \\\\ \\int_{0}^\\infty x^ne^{-\\alpha x}dx\u0026amp;=\\frac{n!}{\\alpha^{n+1}} \\end{align*} $$ ここで$\\alpha =1$と置くと、 $$ \\int_{0}^\\infty x^n e^{-x}dx=n! \\quad n=1,2,3,\\cdots $$ 上記の式から$0!=1$である理由も自然に説明できる。$n=0$とすると、 $$ 0!=\\int_{0}^\\infty e^{-x}dx=\\left[-e^{-x}\\right]_{0}^\\infty=1 $$ 従って、$0!:=1$と自然に定義できる。\nガンマ関数の再帰関係 $n$が整数でなくても上記の積分値で関数を定義できる。これをガンマ関数と呼ぶ。通常整数の場合は$n$と書き、そうでない場合は$p$と書く。 $$ \\Gamma (p)=\\int_{0}^\\infty x^{p-1}e^{-x}dx,\\quad p\u0026gt;0 \\tag{1} $$ 範囲が$p\u0026gt;0$である理由は、その範囲でのみ不適切積分が収束するからである。$p\\le 0$の場合、上記の積分は発散するので、$\\Gamma (p)$を定義するのに使用できない。$p\\le 0$の場合のガンマ関数の定義方法はさらに下で紹介する。また、$\\Gamma (p)=(p-1)!$であり$\\Gamma (p)= p!$でないことに注意。$p$が整数であれば、ガンマ関数は階乗と同じになるので、$\\Gamma (n+1)=n \\Gamma (n)$が成り立つのは明らかだ。しかし、これは$p$が整数でない場合にも成り立つ。まず$(1)$で$p$の代わりに$p+1$を入れると、 $$ \\Gamma (p+1)=\\int_{0}^\\infty x^pe^{-x}dx,\\quad p\u0026gt;-1, \\tag{2} $$ $(2)$の右辺を部分積分すると、 $$ \\begin{align*} \\int_{0}^{\\infty} x^{p}e^{-x}dx\u0026amp;= \\int_{0}^{\\infty} (-x^{p})(-e^{-x})dx \\\\ \u0026amp;= \\left[-x^{p}e^{-x}\\right]_{0}^{\\infty}+\\int_{0}^{\\infty} px^{p-1}e^{-x}dx \\\\ \u0026amp;= p\\int_{0}^{\\infty} x^{p-1}e^{-x}dx \\\\ \u0026amp;=p\\Gamma (p) \\end{align*} $$ 従って、上記の結果と$(2)$を組み合わせると、 $$ \\Gamma (p+1)=p\\Gamma (p),\\quad p\u0026gt;-1 \\tag{3} $$ $(3)$をガンマ関数に対する再帰関係と言う。これにより、ガンマ関数を含んだ式を簡単に表現できる。例えば、 $$ \\frac{\\Gamma (1/4)}{\\Gamma (9/4)}=\\frac{\\Gamma (1/4)}{\\frac{5}{4}\\Gamma (5/4)}=\\frac{\\Gamma (1/4)}{\\frac{5}{4}\\frac{1}{4}\\Gamma (1/4)}=\\frac{16}{5} $$\n負数まで拡張されたガンマ関数 再帰関係を利用して負数に対するガンマ関数を定義できる。$(3)$をよく見ると、右側のガンマ関数に$-1\u0026lt;p\u0026lt;0$が入ることがわかる。従って、これを利用して$p\u0026lt;0$に対するガンマ関数は以下のように定義される。 $$ \\Gamma (p)=\\frac{1}{p}\\Gamma (p+1),\\quad p\u0026lt;0 $$ 例えば、$\\Gamma (-3/5)=-\\frac{5}{3}\\Gamma (2/5 )$であり、$\\Gamma (-8/5)=-\\frac{5}{8}\\Gamma (-3/5)=\\frac{25}{24}\\Gamma (2/5)$である。$p=0$の場合は以下のように発散することが示される。$\\Gamma (1)=0!=1$であるため、 $$ \\lim \\limits_{p \\rightarrow 0} \\Gamma (p)=\\lim \\limits_{p \\rightarrow 0} \\frac{\\Gamma (p+1)}{p}=\\infty $$\n参照 オイラー積分 ガンマ関数をもっと簡単に知りたい場合は：階乗の一般化、ガンマ関数 物理学におけるガンマ関数 ","id":1476,"permalink":"https://freshrimpsushi.github.io/jp/posts/1476/","tags":null,"title":"ガンマ関数の導出"},{"categories":"해석개론","contents":"要約 $f(x,t)$と$\\dfrac{\\partial f}{\\partial x}(x,t)$が連続だとしよう。すると、以下の式が成り立つ。\n$$ \\frac{d}{dx} \\int_{a}^b f(x,t)dt = \\int_{a}^b\\frac{\\partial f}{\\partial x}(x,t)dt $$\n説明 微分と積分の順序を交換できるのは、言うまでもなく便利である。\n他にも、ライプニッツの名前が付いた微分と積分に関連する定理や公式が多い。\n証明 連続ならば積分可能なので、$u$を以下のようにしよう。\n$$ u(x):=\\int_{a}^b f(x,t)dt $$\nすると、以下が成り立つ。\n$$ \\begin{equation} \\begin{aligned} \\frac{ u(x+h)-u(x)}{h} \u0026amp;= \\frac{\\int_{a}^{b} f(x+h,t)dt -\\int_{a}^{b}f(x,t)dt}{h} \\\\ \u0026amp;= \\frac{ \\int_{a}^{b} \\big[f(x+h,t)-f(x,t) \\big] dt}{h} \\\\ \u0026amp;= \\int_{a}^{b} \\frac{f(x+h,t)-f(x,t)}{h}dt \\end{aligned} \\end{equation} $$\nまた、固定された$y$に対して、$f(x,y)$に平均値の定理を適用すると、以下の式を満たす$c\\in[x,x+h]$が存在する。\n$$ \\frac{f(x+h,t)-f(x,t)}{h}=\\frac{\\partial f}{\\partial x}(c,t) $$\n両辺を$t$に関して定積分すると、$(1)$により以下が成り立つ。\n$$ \\frac{u(x+h)-u(x)}{h}=\\int_{a}^{b}\\frac{\\partial f}{\\partial x}(c,t) dt $$\n今、任意の$\\epsilon \u0026gt;0$に対して、$\\epsilon_{0}=\\dfrac{\\epsilon}{b-a}$としよう。$[x,x+h]\\times [a,b]$はコンパクトであり、仮定により$\\dfrac{\\partial f}{\\partial x}$はコンパクト区間上で連続なので、一様連続である。すると、十分に小さな$h$に対して、以下が成り立つ。\n$$ \\left| \\frac{\\partial f}{\\partial x}(x+h,t)-\\frac{\\partial f}{\\partial x} (x,t)\\right| \u0026lt; \\epsilon_{0} $$\nまた、$c\\in[x,x+h]$だから、一様連続の定義により、以下が成り立つ。\n$$ \\left| \\frac{\\partial f}{\\partial x}(c,t)-\\frac{\\partial f}{\\partial x} (x,t)\\right| \u0026lt; \\epsilon_{0} $$\n今、計算してみると、以下が得られる。\n$$ \\begin{align*} \u0026amp; \\left| \\lim \\limits_{h\\rightarrow 0}\\frac{u(x+h,t)-u(x,t)}{h} - \\int_{a}^{b} \\frac{\\partial f}{\\partial x}(x,t)dt\\right| \\\\ =\u0026amp;\\ \\left| \\lim \\limits_{h\\rightarrow 0}\\int_{a}^{b}\\frac{\\partial f}{\\partial x}(c,t) dt - \\int_{a}^{b} \\frac{\\partial f}{\\partial x}(x,t)dt\\right| \\\\ =\u0026amp;\\ \\left| \\lim \\limits_{h \\rightarrow 0} \\int_{a}^{b}\\left[ \\frac{\\partial f}{\\partial x}(c,t)-\\frac{\\partial f}{\\partial x}(x,t) \\right] dt\\right| \\\\ =\u0026amp;\\ \\lim \\limits_{h \\rightarrow 0}\\left| \\int_{a}^{b}\\left[ \\frac{\\partial f}{\\partial x}(c,t)-\\frac{\\partial f}{\\partial x}(x,t) \\right] dt\\right| \\\\ \\le\u0026amp; \\lim \\limits_{h \\rightarrow 0} \\int_{a}^{b}\\left| \\frac{\\partial f}{\\partial x}(c,t)-\\frac{\\partial f}{\\partial x}(x,t) \\right| dt \\\\ \\le\u0026amp; \\lim \\limits_{h \\rightarrow 0} \\int_{a}^{b} \\epsilon_{0}dt \\\\ =\u0026amp;\\ \\lim \\limits_{h \\rightarrow 0} (b-a)\\epsilon_{0} \\\\ =\u0026amp;\\ \\epsilon \\end{align*} $$\nこの式は、任意の$\\epsilon\u0026gt;0$に対して成り立つので、以下が得られる。\n$$ \\lim \\limits_{h\\rightarrow 0} \\frac{u(x+h,t)-u(x,t)}{h}=\\int_{a}^{b}\\frac{\\partial f}{\\partial x}(x,t)dt $$\nまた、$\\displaystyle u(x):=\\int_{a}^b f(x,t)dt$だから、以下が成り立つ。\n$$ \\frac{d}{dx} \\int_{a}^b f(x,t)dt = \\int_{a}^b\\frac{\\partial f}{\\partial x}(x,t)dt $$\n■\n関連項目 ライプニッツの微分規則 ","id":1475,"permalink":"https://freshrimpsushi.github.io/jp/posts/1475/","tags":null,"title":"ライプニッツの積分則"},{"categories":"줄리아","contents":"概要 ジュリアはデータを扱う上での強みを生かして、パイプラインオペレーターをサポートしている。\nコード julia\u0026gt; (1:5) .|\u0026gt; (x -\u0026gt; sqrt(x+2)) .|\u0026gt; sin |\u0026gt; minimum\r0.4757718381527513\rjulia\u0026gt; minimum(sin.((x -\u0026gt; sqrt(x+2)).(1:5)))\r0.4757718381527513 上のサンプルコードは、配列 $[1,2,3,4,5]$ を $\\sqrt{x + 2}$ に入れ、その結果を $\\sin$ に入れて、その中の最小値を得るコードであり、上記のコードと以下のコードは完全に同じ結果を出す。複雑なコードを書く中でパイプラインがどれだけ便利であるかは、説明するまでもないだろう。配列を入れる時は、各要素を個別に扱うために必ずドットを使用する点だけ注意すれば、他の言語のパイプラインと同じように使用できる。\n他の言語 R でのパイプラインオペレーター 環境 OS: Windows julia: v1.5.0 ","id":1450,"permalink":"https://freshrimpsushi.github.io/jp/posts/1450/","tags":null,"title":"ジュリアでパイプオペレータを使用する方法"},{"categories":"수리통계학","contents":"定義 1 標本空間 $\\Omega$で定義された$n$個の確率変数 $X_{i}$に対し$X = (X_{1} , \\cdots , X_{n})$を$n$次元ランダムベクトルRandom Vectorという。$X$の値域$X(\\Omega)$を空間とも呼ぶ。 次のを満たす関数$F_{X} : \\mathbb{R}^{n} \\to [0,1]$を$X$のジョイントJoint累積分布関数という。 $$ F_{X}\\left( x_{1}, \\cdots , x_{n} \\right) := P \\left[ X_{1} \\le x_{1} , \\cdots , X_{n} \\le x_{n} \\right] $$ ある$h_{1} , \\cdots , h_{n} \u0026gt;0$に対し、次のを満たす関数$M_{X}$が存在するなら、$X$の積率生成関数という。 $$ M_{X} (t_{1}, \\cdots , t_{n}) := E \\left[ e^{\\sum_{k=1}^{n} t_{k} X_{k} } \\right] = E \\left[ \\prod_{k=1}^{n} e^{t_{k} X_{k}} \\right] \\\\ |t_{1}| \u0026lt; h_{1} , \\cdots , |t_{n} | \u0026lt; h_{n} $$ 離散 D1: $X$の空間が可算集合なら、$X$は離散ランダムベクトルという。 D2: 次を満たす$p_{X} : \\mathbb{R}^{n} \\to [0,1]$を離散ランダムベクトル$X$のジョイント確率質量関数という。 $$ p_{X} (x_{1} , \\cdots , x_{n}) := P \\left[ X_{1} = x_{1} , \\cdots , X_{n} = x_{n} \\right] $$ D3: $1 \\le k \\le n$に対し、次のような$P_{X_{k}} (x_{k})$をマージナル確率質量関数という。 $$ P_{X_{k}} (x_{k}) := \\sum_{x_{1}} \\cdots \\sum_{x_{k-1}}\\sum_{x_{k+1}} \\cdots \\sum_{x_{n}} p_{X} (x_{1} , \\cdots , x_{n}) $$ D4: $S_{X}:= \\left\\{ \\mathbb{x} \\in \\mathbb{R}^{n} : p_{X}(\\mathbb{x}) \u0026gt; 0 \\right\\}$を$X$のサポートという。 連続 C1: 確率変数$X$の累積分布関数$F_{X} = F_{X_{1} , \\cdots , X_{n}}$が全ての$\\mathbb{x} \\in \\mathbb{R}^{n}$で連続なら、$X$は連続ランダムベクトルという。 C2: 次を満たす$f_{X} : \\mathbb{R}^{n} \\to [0,\\infty)$を、連続ランダムベクトル$X$のジョイント確率密度関数という。 $$ F_{X} (x_{1}, \\cdots, x_{n}) = \\int_{-\\infty}^{x_{1}} \\cdots \\int_{-\\infty}^{x_{n}} f_{\\mathbb{x}} (t_{1} , \\cdots , t_{n}) dt_{1} \\cdots d t_{n} $$ C3: $1 \\le k \\le n$に対し、次のような$f_{X_{k}} (t_{k})$をマージナル確率密度関数という。 $$ f_{X_{k}}(t_{k}) := \\int_{\\infty}^{x_{1}} \\cdots \\int_{\\infty}^{x_{k-1}} \\int_{\\infty}^{x_{k+1}} \\cdots \\int_{\\infty}^{x_{n}} f_{X}(t_{1} , \\cdots , t_{n}) dt_{1} \\cdots d_{k-1} d_{k+1} \\cdots d_{n} $$ C4: $S_{X} := \\left\\{ \\mathbb{t} \\in \\mathbb{R}^{n} : f_{X} ( \\mathbb{t} ) \u0026gt; 0 \\right\\}$を$X$のサポートという。 元々ランダムベクトルRandom Vectorは、確率ベクトルと訳されるが、高校卒業以上でStochasticやProbabilisticなどと混同されることを避けるため、原語をそのまま使う。 元々ジョイント累積分布関数Joint Cumulative Distribution Functionは、結合確率分布と訳されるが、独立や依存に対する誤解を招く可能性があるため、原語をそのまま使う。 元々マージナル分布Marginal Distributionは、周辺分布と訳されるが、経済学の限界Marginalのようにその意味が伝わりにくいと思われるため、原語をそのまま使う。 説明 多変量確率分布は、一変量確率分布を多次元に一般化したものであり、変数が複数ある点で根本的に大きな違いがあるが、少なくとも学部レベルの数理統計学では、微積分学的なスキルでも十分に異なることができる。どのように異なるか見てみよう：\n1: 混同してはいけないのは、ランダムベクトル$X : \\Omega^{n} \\to \\mathbb{R}^{n}$も依然として関数であることだ。そのため、その値域を考えることができ、これにより多変量に関しても離散型と連続型に分類する。 C2: 連続のジョイント密度関数は、一般的に確率が$0$の$A \\subset \\mathbb{R}^{n}$を除き、微積分学の基本定理に従って次のを満たすように定義される。 $$ {{ \\partial^{n} } \\over { \\partial x_{1} \\cdots \\partial x_{n} }} F_{X} (\\mathbb{x}) = f(\\mathbb{x}) $$ D3, C3: 式は複雑だが、簡単に言えば、ジョイント確率分布を純粋に確率変数$X_{k}$に関する分布に変えたものだ。経済学でマージナルという言葉が微分の概念と通じるのと反対に、数理統計学では関心のない変数を一掃するために積分や合計をすることだ。 Hogg et al. (2013). Introduction to Mathematical Statistics(7th Edition): p75~84.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1449,"permalink":"https://freshrimpsushi.github.io/jp/posts/1449/","tags":null,"title":"数理統計学における多変量確率分布"},{"categories":"줄리아","contents":"概要 Juliaでは、ラムダ式は以下のように定義される。\n(x -\u0026gt; 3x^2 - 2x + 3)(1) これは、匿名関数$\\lambda : \\mathbb{Z} \\to \\mathbb{Z}$を定義し、そこに$1$を代入して、$4$という関数値を得たものに相当する。 $$ \\lambda : x \\mapsto ( 3 x^{2} - 2 x + 3 ) \\\\ \\lambda (1) = 4 $$ 実際、ラムダ式自体はJuliaの特徴ではなく、MATLABやPythonを含む関数型言語に影響を受けて、ほぼ自然にサポートされているもので、Juliaに興味を持つ学習者ならすでにラムダ式を使った経験が多いかもしれない。しかし、これまで使ってきた「それ」がラムダ式であるかどうか知らなかったり、まだその真の価値を知らない読者のために、特にすぐに適用できるであろう例を2つ紹介する。\n例1：リストを異なる尺度でソート リストをソートするのは組み込み関数を使えば非常に簡単だが、多次元配列でカテゴリー別に優先度を設定してソーティングしたり、元のデータを異なる基準でソートしたい場合がある。そんな時は、sort()関数のbyオプションに該当する関数をラムダ式として入れることで、簡単にコードを書くことができる。\njulia\u0026gt; # Example 1\rjulia\u0026gt; example = rand(-20:20,10)\r10-element Array{Int64,1}:\r3\r8\r19\r-12\r-20\r9\r-13\r19\r13\r2\rjulia\u0026gt; sort(example, by=(x -\u0026gt; abs(x)))\r10-element Array{Int64,1}:\r2\r3\r8\r9\r-12\r-13\r13\r19\r19\r-20 上記の作業は、ランダムに選ばれた整数を絶対値が小さいものから大きいものに基づいてソートすることを示している。ラムダ式がなくても不可能ではないが、思ったより単純ではない。与えられたラムダ式(x -\u0026gt; abs(x))を上手く変えれば、コーダーが望むコードを簡単に書くことができるだろう。\n例1の応用 valueという辞書が以下のように作成されているとする。 この時、辞書の値の大きさ順でソートするコードは、ラムダ式を活用してsort(value,by=(x -\u0026gt; value[x]))のように簡単に組むことができ、その実行結果は以下の通りだ。 例2：リストの頻度計算 Rのようにデータを最優先にする言語では、最初から組み込み関数で作られているが、この頻度計算が思うほど単純ではない。アルゴリズムと呼べるほど複雑な作業ではないが、実際にやってみるとかなり手がかかる。これもまた、ラムダ関数を利用して簡単に解決できる！\njulia\u0026gt; # Example 2\rjulia\u0026gt; example = rand(1:3,10); println(example)\r[3, 1, 2, 2, 3, 2, 3, 1, 3, 3]\rjulia\u0026gt; uexample = sort(unique(example))\r3-element Array{Int64,1}:\r1\r2\r3\rjulia\u0026gt; counts = map(x-\u0026gt;count(y-\u0026gt;x==y,example),uexample)\r3-element Array{Int64,1}:\r2\r3\r5 上記の作業は、ランダムに選ばれた整数の頻度を数えたものである。unique()でデータの階級を把握し、それぞれの階級に該当する要素をカウントする方式で、単純に問題を解決した。\n環境 OS: Windows julia: v1.5.0 ","id":1448,"permalink":"https://freshrimpsushi.github.io/jp/posts/1448/","tags":null,"title":"ジュリアでのラムダ式"},{"categories":"매트랩","contents":"方法 tic\rX1=rand(2^7);\rX2=rand(2^8);\rX3=rand(2^9);\rX4=rand(2^10);\rX5=rand(2^11);\rtoc\rY1=imrotate(X1,45,\u0026#39;bicubic\u0026#39;,\u0026#39;crop\u0026#39;);\rtoc\rY2=imrotate(X2,45,\u0026#39;bicubic\u0026#39;,\u0026#39;crop\u0026#39;);\rtoc\rY3=imrotate(X3,45,\u0026#39;bicubic\u0026#39;,\u0026#39;crop\u0026#39;);\rtoc\rY4=imrotate(X4,45,\u0026#39;bicubic\u0026#39;,\u0026#39;crop\u0026#39;);\rtoc\rY5=imrotate(X5,45,\u0026#39;bicubic\u0026#39;,\u0026#39;crop\u0026#39;);\rtoc tic：実行時間を測定するためのストップウォッチを開始する。 toc：ストップウォッチの現在時間を返す。tocとtocの間の時間を測るわけではないことに注意。 上記の例示コードでY1〜Y6を計算する時間をそれぞれ計りたい場合、以下のようにコードを入力する必要がある。\ntic\rX1=rand(2^7);\rX2=rand(2^8);\rX3=rand(2^9);\rX4=rand(2^10);\rX5=rand(2^11);\rtoc\rtic\rY1=imrotate(X1,45,\u0026#39;bicubic\u0026#39;,\u0026#39;crop\u0026#39;);\rtoc\rtic\rY2=imrotate(X2,45,\u0026#39;bicubic\u0026#39;,\u0026#39;crop\u0026#39;);\rtoc\rtic\rY3=imrotate(X3,45,\u0026#39;bicubic\u0026#39;,\u0026#39;crop\u0026#39;);\rtoc\rtic\rY4=imrotate(X4,45,\u0026#39;bicubic\u0026#39;,\u0026#39;crop\u0026#39;);\rtoc\rtic\rY5=imrotate(X5,45,\u0026#39;bicubic\u0026#39;,\u0026#39;crop\u0026#39;);\rtoc 他の言語 Rで ","id":1467,"permalink":"https://freshrimpsushi.github.io/jp/posts/1467/","tags":null,"title":"MATLABでコード実行時間を計測する方法"},{"categories":"줄리아","contents":"画像サイズの変更 Images パッケージの imresize を使えばいい。関数名はMatlabと同じだ。\nimresize(X, ratio=a): 配列Xをa倍に調整した画像を返す。Matlabとは違って、ただ比率を書くだけではなく、必ず ratio=a と書かなければならない。\nimresize(X, m, n): 配列Xをm行n列に拡大/縮小した画像を返す。以下は例示コードとその結果だ。\nusing Images\rX=load(\u0026#34;example\\_{i}mage2.jpg\u0026#34;)\rY1=imresize(X, ratio=0.5)\rY2=imresize(X,500,500)\rY3=imresize(X,1500,1500)\rY4=imresize(X,700,1000)\rY5=imresize(X,1000,1300)\rY6=imresize(X,300,300)\rsave(\u0026#34;X.png\u0026#34;,colorview(RGB,X))\rsave(\u0026#34;Y1=imresize(0.5).png\u0026#34;,colorview(RGB,Y1))\rsave(\u0026#34;Y2=imresize(500,500).png\u0026#34;,colorview(RGB,Y2))\rsave(\u0026#34;Y3=imresize(1500,1500).png\u0026#34;,colorview(RGB,Y3))\rsave(\u0026#34;Y4=imresize(700,1000).png\u0026#34;,colorview(RGB,Y4))\rsave(\u0026#34;Y5=imresize(1000,1300).png\u0026#34;,colorview(RGB,Y5))\rsave(\u0026#34;Y6=imresize(300,300).png\u0026#34;,colorview(RGB,Y6)) 参照 Matlabで画像サイズを調整する方法 環境 OS: Windows10 Version: 1.5.3 (2020-11-09) ","id":1466,"permalink":"https://freshrimpsushi.github.io/jp/posts/1466/","tags":null,"title":"ジュリアで画像サイズを変更する方法"},{"categories":"줄리아","contents":"コード using Images\rcd(\u0026#34;C:/Users/rmsms/OneDrive/examples\u0026#34;)\rpwd()\rexample = load(\u0026#34;example.jpg\u0026#34;)\rtypeof(example)\rsize(example)\rgray1 = Gray.(example)\rtypeof(gray1)\rsize(gray1)\rM = convert(Array{Float64},gray1)\rtypeof(M)\rsize(M)\rcolorview(Gray, M.^(1/2))\rsave(\u0026#34;rgb.png\u0026#34;, colorview(RGB, example))\rsave(\u0026#34;gray1.png\u0026#34;, colorview(Gray, gray1))\rsave(\u0026#34;gray2.png\u0026#34;, colorview(Gray, transpose(gray1)))\rsave(\u0026#34;gray3.png\u0026#34;, colorview(Gray, M.^(1/2))) 上から順にサンプルコードを簡単に理解してみよう:\ncd() : Change Directory, 作業ディレクトリを望む場所に変える。\npwd() : Print Working Directory, 作業ディレクトリを出力する。例をそのまま試したいなら、上のファイルを作業ディレクトリにダウンロードして、ファイル名をexample.jpgに変更しよう。\nload() : 作業ディレクトリ内の画像ファイルを読み込む。読み込まれた画像のタイプは Array{RGB{Normed{UInt8,8}},2}だ。 これは他の言語でカラー画像を3つの行列を1つのテンソルとして表現するのとは少し違う。実際、そのような方法は数学的には理解しやすいかもしれないが、色空間やライブラリによって統一された規格がなく多くの混乱を招いてきた。Juliaではタイプを1つにして幅と高さが2次元の配列として理解する。画像は読み込まれた瞬間にPlotパネルにプリントされる。 Gray() : 画像を白黒に変換するのに使われた。実際に使われるのはGray()ではなくGray.()であることに注意しよう。この関数自体は1つのピクセルを白黒に変えるもので、ドットをつけることで全てのピクセルに適用されることを意味する。\nsize() : 画像のサイズを返す。述べたように、カラーと白黒を形が違う別のテンソルとして扱わず、データタイプが異なりサイズが同じ配列として扱っている。 Convert() : Array{Float64}、すなわち行列にgray1画像を変換した。すると、0が黒色で1が白色の行列によって白黒画像が表される。\ncolorview() : この関数自体が画像や行列を出力する関数だ。わざわざ行列を画像に再変換する必要はなく、直接Plotパネルで画像を確認できる。サンプルコードでは、行列 $M$ の各成分にルートを取っている。行列の全ての成分は $[0,1]$ に属するので、この変換は画像を全体的に明るく補正することに該当する。 save() : 作業ディレクトリに画像を保存する:gray1.png : 白黒で保存された。gray2.png : 白黒であり、転置行列状態で保存された。gray3.png : 白黒であり、明るく補正された状態で保存された。rgb.png : オリジナルの色をそのまま持った状態で保存された。\n環境 OS: Windows julia: v1.5.0 ","id":1446,"permalink":"https://freshrimpsushi.github.io/jp/posts/1446/","tags":null,"title":"ジュリアで画像を読み込み、行列として保存する方法"},{"categories":"매트랩","contents":"方法 imresize(A, scale): Aのサイズをscale倍調整して新しい画像を返す。 Aが10x10の画像である場合、scaleに0.5を入力すると5x5の画像を返す。以下のように直接サイズを調整することもできる。\nimresize(A, [m n]): m行n列の画像を返す。以下は例のコードとその結果だ。 X=imread(\u0026#39;test\\_{i}mage.jpg\u0026#39;);\rfigure()\rimshow(X)\rsaveas(gcf,\u0026#39;X.png\u0026#39;)\rtitle(\u0026#39;X\u0026#39;)\rY1=imresize(X,0.5);\rY2=imresize(X,[500 500]);\rY3=imresize(X,[700 500]);\rY4=imresize(X,[500,700]);\rfigure()\rimshow(Y1)\rsaveas(gcf,\u0026#39;Y1.png\u0026#39;)\rtitle(\u0026#39;Y1=imresize(X,0.5)\u0026#39;)\rfigure()\rimshow(Y2)\rsaveas(gcf,\u0026#39;Y2.png\u0026#39;)\rtitle(\u0026#39;Y2=imresize(X,[500 500])\u0026#39;)\rfigure()\rimshow(Y3)\rsaveas(gcf,\u0026#39;Y3.png\u0026#39;)\rtitle(\u0026#39;Y3=imresize(X,[700 500])\u0026#39;)\rfigure()\rimshow(Y4)\rsaveas(gcf,\u0026#39;Y4.png\u0026#39;)\rtitle(\u0026#39;Y4=imresize(X,[500,700])\u0026#39;) 他の言語で ジュリアで ","id":1465,"permalink":"https://freshrimpsushi.github.io/jp/posts/1465/","tags":null,"title":"MATLABでの画像サイズの変更方法"},{"categories":"줄리아","contents":"画像の回転 imrotate(X, theta) : 配列Xをthetaラジアンで回転させる。ここで注意すべき点は、角度の単位が度（$^{\\circ})$のMATLABと異なり、角度の単位はラジアンであることだ。さらに、MATLABとは異なり時計回りに回転する。他の変数を入力しない場合の補間法はバイリニアであり、回転された画像は切り取られない。元の画像Xを$90^\\circ=\\pi/2$、$180^\\circ=\\pi$、$270^\\circ=\\frac{3}{2}\\pi$だけ回転させた例とその結果は下のようになる。\nusing Images\rusing Interpolations\rX=load(\u0026#34;example\\_{i}mage.png\u0026#34;)\rY1=imrotate(X,pi/2)\rY2=imrotate(X,pi)\rY3=imrotate(X,pi*3/2) 上のように回転させると、元の配列がぴったりと収まるので、画像のサイズは変わらない。しかし、$90$の倍数ではない角度で回転させると、元の形に合わなくなる。そのため、元の画像のすべての点を表現するために、以下のように画像が大きくなる。元の画像のサイズを維持したい場合は、変数axes()を追加すればいい。\nY4=imrotate(X,pi/6)\rY5=imrotate(X,pi/6,axes(X)) また、InterpolationsパッケージのConstant()を使用すると、補間法をnearest1に適用することができる。計算に最も近い点のみを使用するので、精度は低下するが、計算は速い。対照的に、bilinear2の場合は周囲の四点すべてを計算に使用するので、相対的に速度は遅いが、より精確である。ここで精確とは、回転した際に画像が損傷する度合いが低いという意味だ。以下の画像を見てみよう。画像が大きく、一見すると違いが分からないかもしれないが、拡大すると二つの補間法の違いがはっきりとわかる。\nY6=imrotate(X,pi/6,Constant()) 参照 MATLABで画像を回転させる方法 環境 OS: Windows10 Version: 1.5.3 (2020-11-09) 最近傍補間法, 最近隣補間法\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n双線形補間法, 二線形補間法\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1462,"permalink":"https://freshrimpsushi.github.io/jp/posts/1462/","tags":null,"title":"ジュリアで画像配列を回転する方法"},{"categories":"줄리아","contents":"$A = \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 1 \\\\ 0 \u0026amp; 3 \u0026amp; 0 \\\\ 2 \u0026amp; 3 \u0026amp; 4\\end{pmatrix}$としよう。\n転置行列 julia\u0026gt; A =[1 2 1;\r0 3 0;\r2 3 4]\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r2 3 4\rjulia\u0026gt; transpose(A)\r3×3 LinearAlgebra.Transpose{Int64,Array{Int64,2}}:\r1 0 2\r2 3 3\r1 0 4\rjulia\u0026gt; A\u0026#39;\r3×3 LinearAlgebra.Adjoint{Int64,Array{Int64,2}}:\r1 0 2\r2 3 3\r1 0 4 行列の要素が実数の場合、transpose()と'は同じ行列を返すが、データ型が微妙に異なる。これは'が正確には転置ではなく共役転置であるためだ。したがって、実数の行列の場合は実質的に同じ行列を返し、複素数の行列の場合は全く異なる結果を返す。\njulia\u0026gt; A_complex=[1+im 2 1+im;\r0 3 0+im;\r2 3+im 4]\r3×3 Array{Complex{Int64},2}:\r1+1im 2+0im 1+1im\r0+0im 3+0im 0+1im\r2+0im 3+1im 4+0im\rjulia\u0026gt; transpose(A_complex)\r3×3 LinearAlgebra.Transpose{Complex{Int64},Array{Complex{Int64},2}}:\r1+1im 0+0im 2+0im\r2+0im 3+0im 3+1im\r1+1im 0+1im 4+0im\rjulia\u0026gt; A_complex\u0026#39;\r3×3 LinearAlgebra.Adjoint{Complex{Int64},Array{Complex{Int64},2}}:\r1-1im 0+0im 2+0im\r2+0im 3+0im 3-1im\r1-1im 0-1im 4+0im 繰り返し乗算 julia\u0026gt; A =[1 2 1;\r0 3 0;\r2 3 4]\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r2 3 4\rjulia\u0026gt; A^2\r3×3 Array{Int64,2}:\r3 11 5\r0 9 0\r10 25 18\rjulia\u0026gt; A*A\r3×3 Array{Int64,2}:\r3 11 5\r0 9 0\r10 25 18\rjulia\u0026gt; A^3\r3×3 Array{Int64,2}:\r13 54 23\r0 27 0\r46 149 82\rjulia\u0026gt; A*A*A\r3×3 Array{Int64,2}:\r13 54 23\r0 27 0\r46 149 82 A^2とA*Aは完全に同じ結果を返す。同様に、A^3とA*A*Aも同じである。\n要素ごとの乗算、要素ごとの除算 julia\u0026gt; A =[1 2 1;\r0 3 0;\r2 3 4]\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r2 3 4\rjulia\u0026gt; A.*A\r3×3 Array{Int64,2}:\r1 4 1\r0 9 0\r4 9 16\rjulia\u0026gt; A./A\r3×3 Array{Float64,2}:\r1.0 1.0 1.0\rNaN 1.0 NaN\r1.0 1.0 1.0 各要素を乗算または除算した結果を返す。\n左右反転、上下反転 julia\u0026gt; A =[1 2 1;\r0 3 0;\r2 3 4]\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r2 3 4\rjulia\u0026gt; reverse(A,dims=1)\r3×3 Array{Int64,2}:\r2 3 4\r0 3 0\r1 2 1\rjulia\u0026gt; reverse(A,dims=2)\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r4 3 2 reverse(A,dims=1)は行列$A$を上下に反転した行列を返し、MATLABでのflipud(A)に相当する。reverse(A,dims=2)は行列$A$を左右に反転した行列を返し、MATLABでのfliplr(A)に相当する。\n逆行列 julia\u0026gt; A =[1 2 1;\r0 3 0;\r2 3 4]\r3×3 Array{Int64,2}:\r1 2 1\r0 3 0\r2 3 4\rjulia\u0026gt; inv(A)\r3×3 Array{Float64,2}:\r2.0 -0.833333 -0.5\r0.0 0.333333 0.0\r-1.0 0.166667 0.5 行列$A$の逆行列を返す。逆行列を見つけることができない場合は、エラーが発生する。\n環境 OS: Windows10 Version: 1.5.3 (2020-11-09) ","id":1460,"permalink":"https://freshrimpsushi.github.io/jp/posts/1460/","tags":null,"title":"ジュリアでの2次元配列操作の関数들"},{"categories":"줄리아","contents":"Heatmap Plots パッケージのheatmap関数を使えば、2次元配列をヒートマップ画像として出力でき、savefig関数でその画像を保存できる。@__DIR__はジュリアコードファイルの位置を教えてくれるマクロだ。\n# code1 だが、配列Aとヒートマップ画像を比較すると、配列の上下が逆さまに作成されたヒートマップ画像になっていることが分かる。出力画像がこのように作成される理由は、それぞれの成分の位置を行と列ではなく直交座標系の座標として考えるからだというのが公式のカニの話だ。つまり、例えば行列$A$では、19という値は4行4列の成分ではなく、直交座標$(4,4)$の成分と見なすわけだ。これが行列と画像が上下逆さまになっている理由を説明する。\nしたがって、配列と同じ見た目に出力されるようにしたいのであれば、yflip=trueオプションを追加すればいい1。\n# code2 また、MATLABに慣れているユーザーは、color=:bgyオプションを使用すれば、MATLABの基本色と似た出力が得られる。\n# code3 色のテーマ 使用できる色のテーマは、次のようなものがある。\n参照 マットラボから 環境 OS: Windows10 バージョン: 1.5.3 (2020-11-09) https://github.com/JuliaPlots/Makie.jl/issues/46#issuecomment-357023505\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1459,"permalink":"https://freshrimpsushi.github.io/jp/posts/1459/","tags":null,"title":"ジュリアで配列をヒートマップ画像として出力保存する方法"},{"categories":"줄리아","contents":"概要 Juliaでは、Pythonと同様にセットデータ型をサポートしています。元来のセットデータ型がそうであるように、使用する人にとっては非常に便利で、使用しない人にはまったく使われないものですが、Juliaは言語設計自体が数学に近いため、セットの概念と操作がしっかりと実装されており、必ず理解しておくべきです。 特に、他の言語、特にPythonと最も異なる点は、ユニコード記号もコードの一部として使用できることです。AtomエディタでJunoを使用している場合、上記のようにTeXコードを自動補完することができます。このコンテキストでは、$\\in$は単なる記号ではなく、実際に要素がセットに属しているかを表しています。\nコード julia\u0026gt; X = Set([1,2,3,1]); print(X)\rSet([2, 3, 1])\rjulia\u0026gt; X[1]\rERROR: MethodError: no method matching getindex(::Set{Int64}, ::Int64)\rStacktrace:\r[1] top-level scope at REPL[23]:1\rjulia\u0026gt; for i in X print(i) end\r231 上記のコードは、セット $X$ を $X := \\left\\{ 1, 2, 3, 1 \\right\\} = \\left\\{ 2,3,1 \\right\\}$ として定義することを意味します。数学でのセットと同じように、重複と順序の概念は存在しません。したがって、最初のインデックスを参照するとエラーが出ます。しかし、データ型自体はPythonと同様にイテラブルであるため、ループ内で使用することができます。\njulia\u0026gt; if 1∈X print(\u0026#34;!\u0026#34;) else print(\u0026#34;?\u0026#34;) end\r!\rjulia\u0026gt; if 0∈X print(\u0026#34;!\u0026#34;) else print(\u0026#34;?\u0026#34;) end\r?\rjulia\u0026gt; if 0∉X print(\u0026#34;!\u0026#34;) else print(\u0026#34;?\u0026#34;) end\r!\rjulia\u0026gt; if [1,2] ⊆ X print(\u0026#34;!\u0026#34;) else print(\u0026#34;?\u0026#34;) end\r!\rjulia\u0026gt; if [0,1,2] ⊆ X print(\u0026#34;!\u0026#34;) else print(\u0026#34;?\u0026#34;) end\r? これが数式を読むような親しみを感じるなら、セットデータ型を有効に使用する準備ができていることです。特に注意すべき点は、上記のような計算がセットデータ型に限定されて動作するわけではないことです。つまり、リストに対しても同様の操作を気軽に使用することができます。セットデータ型が不慣れであっても、セットにだけ慣れていれば、Juliaのセット演算子を利用することに問題はありません。なお、包含関係は \\subset $\\subset$ ではなく \\subseteq $\\subseteq$ を使用する必要があります。\njulia\u0026gt; Y = [\u0026#34;1\u0026#34;,\u0026#34;2\u0026#34;,3]\r3-element Array{Any,1}:\r\u0026#34;1\u0026#34;\r\u0026#34;2\u0026#34;\r3\rjulia\u0026gt; ∪(X,Y)\rSet{Any} with 5 elements:\r\u0026#34;1\u0026#34;\r2\r3\r\u0026#34;2\u0026#34;\r1\rjulia\u0026gt; ∩(X,Y)\rSet{Int64} with 1 element:\r3\rjulia\u0026gt; ∩(Y,X)\r1-element Array{Any,1}:\r3\rjulia\u0026gt; setdiff(X,Y); X\rSet{Int64} with 3 elements:\r2\r3\r1\rjulia\u0026gt; setdiff!(X,Y); X\rSet{Int64} with 2 elements:\r2\r1 基本的にセットを扱うため、和集合と交差点も数式と同様に表現することができます。2行目と3行目の違いは操作の順序です。$X$ はJuliaでセットデータ型、$Y$はただの配列として定義されており、返される値は最初の引数のデータ型に従います。このような違いは、Juliaのような強い型付け言語では非常に重要であるため、必ず理解しておく必要があります。差集合は、setdiff()はただの差集合を返し、setdiff!()はセット自体を更新するという違いがあります。\n環境 OS: Windows julia: v1.5.0 ","id":1442,"permalink":"https://freshrimpsushi.github.io/jp/posts/1442/","tags":null,"title":"ジュリアでの集合データ型と演算子"},{"categories":"수리통계학","contents":"定義 1 確率変数$X$とある正の数$h\u0026gt;0$に対して、$E(e^{tX})$が$-h\u0026lt; t \u0026lt; h$で存在するならば、$M(t) = E( e^{tX} )$を$X$のモーメント生成関数と定義する。\n説明 モーメント生成関数(mgf)は、数理統計学で比較的早い段階に出会う概念だが、その見慣れない定義と文脈のない導入は、時としてこの分野を嫌いにさせる要因となる。mgfを理解する難しさは、教科書がその定義と応用に直行し、読者が何であるかは知っているが、その形式や目的はまったくわからないままになるからだ。基本的に、「モーメント生成関数」という語は、\u0026lsquo;モーメント\u0026rsquo;と\u0026rsquo;生成関数\u0026rsquo;を組み合わせて作られる。忙しい読者のためにポイントをまとめると以下の通り：\nモーメントが何かを理解する必要はない：基本的にモーメントは平均や分散などを包含する抽象的な概念である。モーメントは、次数に応じて適切な操作を加えることで意味のある統計量になることができるが、それ自体では統計的な意味を持たない。無理に何かの統計量と結びつけることなく、モーメントそのものとして十分に理解できる。 モーメント生成関数は単なる生成関数の一種である：生成関数は、多項式関数を一般的な形で表したものに過ぎない。モーメント生成関数がモーメントを生成する関数であるという説明も悪くないが、モーメント生成関数が生成関数の一つとして、その係数がモーメントであることを知っておけば、その性質をより正確に理解できる。 モーメント生成関数をマクローリン展開で解くと次のようになる。[ 注意: 定義で$-h\u0026lt;t\u0026lt;h$を収束半径として設定する理由がここにある。] $$ \\begin{align*} M(t) =\u0026amp; E(e^{tX}) \\\\ =\u0026amp; 1 + E(tX) + {{E(t^2 X^2)} \\over {2!}} + \\cdots \\end{align*} $$ 期待値は線形性を持つので、以下のように$t$に対する生成関数として表現できる。 $$ M(t) = 1 + E(X) t+ {{E( X^2) t^2 } \\over {2!}} + \\cdots $$ $t^k$項の係数は、$k$次モーメントの定数倍である$\\displaystyle {{E(X^{k})} \\over {k!}} $であることに注意しよう。これで、両辺を$t$に対して$n$回微分し、$t=0$を代入すると $$ M^{(n)} (0) = E(X^{n}) $$ したがって、関数$M$はモーメントを生成すると言え、このためにモーメント生成関数と呼ばれると見なしても問題ない。$M$が定義で直接与えられていなかったり、生成関数に対する言及のみがあった場合、理解するのがずっと簡単だったであろう。\n一方、確率変数$X$と$Y$に関するモーメント生成関数$M_{X}$と$M_{Y}$がそれぞれ存在するとしよう。モーメントは、統計学で我々が究極的に知りたい統計量を得るために考案された概念である。そして、すべての項のモーメントが同じであれば、その$X$と$Y$は同じ分布に従うと言えるだろう。この定理に従えば、モーメント生成関数が存在する分布同士であれば、モーメント生成関数自体を分布と同じ概念として比較することが許される。実際、分布関数は確率を表すのには便利だが、分布自体を扱うのにはそれほど良くない。その代わりに、モーメント生成関数がこのような性質を持つため、どのような確率変数がどのような分布に従うかを数式的に議論する際に最も頻繁に使用される。\n要約 $X$と$Y$を、それぞれモーメント生成関数$M_{X}$、$M_{Y}$および累積分布関数$F_{X}$、$F_{Y}$を持つ確率変数としよう。すべての$ z \\in \\mathbb{R}$に対して$F_{X} (z) = F_{Y}(z)$が真であることと、ある$h\u0026gt;0$とすべての$t \\in (-h,h)$に対して$M_{X}(t) = M_{Y}(t)$が真であることは同等である。\n$\\mathbb{R}$は実数集合を意味する。 Hogg et al. (2013). Introduction to Mathematical Statistics(7th Edition): p59.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":248,"permalink":"https://freshrimpsushi.github.io/jp/posts/248/","tags":null,"title":"積率母関数とは何か？"},{"categories":"줄리아","contents":"概要 ジュリアは、R、パイソン、マトラボの利点が混在する言語だ。配列はプログラミングの基本であり、その利用で複数の言語の痕跡が見られる。\nコード 行列 julia\u0026gt; M = [1. 2. ; 3. 4.]\r2×2 Array{Float64,2}:\r1.0 2.0\r3.0 4.0\rjulia\u0026gt; size(M)\r(2, 2)\rjulia\u0026gt; length(M)\r4 行列の場合、ほぼマトラボの文法で定義され、使われる。size()関数はマトラボと同じように使用され、パイソンのnumpyパッケージのプロパティ.shapeに相当する機能をする。length()はマトラボと異なり、全要素の数を返す。\n二次配列 julia\u0026gt; x = [[1,2,3,4] for _ in 1:4]; x\r4-element Array{Array{Int64,1},1}:\r[1, 2, 3, 4]\r[1, 2, 3, 4]\r[1, 2, 3, 4]\r[1, 2, 3, 4] 配列内にループを配置して使用することは、パイソンでよく見られる方法だ。これにより、Rのrep()関数などを似たように再現できる。\nスライシング julia\u0026gt; y = [3,2,5,1,4]\r5-element Array{Int64,1}:\r3\r2\r5\r1\r4\rjulia\u0026gt; y[[4,2,1,5,3]]\r5-element Array{Int64,1}:\r1\r2\r3\r4\r5\rjulia\u0026gt; y[3:end]\r3-element Array{Int64,1}:\r5\r1\r4\rjulia\u0026gt; y[3:4] .= -1; y\r5-element Array{Int64,1}:\r3\r2\r-1\r-1\r4 インデキシングは、Rと似ており、インデックスの配列を与えると、その順序で出力する。配列の最後のインデックスをendで表現することから、スライシングはマトラボからの影響があると分かる。最後に、.=を使って、3、4番目の要素に-1を直接代入することも、マトラボに似ている。\nインデキシング julia\u0026gt; x = [1 2; 3 4]\r2×2 Array{Int64,2}:\r1 2\r3 4\rjulia\u0026gt; x[1,:]\r2-element Array{Int64,1}:\r1\r2\rjulia\u0026gt; x[[1],:]\r1×2 Array{Int64,2}:\r1 2\rjulia\u0026gt; x[1,1] = -1; x\r2×2 Array{Int64,2}:\r-1 2\r3 4 特殊なのは、インデキシングの方法によって、結果が異なる可能性があることだ。概念的には、同じものを入れても、結果が同じになると思うが、入れる時に要素が入れば、結果も要素として得られ、配列が入れば、結果も配列として得ることができる。これはジュリアを使いにくくするが、同時に高度な機能を実装するのに大いに役立つ。\n環境 OS: Windows julia: v1.5.0 ","id":1437,"permalink":"https://freshrimpsushi.github.io/jp/posts/1437/","tags":null,"title":"ジュリアにおける配列のスライシングとインデックス化"},{"categories":"수리통계학","contents":"定義 1 二つの確率変数 $X, Y$に対して、次のように定義された$\\rho = \\rho (X,Y)$をピアソン相関係数Pearson Correlationと呼ぶ。 $$ \\rho = { {\\text{Cov} (X,Y)} \\over {\\sigma_X \\sigma_Y} } $$\n$\\sigma_{X}$、$\\sigma_{Y}$はそれぞれ $X$、$Y$の標準偏差だ。 説明 ピアソン相関係数(Pearson) Correlation Coefficientは、二つの変数がお互いに**（線形）相関関係**を持っているかを確認する尺度になる。$1$や$–1$に近ければ相関関係があると見なし、$0$ならばないとされる。\n相関関係と独立は同じ概念ではないことに注意が必要だ。相関関係は、二つの変数が直線形のグラフを描くかだけを確認する。相関関係がないとしても必ずしも独立とは限らない。しかし、独立であれば相関関係がないと言える。この逆が成立するのは二つの変数が正規分布に従う場合に限られる。\n性質 ピアソン相関係数は$[-1,1]$を超えない。つまり、 $$ – 1 \\le \\rho \\le 1 $$\n証明 証明は二つの方法を紹介したい。\nコーシー・シュワルツの不等式を用いた証明 $$ \\rho = { {\\text{Cov} (X,Y)} \\over {\\sigma_X \\sigma_Y} } = {1 \\over n} \\sum_{k=1}^{n} { \\left( { { x_k - \\mu_{X} } \\over {\\sigma_X} } \\right) \\left( { { y_k - \\mu_{Y} } \\over {\\sigma_Y} } \\right) } $$ 両辺を二乗すると $$ \\rho ^2 = {1 \\over {n^2} } \\left\\{ \\sum_{k=1}^{n} { \\left( { { x_k - \\mu_{X} } \\over {\\sigma_X} } \\right) \\left( { { y_k - \\mu_{Y} } \\over {\\sigma_Y} } \\right) } \\right\\} ^ 2 $$\nコーシー・シュワルツの不等式： $$ ({a}^{2}+{b}^{2})({x}^{2}+{y}^{2})\\ge { (ax+by) }^{ 2 } $$\nコーシー・シュワルツの不等式により、 $$ {1 \\over {n^2} } \\left\\{ \\sum_{k=1}^{n} { \\left( { { x_k - \\mu_{X} } \\over {\\sigma_X} } \\right) \\left( { { y_k - \\mu_{Y} } \\over {\\sigma_Y} } \\right) } \\right\\} ^ 2 \\le {1 \\over {n^2} } \\sum_{k=1}^{n} { \\left( { { x_k - \\mu_{X} } \\over {\\sigma_X} } \\right) ^ 2 } \\sum_{k=1}^{n} { \\left( { { y_k - \\mu_{Y} } \\over {\\sigma_Y} } \\right) ^ 2 } $$ 右辺を整理すると、 $$ \\begin{align*} \u0026amp; {1 \\over {n^2} } \\sum_{k=1}^{n} { \\left( { { x_k - \\mu_{X} } \\over {\\sigma_X} } \\right) ^ 2 } \\sum_{k=1}^{n} { \\left( { { y_k - \\mu_{Y} } \\over {\\sigma_Y} } \\right) ^ 2 } \\\\ =\u0026amp; {1 \\over { {\\sigma_X}^2 {\\sigma_Y}^2 } } \\sum_{k=1}^{n} { \\left( { { x_k - \\mu_{X} } \\over { \\sqrt{n} } } \\right) ^ 2 \\sum_{k=1}^{n} \\left( { { y_k - \\mu_{Y} } \\over {\\sqrt{n}} } \\right) ^ 2 } \\\\ =\u0026amp; {1 \\over { {\\sigma_X}^2 {\\sigma_Y}^2 } } {\\sigma_X}^2 {\\sigma_Y}^2 \\\\ =\u0026amp; 1 \\end{align*} $$ $\\rho ^2 \\le 1$であるため、 $$ -1 \\le \\rho \\le 1 $$\n■\n共分散の定義を用いた証明 $\\text{Var}(Y)={ \\sigma _ Y }^2, \\text{Var}(X)={ \\sigma _ X }^2$、$\\displaystyle Z= \\frac { Y }{ \\sigma _Y } - \\rho \\frac { X }{ \\sigma _X }$を共分散の定義とすると、 $$ \\begin{align*} \\text{Var}(Z)\u0026amp;=\\frac { 1 }{ { \\sigma _ Y }^2 }\\text{Var}(Y)+\\frac { { \\rho ^ 2 } }{ { \\sigma _ X }^2 }\\text{Var}(X)-2\\frac { \\rho }{ { \\sigma _X } { \\sigma _Y } }\\text{Cov}(X,Y) \\\\ =\u0026amp; \\frac { 1 }{ { \\sigma _ Y }^2 }{ \\sigma _ Y }^2+\\frac { { \\rho ^ 2 } }{ { \\sigma _ X }^2 }{ \\sigma _ X }^2-2\\rho \\cdot \\rho \\\\ \u0026amp;=1+{ \\rho ^ 2 }-2{ \\rho ^ 2 } \\\\ \u0026amp;=1-{ \\rho ^ 2 } \\end{align*} $$ $\\text{Var}(Z)\\ge 0$であるから、 $$ \\begin{align*} 1-{ \\rho ^ 2 }\\ge 0 \\implies\u0026amp; { \\rho ^ 2 }-1\\le 0 \\\\ \\implies\u0026amp; (\\rho +1)(\\rho –1)\\le 0 \\\\ \\implies\u0026amp; -1\\le \\rho \\le 1 \\end{align*} $$\n■\nHogg et al. (2013). Introduction to Mathematical Statistics (7th Edition): p104.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":57,"permalink":"https://freshrimpsushi.github.io/jp/posts/57/","tags":null,"title":"ピアソン相関係数"},{"categories":"수리통계학","contents":"定義と性質 平均がそれぞれ$\\mu_{X}$、$\\mu_{Y}$である確率変数$X$と$Y$について、$\\text{Cov} (X ,Y) : = E \\left[ ( X - \\mu_{X} ) ( Y - \\mu_{Y} ) \\right]$は$X$と$Y$の共分散Covarianceと定義される。共分散は以下の性質を持っている。\n[1]: $\\text{Var} (X) = \\text{Cov} (X,X)$ [2]: $\\text{Cov} (X,Y) = \\text{Cov} (Y, X)$ [3]: $\\text{Var} (X + Y) = \\text{Var} (X) + \\text{Var} (Y) + 2 \\text{Cov} (X,Y)$ [4]: $\\text{Cov} (X + Y , Z ) = \\text{Cov}(X,Z) + \\text{Cov}(Y,Z)$ [5]: $\\text{Cov} (aX + b , cY + d ) = ac \\text{Cov}(X,Y)$ 説明 共分散は二つの変数の線形の相関関係を示し、分散と異なり$0$はもちろん負の値も取り得る。\n証明 [1] $$ \\begin{align*} \\text{Cov} (X ,X) =\u0026amp; E[ ( X - \\mu_{X} ) ( X - \\mu_{X} ) ] \\\\ =\u0026amp; E[ ( X - \\mu_{X} )^2 ] \\\\ =\u0026amp; \\text{Var} (X) \\end{align*} $$\n■\n[2] $$ \\begin{align*} \\text{Cov} (X ,Y) =\u0026amp; E[ ( X - \\mu_{X} ) ( Y - \\mu_{Y} ) ] \\\\ =\u0026amp; E[ ( Y - \\mu_{Y} ) ( X - \\mu_{X} ) ] \\\\ =\u0026amp; \\text{Cov} (X ,Y) \\end{align*} $$\n■\n[3] $$ \\begin{align*} \\text{Var} (X + Y) =\u0026amp; E [ ( X + Y - \\mu_{X} - \\mu_{Y} )^2 ] \\\\ =\u0026amp; E \\left[ \\left\\{ ( X - \\mu_{X} ) + (Y - \\mu_{Y} ) \\right\\} ^2 \\right] \\\\ =\u0026amp; E \\left[ ( X - \\mu_{X} )^2 + 2 ( X - \\mu_{X} ) (Y - \\mu_{Y} )+ (Y - \\mu_{Y} )^2 \\right] \\\\ =\u0026amp; E[ ( X - \\mu_{X} )^2] + 2 E [ ( X - \\mu_{X} ) (Y - \\mu_{Y} ) ] + E [ (Y - \\mu_{Y} )^2 ] \\\\ =\u0026amp; \\text{Var} (X) + 2 \\text{Cov} (X,Y) + \\text{Var} (Y) \\end{align*} $$\n■\n[4] $$ \\begin{align*} \\text{Cov} (X + Y , Z ) =\u0026amp; E \\left[ ( X + Y - \\mu_{X} - \\mu_{Y} ) ( Z - \\mu_{Z} ) \\right] \\\\ =\u0026amp; E \\left[ \\left\\{ ( X - \\mu_{X} ) + ( Y - \\mu_{Y} ) \\right\\} ( Z - \\mu_{Z} ) \\right] \\\\ =\u0026amp; E \\left[ ( X - \\mu_{X} ) ( Z - \\mu_{Z} ) \\right] + E \\left[ ( Y - \\mu_{Y} ) ( Z - \\mu_{Z} ) \\right] \\\\ =\u0026amp; \\text{Cov}(X,Z) + \\text{Cov}(Y,Z) \\end{align*} $$\n■\n[5] $$ \\begin{align*} \\text{Cov} (aX + b , cY + d ) =\u0026amp; E \\left[ ( aX + b - a \\mu_{X} - b ) ( cY + d - c \\mu_{Y} - d ) \\right] \\\\ =\u0026amp; E \\left[ ( aX - a \\mu_{X} ) ( cY - c \\mu_{Y} ) \\right] \\\\ =\u0026amp; E \\left[ a c ( X - \\mu_{X} ) ( Y - \\mu_{Y} ) \\right] \\\\ =\u0026amp; ac E \\left[( X - \\mu_{X} ) ( Y - \\mu_{Y} ) \\right] \\\\ =\u0026amp; ac \\text{Cov}(X,Y) \\end{align*} $$\n■\n併せて見る 共分散行列 $\\Sigma$ ","id":425,"permalink":"https://freshrimpsushi.github.io/jp/posts/425/","tags":null,"title":"共分散の様々な性質"},{"categories":"수리통계학","contents":"まとめ 平均 $E ( X ) = \\mu_{X}$ と分散 $\\text{Var} (X) = E [ ( X - \\mu_{X} )^2 ]$ は以下の性質を持っている。\n[1]: $E(X + Y) = E(X) + E(Y)$ [2]: $E(aX + b) = a E(X) + b$ [3]: $\\text{Var} (X) \\ge 0$ [4]: $\\text{Var} ( X ) = E(X^2) - \\mu_{X}^2$ [5]: $\\text{Var} (aX + b) = a^2 \\text{Var} (X)$ 説明 平均と分散に関するものなので、非常に重要な性質だ。特に[1]と[2]はいわゆる線形性Linearityと呼ばれる性質で、式を扱う時にとても便利にしてくれる。\n証明 [1] 離散的な場合 $$ \\begin{align*} E ( X + Y ) =\u0026amp; \\sum (xp(x) + yp(y) ) \\\\ =\u0026amp; \\sum xp(x) + \\sum yp(y) \\\\ =\u0026amp; E(X) + E(Y) \\end{align*} $$ 連続的な場合 $$ \\begin{align*} E ( X + Y ) =\u0026amp; \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} (x + y) f(x,y) dx dy \\\\ =\u0026amp; \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} x f(x,y) dx dy + \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} y f(x,y) dx dy \\\\ =\u0026amp; E(X) + E(Y) \\end{align*} $$\n■\n[2] 離散的な場合 $$ \\begin{align*} E ( aX + b ) =\u0026amp; \\sum \\left( a x p(x) + b p(x) \\right) \\\\ =\u0026amp; a \\sum x p(x) + b \\sum p(y) \\\\ =\u0026amp; a E(X) + b \\end{align*} $$ 連続的な場合 $$ \\begin{align*} E ( aX + b ) =\u0026amp; \\int_{-\\infty}^{\\infty} (ax+b) f(x) dx \\\\ =\u0026amp; \\int_{-\\infty}^{\\infty} a xf(x) dx + \\int_{-\\infty}^{\\infty} b f(x) dx \\\\ =\u0026amp; a \\int_{-\\infty}^{\\infty} xf(x) dx + b \\int_{-\\infty}^{\\infty} f(x) dx \\\\ =\u0026amp; a E(X) + b \\end{align*} $$\n■\n[3] $( X - \\mu_{X} )^2 \\ge 0$ であるから $\\text{Var} (X) = E [ ( X - \\mu_{X} )^2 ] \\ge 0$\n■\n[4] $$ \\begin{align*} \\text{Var} (X) =\u0026amp; E [ ( X - \\mu_{X} )^2 ] \\\\ =\u0026amp; E (X^2 - 2 \\mu_{X} X + \\mu_{X}^2 ) \\\\ =\u0026amp; E (X^2) - 2 \\mu_{X} E(X) + \\mu_{X}^2 \\\\ =\u0026amp; E(X^2) - \\mu_{X}^2 \\end{align*} $$\n■\n[5] 定理 [2] によると $Y = a X + b$ だったら $\\mu_{Y} = a \\mu_{X} + b$ で $$ \\begin{align*} \\text{Var} (Y) =\u0026amp; E [ ( Y - \\mu_{Y} )^2 ] \\\\ =\u0026amp; E [ ( aX + b - a \\mu_{X} - b )^2 ] \\\\ =\u0026amp; E [ a^2 ( X - \\mu_{X} )^2 ] \\\\ =\u0026amp; a^2 \\text{Var} (X) \\end{align*} $$\n■\n","id":424,"permalink":"https://freshrimpsushi.github.io/jp/posts/424/","tags":null,"title":"平均と分散の性質들"},{"categories":"수리통계학","contents":"要約 データ $X = \\left\\{ x_{1} , \\cdots , x_{n} \\right\\}$ が与えられているとしよう。\n0: $\\displaystyle h(\\theta)=\\sum_{i=1}^{n} {|x_i - \\theta|}^{0}$ を最小にする $\\theta$ は $$ \\argmin_{\\theta} h \\left( \\theta \\right) = \\text{mode}(X) $$ 1: $\\displaystyle h(\\theta)=\\sum_{i=1}^{n} {|x_i - \\theta|}^{1}$ を最小にする $\\theta$ は $$ \\argmin_{\\theta} h \\left( \\theta \\right) = \\text{median}(X) $$ 2: $\\displaystyle h(\\theta)=\\sum_{i=1}^{n} {|x_i - \\theta|}^{2}$ を最小にする $\\theta$ は $$ \\argmin_{\\theta} h \\left( \\theta \\right) = \\text{mean}(X) $$ 説明 線形代数の用語で難しく言うと次のようになる:\n上記の定理は、なぜ特定の値が代表値として考えられるのかについての数理的根拠となる。特に2の場合、分散を最小化する代表値が平均であることを含意しており、これまでの\u0026lsquo;なぜ分散をこのように定義するのか\u0026rsquo;についての答えになり得るだろう。\n証明 最頻値 戦略: $l^{0}$-ノルムは不等である度合いではなく、不等な数をカウントするノルムである。\n$$ \\left| x_{i} - \\theta \\right|^{0} := \\begin{cases} 1 \u0026amp; , \\theta \\ne x_{i} \\\\ 0 \u0026amp; , \\theta = x_{i} \\end{cases} $$ したがって、$\\displaystyle h(\\theta)=\\sum_{i=1}^{n} {|x_i - \\theta|}^{0} = 1 + 0 + 1 + \\cdots 1+ 1$ を最小化する $\\theta$ は $\\text{mode}(X)$\n■\n中央値 戦略: 絶対値の定義に従って、まずは計算が簡単になるように分解する。データの最大項と最小項を一つずつペアにして、未知数を消去し、定数項にする。そうすると、最後に最小にしなければならない未知数項が簡単になる。\n$ x_{(1)} \\le x_{(2)} \\le \\cdots \\le x_{(n)}$ とする。\nパート1. $\\theta \\in [x_{(1)} , x_{(n)} ]$ $\\theta \u0026lt; x_{(1)}$ と仮定すると、すべての $x_{(i)}$ より $\\theta$ が小さいので $$ h(\\theta)=\\sum_{i=1}^{n} {\\left( x_{(i)} - \\theta \\right) } \u0026gt; \\sum_{i=1}^{n} { \\left( x_{(i)} - x_{(1)} \\right) } $$ $ x_{(n)} \u0026lt; \\theta$ と仮定すると、すべての $x_{(i)}$ より $\\theta$ が大きいので $$ h(\\theta)=\\sum_{i=1}^{n} { \\left( \\theta - x_{(i)} \\right) } \u0026gt; \\sum_{i=1}^{n} { \\left( x_{(n)} - x_{(i)} \\right) } $$ したがって、$\\theta$ が具体的に何であれ、まずは$\\theta \\in [x_{(1)} , x_{(n)} ]$ でなければならない。\nパート2.\n$\\theta_{0} \\in [x_{(1)} , x_{(n)} ]$ に対して $$ \\begin{align*} h(\\theta_{0}) =\u0026amp; \\sum_{i=1}^{n} | x_{(i)} - \\theta_{0} | \\\\ =\u0026amp; \\sum_{i=2}^{n-1} | x_{(i)} - \\theta_{0} | + ( \\theta_{0} - x_{(1)} ) + ( x_{(n)} - \\theta_{0} ) \\\\ =\u0026amp; \\sum_{i=2}^{n-1} | x_{(i)} - \\theta_{0} | + ( x_{(n)} - x_{(1)} ) \\end{align*} $$\n$\\theta_{1} \\in [x_{(2)} , x_{(n-1)} ] \\subset [x_{(1)} , x_{(n)} ]$ に対して $$ \\begin{align*} h(\\theta_{1}) =\u0026amp; \\sum_{i=1}^{n} | x_{(i)} - \\theta_{1} | \\\\ =\u0026amp; \\sum_{i=2}^{n-1} | x_{(i)} - \\theta_{1} | + ( x_{(n)} - x_{(1)} ) \\\\ =\u0026amp; \\sum_{i=3}^{n-2} | x_{(i)} - \\theta_{1} | + ( x_{(n-1)} - x_{(2)} ) + ( x_{(n)} - x_{(1)} ) \\end{align*} $$\nこのように適切な $\\theta_{k} \\in [x_{(1+k)} , x_{(n-k)} ]$ を選ぶたびに、$( x_{(n-k)} - x_{(1+k)} )$ がシグマの外に出ることができる。これらの項はデータ $X$ によって決定されているため、定数項であり、便宜上これらの和を次のように示そう。 $$ C_{k} : = \\sum_{j=0}^{k} \\left( x_{(n-j)} - x_{(j+1)} \\right) $$\nパート3.\nケース3-1. $ n$ が奇数の場合\nパート2. に従い $$ \\begin{align*} h ( \\theta ) =\u0026amp; \\sum_{i=1}^{n} | x_{(i)} - \\theta | \\\\ =\u0026amp; \\sum_{i=1+k}^{n-k} | x_{(i)} - \\theta | + C_{k} \\\\ =\u0026amp; \\left| x_{\\left( {{n+1} \\over {2}} \\right)} - \\theta \\right| + C_{{{n-1} \\over {2}} - 1} \\end{align*} $$ したがって、$h( \\theta )$ が最小になる値は $\\theta = x_{\\left( {{n+1} \\over {2}} \\right)}$ である。 ケース3-2. $ n$ が偶数の場合\nパート2. に従い $$ \\begin{align*} h ( \\theta ) =\u0026amp; \\sum_{i=1}^{n} | x_{(i)} - \\theta | \\\\ =\u0026amp; \\sum_{i=1+k}^{n-k} | x_{(i)} - \\theta | + C_{k} \\\\ =\u0026amp; \\left| x_{\\left( {{n} \\over {2}} \\right)} - \\theta \\right| + \\left| x_{\\left( {{n} \\over {2}} + 1 \\right)} - \\theta \\right| + C_{{{n} \\over {2}} - 2} \\end{align*} $$ この場合、すべての $\\displaystyle \\theta \\in \\left[ x_{ \\left( {{n} \\over {2}} \\right)} , x_{ \\left( {{n} \\over {2}} + 1 \\right)} \\right]$ は $h ( \\theta )$ を最小にする。 結局、$n$ が偶数であろうと奇数であろうと、$h ( \\theta)$ を最小にする $\\theta$ は $X$ の中央値である。\n■\n平均 戦略: 微分によって簡単に導出できる。\n$$ {{ d } \\over { d \\theta }} \\sum_{i=1}^{n} \\left( x_{i} - \\theta \\right) = \\sum_{i=1}^{n} 2 \\left( x_{i} - \\theta \\right) = 0 $$ 上記の式を満たす $\\theta$ が $\\displaystyle h(\\theta)=\\sum_{i=1}^{n} {|x_i - \\theta|}^{2}$ を最小化するため $$ \\displaystyle\\sum_{i=1}^{n} 2 \\left( x_{i} - \\theta \\right) = 0 \\implies \\sum_{i=1}^{n} x_{i} = n \\theta \\implies \\theta = {{ 1 } \\over { n }} \\sum_{i=1}^{n} x_{i} $$\n■\n同じく見る 統計学の三つの代表値：最頻値、中央値、平均 ","id":49,"permalink":"https://freshrimpsushi.github.io/jp/posts/49/","tags":null,"title":"代表値の数理的性質の証明"},{"categories":"알고리즘","contents":"定義 グリーディアルゴリズムは、その瞬間のみを考慮して最も良い選択をする方法だ。\n説明 グリードアルゴリズムは、名前が示す通り、長期的な視点を持たずにその瞬間だけを考える。良いことには、常に最善を尽くそうとするが、大局を見たときにこれは賢明ではないかもしれない。次の例を見てみよう：\n左の0から始まって右の1に到達するパスを見つける問題を想像してみよう。ただ見つけるだけではなく、最も少ないノードを経由するという条件で、最適解は 0-A-C-E-1(3) になるだろう。しかし、グリーディアルゴリズムは、その瞬間に最も遠くへ行ける道を選ぶ。そのため、この問題をグリーディアルゴリズムで解いた場合、解は 0-B-D-F-G-H-1(5) となり、最適解と比較して2回のコストが余分に発生する。最初の選択が0-Bでなかったり、D-FではなくD-Eを選んでいたら、もっと良い解を選べる可能性があった。\nこのように、グリーディアルゴリズムは、ほとんどのコンテキストで「良いアルゴリズム」というよりは「愚かな方法」の可能性が高い。しかし、アルゴリズムを動員するほど難しい問題の場合、上の例のように一目で済ませられるほど簡単ではないだろう。そのため、通常、グリーディアルゴリズムを使ってナイーブに解いた後、改良されたアルゴリズムを開発する。\n","id":1434,"permalink":"https://freshrimpsushi.github.io/jp/posts/1434/","tags":null,"title":"グリーディアルゴリズム"},{"categories":"수리통계학","contents":"定義: 期待値、平均、分散 確率変数 $X$ が与えられたとする。\n連続確率変数$X$ の確率密度関数$f(x)$ が $\\displaystyle \\int_{-\\infty}^{\\infty} |x| f(x) dx \u0026lt; \\infty$ を満たす場合、以下のように定義された $E(X)$ を$X$ の期待値Expectationと言う。 $$ E(X) := \\int_{-\\infty}^{\\infty} x f(x) dx $$\n離散確率変数$X$ の確率質量関数$p(x)$ が $\\displaystyle \\sum_{x} |x| p(x) \u0026lt; \\infty$ を満たす場合、以下のように定義された $E(X)$ を$X$ の期待値Expectationと言う。 $$ E(X) := \\sum_{x} x p(x) $$\n$\\mu = E(X)$ が存在する場合、$X$ の 平均Meanと定義する。\n$\\sigma^2 = E((X - \\mu)^2)$ が存在する場合、$X$ の 分散Varianceと定義する。\n抽象化の意味 統計学を簡単な言葉で一言で言い表すなら、「だから平均的にどうなの？」を研究する学問だと言えるかもしれない。「平均」は代表値としてかなり直感的で、計算も簡単な統計量である。しかし、この世にある様々な現象を説明するには、そんな単純なレベルでは足りず、「期待値」という形で抽象化される。期待値は離散分布だけでなく区分求積法の発想を借りて連続分布にも対応する概念となる。この文章で議論するのはまさにその「抽象化」についての話である。\n数理統計学は確かに統計理論に関する内容を含んでいるが、学問の性質を見たとき、本質的には数学の一分野に近い。従って、他の数学の分野のように数学者と同じマインドセットで数理統計学を理解しようとする努力が必要である。数学者の仕事の一つは、直感や既に世に出ている理論と矛盾せず、厳格な定義を考案して、万物、対象物、現象、またそれらを超えたすべてを記号に変え、そのすべてをその場で研究できるようにすることである。\n平均と分散の定義を見ると、直感的な意味は全く残っておらず、単に確率変数に期待値を取った形で表されている。定義からこれらがどのように平均や実際につながるか、どのように分散と実際につながるかには関心がない。むしろ、平均と分散という概念を当たり前のように受け入れているほど習熟した学習者が、記号に逆らって適応することが期待される。実際、数理統計学を学ぶレベルの学習者であれば、それらの定義に大きな疑問を持つことはないだろう。\n平均の抽象化と期待値という概念が定着すると、学者たちはより多くの可能性を発見する。単純な合計や積分といった限界を超え、基本的な代数学や解析学の理論を導入してこれらを扱い始めたのである。自然と、学者たちの関心は抽象化から一般化へと移行する。\n定義: モーメント 自然数 $m$ に対して、$E( X^m )$ を$X$ の**$m$番目のモーメント**$m$th Momentと定義する。\n一般化の意味 モーメントの定義を読むだけで即座に分かる事実は、第一モーメント、つまり$E(X^1)$ が$X$ の平均になるということである。もう少し考えると、第二モーメントである$E(X^2)$ も少しの操作を通じて分散になることもわかる。\n平均は第一モーメント、分散は第二モーメントと実質的に同じ概念と見ることができる。逆に考えれば、モーメントの中で第一が平均に対応し、第二が分散に対応すると言えるだろう。\nそれでは、直感を持つ学者はもちろん、第三や第四のモーメントも何か重要な情報を与えることができるだろうと推測することになる。[ NOTE: 具体的には、これを歪度、尖度と言う。] 今では研究の方向性が逆転し、明らかになった事実を理論で説明するのではなく、理論で隠された事実を見つけ出そうとするものになっている。\nこのような方法論は、統計学だけでなく、自然科学全般で容易に見ることができる。数理統計学は統計よりも数学に近いといえるが、学問の存在意義に近づくと再び統計学の姿を取り戻す。モーメントMomentという言葉自体は物理学などでも使用されるが、統計学ではモーメントがどのような意味を持つか考える必要はない。ただ、数理統計学を支える理論を説明する際に使用する言葉として知っておけば十分である。\n一方で、期待値の存在条件である$\\displaystyle \\int_{-\\infty}^{\\infty} |x| f(x) dx \u0026lt; \\infty$、つまり$x$ でわざわざ絶対値を取る必要があるのか疑問に思うかもしれない。存在を保証することと具体的な値を計算することは別としても、$X$ の期待値$E(X)$ は別の式を使う必要がないように見えるからである。\nこの点については、次の定理を見れば少しは役立つかもしれない。最も単純な$X$ の議論ではなく、$g(X)$ への一般化まで考慮すると、むしろ$E(X)$ こそ恒等関数 $g(x) = x$ に対する特別なケースとして定義されたと見ることもできるだろう。\n定理 確率変数$X$ について、$Y$ が何らかの関数$g$ に対して$Y := g(X)$ のように表れるとする。\n[1]: $X$ が確率密度関数$f_{X}$ を持つ連続確率変数で、$\\displaystyle \\int_{-\\infty}^{\\infty} |g(x)| f_{X} (x) dx \u0026lt; \\infty$ を満たす場合、 $$ E (Y) = \\int_{-\\infty}^{\\infty} g(x) f_{X} (x) dx $$ [2]: $X$ が確率質量関数$p_{X}$ を持つ離散確率変数で、$\\displaystyle \\sum_{x} |g(x)| p_{X} (x) \u0026lt; \\infty$ を満たす場合、 $$ E (Y) = \\sum_{x} g(x) p_{X} (x) $$ 参照 代表値としての平均 測度論で定義される期待値 ","id":246,"permalink":"https://freshrimpsushi.github.io/jp/posts/246/","tags":null,"title":"数理統計学における期待値、平均、分散、モーメントの定義"},{"categories":"수리통계학","contents":"定義 1 標本空間 $\\Omega$ で 確率 $P$ が定義されているとする。\n定義域が標本空間の関数 $X : \\Omega \\to \\mathbb{R}$ を 確率変数Random Variableと呼ぶ。確率変数の値域 $X(\\Omega)$ を 空間Spaceとも呼ぶ。 以下を満たす関数 $F_{X} : \\mathbb{R} \\to [0,1]$ を $X$ の累積分布関数(Cumulative Distribution Function, cdf) とする。 $$ F_{X}(x) = P_{X}\\left( (-\\infty,x] \\right) = P \\left( \\left\\{ \\omega \\in \\Omega : X(\\omega) \\le x \\right\\} \\right) $$ 離散 D1: 確率変数 $X$ の空間が可算集合ならば $X$ を 離散確率変数Discrete Random Variableと呼び、離散確率分布に従うとする。 D2: 以下を満たす $p_{X} : \\mathbb{R} \\to [0,1]$ を離散確率変数 $X$ の確率質量関数(Probability Mass Function, pmf) と呼ぶ。 $$ p_{X}(x) := P\\left( X=x \\right) $$ D3: $\\mathcal{S}_{X} := \\left\\{ x \\in \\mathbb{R} : p_{X}(x) \u0026gt; 0 \\right\\}$ を $X$ のサポートSupportと呼ぶ。 連続 C1: 確率変数 $X$ の累積分布関数 $F_{X}$ が全ての $x \\in \\mathbb{R}$ で連続ならば $X$ を 連続確率変数Continuous Random Variableと呼び、連続確率分布に従うとする。 C2: 以下を満たす関数 $f_{X} : \\mathbb{R} \\to [0,\\infty)$ を連続確率変数 $X$ の確率密度関数(Probability Density Function, pdf) と呼び、$X$ が 絶対連続Absolutely Continuousであるとする。 $$ F_{X}(x) = \\int_{-\\infty}^{x} f_{X}(t) dt $$ C3. $\\mathcal{S}_{X} := \\left\\{ t \\in \\mathbb{R} : f_{X}(t) \u0026gt; 0 \\right\\}$ を $X$ のサポートSupportと呼ぶ。 解説 サポート 、または 支持集合 は、簡単に言えば、私たちが興味を持つ部分だけを選び出した集合である。よく使われる表現ではないが、確率論が何を表現したいのかを確かに伝える。確率は確定的な何かに関心がなく、確率が $0$ とは決して起こらないということなので、無関心で良い。だから $\\mathcal{S}$ は「本当に重要な集合」や「私たちが知るべき集合」と見なせるようになり、限られたエネルギーを $\\Omega$ 全体ではなく $\\mathcal{S}$ にだけ注ぐことができるようになる。\n高校で確率に触れたときも、教師が「確率変数は関数だ」と強調した記憶があるだろう。しかし、それとは別に、本当に確率変数を関数として考えて扱うことは、もう少し高いレベルの抽象化能力を必要とする。ここで紹介されている定義はまだ数学的に厳密ではないが、集合と関数で確率の概念を描写することは簡単ではない。わからないからといって絶望することもなければ、わかったと思って軽く見ることもない。\n定義を読めば、離散確率変数と連続確率変数には本質的な違いがあり、それが形式的な違いにもつながることがわかる。学部生レベルでは混乱することもあるかもしれないが、連続確率変数を扱うときのみヤコビアンが付くことをしっかり理解しておこう。\n要約 サポート $\\mathcal{S}_{X}$ を持つ連続確率変数 $X$ と微分可能な単射関数 $g$ に対して、確率変数 $Y$ を $Y:=g(X)$ のように定義すると、$Y$ の確率密度関数は $y \\in \\mathcal{S}_{Y}$ に関して次のように求められる。[ 注: 実際には$g$ は全単射とは仮定されていないため、逆関数 $g^{-1}$ の存在が常に保証されるわけではない。] $$ f_{Y} (y) = f_{X} \\left( g^{-1}(y) \\right) \\left| {{ d x } \\over { d y }} \\right| $$\nここで $\\mathcal{S}_{Y}$ は $Y$ のサポート、$x$ は $x = g^{-1}(y)$ を意味する。 証明 $g$ は単射で連続なので、増加関数か減少関数である。ケースを分けて考えよう。\nケース 1. $g$ が増加関数の場合 $$ \\begin{align*} F_{Y}(y) =\u0026amp; P \\left( Y \\le y \\right) \\\\ =\u0026amp; P \\left( g(X) \\le y \\right) \\\\ =\u0026amp; P \\left( X \\le g^{-1}(y) \\right) \\\\ =\u0026amp; F_{X}\\left( g^{-1}(y) \\right) \\end{align*} $$ 微積分の基本定理により、$Y$ の確率密度関数は $$ \\begin{align*} f_{Y}(y) =\u0026amp; {{ d } \\over { d y }} F_{Y}(y) \\\\ =\u0026amp; {{ d } \\over { d y }} \\int_{-\\infty}^{x} f_{X}(t) dt \\\\ =\u0026amp; {{ d } \\over { d x }} \\int_{-\\infty}^{x} f_{X}(t) dt {{ d x } \\over { d y }} \\\\ =\u0026amp; f_{X} \\left( x \\right) {{ d x } \\over { d y }} \\\\ =\u0026amp; f_{X} \\left( g^{-1} (y) \\right) {{ d x } \\over { d y }} \\end{align*} $$ $g$ が増加関数なので $\\displaystyle {{ d x } \\over { d y }} = {{ d g^{-1}(y) } \\over { d y }} \u0026gt;0$ であり、したがって $$ {{ d x } \\over { d y }} = \\left| {{ d x } \\over { d y }} \\right| $$\nケース 2. $g$ が減少関数の場合 $$ \\begin{align*} F_{Y}(y) =\u0026amp; P \\left( Y \\le y \\right) \\\\ =\u0026amp; P \\left( g(X) \\le y \\right) \\\\ =\u0026amp; P \\left( X \\le g^{-1}(y) \\right) \\\\ =\u0026amp; 1- F_{X}\\left( g^{-1}(y) \\right) \\end{align*} $$ 同様に $\\displaystyle f_{Y}(y) = - f_{X} \\left( g^{-1} (y) \\right) {{ d x } \\over { d y }}$ である。$g$ が減少関数なので $\\displaystyle {{ d x } \\over { d y }} \u0026lt; 0$ であり、したがって $$ - {{ d x } \\over { d y }} = \\left| {{ d x } \\over { d y }} \\right| $$\n■\n厳密な定義 測度論で定義される確率変数と確率分布 測度論で定義される累積分布関数 測度論で定義される離散確率分布 測度論における絶対連続 Hogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p32~41.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1433,"permalink":"https://freshrimpsushi.github.io/jp/posts/1433/","tags":null,"title":"数理統計学における確率変数と確率分布"},{"categories":"확률론","contents":"定義 距離空間 $S$ の ボレルシグマ場 $\\mathcal{S}:= \\mathcal{B}(S)$ に関して 可測空間 $(S,\\mathcal{S})$ を定義しよう。\n確率空間 $(\\Omega, \\mathcal{F}, P)$ で定義された 確率変数 $X$ と 確率過程 $\\left\\{ X_{n} \\right\\}_{n \\in \\mathbb{N}}$ が $n \\to \\infty$ のとき、すべての $f \\in C_{b}(S)$ に対して以下を満たす場合、$X$ は分布収束するConverge in Distributionと言い、$X_{n} \\overset{D}{\\to} X$ と示される。 $$ \\int_{\\Omega} f(X_{n}) dP \\to \\int_{\\Omega} f(X) dP $$\n$C_{b}(S)$ は、$S$ で定義される有界連続関数の集合を表す。 $$ C_{b}(S) := \\left\\{ f:S \\to \\mathbb{R} \\mid f\\text{ is bounded and continuous} \\right\\} $$ 定理 [1]: $(S,\\mathcal{S})$ で定義された 確率測度の 列 $\\left\\{ P_{n} \\right\\}_{n \\in \\mathbb{N}}$ がすべての ボレル集合 $B \\in \\mathcal{B} \\left( \\mathbb{R} \\right)$ に対して $$ P_{n} \\left( X^{-1} (B) \\right) := P \\left( X_{n}^{-1} (B) \\right) $$ を満たすように定義されている場合、次のことが成立する。 $$ X_{n} \\overset{D}{\\to} X \\iff P_{n} \\overset{W}{\\to} P $$ 2: $X_{n} \\overset{D}{\\to} X$ と、すべての $\\left\\{ X_{n} \\right\\}$ のサブシーケンス $\\left\\{ X_{n '} \\right\\} \\subset \\left\\{ X_{n} \\right\\}$ が $X_{n ''} \\overset{D}{\\to} X$ を満たす $\\left\\{ X_{n '} \\right\\}$ のサブシーケンス $\\left\\{ X_{n ''} \\right\\} \\subset \\left\\{ X_{n '} \\right\\}$ を持つことは同値である。数式で再び書くと、次のようになる。 $$ X_{n} \\overset{D}{\\to} X \\iff \\forall \\left\\{ X_{n '} \\right\\} \\subset \\left\\{ X_{n} \\right\\}, \\exists \\left\\{ X_{n ''} \\right\\} \\subset \\left\\{ X_{n '} \\right\\} : X_{n ''} \\overset{D}{\\to} X $$ [3] 連続写像定理: 可測関数 $h : (S , \\mathcal{S}) \\to (S ' , \\mathcal{S} ')$ について、$h$ が連続である点の集合 $C_{h}$、言い換えると $C_{h} : = \\left\\{ x \\in S : h \\text{ is continuous at } x \\right\\}$ を定義しよう。もし$X_{n} \\overset{D}{\\to} X$ かつ $P(X \\in C_{h}) = 1$ なら $h(X_{n}) \\overset{D}{\\to} h(X)$ である。数式で示すと、次のようになる。 $$ X_{n} \\overset{D}{\\to} X \\land P(X \\in C_{h}) = 1 \\implies h(X_{n}) \\overset{D}{\\to} h(X) $$ 説明 [1]: 定理で紹介したように、このように定義された $P_{n}$ は誘導された確率測度Induced Probability Measureと呼ばれる。確率 \u0026lsquo;変数\u0026rsquo;の収束と、$P_{n} \\overset{W}{\\to} P$ すなわち確率 \u0026lsquo;測度\u0026rsquo;の収束と区別されることが重要である。 [3]: 連続写像定理は、実際には確率収束やほぼ確実に収束に対しても一般化可能である。$h$ も関数であり、確率変数も関数である点から、合成関数 $h \\circ X$ の使用が自然に考えられるべきである。$A \\in \\mathcal{S} ' $ に対して、次の式が意味を成すかどうかを考え、納得する過程が必要である。 $$ P \\left( h(X)^{-1} (A) \\right) = P \\left( X \\in h^{-1}(A) \\right) $$ すべての $f \\in C_{b}(S)$ において分布が同じであるという表記は $\\overset{D}{=}$ を使うこともある。その定義は、すべての $A \\in \\mathcal{S} ' $ と連続関数 $h:S \\to S'$ に対して次のようである。 $$ h(X) \\overset{D}{=} h(Y) \\overset{\\text{def}}{\\iff} P \\left( h(X)^{-1}(A) \\right) = P \\left( h(Y)^{-1}(A) \\right) $$ 再び収束の表現を考えると、次のようになる。 $$ h\\left( X_{n} \\right) \\overset{D}{\\to} h(X) \\iff P \\left( h\\left( X_{n} \\right)^{-1}(A) \\right) \\to P \\left( h(X)^{-1}(A) \\right) $$ 証明 [1] すべての $f \\in C_{b}(S)$ に対して $$ \\begin{align*} P_{n} \\overset{W}{\\to} P \\iff \u0026amp; \\int_{S} f dP_{n} \\to \\int_{S} f dP \\\\ \\iff \u0026amp; \\int_{\\Omega} f(X_{n}) dP \\to \\int_{\\Omega} f(X) dP \\\\ \\iff \u0026amp; X_{n} \\overset{D}{\\to} X \\end{align*} $$\n■\n2 $(\\implies)$ $X_{n} \\overset{D}{\\to} X$ が成立しない場合 $$ \\int_{\\Omega} f(X_{n}) dP \\to \\int_{\\Omega} f(X) dP $$ $f \\in C_{b}(S)$ が存在すると仮定する。つまり、 $$ \\left| \\int_{\\Omega} f(X_{n '}) dP - \\int_{\\Omega} f(X) dP \\right| \u0026gt; \\varepsilon $$ を満たす $\\varepsilon \u0026gt; 0$ とサブシーケンスのインデックス $\\left\\{ n' \\right\\}$ が存在すると仮定するのだ。しかし、これは $$ \\int_{\\Omega} f(X_{n ''}) dP \\to \\int_{\\Omega} f(X) dP $$ を満たすサブシーケンスのインデックスのサブシーケンス $\\left\\{ n'' \\right\\}$ が常に存在するため矛盾である。\n$(\\impliedby)$ $\\left\\{ n'' \\right\\} = \\left\\{ n \\right\\}$ とすると、自明に成立する。\n■\n[3] $P_{X}$ を $X$ から誘導された確率測度、すなわち $P_{X}(A) := P \\left( X^{-1}(A) \\right) = P(X \\in A)$ としよう。 $$ \\overline{h^{-1}(B)} \\subset h^{-1}(B) \\cup C_{h}^{c} $$ $S'$ のすべての閉集合 $B$ に対して、上記の包含関係が成立する。任意の $x \\in \\overline{h^{-1}(B)}$ を考えると、$h$ が連続部分に対して閉包を保持して $h^{-1}(B)$ を含み、連続でない部分の逆像は $C_{h}^{c}$ を含むからである。閉包 $\\overline{h^{-1}(B)}$ は $S$ で閉集合であるので $$ \\begin{align*} \u0026amp; \\limsup_{n \\to \\infty} P \\left( h ( X_{n} ) \\in B \\right) \\\\ =\u0026amp; \\limsup_{n \\to \\infty} P \\left( X_{n} \\in h^{-1} (B) \\right) \\\\ =\u0026amp; \\limsup_{n \\to \\infty} P_{X} \\left( h ( X_{n} )^{-1}(B) \\right) \\\\ =\u0026amp; \\limsup_{n \\to \\infty} P_{X} \\left( \\left[ X_{n}^{-1} \\circ h^{-1} \\right] (B) \\right) \\\\ =\u0026amp; \\limsup_{n \\to \\infty} P_{X} \\left( X_{n}^{-1} \\left( h^{-1} (B) \\right) \\right) \\\\ =\u0026amp; \\limsup_{n \\to \\infty} P_{n} \\left( h^{-1} (B)\\right) \\\\ \\le \u0026amp; \\limsup_{n \\to \\infty} P_{n} \\left( \\overline{h^{-1} (B)} \\right) \\end{align*} $$\nポートマントー定理: 空間 $S$ が 距離空間 $( S , \\rho)$ でありながら 可測空間 $(S,\\mathcal{B}(S))$ でもあるとしよう。次のすべては同値である。\n(1): $P_{n} \\overset{W}{\\to} P$ (2): すべての有界な、一様連続関数 $f$ に対して $\\displaystyle \\int_{S} f dP_{n} \\to \\int_{S}f d P$ (3): すべての閉集合 $F$ に対して $\\displaystyle \\limsup_{n\\to\\infty} P_{n}(F) \\le P(F)$ (4): すべての開集合 $G$ に対して $\\displaystyle P(G) \\le \\liminf_{n\\to\\infty} P_{n}(G)$ (5): $P(\\partial A) = 0$ であるすべての $A$ に対して $\\displaystyle \\lim_{n\\to\\infty} P_{n}(A) = P(A)$ [1]に従って $X_{n} \\overset{D}{\\to} X$ であれば $P_{n} \\overset{W}{\\to} P_{X}$ であり、ポートマントー定理の $(1) \\implies (3)$ と仮定 $P_{X}(X \\in C_{h}^{c}) = 0$ によって $$ \\begin{align*} \\limsup_{n \\to \\infty} P_{X} \\left( h ( X_{n} )^{-1}(B) \\right) \\le \u0026amp; \\limsup_{n \\to \\infty} P_{n} \\left( \\overline{h^{-1} (B)} \\right) \\\\ \\le \u0026amp; P_{X} \\left( \\overline{h^{-1} (B)} \\right) \\\\ \\le \u0026amp; P_{X} \\left( h^{-1} (B) \\cup C_{h}^{c} \\right) \\\\ \\le \u0026amp; P _{X}\\left( h^{-1} (B) \\right) + P_{X} \\left( C_{h}^{c} \\right) \\\\ \\le \u0026amp; P_{X} \\left( h^{-1} (B) \\right) \\\\ \\le \u0026amp; P_{X} \\left( X^{-1} \\left( h^{-1} (B) \\right) \\right) \\\\ \\le \u0026amp; P_{X} \\left( \\left( h(X) \\right)^{-1} (B) \\right) \\end{align*} $$ 同じ方法で $\\displaystyle P_{X} \\left( \\left( h(X) \\right)^{-1} (B) \\right) \\le \\liminf_{n \\to \\infty} P_{X} \\left( h ( X_{n} )^{-1}(B) \\right)$ を示せば $$ \\lim_{n \\to \\infty} P_{X} \\left( h ( X_{n} )^{-1}(B) \\right) = P_{X} \\left( \\left( h(X) \\right)^{-1} (B) \\right) $$\n■\n参考文献 数理統計学で定義された分布収束 ほとんど確実に収束 $\\implies$ 確率収束 $\\implies$, 分布収束(弱収束) 更新情報\n2023年8月19日、リュデシクによる、定理 [1] ステートメントの誤字修正($P_{n} (A) := P \\left( X_{n}^{-1} (A)\\right)$ → $P_{n} \\left( X^{-1} (B) \\right) := P \\left( X_{n}^{-1} (B)\\right)$) ","id":1432,"permalink":"https://freshrimpsushi.github.io/jp/posts/1432/","tags":null,"title":"測度論によって定義される分布の収束"},{"categories":"수리통계학","contents":"定義 1 同じ条件下で繰り返しできる試行を無作為試行Random Experimentと呼ぶ。 無作為試行で得られる全ての結果Outcomeを集めた集合$\\Omega$を標本空間Sample Spaceと呼ぶ。 標本空間の中で我々が興味を持っている結果の集合、即ち$B \\subset \\Omega$を事象Eventと言い、これらの集合を$\\mathcal{B}$のように表す。 次の三つの条件を満たす関数$P : \\mathcal{B} \\to \\mathbb{R}$を確率probabilityと呼ぶ： (i): 全ての$B \\in \\mathcal{B}$に対して$P(B) \\ge 0$ (ii): 全体空間$\\Omega \\in \\mathcal{B}$に対して$P(\\Omega) = 1$ (iii) 確率の加法定理Additive Law of Probability：相互に素な事象の列$\\left\\{ B_{i} \\right\\}_{i=1}^{\\infty}$、即ち$n \\ne m \\implies B_{n} \\cap B_{m} = \\emptyset$の$\\left\\{ B_{i} \\right\\}$に対して $$ P \\left( \\bigcup_{i=1}^{\\infty} B_{i} \\right) = \\sum_{i=1}^{\\infty} P \\left( B_{i} \\right) $$ 説明 数理統計学だとしても、基本的にその概念自体は教育課程内の確率、大学レベルの確率論で使用するものと変わらない。理論の基礎がどうであれ、表現や論法が異なることはあっても概念は変わらない。集合と関数に圧倒されず、ゆっくりと説明を読んでみよう：\n事象と標本空間 高校レベルの確率統計と異なる点があるとすれば、もう少し積極的に集合を使用して確率という概念を描写するということだ。実際、大学レベルの数理統計学で扱う確率の概念でさえまだ「無作為試行」とか「興味を持っている」などの曖昧な表現が残っているが、初めて接する立場からするとこれでも厳格で難しく感じることがあるかもしれない。正常だから心配するな。\n人間の身長が正規分布に従うと仮定したら、標本空間 $\\Omega$ は実数集合$\\mathbb{R}$そのものになる。確かに身長は必ず正の値でなければならないだろうが、そのような不必要な厳密さは一旦置いておこう。それではある事象$B$とは、アダムAdamという男性の身長$x$を測定した時に、それを含む集合として表される。例えば$[172,190] \\subset \\Omega$は、身長を測定した時にそれが172以上かつ190以下である事象となる。この測定は、定義で説明された無作為試行であり、そのように測定された値$x$は結果であり、そのような結果として得られる全てのケースを集めたものが標本空間である。このような抽象化を理解できなくても、数理統計学を学ぶ上で大きな問題にはならないかもしれない。しかし、それだけで基盤が不安定になることを覚悟しなければならない。\n抽象化の次のステップは形式化である。事象$B \\subset \\Omega$が$\\Omega$のべき集合$\\mathscr{P}(\\Omega)$に属する。これらを集めた$\\mathcal{B}$について、いくつかの関係をチェックしてみよう。 $$ B \\subset \\Omega \\\\ \\mathcal{B} \\not\\subset \\Omega \\\\ B \\in \\mathscr{P}(\\Omega) \\\\ B \\in \\mathcal{B} \\\\ B \\notin \\Omega \\\\ \\mathcal{B} \\subset \\mathscr{P}(\\Omega) $$\n確率 このような複雑な表現を使用する理由は、確率（関数）$P$の定義域が標本空間$\\Omega$そのものではなく、事象でなければならないためである。高校レベルで言えば、アダムの身長が正確に181である（$x=181$）確率$\\displaystyle \\int_{181}^{181} f(x) dx = 0$は関係なく、180より大きく182より小さい（$180\u0026lt;x\u0026lt;182$）確率$\\displaystyle \\int_{180}^{182}f(x) dx \u0026gt; 0$のように計算すべきだと理解してもよい。確率はある事象の可能性を$0$から$1$までの数値で量る関数である。\n全体空間、つまり$\\Omega$に対して$P(\\Omega)=1$ということは直感的に言えば「何かが起こる確率は100%だ」ということになる。数式的には「確実に起こることよりも確実なことはない」と説明できるだろう。\n相互排他的事象 事象$B \\subset \\Omega$に対して次を満たす事象$A \\subset \\Omega$を$B$の相互排他的事象Exclusive Eventと呼ぶ。 $$ P \\left( B \\cap A \\right) = 0 $$ 相互排他的事象の明白な例には$\\emptyset$や$B^{C}$などがあるが、定義が正確に$B \\cap A = \\emptyset$を言っているわけではないことを覚えておく必要がある。どこまでも相互排他的事象は確率によって定義され、具体的に集合としてこれらがどのように見えるかは関係ない。\n厳密な定義 測度論によって厳密に定義された確率 Hogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p11.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1431,"permalink":"https://freshrimpsushi.github.io/jp/posts/1431/","tags":null,"title":"数理統計学における確率と確率の加法定理"},{"categories":"줄리아","contents":"코드 julia\u0026gt; x1=[1 2 3]\r1×3 Array{Int64,2}:\r1 2 3\rjulia\u0026gt; x2=[1, 2, 3]\r3-element Array{Int64,1}:\r1\r2\r3\rjulia\u0026gt; x3=[i for i in 1:3]\r3-element Array{Int64,1}:\r1\r2\r3\rjulia\u0026gt; x4=[i for i in 1:3:10]\r4-element Array{Int64,1}:\r1\r4\r7\r10\rjulia\u0026gt; x5=[i for i in 1:3:11]\r4-element Array{Int64,1}:\r1\r4\r7\r10 x1は2次元配列です。行ベクトルと同じように見えるため、成分座標を1つだけ入力すると、行ベクトルのように認識されます。x2, x3, x4, x5は1次元配列です。\nx=[i for i in n:m]と入力すると、$n$から$m$までの間隔が$1$の配列を返します。 x=[i for i in n:k:m]と入力すると、$n$から$m$までの間隔が$k$の配列を返します。 最後の要素は$m$以下で最も大きい数です。このようにリスト内にfor文を含めてリストを作ることを、Pythonなどでリスト内包List Comprehensionと言います。\njulia\u0026gt; x6=1:3\r1:3\rjulia\u0026gt; x7=1:3:10\r1:3:10\rjulia\u0026gt; x9=1:3:11\r1:3:10 上記のように書くと実際にはそうではないが、事実上x3, x4, x5のような配列が作成されたと考えても良いです。下の写真に示されるように、データタイプは異なるが上で作成されたものと同様に使用できます。\nrange(n,stop=m,length=k): これはマットラボでのlispace(n,m,k)と全く同じです。具体的な違いは以下の例のコードと結果を通じて確認しましょう。 julia\u0026gt; x9=range(1,stop=10)\r1:10\rjulia\u0026gt; x10=range(1,length=15)\r1:15\rjulia\u0026gt; x11=range(1,stop=10,length=15)\r1.0:0.6428571428571429:10.0\rjulia\u0026gt; x12=range(1,length=15,stop=10)\r1.0:0.6428571428571429:10.0 このコードも同様に、実際のデータタイプは異なるが事実上同じベクトルを生成すると考えても良いです。マットラボとは異なり、2番目、3番目の変数のうちの1つだけを入力することもでき、入力する順序を変えても構いません。\n最初の行 は、最初の要素が$1$、最後の要素が$10$のベクトルを返します。他に入力されたものがないので、要素間の間隔は$1$です。 二番目の行 は、最初の要素が$1$で、合計$15$個の要素を持つベクトルを返します。間隔は自動的に$1$となり、x=range(1,stop=15)あるいはx=1:15で作成されたベクトルと同じです。 三番目の行 は、最初の要素が$1$、最後の要素が$10$で、合計$15$個の要素を持つベクトルを返します。したがって、間隔は自動的に$9/14=0.6428571428571429$となります。これは整数ではないので、自然と実数要素を持つベクトルを返します。また、x=1.0:0.6428571428571429:10.0で作成されたものと同じです。 四番目の行 は、三番目の行で返されたベクトルと正確に同じベクトルを返します。 타언어 マットラボで等間隔の行ベクトルを生成する方法 환경 OS: Windows10 Version: 1.5.0 ","id":1452,"permalink":"https://freshrimpsushi.github.io/jp/posts/1452/","tags":null,"title":"ジュリアでベクターを生成する様々な方法"},{"categories":"줄리아","contents":"説明 circshifr(A, (n,m))を使用すると、配列Aの行を$n$カン下にシフトさせ、列を$m$カン右にシフトさせることができる。(n,m)は整数から成るタプルでなければならず、もちろん負の数も可能だ。負の数の場合は逆方向に平行移動される。\n3次元以上の配列の場合は、一番小さい2次元配列にそれぞれ適用される。\nコード 2次元配列 julia\u0026gt; A = transpose(reshape(1:25,5,5))\r5×5 LinearAlgebra.Transpose{Int64,Base.ReshapedArray{Int64,2,UnitRange{Int64},Tuple{}}}:\r1 2 3 4 5\r6 7 8 9 10\r11 12 13 14 15\r16 17 18 19 20\r21 22 23 24 25\rjulia\u0026gt; circshift(A, (-1,0))\r5×5 Array{Int64,2}:\r6 7 8 9 10\r11 12 13 14 15\r16 17 18 19 20\r21 22 23 24 25\r1 2 3 4 5\rjulia\u0026gt; circshift(A, (0,3))\r5×5 Array{Int64,2}:\r3 4 5 1 2\r8 9 10 6 7\r13 14 15 11 12\r18 19 20 16 17\r23 24 25 21 22\rjulia\u0026gt; circshift(A, (-1,3))\r5×5 Array{Int64,2}:\r8 9 10 6 7\r13 14 15 11 12\r18 19 20 16 17\r23 24 25 21 22\r3 4 5 1 2 高次元配列 julia\u0026gt; B = reshape(1:4*4*3,4,4,3)\r4×4×3 reshape(::UnitRange{Int64}, 4, 4, 3) with eltype Int64:\r[:, :, 1] =\r1 5 9 13\r2 6 10 14\r3 7 11 15\r4 8 12 16\r[:, :, 2] =\r17 21 25 29\r18 22 26 30\r19 23 27 31\r20 24 28 32\r[:, :, 3] =\r33 37 41 45\r34 38 42 46\r35 39 43 47\r36 40 44 48\rjulia\u0026gt; circshift(B,(-1,0))\r4×4×3 Array{Int64,3}:\r[:, :, 1] =\r2 6 10 14\r3 7 11 15\r4 8 12 16\r1 5 9 13\r[:, :, 2] =\r18 22 26 30\r19 23 27 31\r20 24 28 32\r17 21 25 29\r[:, :, 3] =\r34 38 42 46\r35 39 43 47\r36 40 44 48\r33 37 41 45\rjulia\u0026gt; circshift(B,(0,2))\r4×4×3 Array{Int64,3}:\r[:, :, 1] =\r9 13 1 5\r10 14 2 6\r11 15 3 7\r12 16 4 8\r[:, :, 2] =\r25 29 17 21\r26 30 18 22\r27 31 19 23\r28 32 20 24\r[:, :, 3] =\r41 45 33 37\r42 46 34 38\r43 47 35 39\r44 48 36 40\rjulia\u0026gt; circshift(B,(-1,2))\r4×4×3 Array{Int64,3}:\r[:, :, 1] =\r10 14 2 6\r11 15 3 7\r12 16 4 8\r9 13 1 5\r[:, :, 2] =\r26 30 18 22\r27 31 19 23\r28 32 20 24\r25 29 17 21\r[:, :, 3] =\r42 46 34 38\r43 47 35 39\r44 48 36 40\r41 45 33 37 julia\u0026gt; C = reshape(1:3*3*2*4,3,3,2,4)\r3×3×2×4 reshape(::UnitRange{Int64}, 3, 3, 2, 4) with eltype Int64:\r[:, :, 1, 1] =\r1 4 7\r2 5 8\r3 6 9\r[:, :, 2, 1] =\r10 13 16\r11 14 17\r12 15 18\r[:, :, 1, 2] =\r19 22 25\r20 23 26\r21 24 27\r[:, :, 2, 2] =\r28 31 34\r29 32 35\r30 33 36\r[:, :, 1, 3] =\r37 40 43\r38 41 44\r39 42 45\r[:, :, 2, 3] =\r46 49 52\r47 50 53\r48 51 54\r[:, :, 1, 4] =\r55 58 61\r56 59 62\r57 60 63\r[:, :, 2, 4] =\r64 67 70\r65 68 71\r66 69 72\rjulia\u0026gt; circshift(C,(1,1))\r3×3×2×4 Array{Int64,4}:\r[:, :, 1, 1] =\r9 3 6\r7 1 4\r8 2 5\r[:, :, 2, 1] =\r18 12 15\r16 10 13\r17 11 14\r[:, :, 1, 2] =\r27 21 24\r25 19 22\r26 20 23\r[:, :, 2, 2] =\r36 30 33\r34 28 31\r35 29 32\r[:, :, 1, 3] =\r45 39 42\r43 37 40\r44 38 41\r[:, :, 2, 3] =\r54 48 51\r52 46 49\r53 47 50\r[:, :, 1, 4] =\r63 57 60\r61 55 58\r62 56 59\r[:, :, 2, 4] =\r72 66 69\r70 64 67\r71 65 68 環境 OS: Windows10 バージョン: 1.5.3 (2020-11-09) ","id":1453,"permalink":"https://freshrimpsushi.github.io/jp/posts/1453/","tags":null,"title":"ジュリアで配列を平行移動する方法"},{"categories":"집합론","contents":"定義 1 集合 $A$ での関係 $\\le$ が反射的であり、推移的であり、反対称的なら、部分順序Partial Orderと言い、$(A,\\le)$ を部分順序集合と呼ぶ。$A$ が部分順序集合であるというのは、全ての要素 $a,b \\in A$ に対して下記を満たすことである。 $$ a \\le b \\land b \\le a \\implies a = b $$ 部分順序集合 $(A, \\le)$ が与えられた場合、全ての $a,b \\in A$ に対して $a \\le b$ か $b \\le a$ なら、$\\le$ を $A$ の全順序Total Orderとし、$(A,\\le)$ を全順序集合Totally Ordered Setという。 説明 定義では、$\\le$ は単なる記号であり、必ずしも大きさを比較する不等号である必要はない。もちろん、不等号や包含関係は部分順序になれるが、逆が成立するわけではない。例えば、アルファベットで a の次は b であり、単なる記号で $a \\le b$ のように示しても構わない。実際には、コンピュータ科学では a はアスキーコード $0000001_{(2)}$ に対応し、b はアスキーコード $00000010_{(2)}$ に対応し、このような2進数の大小関係で文字間の順序も表せる。\n実際、全順序集合は義務教育を受けた人なら容易に思い浮かべられるもので、そうでない集合がすぐに思い浮かばないかもしれない。全順序集合の良い例は自然数の集合 $\\mathbb{N}$ であり、これは整数の集合 $\\mathbb{Z}$、有理数の集合 $\\mathbb{Q}$、実数の集合 $\\mathbb{R}$ にも同じことが言える。しかし、複素数 $\\mathbb{C}$ になると、自然な順序は特に定義されていない。\n全順序を定義する前に部分順序を定義するのは、その方が数学的にはるかに自然だからである。以下の五つの集合を考えてみれば、これらは自然に部分順序を持つ。 $$ A = \\left\\{ 1 \\right\\} \\\\ B = \\left\\{ 1,2 \\right\\} \\\\ C = \\left\\{ 1,3 \\right\\} \\\\ D = \\left\\{ 1,2,3 \\right\\} \\\\ E = \\left\\{ 1,2,4 \\right\\} $$ 集合の包含関係を考えると、 $$ A \\subset B \\subset D \\\\ A \\subset B \\subset E \\\\ A \\subset C \\subset D $$ 図で見ると、この複雑な形を一目で確認できる。 こういった非線形の形は、私たちの日常生活でもよく見かけるほど自然な関係を表している。考え直してみれば、完全に線形に積み重なる関係が奇妙に思えるかもしれない。自然数の集合 $\\mathbb{N}$ でさえ、フォン・ノイマンの構成法に従えば、「直感的」とはちょっと違う。だから、全順序は単に集合全体で線形に定義される関係と言った方が理解しやすいかもしれない。\n이흥천 역, You-Feng Lin. (2011). 집합론(Set Theory: An Intuitive Approach): p289.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1421,"permalink":"https://freshrimpsushi.github.io/jp/posts/1421/","tags":null,"title":"部分順序集合"},{"categories":"위상수학","contents":"定義 1 距離空間 $(X,d)$ と $\\varepsilon\u0026gt;0$ が与えられたとする。\n全ての $x \\in X$ に対して $B_{d}(x,\\varepsilon) \\cap A_{\\varepsilon} \\ne \\emptyset$ を満たす有限集合 $A_{\\varepsilon} \\subset X$ を$X$ の**$\\varepsilon$-ネット**と言う。 全ての $\\varepsilon \u0026gt; 0$ に対して、$X$ の$\\varepsilon$-ネット $A_{\\varepsilon}$ が存在するなら、$X$ は完全有界と言う。 説明 完全有界空間は、一般にプリコンパクト空間とも呼ばれる。\n$\\varepsilon$-ネット $A_{\\varepsilon}$ をネットと言うのは、条件 $B_{d}(x,\\varepsilon) \\cap A_{\\varepsilon} \\ne \\emptyset$ を考慮した時、非常に直感的である。式を言葉に訳して考えると、与えられた空間 $X$ で任意の点を取っても、$A_{\\varepsilon}$ に引っかかるということだ。全ての点が許容誤差 $\\varepsilon$ の内側で引っかかるなら、これをネットと呼ぶのはかなり妥当である。\n完全有界 全ての $\\varepsilon\u0026gt;0$ に対して、有限カバーだけを考えても $X$ をカバーできるということは、$X$ が本当に小さくて扱いやすいという意味である。ある空間が完全有界であるということは、これを有限に分割して考えることができ、同時に距離空間であるから、その一つ一つも想像しやすい条件を備えているということである。\n$A_{\\varepsilon}$ が $\\varepsilon$-ネットになる条件 $B_{d}(x,\\varepsilon) \\cap A_{\\varepsilon} \\ne \\emptyset$ で既視感を感じるなら、自分が位相数学とかなり仲がいいと思ってもいい。この条件は、ある空間が可分空間であるかを判定する方法でもほぼ同じ形で現れる。実際に、密度性との概念上の違いは、有限か無限かの違いである。無限集合に対して条件を満たすより、有限集合に対して条件を満たす方が難しく、次の定理も当然成立する。\n定理 証明 1 有限集合は可算であるから、可分空間の定義から自明である。\n■\n2 列コンパクトを迂回して演繹する。\n■\nMunkres. (2000). Topology(2nd Edition): p275.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1420,"permalink":"https://freshrimpsushi.github.io/jp/posts/1420/","tags":null,"title":"完全有界空間"},{"categories":"줄리아","contents":"方法1 using LinearAlgebra\rusing Pkg\rPkg.add(\u0026#34;Plots\u0026#34;)\rPkg.add(\u0026#34;Distributions\u0026#34;)\rusing Plots 上のコードは、LinearAlgebraパッケージとPkgパッケージを読み込むこと、そして.add()関数を使ってPlots、Distributionパッケージをインストールするコードを示している。パッケージを読み込むキーワードusingは、数学である定理や論法を使う時に使う言葉に似ている。パッケージをインストールすること自体はPythonよりはRにもっと近いけど、使用法はPythonにもっと似ている。Rと同じようにパッケージ名をダブルクオートで囲む必要があり、一般的にパッケージ名はパスカルケース1で書き、-sをよく付けて複数形にすることが多く2混乱することがある。\n方法2 REPLで]を入力すると、上記のようにパッケージマネージャ環境に切り替わる。バックスペースを押すと、再びREPL環境に戻る。パッケージマネージャ環境でadd package_nameを入力すると、指定したパッケージがインストールされる。\n(@v1.5) pkg\u0026gt; add Plots Resolving package versions... Updating `C:\\Users\\rydbr\\.julia\\environments\\v1.5\\Project.toml` [91a5bcdd] + Plots v1.0.14 No Changes to `C:\\Users\\rydbr\\.julia\\environments\\v1.5\\Manifest.toml 単語の最初の文字を大文字で書く表示方法を言う。例のコードで確認できるように、linear algebraはLinearAlgebraのように各単語の最初の文字を大文字にして、スペースを省略する。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n例のコードで見るように、PlotとDistributionはPlotsとDistributionsとして呼ばなければならない。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1416,"permalink":"https://freshrimpsushi.github.io/jp/posts/1416/","tags":null,"title":"Juliaでパッケージをインストールして使用する方法"},{"categories":"위상수학","contents":"定義 1 位相空間 $(X,\\mathscr{T})$と部分集合 $A \\subset X$が与えられたとしよう。すると以下の集合 $$ \\mathscr{T}_{A} =\\left\\{ A\\cap U\\ :\\ U\\in \\mathscr{T} \\right\\} $$ は $A$上の位相である。このとき $\\mathscr{T}_{A}$を部分空間位相Subspace Topologyまたは相対位相と呼ぶ。また、位相空間 $(A, \\mathscr{T}_{A})$を $(X,\\mathscr{T})$の部分空間Subspaceと呼ぶ。\n定理 [0]: 位相空間 $(X, \\mathscr{T}$) と部分集合 $A \\subset X$に対して $$ \\mathscr{T}_{A} = \\left\\{ A\\cap U\\ :\\ U \\in \\mathscr{T}\\right\\} $$ は $A$上の位相となる。 位相空間 $X$と部分空間 $A$が与えられたとしよう。部分空間 $A$で開集合、閉集合の同値条件は以下のようである。\n[a1]: $V\\subset A$が $A$で開集合であるための必要十分条件は、$V= A\\cap U$を満たす $X$での開集合 $U$が存在することである。 [b1]: $F\\subset A$が $A$で閉集合であるための必要十分条件は、$F=A\\cap E$を満たす $X$での閉集合 $E$が存在することである。 部分空間で開集合（閉集合）であっても、全体空間で開集合（閉集合）である保証はない。部分空間が全体空間に対して開集合（閉集合）であれば、その性質が全体空間でも保証される。位相空間 $(X,\\mathscr{T})$と部分空間 $(A,\\mathscr{T}_{A})$、部分集合 $B\\subset A\\subset X$が与えられたとしよう。\n[a2]: $B$が部分空間 $A$で開集合であり、$A$が $X$で開集合であれば、$B$は $X$で開集合である。 [b2]: $B$が部分空間 $A$で閉集合であり、$A$が $X$で閉集合であれば、$B$は $X$で閉集合である。 [3]: $\\mathscr{B}$を位相空間 $(X,\\mathscr{T})$の基底としよう。すると $$ \\mathscr{B}_{A} =\\left\\{ A\\cap B\\ :\\ B\\in \\mathscr{B} \\right\\} $$ は部分空間 $(A,\\mathscr{T}_{A})$の基底である。 説明 混乱しないように、いくつかの表記についてはっきりと確認しておこう。$(X,\\mathscr{T})$が全体空間で $$ \\mathscr{T}_{A}=\\left\\{A\\cap U\\ :\\ U \\in \\mathscr{T} \\right\\} $$ は部分集合 $A$の位相である。つまり、部分空間 $(A,\\mathscr{T}_{A})$を形成する。$\\mathscr{B}$は全体集合 $X$の基底である。$\\mathscr{B}_{A}$は全体集合の基底の各要素と $A$の交差集合のコレクションである。これが部分集合 $A$の基底となるというのが定理の内容である。 $$ \\mathscr{T}_{\\mathscr{B}_{A}}=\\left\\{U_{A}\\subset A\\ :\\ \\forall\\ x \\in U_{A},\\ \\exists\\ (A\\cap B) \\in \\mathscr{B}_{A}\\ \\ \\text{s.t.}\\ x\\in (A\\cap B) \\subset U_{A}\\right\\} $$ さらに進んで、$\\mathscr{T}_{\\mathscr{B}_{A}}$が $\\mathscr{T}_{A}$と同じであることが核心である。内容が複雑に感じられるかもしれない。整理して説明すると次のようになる。\n全体空間 $X$の基底の要素と $A$を交差させたものを集めると、$A$の基底になる。 その基底 $\\mathscr{B}_{A}$で生成した位相は $\\mathscr{T}_{\\mathscr{B}_{A}}$である。 2で生成した位相 $\\mathscr{T}_{\\mathscr{B}_{A}}$は $X$の開集合と $A$を交差させたものの集合である $\\mathscr{T}_{A}$と同じである。 証明 [0] $(T1)$: $A \\cap \\varnothing =\\varnothing$、$A \\cap X=A$であるから、空集合と全体集合が $\\mathscr{T}_{A}$に属する。 $(T2)$: $V_\\alpha \\in \\mathscr{T}_{A}( \\alpha \\in \\Lambda)$としよう。$\\mathscr{T}_{A}$の定義により、各々の $V_\\alpha$に対して、$V_\\alpha = A \\cap U_\\alpha$を満たす $U_\\alpha$が存在する。位相の定義により $U=\\cup_{\\alpha \\in \\Lambda} U_\\alpha \\in \\mathscr{T}$である。そうすると $$ \\bigcup_{\\alpha \\in \\Lambda} V_\\alpha = \\bigcup_{\\alpha \\in \\Lambda} (A \\cap U_\\alpha ) =A\\cap (\\cup_{\\alpha \\in \\Lambda} U_\\alpha ) =A\\cap U \\in \\mathscr{T}_{A} $$ であるから $\\bigcup _{\\alpha \\in \\Lambda} V_\\alpha \\in \\mathscr{T}_{A}$である。 $(T3)$: $V_{1},\\ \\cdots\\ ,V_{n} \\in \\mathscr{T}_{A}$としよう。同様に、各々の $V_{i}$に対して、$V_{i} =A \\cap U_{i}$を満たす $U_{i}$が存在する。そして $U=\\cap _{i} U_{i} \\in \\mathscr{T}$であるから $$ \\bigcap _{i=1}^n V_{i} = \\bigcap_{i=1}^n (A\\cap U_{i}) = A\\cap \\left( \\bigcap_{i=1}^n U_{i} \\right) =A\\cap U \\in \\mathscr{T}_{A} $$ である。従って $\\bigcap_{i=1}^n V_{i} \\in \\mathscr{T}_{A}$である。 位相の条件 三つを満たすので $\\mathscr{T}_{A}$は $A$の上の位相である。\n■\n[a1] 距離空間での証明を参照せよ。$\\mathscr{T}_{A}$の定義により自明である。\n■\n[b1] $(\\implies)$ $F$が $A$で閉集合であるため、$A-F$は $A$で開集合である。そこで[a1]により、$A-F=A\\cap U$を満たす $X$での開集合 $U$が存在する。$U$が開集合であるため、$E=X-U$は $X$で閉集合である。それで $$ A\\cap E=A\\cap (X-U)=A-(A\\cap U)=A-(A-F)=F $$\n$(\\Longleftarrow )$ $E$は $X$で閉集合であるため、$X-E$は $X$で開集合である。そこで[a1]により、$A \\cap (X-E)$は $A$で開集合である。$F^c=A-(A\\cap E)=A\\cap(X-E)$であるから、$F ^c$は $A$で開集合である。従って、$F$は $A$で閉集合である。\n■\n[a2] $B$が $A$で開集合である場合、[a1]により、$B=A\\cap U\\ (U\\in \\mathscr{T})$を満たす $X$での開集合 $U$が存在する。仮定により、$A$は $X$で開集合である。従って、$B$は $X$で開集合の交差であり、$X$で開集合である。\n■\n[b2] $B$が $A$で閉集合である場合、[b1]により、$B=A\\cap E$を満たす $X$での閉集合 $E$が存在する。仮定により、$A$は $X$で閉集合であり、$B$は閉集合同士の交差であるため、$B$も $X$で閉集合である。\n■\n[3] パート1. $\\mathscr{B}_{A}$は $A$の基底である。\n[b1]: 任意の $x\\in A$に対して、$A\\subset X$であるから、$x\\in X$である。$\\mathscr{B}$が $X$の基底であるため、定義により、$x \\in B \\in \\mathscr{B}$を満たす $B$が存在する。従って、$x\\in (A\\cap B ) \\in \\mathscr{B}_{A}$を満たす $A\\cap B \\in \\mathscr{B}_{A}$が存在する。[b2]: 任意の $A\\cap B_{1}$、$A\\cap B_{2}$と $x\\in \\Big( (A\\cap B_{1} ) \\cap (A \\cap B_{2}) \\Big)$に対して $$ (A\\cap B_{1})\\cap (A \\cap B_{2})=A\\cap B_{1}\\cap B_{2} $$ であるため、$x\\in (B_{1}\\cap B_{2})$である。$\\mathscr{B}$が $X$の基底であるため、定義により、$x\\in B_{3} \\subset ( B_{1}\\cap B_{2})$を満たす $B_{3}$が存在する。従って $$ x \\in (A\\cap B_{3})\\subset \\Big( A\\cap (B_{1}\\cap B_{2}) \\Big)=(A\\cap B_{1}) \\cap (A\\cap B_{2}) $$ である。基底となる二つの条件を満たしたので、$\\mathscr{B}_{A}$は部分集合 $A$の基底である。\nパート2. $\\mathscr{T}_{\\mathscr{B}_{A}}=\\mathscr{T}_{A}$である。\n$(\\subset)$ $\\mathscr{B}$が $(X,\\mathscr{T})$の基底であるため、$\\mathscr{T}_{\\mathscr{B}}=\\mathscr{T}$であり、$\\mathscr{B}\\subset \\mathscr{T_{\\mathscr{B}}}=\\mathscr{T}$である。従って、全ての $B \\in \\mathscr{B}$に対して、$B\\in \\mathscr{T}$である。$\\mathscr{T}_{A}$の定義によって、$A\\cap B \\in \\mathscr{T}_{A}$である。従って $$ \\mathscr{B}_{A} \\subset \\mathscr{T}_{A} $$ $\\mathscr{T}_{\\mathscr{B}_{A}}$は $\\mathscr{B}_{A}$を含む最も小さい位相であるため $$ \\mathscr{T}_{\\mathscr{B}_{A}} \\subset \\mathscr{T}_{A} $$\n$(\\supset )$ $V \\in \\mathscr{T}_{A}$と仮定する。[a1]によって、$V=A\\cap U$を満たす $U\\in \\mathscr{T}$が存在する。また、$V$の任意の点 $x\\in V \\subset A$に対して、$x \\in U$である。$\\mathscr{B}$は$\\mathscr{T}$を生成する基底であるため、$x\\in B \\subset U$を満たす $B\\in \\mathscr{B}$が存在する。従って、$A \\cap B \\in \\mathscr{B}_{A}$が $$ x\\in (A\\cap B) \\subset (A\\cap U) =V $$ を満たす。これは、$V$がその $\\mathscr{B}_{A}$が生成する $A$上の位相 $\\mathscr{T}_{\\mathscr{B}_{A}}$に属する条件であるため、$V \\in \\mathscr{T}_{\\mathscr{B}_{A}}$である。従って $$ \\mathscr{T}_{\\mathscr{B}_{A}} \\supset \\mathscr{T}_{A} $$ ■\nMunkres. (2000). 『Topology(2nd Edition)』: p89.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1439,"permalink":"https://freshrimpsushi.github.io/jp/posts/1439/","tags":null,"title":"部分空間トポロジー、相対トポロジー"},{"categories":"측도론","contents":"定義 空間 $S$ が距離空間 $( S , \\rho)$ であり、かつ可測空間 $(S,\\mathcal{B}(S))$ であるとする。\n測度論 $S$ 上で定義される測度 $\\mu$ とその測度の列 $\\left\\{ \\mu_n \\right\\}_{n \\in \\mathbb{N}}$ が $n \\to \\infty$ のとき、全ての $f \\in C_{b}(S)$ に対して以下を満たすとき、$\\left\\{ \\mu_{n} \\right\\}$ が測度 $\\mu$ に弱く収束するConverge Weaklyと言い、$\\mu_{n}\\overset{W}{\\to}\\mu$ のように表される。 $$ \\int_{S} f d\\mu_{n} \\to \\int_{S} f d\\mu $$\n確率論 $S$ 上で定義される確率 $P$ と確率の列 $\\left\\{ P_n \\right\\}_{n \\in \\mathbb{N}}$ が $n \\to \\infty$ のとき、全ての $f \\in C_{b}(S)$ に対して以下を満たすとき、$\\left\\{ P_{n} \\right\\}$ が確率 $P$ に弱く収束するConverge Weaklyと言い、$P_{n}\\overset{W}{\\to}P$ のように表される。 $$ \\int_{S} f dP_{n} \\to \\int_{S} f dP $$\n$C_{b}(S)$ は $S$ で定義される有界連続関数の集まりを以下のように表す。 $$ C_{b}(S) := \\left\\{ f:S \\to \\mathbb{R} \\mid f\\text{ is bounded and continuous} \\right\\} $$ $\\displaystyle \\int_{S} f dP$ は簡単に $\\displaystyle Pf := \\int_{S} f dP$ のように表されることもある。 説明 測度の弱収束に関する代表的な応用は、確率論を挙げることができる。本ポストでは、主に統計学のバックグラウンドを持つ人が確率論に触れることを想定し、位相数学に不慣れな読者向けに説明した。バックグラウンドが数学であれば、位相数学の内容は気軽に読んで、確率論で何が必要かを探る感じで読めばいい。\n統計学 確率（測度）の弱収束とは、測度論で説明される分布収束と見なすことができる。数理統計学などで接する分布収束では、単変量確率変数の場合、$X_{n}$ の累積分布関数 $F_{X_{n}}$ が存在して、$X$ の累積分布関数 $F_{X}$ が連続となる全ての点 $x \\in X$ において以下を満たす場合、$X_{n} \\overset{D}{\\to} X$ とされた。 $$ \\lim_{n\\to\\infty} F_{X_{n}}(x) = F(x) $$ 全ての $f \\in C_{b}(S)$ に対して以下を満たす場合、弱収束するとされているが、上下の二つの式が似ていることに注目しよう。 $$ \\lim_{n\\to\\infty} P_{n} f = P f $$ 確率（測度）の列を難しく考えることなく、まずは直感的に接近しよう。式を見ると、$f$ が $P_{n}$ の重みを持って$P f$ に収束すると見ることができる。確率論で扱う関数とは、結局確率変数なのだが、これを $X_{n}\\equiv P_{n}f$、$X \\equiv Pf$ のように表せば、以下のように表せるかもしれない。 $$ X_{n} \\overset{D}{\\to}X \\overset{?}{\\iff} P_{n} \\overset{W}{\\to} P $$ 左の式で見えないものは、$X$ が連続となる全ての点 $x \\in S$ であり、右の式で見えないものは、有界連続の全ての関数 $f \\in C_{b}(S)$ だ。\n位相数学 弱収束について論じる上で、基本的な位相数学を必ず把握していなければならない。幸いにして、測度論で論じられる空間はかなり直感的で、距離空間くらいは知っていれば十分だ。空間 $S$ が距離空間 $( S , \\rho)$ であり、かつ $(S,\\mathcal{B}(S))$ が可測空間であるということは、$S$ で全ての $x \\in S$ と全ての $\\varepsilon\u0026gt;0$ に対して距離関数 $\\rho$ で作られる全てのオープンボール $B_{\\rho}(x , \\varepsilon) := \\left\\{ y \\in S : \\rho (x,y) \u0026lt; \\varepsilon \\right\\}$ を含む最小のシグマフィールド、つまりボレルシグマフィールド $\\mathcal{B}(S)$ をシグマフィールドとして持つ可測空間 $(S,\\mathcal{B}(S))$ となるということだ。\nオープンセットはオープンボールの合集であり、シグマフィールドの性質により、その補集合も全て$\\mathcal{B}(S)$に含まれるので、全てのクローズドセットも$\\mathcal{B}(S)$に属する必要がある。単純な例として、$S=\\mathbb{R}$ を考えてみよう。$\\mathcal{B}(\\mathbb{R})$ は以下のような開集合と閉集合を要素として持つ。 $$ \\emptyset, \\mathbb{R}, [0,7], (-\\infty, \\pi) , \\left\\{ 1 \\right\\}, (-1,1) \\cap (0,9) $$ それだけでなく、シグマフィールドの他の条件により、以下のように開集合と閉集合が混在する集合も要素として持つ。ここで例に挙げた集合が比較的直感的に作られていると感じることができれば良い。 $$ [0,1), (-\\infty,\\pi], [\\pi , \\infty) , \\left\\{ 1 \\right\\} \\cup (-3,-2) $$ まだ難しく感じ、定義を納得できない場合は、時間をかけてでも位相数学を勉強してみることをお勧めする。もちろん、確率論を勉強しようとして最初に位相数学の基礎を固めるのは非効率的かもしれない。しかし、効率を求めるのではなく、その経験自体が大きな助けになる。位相数学には非常に衝撃的で変態的な例がたくさんある。抽象数学の辛さを味わえば、距離空間のような良い空間を勉強することに感謝の気持ちを持つようになるだろう。もちろん、数学的なセンスも同時に鍛えることができる。\nこのトピックに関して、次の有用な定理を紹介する。\n定理 $P$ が $(S,\\mathcal{B}(S))$ で定義された確率だとしよう。すると、全ての $A \\in \\mathcal{B}(S)$ と $\\varepsilon\u0026gt;0$ に対して以下を満たす閉集合 $F_{\\varepsilon}$ と開集合 $G_{\\varepsilon}$ が存在する。 $$ F_{\\varepsilon}\\subset A \\subset G_{\\varepsilon} \\\\ P ( G_{\\varepsilon} \\setminus F_{\\varepsilon}) \u0026lt; \\varepsilon $$ 上の定理に基づいて、次の系を得ることができる。 $$ \\begin{align*} P(A) =\u0026amp; \\sup \\left\\{ P(F) : F \\in \\mathcal{B}(S) \\text{ is closed in S} \\right\\} \\\\ =\u0026amp; \\inf \\left\\{ P(G) : G \\in \\mathcal{B}(S) \\text{ is open in S} \\right\\} \\end{align*} $$\n言うまでもなく、どのような位相空間でも元々$A^{\\circ} \\subset A \\subset \\overline{A}$であったため、$A$よりも少し小さいオープンセット、少し大きいクローズドセットの存在は断言できるが、上のように$P ( G_{\\varepsilon} \\setminus F_{\\varepsilon}) \u0026lt; \\varepsilon$を満たしながら、少し小さいクローズドセットと少し大きいオープンセットが存在することは、思うほど当たり前ではない。これは、確率$P$が測度であり、連続であるという点を明確にした後に当たり前になる。\n実際、百マイル以上語るよりも、上の画像を見て理解する方が簡単だろう。この性質により、$A$の境界 $\\partial A$ の周りをぎりぎりで通りながら、$\\varepsilon \u0026gt; 0$が与えられるたびに十分に狭い帯を見つけることができる$F_{\\varepsilon}$と$G_{\\varepsilon}$が存在することになる。\n併せて見る ほぼ確実に収束する $\\implies$ 確率収束 $\\implies$ 分布収束(弱収束) [ヒルベルト空間での弱 ","id":1410,"permalink":"https://freshrimpsushi.github.io/jp/posts/1410/","tags":null,"title":"測度の弱収束"},{"categories":"측도론","contents":"要約 パイシステム$\\mathcal{P}$がラムダシステム$\\mathcal{L}$の部分集合である場合、$\\mathcal{P} \\subset \\sigma ( \\mathcal{P} ) \\subset \\mathcal{L}$を満たすシグマフィールド$\\sigma ( \\mathcal{P} )$が存在する。\n$\\sigma ( \\mathcal{P} )$は$\\mathcal{P}$の全ての要素を含む最小のシグマフィールドを示す。 説明 このステートメントだけ見ると非常に単純に見えるが、このような定理の証明はかなり長くて複雑だ。ここでパイシステム$\\mathcal{P}$とラムダシステム$\\mathcal{L}$の役割が何かを考えてみよう。\nパイシステムとラムダシステム:\n以下を満たす$\\mathcal{P}$を**$\\pi$-システム**という。 $$ A, B \\in \\mathcal{P} \\implies A \\cap B \\in \\mathcal{P} $$ 以下の条件を満たす$\\mathcal{L}$を**$\\lambda$-システム**という。 (i): $\\emptyset \\in \\mathcal{L}$ (ii): $A \\in \\mathcal{L} \\implies A^{c} \\in \\mathcal{L}$ (iii): 全ての$i \\ne j$に対して、$\\displaystyle A_{i} \\cap A_{j} = \\emptyset$の時、$\\displaystyle \\left\\{ A_{n} \\right\\}_{n \\in \\mathbb{N}} \\subset \\mathcal{L} \\implies \\bigcup_{n \\in \\mathbb{N}} A_{n} \\in \\mathcal{L}$ 測度論はその名の通り、何かを測る理論だから、どうしても可測空間を考えなくてはならないし、何をするにしてもシグマフィールドが必要だ。しかし、与えられた問題にぴったりのシグマフィールドを見つけるのは思ったほど簡単なことではないかもしれない。この問題のシグマフィールドがパイシステムより大きくなければならないがラムダシステムより大きくなってはいけないという制約がある場合、この定理を通して具体的なシグマフィールド$\\sigma ( \\mathcal{P} )$を得ることができるだろう。\nこれはシグマフィールドを見つける難しい問題に直面した時、比較的作りやすいパイシステムから始めてアプローチすることができるという意味になる。ラムダシステムはこのような簡単なパイシステムを含むように構築されなければならず、この作業は通常、「ただ何かのシグマフィールドを見つけること」より具体的で簡単だ。\n","id":1405,"permalink":"https://freshrimpsushi.github.io/jp/posts/1405/","tags":null,"title":"ディンキンのパイ-ラムダ定理"},{"categories":"확률론","contents":"確率収束の難しい定義 確率空間 $( \\Omega , \\mathcal{F} , P)$ が与えられたとしよう。\n確率変数のシーケンス $\\left\\{ X_{n} \\right\\}_{n \\in \\mathbb{N}}$ が確率変数 $X$ へ測度収束するならば、確率収束すると言い、$X_{n} \\overset{P}{\\to} X$として示される。\n測度論にまだ触れていなければ、確率空間という言葉を無視しても良い。 説明 $\\left\\{ X_{n} \\right\\}_{n \\in \\mathbb{N}}$ が $X$ に収束するというのは、全ての $\\varepsilon \u0026gt; 0$ に対して $$ \\lim_{n \\to \\infty} P \\left( \\left\\{ \\omega \\in \\Omega : | X_{n}(\\omega) - X(\\omega) | \\ge \\varepsilon \\right\\} \\right) = 0 $$ ということで、もう少し馴染みやすい形に変えると次のようになる。 $$ \\lim_{n \\to \\infty} P \\left( | X_{n}(\\omega) - X(\\omega) | \u0026lt; \\varepsilon \\right) = 1 $$ 確率変数のシーケンスは確率過程なので、確率過程論で有用に使えると推測できる。\n測度収束から受け継がれた確率収束の性質:\n[3] $X_{n}$ が $X$ へほとんど確実に収束するならば、確率収束する。 [4] $X_{n}$ が $X$ へ$\\mathcal{L}_{p}$ 収束するならば、確率収束する。 確率 $P$ は測度なので、測度収束の性質をそのまま受け継ぐ。\n参照 数理統計学で定義された確率収束 ほとんど確実に収束する $\\implies$ 確率収束 $\\implies$ 分布収束 $\\mathcal{L}_{p}$ 収束 $\\implies$ 確率収束 ","id":1397,"permalink":"https://freshrimpsushi.github.io/jp/posts/1397/","tags":null,"title":"測度論によって定義される確率の収束"},{"categories":"집합론","contents":"定義 1 任意の集合 $X$ に対して、次の性質を持つ $\\text{card} X$ を $X$ の濃度Cardinalityと定義する。\n(i): $X = \\emptyset \\iff \\text{card} X = 0$ (ii): $A \\sim B \\iff \\text{card} A = \\text{card} B$ (iii): 何らかの自然数 $k$ について、$X \\sim \\left\\{ 1 , 2, \\cdots , k \\right\\}$ ならば $\\text{card} X = k$ 特に、有限集合の濃度を有限濃度と言い、無限集合の濃度を超限濃度という。\n二つの集合 $A$、$B$ において、$A$ が $B$ の何らかの部分集合と等号であるが、$B$ は $A$ のどの部分集合とも等号でない場合、$\\text{card} A$ は $\\text{card} B$ より小さいと言って、以下のように表示される。 $$ \\text{card} A \u0026lt; \\text{card} B $$ 相互に素な二つの集合 $A$、$B$ がそれぞれ濃度 $a = \\text{card} A$、$b =\\text{card} B$ を持つ場合、その和集合の濃度を**$a$、$b$ の（濃度）和**と言い、以下のように表示される。 $$ \\text{card} \\left( A \\cup B \\right):= a+b $$ 二つの集合 $A$、$B$ がそれぞれ濃度 $a = \\text{card} A$、$b =\\text{card} B$ を持つ場合、そのデカルト積の濃度を**$a$、$b$ の（濃度）積**と言い、以下のように表示される。 $$ \\text{card} \\left( A \\times B \\right):= ab $$ 二つの集合 $A$、$B$ がそれぞれ濃度 $a = \\text{card} A$、$b =\\text{card} B$ を持つ場合、定義域 $A$ と値域 $B$ を持つ全ての関数の集合 $B^{A}$ の濃度を**$b$ の $a$ （濃度）乗**と言い、以下のように表示される。 $$ \\text{card} \\left( B^{A} \\right):= b^{a} $$ 説明 濃度は「集合のサイズ」を抽象化したもので、無限集合に対しても数学的に意味のある比較をするために導入されたと考えても差し支えない。集合のサイズから来た概念であるため、集合論が核でないか、便宜上 $|X| := \\text{card} X$ のように簡潔に表示されることもある。\n濃度は自然数に似た以下のような代数的性質を持つ。\n基本性質 1 $x,y,z$ を濃度と仮定する。\n[1]: $$|A| \\le |B| \\land |A| \\ge |B| \\implies |A| = |B|$$ [2]: $$x + y = y+x \\\\ (x+y) + z = x + (y + z)$$ [3]: $$xy = yx \\\\ (xy)z = x(yz) \\\\ x ( y+z) = xy + xz$$ [4]: $$z^{x} z^{y} = z^{x+y} \\\\ \\left( z^{y} \\right)^{x} = z^{yx}$$ 李興天 訳, You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p241.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1395,"permalink":"https://freshrimpsushi.github.io/jp/posts/1395/","tags":null,"title":"集合の濃度"},{"categories":"측도론","contents":"定義 測度空間 $( X , \\mathcal{E} , \\mu)$が与えられたとする。\nルベーグ積分可能な関数の集合 $\\Phi \\subset \\mathcal{L}^{1}$が与えられた時、全ての $\\varepsilon\u0026gt;0$ に対して、 $$ \\mu (E) \u0026lt; \\delta \\implies \\sup_{f \\in \\Phi} \\int_{ E } \\left| f \\right| d \\mu \u0026lt; \\varepsilon $$ を満たす $\\delta \u0026gt; 0$が存在するならば、$\\Phi$は一様積分可能であるという。\n説明 一様積分可能性は一様Uniformlyという言葉がついている通り、セットの概念で近づいていて、$\\Phi$に属していれば、どのような関数でもその$l_{1}$の値を同時に$\\varepsilon$より小さくすることができるようにする、つまり狭い$E$、または言い換えれば小さい$\\delta \u0026gt; \\mu (E)$が存在しなければならない。セットとしてこのように説明すると数学的には厳密かもしれないが、直感的には理解しにくい。関数の集合の例として、シーケンス$\\left\\{ f_{n} \\right\\}_{n=1}^{\\infty}$を考えれば、次のように説明するともっと理解しやすくなる。 $$ \\mu (E) \u0026lt; \\delta \\implies \\sup_{n \\in \\mathbb{N}} \\int_{ E } \\left| f_{n} \\right| d \\mu \u0026lt; \\varepsilon $$ しかし、この表示を嫌う理由は、シーケンスが結局はカウンタブルセットであるからだ。様々な理論の基礎になるべき実解析の立場では、可能性を過度に限定している感じは否めない。\n一方で、一様積分可能性を考える良い例としては、確率過程論の一様積分可能なマルチンゲールがある。\n","id":1392,"permalink":"https://freshrimpsushi.github.io/jp/posts/1392/","tags":null,"title":"一様可積分性"},{"categories":"집합론","contents":"要約 1 開区間 $(0,1)$ は非可算集合だ。\n証明 実数集合 $\\mathbb{R}$ は可算集合ではなく、これは実数集合と任意の可算集合の間に「一対一の対応」が存在しないことから示される。これは自然数集合と開区間 $(0,1)$ の間に一対一の対応が存在しないことを示し、それを系として得られる。\nカントールはこれを驚くべき方法で証明し、その方法は「対角線論法」という名前でカントールの成果として残った。結果を抜きにしても、その自体で美しさを感じることができる証明なので、何回読んでも理解できない場合でも、理解できるまで読み続けること。\n証明 一対一の対応 $f : \\mathbb{N} \\to (0,1)$ が存在すると仮定すると、${ a } _{ ij }$ は小数点以下$j$番目の数字で次のように表せる。 $$ f(i)=0. { a } _{ i1 } { a } _{ i2 } { a } _{ i3 } { a } _{ i4 } \\cdots $$ すると、自然数 $i \\in \\mathbb{N}$ に対して $$ f(1)=0. { a } _{ 11 } { a } _{ 12 } { a } _{ 13 } { a } _{ 14 } \\cdots \\\\ f(2)=0. { a } _{ 21 } { a } _{ 22 } { a } _{ 23 } { a } _{ 24 } \\cdots \\\\ f(3)=0. { a } _{ 31 } { a } _{ 32 } { a } _{ 33 } { a } _{ 34 } \\cdots \\\\ \\vdots \\\\ f(k)=0. { a } _{ k1 } { a } _{ k2 } { a } _{ k3 } { a } _{ k4 } \\cdots \\\\ \\vdots $$ のような配列で表せるだろう。ここで$z \\in (0,1)$ を次のように定義しよう。 $$ z=0. { z } _{ 1 } { z } _{ 2 } { z } _{ 3 } { z } _{ 4 } \\cdots, \\left( { z } _{ j } = \\begin{cases} 2 \u0026amp; { a } _{ jj } \\text{가 홀수일 때} \\\\ 1 \u0026amp; { a } _{ jj } \\text{가 짝수일 때} \\end{cases} \\right) $$ これは上の配列の対角線上にある数、$a_{11} , a_{22} , \\cdots$ たちと奇偶が反対の数を選ぶことだ。$z$ と$f(i)$ の小数点以下$i$番目の数字が奇数か偶数かだけ見てみよう。$ { z }_{ i }$ が偶数なら${ a } _{ ii }$ は奇数で、$\\ { z } _{ i }$ が奇数なら${ a } _{ ii }$ は偶数なので $$ z\\neq f(1) \\\\ z\\neq f(2) \\\\ z\\neq f(3) \\\\ \\vdots \\\\ z\\neq f(k) \\\\ \\vdots $$ になる。全ての自然数 $i$ に対して$z \\neq f(i)$ であり$z \\notin f(\\mathbb{N}) $ とすると、$f$ は一対一の対応なので$f(\\mathbb{N})=(0,1)$ であり$z \\in (0,1)$ のため$z\\in f(\\mathbb{N})$ でなければならない。これは仮定に矛盾するので、一対一の対応 $f : \\mathbb{N} \\to (0,1)$ は存在しない。\n■\n이흥천 역, You-Feng Lin. (2011). 집합론(Set Theory: An Intuitive Approach): p231.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":109,"permalink":"https://freshrimpsushi.github.io/jp/posts/109/","tags":null,"title":"カントールの対角線論法"},{"categories":"집합론","contents":"定義 1 集合 $X$ が有限集合あるいは $X \\sim \\mathbb{N}$ の場合、可算集合と呼ぶ。 可算集合でない集合を非可算集合と呼ぶ。 $\\mathbb{N}$ は自然数の集合だ。 $X \\sim Y$ の $\\sim$ は集合の等価を言う。 説明 可算集合という概念は、韓国人を含む東洋人にとって受け入れやすいわけではない。これは英語を含むインド・ヨーロッパ語族の思考方法と私たちのマインドが根本的に異なることによる。あるのは、ヨーロッパの言語は名詞に性があり、数や格に応じて動詞と形容詞が変化するなど、「共感しにくい」文法特性を持つことが多い。特にこの数については、文法に触れる前に、そもそもなぜ区分されるのか理解しにくいものが多いが、実はこの言語的思考方法の違いが東西の数学の違いとして表れる。\n可算、つまり「一つ、二つ、\u0026hellip;」と「いくつ」か数えることができる対象を言う。例えば人間や腕時計、オレンジなどがそうだ。数えられないものとしては、水やパンのように数ではなく量があり、任意に分割できるものを言う。だから A dogは犬一匹で、Dogsは複数の犬だが、Dogは犬肉を意味することになる。もちろん言葉には文脈があり、これほど極端に解釈はされないが、可能であるという話だ。\nこの違いをなぜ区別するかを理解させるよりも、私たちも変なものを使っていると説明する方がよかった。例えば、韓国を含む東アジア国家は「単位」というまったく役に立たなさそうな言葉を使う。例えば人は人、動物は匹、長く細いものは本、薄く広いものは枚、建物は棟などだ。韓国語は必ずこのように使い、英語ではこれらの表現を全く使わないという意味ではなく、思考の基底でこれを自然に受け入れているという点が重要だ。\nつまり、鉛筆を数えるとき「鉛筆一本」と単に言うこともできるけど、「鉛筆一本貸して」と言ったとき、なぜ鉛筆を本で数えるのか変だと感じない「感覚」が実際の言語習慣を決定づける。一方で、英語を骨まで受け入れないと、コミュニケーションに問題がないほど英語を上手に話せても、a, theなどの冠詞の使い方がめっちゃくちゃで、どこかおかしくならざるを得ない。言語って本来こんなものだ。受け入れればいいし、受け入れざるを得ない。そして言語間の違いは、元々使っていたものだから、そういうものだと思ってスルーしても実は全く問題ない。\n[1]：$X \\sim \\mathbb{Z}$ ならば、$X$ は可算集合だ。 [2]：$X \\sim \\mathbb{Q}$ ならば、$X$ は可算集合だ。 [3]：$X \\sim \\mathbb{R}$ ならば、$X$ は非可算集合だ。 しかし驚くべきことに、こうした違いは実際の数学でも現れ、[3]のように具体的に非可算集合を提案できる。この事実は集合論の父、ゲオルク・カントールによって証明され、彼自身もそうした非可算集合の存在に驚いたという。しかし、可算と非可算を抜きにしても、東アジアの数学ではこれらの概念が思い浮かぶことがあったのだろうか？不可能だと断言はできないが、ピタゴラス以後、非可算集合を発見するまでにかかった時間はなんと約2500年である。東洋で純粋数学が発展していたら、東洋独自の視点で驚くべき成果を見つけ出していたかもしれないが、可算と非可算の概念は間違いなくインド・ヨーロッパ言語族が残した遺産だ。\n証明 おそらくカントールは、すべての無限集合が可算集合であることを証明しようとしたのかもしれない。直感的にもその方が簡単で、本当であればすべての集合を自然数に引き下げて考えることができるからだ。カントールの旅を追って、まず[1]と[2]の証明を見てみよう。\n[1] $\\mathbb{N}$ と $\\mathbb{Z}$ の間に全単射が存在することを示せばよい。次のような対応関係を定義すると全単射となる。 $$ (1,2,3,4,5, \\cdots ) \\mapsto (0,-1,1,-2,2, \\cdots ) $$\n■\n[2] $\\mathbb{N}$ と $\\mathbb{Q}$ の間に全単射が存在することを示せばよい。次のような対応関係を定義してみよう。 $$ \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 6 \u0026amp; 7 \u0026amp; \\cdots \\\\ 3 \u0026amp; 5 \u0026amp; 8 \u0026amp; \\ddots \u0026amp; \\\\ 4 \u0026amp; 9 \u0026amp; \\ddots \u0026amp; \u0026amp; \\\\ 10 \u0026amp; \\ddots \u0026amp; \u0026amp; \\\\ \\vdots \u0026amp; \u0026amp; \\end{bmatrix} \\mapsto \\begin{bmatrix} 1/1 \u0026amp; 1/2 \u0026amp; 1/3 \u0026amp; 1/4 \u0026amp; \\cdots \\\\ 2/1 \u0026amp; 2/2 \u0026amp; 2/3 \u0026amp; \\ddots \u0026amp; \\\\ 4/1 \u0026amp; 4/2 \u0026amp; \\ddots \u0026amp; \u0026amp; \\\\ 5/1 \u0026amp; \\ddots \u0026amp; \u0026amp; \\\\ \\vdots \u0026amp; \u0026amp; \\end{bmatrix} $$ ここで約分を行なった後、重複する要素とそうでない要素が生じる。例えば、$2/2 = 1/1$ より重複である。これから、このような要素を重複しない要素に順番に$-1$ を掛けた要素に対応させてみよう。 $$ \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 6 \u0026amp; 7 \u0026amp; \\cdots \\\\ 3 \u0026amp; 5 \u0026amp; 8 \u0026amp; \\ddots \u0026amp; \\\\ 4 \u0026amp; 9 \u0026amp; \\ddots \u0026amp; \\\\ 10 \u0026amp; \\ddots \u0026amp; \\\\ \\vdots \u0026amp; \u0026amp; \\end{bmatrix} \\mapsto \\begin{bmatrix} 1/1 \u0026amp; 1/2 \u0026amp; 1/3 \u0026amp; 1/4 \u0026amp; \\cdots \\\\ 2/1 \u0026amp; -(1/1) \u0026amp; 2/3 \u0026amp; \\ddots \u0026amp; \\\\ 4/1 \u0026amp; -(1/2) \u0026amp; \\ddots \u0026amp; \u0026amp; \\\\ 5/1 \u0026amp; \\ddots \u0026amp; \u0026amp; \\\\ \\vdots \u0026amp; \u0026amp; \\end{bmatrix} $$ すると、この対応は全単射となる。\n■\n[3] 中学校から数を学ぶときは、自然数、整数、有理数、実数、複素数の順で学ぶが、ここまで証明したカントールがやろうとしたのは明白である。つまり、$\\mathbb{N} \\sim \\mathbb{R}$ であることを示す全単射を見つけようということだ。しかし、推測するにその試みは次々に失敗し、結局そのような全単射が存在しないことを証明しようとしたのだろう。このとき使用した方法が、その有名なカントールの対角線論法である。\n李興天 訳、林游鳳. (2011). 集合論(Set Theory: An Intuitive Approach): p219.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1383,"permalink":"https://freshrimpsushi.github.io/jp/posts/1383/","tags":null,"title":"可算集合と不可算集合"},{"categories":"집합론","contents":"## 定義 [^1] [^1]: ユウ・フォン・リン 著、イ・フンチョン 訳 (2011). 集合論(Set Theory: An Intuitive Approach): p205, 215。 1. 二つの[集合](../1316) $X,Y$ に対して[全単射](../471) $f : X \\to Y$ が存在する場合、$X$ と $Y$ が **等势**\u0026lt;sup\u0026gt;Equipotent\u0026lt;/sup\u0026gt;であるといい、$X \\sim Y$ と表示される。 2. [空集合](../1337)ではない $X$ のどの[真部分集合](../1329) $Y \\subsetneq X$ に対しても、$X \\sim Y$ が成立する場合、$X$ を **無限集合**という。 3. 無限集合でない集合を **有限集合**という。 ## 説明 1. 集合論を用いずに無限を説明しようとする時、「等势」という表現は、柵から羊を出す羊飼いに例えられることがある。十分な数の小石を用意しておき、羊が一匹柵から出るたびに小石を一つかごに入れる。そうすれば、柵から出た羊の数と小石の数は同じになるだろう。逆に、羊を再び柵に入れる際には、かごから小石を一つずつ取り出しながら数えればいい。この数が正確に一致すれば、羊飼いは羊を失くさなかったということになる。 この抽象化されたものは、かごに小石を入れたり取り出したりすることが全単射 $f$ が示す対応に相当する。例えば、自然数の集合 $\\mathbb{N}$ は全単射 $g(n) = 2n$ が存在し、偶数の集合 $2 \\mathbb{N}$ と等势である。もちろん、このような対応は閉区間 $[0,1]$ に対しても $g(x) = 2x$ を通して存在し$[0,1] \\sim [0,2]$ を示す。ここで、$2 \\mathbb{N} \\subsetneq \\mathbb{N}$ があり、$[0,1] \\subsetneq [0,2]$ であることに注目しろ。等势という概念が集合の大きさを説明するために導入されたことは確かだが、それが包含関係に繋がるわけではない。 2. 無限集合の定義が「無限」という言葉を使わずに済ませていることに注目しろ。これは、無限の本質自体が「全体と部分という概念の間に何かを許す」ということを意味している。このようにして、人間の直感に自然に浮かぶ無限は異なり、集合論が生まれた。これは **ヒルベルトのホテル** をはじめとする比喩で説明されることが多い。 ## 基本性質 以下は有限集合と無限集合が持つ基本的な性質である。等势の概念を理解すれば、それほど難しく考える必要はないだろう。 - [0] [空集合](../1337)は[有限集合](../1381)である。 - [1] 無限集合の上位集合は無限集合である。 - [2] 有限集合の部分集合は[有限集合](../1381)である。 - [3] 無限集合と等势ならば無限集合である。 - [4] [有限集合](../1381)と等势ならば有限集合である。 - [5] 無限集合から有限集合を引いた差集合は無限集合である。 ","id":1381,"permalink":"https://freshrimpsushi.github.io/jp/posts/1381/","tags":null,"title":"集合論により厳格に定義される有限集合と無限集合"},{"categories":"줄리아","contents":"julia\u0026gt; function add1(a,b)\rreturn a+b\rend\radd1 (generic function with 1 method)\rjulia\u0026gt; add1(1,2)\r3\rjulia\u0026gt; add(1,2.0)\rERROR: UndefVarError: add not defined\rStacktrace:\r[1] top-level scope at REPL[43]:1\rjulia\u0026gt; function add2(a::Int64, b::Float64)\rreturn a+b\rend\radd2 (generic function with 1 method)\rjulia\u0026gt; add2(1,2)\rERROR: MethodError: no method matching add2(::Int64, ::Int64)\rClosest candidates are:\radd2(::Int64, ::Float64) at REPL[44]:1\rStacktrace:\r[1] top-level scope at REPL[45]:1\rjulia\u0026gt; add2(1,2.0)\r3.0 上のように :: を使って変数が具体的に何であるかを知らせると、タイプが合わない時にエラーが出る。こうしてアノテーションがされていれば、タイプをチェックする必要がなくなるから、当然パフォーマンスが向上する。\nEnvironment OS: Windows julia: v1.5.0 ","id":1379,"permalink":"https://freshrimpsushi.github.io/jp/posts/1379/","tags":null,"title":"ジュリアのタイプとアノテーション"},{"categories":"집합론","contents":"定義 1 $x \\in X$ と $y \\in Y$ と $f: X \\to Y$が関数だとしよう。\nあらゆる $x_{1}, x_{2} \\in X$ に対して $x_{1} \\ne x_{2} \\implies f(x_{1}) \\ne f(x_{2})$ ならば $f$ を単射injectiveという。 $f(X) = Y$ ならば $f$ を全射surjectiveという。 $f$ が単射であり、かつ全射ならば全単射bijectiveという。 $I(x) = x$ を満たす $I : X \\to X$ を恒等関数Identity Functionという。 あらゆる $x, y$ に対して $f(x) = y$ かつ $f^{-1} (y) = x$ を満たす時、$f^{-1} : Y \\to X$ を$f$ の逆関数Inverse Functionという。 基礎的性質 [2]: $f$ が全単射であることと逆関数 $f^{-1}$ が存在することは同値だ。 説明 単射を一対一one-to-one、あるいは一対一関数ともいう。 全射をontoともいう。 全単射を一対一対応1-1 correspondingともいう。 入試数学では本当に重要でないと思われがちだが、一対一対応はとても重要な概念である。数学で苦労してる多くの学生は、この事実を聞いたことがないか、聞いても問題解決に役立たないと思うことが多い。まったく間違っているわけではないが、このレベルを知らなければ、問題解決に必要な他のこともよく知らない可能性が高い。\nそれなりにできる学生も、大学レベルの数学に触れた時に初めて全単射を本当に理解することが多い。一対一対応は集合論に限らず、広大な数学の世界で、それがどんな科目であれ、最も重要な概念である。しかし、そのほど強力で良好な条件なので、逆説的に、数学者は全単射の条件を緩和する方向で研究することになる。どのようにして他の条件で全単射であるかを導出するか、実際には全単射ではないが全単射として使うことができるかなどだ。\nどれほど重要かを簡単に説明すると、「本当に重要だから正確に知っておくべきだ」と言う必要もないほどだ。好むと好まざるとにかかわらず、全単射はあらゆる科目で出てくるので、むしろ卒業するまでに全単射を正確に知らない方が難しいだろう。\n李興天 訳, 林游峰. (2011). 集合論(Set Theory: An Intuitive Approach): p165, 181~187.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":471,"permalink":"https://freshrimpsushi.github.io/jp/posts/471/","tags":null,"title":"単射, 全射, 全単射, 逆関数"},{"categories":"줄리아","contents":"概要 ジュリアはMITで開発され、2012年に公開されたプログラミング言語で、高い生産性と速度を目指している。Cやフォートランと同等の速度を実現しながらも、PythonやRのような高レベルの構文を備えており、その他の多くの言語の利点を取り入れている。2019年11月現在、GPUの急速な発展とディープラーニングの流行により少し遅れているのは事実だが、ジュリアを超えるほど便利で速い言語は学界にはない。\nジュリアについて質問やコメントがあれば、生エビ寿司屋コミュニティ 💬に投稿してみよう。\n特長 ジュリアの特長を見てみよう：\n速い。他の言葉は必要ない。ベンチマークを見れば、Cやフォートランと肩を並べていることがわかる。 簡単だ。最適化まで気にしなければ、ジュリアのプログラミングは、マットラボ、R、パイソンと似たり寄ったりだ。ただし、これらの言語で何かプログラムを実装した後、最適化のためにCで同じコードを書き直す必要がない。ジュリアに適したコーディング技術を通じて改善されるだけだ。同じ理由で、自然科学系のユーザーがマットラボ、R、パイソンを知っていれば、ジュリアもすぐに慣れることができる。実際、これらの言語の生産性は非常に高いため、どんなプログラミング言語がベースであっても、ジュリアは簡単に感じられる。 無料だ。マットラボは強力な線形代数をサポートしているが、まず高価であり、実際には最適化によってジュリアがマットラボよりも10倍から1,000倍速いと言われている。さらに、MATLAB.jlパッケージを使用すると、マットラボと同じスタイルでコードを書くことができ、マットラボに熟練したユーザーが移行しやすい。 他言語のパッケージを簡単に取り込める。新しい言語の最大の弱点であり、フォートランやパイソンなどの言語が使用される主な理由の一つは、まさにパッケージである。パイソンの場合は、PyCall.jlパッケージを使用することで、パイソンの関数を直接呼び出すことができる。もちろん、ジュリアはすでに十分な高性能を備えているが、ccall()関数を使用することでCやフォートランの関数を呼び出すことができる。C++もCpp.jlパッケージでサポートされている。 並行処理に特化している。他の言語がその後に関連するパッケージを開発したのとは異なり、ジュリアは最初から並行処理を念頭に置いて開発された。実際、プログラムによっては、便利というよりも最適化の核心となることもある。 ","id":1374,"permalink":"https://freshrimpsushi.github.io/jp/posts/1374/","tags":null,"title":"ジュリアプログラミング言語"},{"categories":"집합론","contents":"定義 1 関数 $f: X \\to Y$ と $B \\subset Y$ について、$f^{-1}(B): = \\left\\{ x \\in X \\ | \\ f(x) \\in B \\right\\}$ を $f$ による $B$ の原像または逆像という。\n解説 表記は似ているが、定義だけで逆像と逆関数が関係しているとは言えず、これらを混同してはならない。\n韓国語で話すときは逆像が自然だが、英語では[プレイメージ]が自然と感じる人がいるだろう。これは、逆を意味する漢字が単に「どこから来たか」という逆像の概念によく合うのに対して、上で述べたように[インバース]という言葉は逆関数を連想させるため、意識的に使うのを避けるからだ。もちろん、単に[プレイメージ]が発音しやすいのでよく使われる、前置詞としての「元」が馴染みがないので使わない、などの単純な理由もある。\n基本性質 [1] 空集合: $$ f ( \\emptyset ) = \\emptyset $$ [2] 単元素集合: $$ x \\in X \\implies f \\left( \\left\\{ x \\right\\} \\right) = \\left\\{ f(x) \\right\\} $$ [3] 単調性: $$ A \\subset B \\subset X \\implies f (A) \\subset f(B) \\\\ C \\subset D \\subset Y \\implies f^{-1} (C) \\subset f^{-1} (D) \\\\ f(X) \\subset Y \\iff X \\subset f^{-1} (Y) $$ [4] 和集合: $$ f \\left( \\bigcup_{\\gamma \\in \\Gamma} A_{\\gamma} \\right)= \\bigcup_{\\gamma \\in \\Gamma } f \\left( A_{\\gamma} \\right) \\\\ f^{-1} \\left( \\bigcap_{\\gamma \\in \\Gamma} A_{\\gamma} \\right) = \\bigcap_{\\gamma \\in \\Gamma } f^{-1} \\left( A_{\\gamma} \\right) $$ [5] 交差点: $$ f^{-1} \\left( \\bigcup_{\\gamma \\in \\Gamma} A_{\\gamma} \\right)= \\bigcup_{\\gamma \\in \\Gamma } f^{-1} \\left( A_{\\gamma} \\right) \\\\ f \\left( \\bigcap_{\\gamma \\in \\Gamma} A_{\\gamma} \\right) {\\color{red}\\subset} \\bigcap_{\\gamma \\in \\Gamma } f \\left( A_{\\gamma} \\right) $$ [6] 差集合: $$ f (A) \\setminus f (B) \\subset f (A \\setminus B) \\\\ f^{-1} (C) \\setminus f^{-1}(D) = f^{-1} (C \\setminus D) $$ 特に [5]、[6]では、関数は交差点をそのまま保持できないことに注意。等号を満たすためには、$f$ が単射でなくてはならない。\n全単射と逆関数の概念は繰り返しを通じて慣れることができるが、逆像に関しては、できるだけ早く、正確に学ぶ必要がある。逆像を大まかに理解しておいても、すぐに線形代数学におけるゼロ空間に対する直感が落ち、そのまま抽象代数学まで影響を及ぼす。関数の像とは異なる性質が多いので、ただ反対だと思って見過ごすのではなく、ちゃんと勉強して、しっかり理解するようにしよう。\n李興天 訳、You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p173.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":472,"permalink":"https://freshrimpsushi.github.io/jp/posts/472/","tags":null,"title":"関数の原像"},{"categories":"집합론","contents":"定義 1 空集合じゃない二つの集合 $X$、$Y$ が与えられたとする。\n二項関係 $f \\subset (X,Y)$ が次を満たすとき、関数と呼び、$f : X \\to Y$ と表す。 $$ (x ,y_{1}) \\in f \\land (x,y_{2}) \\in f \\implies y_{1} = y_{2} $$ 関数 $f : X \\to Y$ において、$\\text{Dom} (f) = X$ を$f$ の定義域Domainといい、$Y$ を$f$ の共変域Codomainという。定義域の部分集合 $A \\subset X$ が与えられたとき、$f(A):= \\left\\{ f(x) \\in Y \\ | \\ x \\in A \\right\\}$ を$f$ に対する$A$ の像Imageという。特に $f$ に対する定義域 $X$ の像 $\\text{Im} f := f(X)$ を$f$ の値域Rangeという。 定義域が自然数の集合 $\\mathbb{N}$ の関数を数列Sequenceという。 定義域 $A$ と共変域 $B$ を持つすべての関数 $\\lambda : A \\to B$ の集合を $B^{A}$ と表す。 説明 教科書レベルでは、\u0026lsquo;全ての元 $x_{1}, x_{2} \\in X$ に対して、$x_{1} = x_{2} \\implies f(x_{1}) = f(x_{2})$ を満たす $f(x_{1})$ と $f(x_{2})$ が $Y$ に存在するなら、対応 $f : X \\to Y$ を$X$ から $Y$ への関数とする\u0026rsquo; と言われがちだ。しかし、この \u0026lsquo;対応\u0026rsquo; あるいは \u0026lsquo;写像\u0026rsquo; は、表現がやや曖昧だった。大学レベル以上の数学では、集合論を用いて、関数に関連する概念を厳格に定義する。入力があれば出力がただ一つだけ出る、という説明は、コンピュータ科学の関数にもっと適した説明だ。 なぜ値域を別に定義するのか疑問に思うかもしれない。例えば、$f(x) = x^2$ を考えると、明らかに関数の値は $\\left[ 0,\\infty \\right)$ に属していて、$f : \\mathbb{R} \\to \\mathbb{R}$ のように無駄に大きくする必要はないと見える。もともと値域は共変域の部分集合であり、実際には使われない値をなぜ別にするのか理解しにくいことだ。これは、あまりにも簡単な例だけを考えることからくる誤解で、すべての関数が定義時から値域を簡単に予測できるわけではない。関数を定義するときに保証できるのは、$x \\in X$ に対して $f(x) \\in Y$ が存在することだけで、それが何かはわからない。もし $$ f(x) = \\sin \\ln \\sqrt{x} + \\int_{1}^{3^x} {{1} \\over {7t+t^2}} dt $$ のような複雑な関数があれば、定義時からその値域を知ることは不可能であり、必要もない。値域を知ることが重要なのは、合成関数を定義するときくらいだ。 数列が単に数を並べたものだという説明よりも、もっとシンプルで一般的な定義ができたことを確認できる。共変域が「数」であることに限らず、関数であれ、いかにも珍しい集合であれ、全てをカバーできるようになった。このような抽象化は、ただちに数列の概念をきれいに表現できるだけでなく、無限を扱う数学のさまざまな分野で柔軟に使用できる。 関数の集合という概念自体が新しいかもしれないが、抽象数学では、関数空間のような集合を日常的に言及する。$B^{A}$ のような記法をなぜ使うのかは、基数を思い出せば理解しやすい。例えば、$B$ が共変域であり、$A$ が定義域であるすべての関数を考えると、 $$ e \\mapsto 1 \\text{ or } 2 \\text{ or } 3 \\\\ \\pi \\mapsto 1 \\text{ or } 2 \\text{ or } 3 $$ 全ての組み合わせは $9 = 3^2 = |B|^{|A|}$ になる。 李興天 訳, You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p157~159.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":470,"permalink":"https://freshrimpsushi.github.io/jp/posts/470/","tags":null,"title":"集合論によって厳密に定義される関数と写像、数列"},{"categories":"집합론","contents":"定義 1 集合 $X$ 上で同値関係 $R$ が定義されているとしよう。$x \\in X$ に対して、$x / R := \\left\\{ y \\in X : y R x \\right\\}$ を$x$ の同値類と呼ぶ。与えられた$X$ の全ての同値類を集めた集合を$X / R := \\left\\{ x / R : x \\in X \\right\\}$ のように表現する。\n説明 表現が少し汚く見えるかもしれないが、例を考えれば全然難しい概念ではない。自然数集合 $\\mathbb{N}$ 上で$3$ で割った余りが同じならば同値とし、$x,y \\in \\mathbb{Z}$ が同値なら$x \\equiv y \\pmod{3}$ のように表現しよう。$5,7$ は$3$ で割った余りがそれぞれ$2,1$ なので同値ではないが、$11,17$ は$3$ で割った余りが$2$ になるので、$11 \\equiv 17 \\pmod{3}$ のように書くことができる。 $$ 1 \\equiv 4 \\equiv 7 \\equiv 10 \\equiv \\cdots \\pmod{3} \\\\ 2 \\equiv 5 \\equiv 8 \\equiv 11 \\equiv \\cdots \\pmod{3} \\\\ 3 \\equiv 6 \\equiv 9 \\equiv 12 \\equiv \\cdots \\pmod{3} $$ 上の計算から分かるように、$1$ の同値類は$( 1 / \\equiv) = \\left\\{ 1, 4, 7, 10, \\cdots \\right\\}$ であり、$2$ の同値類は$(2 / \\equiv) = \\left\\{ 2, 5, 8, 11, \\cdots \\right\\}$ であり、$3$ の同値類は$(3 / \\equiv) = \\left\\{ 3, 6, 9, 12, \\cdots \\right\\}$ であることが分かる。$3$ より大きい数からは上記の三つの同値類が繰り返され、したがって、 $$ (\\mathbb{N} / \\equiv) = \\left\\{ 1 / \\equiv , 2 / \\equiv , 3 / \\equiv \\right\\} $$ 上の例から確認できるように、同値類は次のような常識的な性質を持つ。\n基本的な性質 [1] $x / R \\ne \\emptyset$ [2] $ x / R \\cap y / R \\ne \\emptyset \\iff xRy$ [3] $x/R = y/R \\iff x R y$ [4] $x / R \\cap y / R \\ne \\emptyset \\iff x/R = y/R $ 証明 [1] $R$ は$X$ 上の同値関係であるため、反射性により、全ての$x \\in X$ に対して$x R x$ かつ、$x \\in x / R$ が成り立つべきだ。\n■\n戦略 [2][3]：$x,y$ と別の$z$ を一つ選び、同値関係の対称性と推移性を利用して式をつなげる。\n[2] $x/R$ と$y/R$ は空集合ではなく、$X$ 上の同値関係であるため、$x/R \\cap y/R \\ne \\emptyset$ であり、これはある$z$ に対して $$ \\begin{align*} \u0026amp; z \\in x / R \\land z \\in y / R \\\\ \\iff \u0026amp; z R x \\land z R y \\\\ \\iff \u0026amp; x R z \\land z R y \\\\ \\iff \u0026amp; x R y \\end{align*} $$ と同値である。\n■\n[3] $( \\implies )$ $x/R = y/R$ であれば$x/R \\cap y/R \\ne \\emptyset$ であるため、[2]により$x R y$\n$( \\impliedby )$ $x R y$ であれば、全ての$z \\in x / R$ に対して$z R x$ だ。$x R y$ であるため、$R$ の推移性により$z R y$ かつ、$z \\in y / R$ である。要約すると、 $$ z \\in x / R \\implies z \\in y / R $$ 集合の包含関係の形で変えてみれば、 $$ x / R \\subset y / R $$ 同じ方法で$ y / R \\subset x / R$ を得ることができるので、 $$ x / R = y / R $$\n■\n[4] 三段論法により、[2]と[3]から、 $$ x / R \\cap y / R \\ne \\emptyset \\iff xRy \\iff x/R = y/R $$\n■\n이흥천 역, You-Feng Lin. (2011). 집합론(Set Theory: An Intuitive Approach): p147.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1050,"permalink":"https://freshrimpsushi.github.io/jp/posts/1050/","tags":null,"title":"同相型"},{"categories":"집합론","contents":"定義 1 集合 $X$ の全ての部分集合 $A,B,C$ について、次の条件を満たす $\\mathscr{P} \\subset 2^{X}$ を $X$ の分割と言う。\n(i): $$A,B \\in \\mathscr{P} \\land A \\ne B \\implies A \\cap B = \\emptyset$$ (ii): $$\\bigcup_{C \\in \\mathscr{P} } C = X$$ 説明 数式で表すと複雑に見えるが、簡単に言うと、単に全体集合を欠けることなくいくつかのピースに分けることに過ぎない。数式的な定義に囚われる余裕があるなら、むしろ$X$の分割$\\mathscr{P}$が$X$の冪集合$2^{X} = \\mathscr{P} (X)$の部分集合であるというようなディテールに気を使う方がいい。\n簡単な例として、整数集合$\\mathbb{Z}$を考えてみよう。偶数の集合$2 \\mathbb{Z} = \\left\\{ \\cdots , -2 , 0 , 2 , \\cdots \\right\\}$と奇数の集合$1 + 2 \\mathbb{Z} = \\left\\{ \\cdots , -3 , -1 , 1 , 3 , \\cdots \\right\\}$を含む$\\mathscr{P} = \\left\\{ 2 \\mathbb{Z} , 1 + 2 \\mathbb{Z} \\right\\}$は$\\mathbb{Z}$の分割となる。ここで$\\mathscr{P} \\subset 2^{\\mathbb{Z}}$は$\\mathbb{Z}$の部分集合を元に持つ集合であり、元の数は$2$個である。何がどこに属しているか、元なのか集合なのかを大まかに見過ごさず、正確に定義に従って理解する練習をすることがいい。\n李興天 訳, You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p147.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1049,"permalink":"https://freshrimpsushi.github.io/jp/posts/1049/","tags":null,"title":"集合の分割"},{"categories":"집합론","contents":"定義 1 反射的であり、対称的であり、推移的な２項関係を同値関係と呼ぶ。\n説明 同値関係を数学的でない言葉で言うなら、「それがそれ」ということだ。\n数学を研究するときにその理由が必ずしも必要ではないが、もし数学を研究する実用的な理由が絶対に必要であるとするならば、「もともと難しく複雑な概念を簡単でシンプルな領域に引き下げて、問題を楽に解決するため」と言えるだろう。この話をするためには「実質的に同じである」と表現することができなければならないが、これがまさに同値関係だ。\n同値関係の例としては$=$, $\\equiv$, $\\iff$ などがあるが、これらの例だけでは数学で同値関係がなぜ重要なのか理解しづらい。新しい視点を持つには自然すぎるためだ。 $$ Q : = \\left\\{ ( n, m ): n\\in \\mathbb{Z} , m \\in \\mathbb{Z} \\setminus \\left\\{ 0 \\right\\} \\right\\} $$ 例えば、次のような集合 $Q$ があるとしよう。ここで、順序ペア $(n,m)$ を $\\displaystyle (n,m) = {{n} \\over {m}}$ のように書くと、$Q$ は私たちがよく知っている有理数の集合 $\\mathbb{Q}$ と実質的に同じであると言える。しかし、詳しく調べてみると、$Q$ と $\\mathbb{Q}$ を同じ集合とは言えない。なぜなら、$Q$ では $(2,3)$ と $(4,6)$ が異なる要素であるにも関わらず、$\\mathbb{Q}$ では $\\displaystyle {{2} \\over {3}}$ と $\\displaystyle {{4} \\over {6}}$ が同じ要素であるからだ。$Q$ と $\\mathbb{Q}$ の違いは、約分があるかどうかである。\n$\\mathbb{Q}$ では、私たちは馴染みのある同値関係を発見できる。この同値関係を $\\sim$ と呼ぶと、次のように定義できるだろう。 $$ {{ a } \\over { b }} \\sim {{ c } \\over { d }} \\iff ad = bc $$ 実際、$\\displaystyle {{2} \\over {3}}$ と $\\displaystyle {{4} \\over {6}}$ を考えると、$2 \\cdot 6 = 12 = 3 \\cdot 4$ であるため、$\\displaystyle {{2} \\over {3}} \\sim {{4} \\over {6}}$ となり、$\\mathbb{Q}$ で二つの要素が同値であると言える。反面、このような同値関係が $Q$ に与えられていない場合、$(2,3)$ と $(4,6)$ が実質的に同じであることを知っていても、同じと言うことができない。\nこのように、元々同じであるべきものを同じと言う作業は、一見無用であり、過度に理論的に感じられるかもしれない。私たちの直感は既に同じであることを知っているが、難しい言葉や記号を使って、言っていたことをまた言う感じだ。しかし、数学では厳密性を最高の価値としており、この過程は避けられないと同時に、それを利用して数学の限界を破ることもできる。 例えば、図のように線分を曲げて両端をくっつけたと考えてみよう。線分だったとき、二つの点は明らかに異なる点だった。しかし、同値関係を適用し、二つの点を実質的に同じ点として扱うことで、これまでとは完全に異なる、新しい輪が作られる。位相数学という分野は、このような現象を研究し、同値関係を用いて「くっつける」という行為を数学的に表現したものである。\n李興天 訳, You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p141.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1033,"permalink":"https://freshrimpsushi.github.io/jp/posts/1033/","tags":null,"title":"数学における同値関係"},{"categories":"집합론","contents":"定義 1 二つの集合$X,Y$に対して、 $$ R := \\left\\{ (x,y): x \\in X , y \\in Y \\right\\} \\subset X \\times Y $$ を (二項) 関係と定義し、次のように表す。 $$ (x,y) \\in R \\iff x R y $$ $x R y \\iff y R^{-1} x$を満たす $$ R^{-1} : \\left\\{ (y,x): (a,b) \\in R \\right\\} $$ を$R$の逆関係という。 すべての$x \\in X$に対して、次を満たす$ R \\subset X^{2}$を反射的と言う。 $$ x R x $$ すべての$x,y \\in X$に対して、次を満たす$ R \\subset X^{2}$を対称的と言う。 $$ x R y \\implies y R x $$ すべての$x,y,z \\in X$に対して、次を満たす$ R \\subset X^{2}$を推移的と言う。 $$ x R y \\land y R z \\implies x R z $$ すべての$x,y \\in X$に対して、次を満たす$ R \\subset X^{2}$を反対称的と言う。 $$ x R y \\land y R x \\implies x = y $$ 説明 二項関係は、「何かと何かが何かの関係を持つ」というような曖昧な表現ではなく、デカルト積を利用して明確に定義される。関係とは正確にデカルト積の部分集合であり、$x R y$を見て「$x$は$y$に対してどうのこうのという意味ではない」と理解しなければならない。直感的に何となく分かった気になって概念をしっかり把握せずにいると、「関係」が登場するたびに本を読むのが難しくなるだろう。\n特に反射的であり、対称的であり、推移的な二項関係を同値関係という。これらの性質は数学全般で非常に重要とされる。\n例 二項関係と逆関係 関数$f : X \\to Y$は、すべての$x$に対して$y = f(x)$を満たす$y \\in Y$が存在し、すべての$x_{1} , x_{2} \\in X$に対して $$ x_{1} = x_{2} \\implies f(x_{1}) = f(x_{2}) $$ を満たす二項関係だ。もちろん、その逆関数$f^{-1}$が存在すれば、$f^{-1}$は関係$f$の逆関係になる。\n反射関係 反射的な関係の例として、等号$=$は$x=x$が常に成り立つ。\n対称関係 対称的な関係の例として、独立$\\perp$は $$ X \\perp Y \\implies Y \\perp X $$ が常に成り立つ。\n推移関係 推移的な関係の例として、不等号$\u0026lt;$は $$ x \u0026lt; y \\land y \u0026lt; z \\implies x \u0026lt; z $$ が常に成り立つ。\n反対称関係 反対称的な関係の例として、包含関係$\\subset$は $$ A \\subset B \\land B \\subset A \\implies A = B $$ が常に成り立つ。\n李興天 訳, Lin You-Feng. (2011). 集合論(Set Theory: An Intuitive Approach): p137~141。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":960,"permalink":"https://freshrimpsushi.github.io/jp/posts/960/","tags":null,"title":"数学における二項関係"},{"categories":"집합론","contents":"定義 1 任意の二つの対象 $a$、$b$ に対して、$(a,b)$ を順序対と言う。 任意の二つの集合 $A$、$B$ について、$a \\in A$、$b \\in B$ の順序対 $(a,b)$ の集合を$A$、$B$ のデカルト積と言い、以下のように表す。 $$ A \\times B := \\left\\{ (a,b): a \\in A \\land b \\in B \\right\\} $$ 説明 デカルト積で「積」という言葉が使用される理由は、集合が持つ元の数を考えた時に$| A \\times B | = | A | \\times |B|$ になるからだ。一つの集合 $X$ について、$X \\times X$ は$X^{2}$ のように表されるが、実数の集合 $\\mathbb{R}$ を考えると、$\\mathbb{R}^2 = \\mathbb{R} \\times \\mathbb{R}$ は座標平面の全ての点を元として持つ集合と見ることができる。これは数直線の全ての点を持つ$\\mathbb{R}$ を掛けることで、座標平面$\\mathbb{R}^2$ を得ることができるということだ。デカルト積で「デカルト」という表現が使われる理由は、この座標平面を考案し、数学界に導入して解析幾何学の世界を開いた人がデカルトだったからだ。デカルトがカントールよりもずっと前の人物であり、集合論に直接的な貢献はなかったが、概念的に前進していたため、この名前の主役になる資格は十分あると言えるだろう。\nこのようなデカルト積は、もちろん一般化が可能であり、例えば、三次元空間 $\\mathbb{R}^{3}$ や一般的なユークリッド空間$\\mathbb{R}^{p}, p \\in \\mathbb{N}$ も考えられる。想像するのは難しいかもしれないが、デカルト積は自然数を越えても拡張される。\n要約 一方、デカルト積に対して、以下の分配法則が成立する。\n分配法則 任意の集合 $A$、$B$、$C$ に対して： $$ A \\times (B \\cap C) = ( A \\times B) \\cap (A \\times C) \\\\ A \\times (B \\cup C) = ( A \\times B) \\cup (A \\times C) \\\\ A \\times (B \\setminus C) = ( A \\times B) \\setminus (A \\times C) $$\n参照 集合のデカルト積 群のデカルト積 位相空間のデカルト積 翻訳：イ・フンチョン、リン・ヨウフォン。(2011)。集合論(Set Theory: An Intuitive Approach)：p129~131。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1360,"permalink":"https://freshrimpsushi.github.io/jp/posts/1360/","tags":null,"title":"集合のデカルト積"},{"categories":"집합론","contents":"定義 要素が集合である集合をファミリーFamilyという。 ファミリーの要素をメンバーMemberという。 一つの集合$\\Gamma$の各$\\gamma \\in \\Gamma$に対して集合$A_{\\gamma}$が対応する時、$\\gamma$をインデックス、$\\Gamma$をインデックス集合、$\\left\\{ A_{\\gamma} : \\gamma \\in \\Gamma \\right\\}$をインデクスファミリーという。 説明 ファミリーは本来、「集合族」という言葉で表されているが、この表現は「集合の集合」という非常に不便な言葉を避けるためだけのもので、それでもなお直感的ではなく不便である。集合が要素として集合を持つという概念を表すためにわざわざ新しい言葉を作るのは奇妙だ。つまり、ファミリーという言葉は純粋に便宜のために導入されたということだ。\n例として、次のファミリーを考えてみよう: $$ \\mathcal{F}=\\left\\{\\left\\{ 1 \\right\\} , \\mathbb{R} , \\mathbb{Q}, \\emptyset , \\mathbb{R} \\right\\} $$ $\\mathcal{F}$はその要素が全て集合であるため、ファミリーと呼べる。注目すべき点は、$\\mathbb{R}$が重複して使われていることだ。このような表記からファミリーは単なる集合の集合ではないし、正確な意味での集合の集合でもないことを知るべきだ。純粋に、純粋に便宜だ。その意味で、「集合族」の族はFamilyから来た家族や族の族しか取り入れておらず、英語の表現が言いたいことを全く生かせていない。さらにメンバーは「構成員」と純化されているが、これはどう見ても便利な表現ではないので、このブログではファミリーとメンバーをそのまま使用する。\n同様に、インデックスも「添え数」と純化されているが、これは行き過ぎだ。上で例として挙げた$\\mathcal{F}$についてインデックスファミリーを構成してみよう。幸いなことに有限集合であるため、$\\Gamma = \\left\\{ 1,2,3,4,5 \\right\\}$に対して $$ A_{1} = \\left\\{ 1 \\right\\} \\\\ A_{2} = \\mathbb{R} \\\\ A_{3} = \\mathbb{Q} \\\\ A_{4} = \\emptyset \\\\ A_{5} = \\mathbb{R} $$ とすると、$\\mathcal{F} = \\left\\{ A_{\\gamma} : \\gamma \\in \\Gamma \\right\\}$を得る。ここで、$A_{2} = \\mathbb{R}$でありながら$A_{5} = \\mathbb{R}$であることに注意しよう。重複を許したのは、集合論の基盤を揺るがすためではなく、ただ表現上便利に使うためであることを覚えておこう。同じ理由で、ファミリーは単にコレクションCollectionとも呼ばれる。英語で集合を定義する際にCollectionという表現が使われているため、意味が循環しているように見えるが、前述のようにそれは単に便利に話すためのものなので、あまり深く考えずに、使用している教材の慣習に従えばいい。\n一方、インデックスは必ずしも上記のような順序を守る必要はなく、具体的に番号が付けられている必要もない。$\\Gamma = \\mathbb{R}$として、$A_{\\gamma}$が$k \\in \\mathbb{Z}$に対して$\\gamma$を含む区間$[k , k+1)$と考えてみよう。すると、$\\gamma \\in \\mathbb{R}$であるため、$A_{\\pi} = [3,4)$、$A_{\\sqrt{10}} = [3,4)$など、全ての$\\gamma \\in \\Gamma$に対して対応する$A_{\\gamma}$を見つける理由がない。こんな変な構成がどうして必要かと思うかもしれないが、位相数学だけを見ても、このような集合を当たり前のように使う。\n任意のファミリー$\\mathcal{F}$に対して、以下の表現を使用する。\n和集合: $$ \\bigcup \\mathcal{F} = \\bigcup_{A \\in \\mathcal{F}} \\left\\{ x \\in U : \\exists A \\in \\mathcal{F} , x \\in A \\right\\} $$ 積集合: $$ \\bigcap \\mathcal{F} = \\bigcap_{A \\in \\mathcal{F}} \\left\\{ x \\in U : \\forall A \\in \\mathcal{F} , x \\in A \\right\\} $$ 基本性質 $\\left\\{ A_{\\gamma} : \\gamma \\in \\Gamma \\right\\}$について、以下が成立する。\n[1] 包含原理の集合形: 全体集合$U$に対して、 $$ \\bigcup_{\\gamma \\in \\emptyset} A_{\\gamma} = \\emptyset \\\\ \\bigcap_{\\gamma \\in \\emptyset} A_{\\gamma} = U $$ [2] ド・モルガンの定理の一般化: $$ \\left( \\bigcup_{\\gamma \\in \\Gamma} A_{\\gamma} \\right)^{c} = \\bigcap_{\\gamma \\in \\Gamma} A_{\\gamma}^{c} \\\\ \\left( \\bigcap_{\\gamma \\in \\Gamma} A_{\\gamma} \\right)^{c} = \\bigcup_{\\gamma \\in \\Gamma} A_{\\gamma}^{c} $$ [3] 分配法則: 集合$B$に対して、 $$ \\left( \\bigcup_{ \\gamma \\in \\Gamma } A_{\\gamma} \\right) \\cap B = \\bigcup_{\\gamma \\in \\Gamma} \\left( A_{\\gamma} \\cap B \\right) \\\\ \\left( \\bigcap_{ \\gamma \\in \\Gamma } A_{\\gamma} \\right) \\cup B = \\bigcap_{\\gamma \\in \\Gamma} \\left( A_{\\gamma} \\cup B \\right) $$ ","id":1358,"permalink":"https://freshrimpsushi.github.io/jp/posts/1358/","tags":null,"title":"集合族と添字"},{"categories":"확률론","contents":"定義 確率空間 $( \\Omega , \\mathcal{F} , P)$が与えられているとしよう。フィルトレーション $\\left\\{ \\mathcal{F}_{n} \\right\\}$に関して$0$以上の整数値を持ち、全ての$n \\in \\mathbb{N}_{0}$に対して$(\\tau = n) \\in \\mathcal{F}_{n}$を満たす確率変数$\\tau$を停止時間Stopping Timeと呼ぶ。\nボレル集合 $B \\in \\mathcal{B}(\\mathbb{R})$に対して、$(\\tau \\in B) = \\tau^{-1} (B)$は、したがって$\\tau^{-1} ( \\left\\{ n \\right\\} )$と同じである。 例 停止時間の直感的な概念は、興味のあるイベントが起こる―観察される瞬間を指す。例えば、$\\tau = 8$は情報$\\mathcal{F}_{8}$を知りながら興味のあるイベントが$n=8$で起きたことを意味する。一見、停止時間の条件は簡単すぎるように見えるかもしれない。しかし、全ての$n \\in \\mathbb{N}_{0}$に対して満足しなければならない点が難題となる。\n$Y_{1}, Y_{2} , \\cdots \\overset{iid}{\\sim} B(1,p)$としよう。つまり、各$Y_{n}$が確率$p$のベルヌーイ分布に従うとし、$Y_{5}$までの結果が以下のようだとしよう。 $$ \\begin{matrix} Y_{1} \u0026amp; Y_{2} \u0026amp; Y_{3} \u0026amp; Y_{4} \u0026amp; Y_{5} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 1 \\end{matrix} $$\n(1) 停止時間でない場合: $\\tau$を$\\tau:= \\max \\left\\{ k: Y_{k} = 0 \\right\\}$と置いた場合、上記のケースでは以下のように$\\tau$が計算される。 $$ \\begin{matrix} Y_{1} \u0026amp; Y_{2} \u0026amp; Y_{3} \u0026amp; Y_{4} \u0026amp; Y_{5} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 1 \\\\ \\tau = 1 \u0026amp; \\tau = 2 \u0026amp; \\tau = 2 \u0026amp; \\tau = 4 \u0026amp; \\tau = 5 \\end{matrix} $$ ここで、$\\tau$は以下を満たさなければ停止時間になれない。 $$ (\\tau = n ) = \\left( Y_{n} = 0 , Y_{n+1} = 1 , \\cdots \\right) $$ これは正確に$Y_{n} = 0$であり、その後は必ず$1$でなければならないが、どんなに$n \\in \\mathbb{N}$があってもまだ試行もしていないので結果を知ることはできない。したがって、$\\tau$は停止時間になれない。\n(2) 停止時間になる場合: $\\tau$を$\\tau:= \\min \\left\\{ k: Y_{k} = 1 \\right\\}$と置いた場合、上記のケースでは以下のように$\\tau$が計算される。 $$ \\begin{matrix} Y_{1} \u0026amp; Y_{2} \u0026amp; Y_{3} \u0026amp; Y_{4} \u0026amp; Y_{5} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 1 \\\\ \\tau = 0 \u0026amp; \\tau = 0 \u0026amp; \\tau = 3 \u0026amp; \\tau = 3 \u0026amp; \\tau = 3 \\end{matrix} $$ $\\tau$はすでに$n=3$で興味のあるイベントが起きてしまっており、未来に何が出ても関係なくなり、停止時間になる。\n説明 上記の例で、$\\max$は停止時間に適さなかったが、$\\min$は停止時間になったことに注目しよう。この意味で、停止時間は「何かが初めて起こるタイミング」と直感的に考えることができるべきだ。一方で、数学的に厳密な定義では、$\\tau$はまだ確率変数であることも忘れてはならない。確率過程 $\\left\\{ X_{n} \\right\\}_{n \\in \\mathbb{N}_{0}}$が与えられている場合、$X_{\\tau}$に対して$\\omega \\in \\Omega$は以下を意味する。 $$ X_{\\tau} = X_{\\tau} ( \\omega )= X_{\\tau (\\omega)} ( \\omega ) $$ 例えば、$\\tau (\\omega_{1}) = 5$ならば、$X_{\\tau} (\\omega_{1}) = X_{5} ( \\omega_{1})$となる式である。$\\tau$は結局のところ「いつかイベントが起こるかもしれない」と示す確率変数であり、\u0026ldquo;停止時間\u0026quot;と呼ばれる前に、すべての$\\omega \\in \\Omega$をそれぞれ何らかの$n \\in \\mathbb{N}_{0}$にマッピングする「関数」である。直感的な理解に固執してこの点を忘れると、停止時間を用いる全ての式展開が苦痛になる。よく覚えておこう。\n","id":1351,"permalink":"https://freshrimpsushi.github.io/jp/posts/1351/","tags":null,"title":"確率過程における停止時間"},{"categories":"르벡공간","contents":"定義 1 関数のシーケンス $\\left\\{ f_{n} \\right\\}_{n \\in \\mathbb{N}}$ がある関数 $f$ に対して次を満たす場合、$\\left\\{ f_{n} \\right\\}$ が$f$ に**$L^{p}$ 収束する**と言う。\n$$ \\lim_{n \\to \\infty} \\left\\| f_{n} - f \\right\\|_{p} = 0 $$\nシーケンス $\\left\\{ f_{n} \\right\\}_{n \\in \\mathbb{N}}$ が次を満たす場合、$L^{p}$ でコーシーCauchy in $L^{p}$と言われる。\n$$ \\lim_{n, m \\to \\infty} \\left\\| f_{n} - f_{m} \\right\\|_{p} = 0 $$\n説明 もちろん、$\\left\\| \\cdot \\right\\|_{p}$ は$p$-ノルムとして次のように定義される。\n$$ \\left\\| f \\right\\|_{p} := \\left( \\int_{E} | f |^{p} dm \\right) ^{{{1} \\over {p}}} $$\n関数のシーケンスが$L^{p}$ 収束するというのは、ノルムの意味で収束することを指す。ルベーグ空間の性質では、$p \\le q$ が$f_{n}$ が$L^{q}$ で収束する場合、$L^{p}$収束すると言える。\n参照 $L^{p}$ 収束 $\\implies$ 測度収束 Bartle. (1995). 非連続積分およびLebesgue測度の要素: p58.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1394,"permalink":"https://freshrimpsushi.github.io/jp/posts/1394/","tags":null,"title":"Lp 収束"},{"categories":"확률론","contents":"定義 確率空間 $( \\Omega , \\mathcal{F} , P)$ が与えられたとする。\n$\\mathcal{F}$ のサブσフィールドのシーケンス $\\left\\{ \\mathcal{F}_{n} \\right\\}_{n \\in \\mathbb{N}}$ が以下を満たす場合、フィルトレーションFiltrationと呼ぶ。 $$ \\forall n \\in \\mathbb{N}, \\mathcal{F}_{n} \\subset \\mathcal{F}_{n+1} $$ フィルトレーション $\\left\\{ \\mathcal{F}_{n} \\right\\}_{n \\in \\mathbb{N}}$ が与えられた時、ルベーグ可積分な $\\mathcal{F}_{n}$-可測確率変数 $X_{n}$ のシーケンス $\\left\\{ X_{n} \\right\\}_{n \\in \\mathbb{N}}$ が形成する順序対のシーケンス $\\left\\{ (X_{n}, \\mathcal{F}_{n}) \\right\\}$ が以下を満たす場合、マルチンゲールと言う。 $$ \\forall n \\in \\mathbb{N}, E \\left( X_{n+1} | \\mathcal{F}_{n} \\right) = X_{n} $$ $\\mathcal{F}_{n}$ が $\\mathcal{F}$ のサブσフィールドであるとは、両方とも $\\Omega$ のσフィールドであるが、$\\mathcal{F}_{n} \\subset \\mathcal{F}$ を意味する。 $X_{n}$ が $\\mathcal{F}_{n}$-可測関数であるとは、全てのボレル集合 $B \\in \\mathcal{B}(\\mathbb{R})$ に対して $X_{n}^{-1} (B) \\in \\mathcal{F}_{n}$ であることを意味する。 説明 それぞれ サブマルチンゲール、スーパーマルチンゲールとは以下のように言う。不等式は右辺が小さくなるとサブ、大きくなるとスーパーと覚えると混乱しない。 $$ \\forall n \\in \\mathbb{N}, E \\left( X_{n+1} | \\mathcal{F}_{n} \\right) \\ge X_{n} \\\\ \\forall n \\in \\mathbb{N}, E \\left( X_{n+1} | \\mathcal{F}_{n} \\right) \\le X_{n} $$ もちろん、サブマルチンゲールでありスーパーマルチンゲールでもある場合は、マルチンゲールと同等である。だから、サブマルチンゲールもしくはスーパーマルチンゲールに当てはまる定理があれば、マルチンゲールにもそのまま適用できる。\nマルチンゲールを直感的に理解することは、σフィールドを事件の集合、「情報」として考えることから始まる：\nフィルトレーション：$\\forall n \\in \\mathbb{N}, \\mathcal{F}_{n} \\subset \\mathcal{F}_{n+1}$、つまりσフィールドが大きいということは、それだけ多くの情報があるという意味だ。マルチンゲールの定義では、プロセス $X_{n}$ が $\\mathcal{F}_{n}$-可測であることは、実際のデータ $x_{n}$ が観測されるにつれてσフィールド $\\mathcal{F}_{n}$ も広がり$n$ 回までの全ての情報を得たと見なしてもよいということだ。 マルチンゲール：$\\forall n \\in \\mathbb{N}, E \\left( X_{n+1} | \\mathcal{F}_{n} \\right) = X_{n}$ とは、$n$ 回までの情報 $\\mathcal{F}_{n}$ を知っている時、次の状況である $X_{n+1}$ も$X_{n}$ と似ていると仮定することを意味する。$X_{n+1}$ の期待値がこれまでに得た $\\mathcal{F}_{n}$ とは無関係に算出されるのであれば、このような確率過程はホワイトノイズと変わらないし、統計的分析の対象にはならない。だから、マルチンゲールの直感的な定義は「私たちが何か有利な情報を持っていて、数学的、統計的により良い結果を知ることができる確率過程」と言える。 起源 「マルティグ」のフランスの村では、いわゆる「二倍返し戦略」が流行していた。一回負けたら、その損失を補うためにより大きな賭けを繰り返す方式で、心理的な面はさておき、これが本当に賢い戦略かどうか考える必要がある。数学的に見ると、このような戦略の本質は $$ E \\left( X_{n+1} | X_{1} , \\cdots , X_{n} \\right) = X_{n+1} $$ の式で要約できる。「これまでずっと負けてきたから、今回は勝つだろう」と賭博師の誤謬を指摘し、なぜマルチンゲールベッティングが意味をなさないかを説明する。\n例 (1) 自己回帰過程 $AR(1)$ $X_{n+1} = X_{n} + \\varepsilon_{n}$ を考えよう。フィルトレーションが与えられている場合、$X_{n}$ に対する情報をすべて知っているので、条件付き期待値の性質により $$ \\begin{align*} E \\left( X_{n+1} | \\mathcal{F}_{n} \\right) =\u0026amp; E \\left( X_{n} + \\varepsilon_{n} | \\mathcal{F}_{n} \\right) \\\\ =\u0026amp; E \\left( X_{n} | \\mathcal{F}_{n} \\right) + E \\left( \\varepsilon_{n} | \\mathcal{F}_{n} \\right) \\\\ =\u0026amp; X_{n} + E \\left( \\varepsilon_{n} | \\mathcal{F}_{n} \\right) \\\\ =\u0026amp; X_{n} + E ( \\varepsilon_{n} ) \\\\ =\u0026amp; X_{n} \\end{align*} $$ となるので、$\\left\\{ (X_{n}, \\mathcal{F}_{n}) \\right\\}$ はマルチンゲールとなる。\n(2) $\\left\\{ X_{n} \\right\\}_{n \\in \\mathbb{N}}$ が互いに独立であり、$E(X_{n}) = 0$ であり、$\\displaystyle S_{n}:= \\sum_{i =1}^{n} X_{i}$ とする。その場合 $$ \\begin{align*} E(S_{n+1} | \\mathcal{F}_{n} ) =\u0026amp; S_{n} + E( X_{n+1} | \\mathcal{F}_{n} ) \\\\ =\u0026amp; S_{n} + E( X_{n+1} ) \\\\ =\u0026amp; S_{n} + 0 \\end{align*} $$ となるので、$\\left\\{ (S_{n}, \\mathcal{F}_{n}) \\right\\}$ はマルチンゲールとなる。\n一方で、マルチンゲールと凸関数 $\\phi$ が与えられた場合、上記のようにサブマルチンゲールを作り出すことができる。\n定理 マルチンゲール $\\left\\{ (X_{n}, \\mathcal{F}_{n}) \\right\\}$ と凸関数 $\\phi: \\mathbb{R} \\to \\mathbb{R}$ に対して、$( \\phi (X_{n}) , \\mathcal{F}_{n} )$ はサブマルチンゲールである。\n証明 条件付きジェンセンの不等式：確率空間 $( \\Omega , \\mathcal{F} , P)$ とサブσフィールド $\\mathcal{G} \\subset \\mathcal{F}$ が与えられ、$X$が確率変数であるとする。凸関数 $\\phi: \\mathbb{R} \\to \\mathbb{R}$ と $\\phi (X) \\in \\mathcal{L}^{1} ( \\Omega ) $に対して $$ \\phi \\left( E \\left( X | \\mathcal{G} \\right) \\right) \\le E \\left( \\phi (X) | \\mathcal{G} \\right) $$\n条件付きジェンセンの不等式により $$ E \\left( \\phi (X_{n+1}) | \\mathcal{F}_{n} \\right) \\ge \\phi \\left( E \\left( X_{n+1} | \\mathcal{F}_{n} \\right) \\right) = \\phi ( X_{n} ) $$\n■\n結論 この定理の結論として、$p \\ge 1$ を $\\phi (x) = | x |^{p}$ として設定すると、$\\left\\{ |X_{n}|^p , \\mathcal{F}_{n} \\right\\}$ は常にサブマルチンゲールであることが分かる。\n関連項目 様々なフィルトレーション $$ A_{1} \\subset A_{2} \\subset \\cdots \\subset A_{n} \\subset \\cdots $$ 一般的に数学全般で、上記のようなネステッドシーケンスNested Sequenceを形成する構造をフィルトレーションFiltrationと使用する。\n確率過程のフィルトレーション コンプレックスのフィルトレーション ベクター空間のフラグ ","id":1349,"permalink":"https://freshrimpsushi.github.io/jp/posts/1349/","tags":null,"title":"マルチンゲールの定義"},{"categories":"집합론","contents":"公理 $$ \\exists U \\left( \\emptyset \\in U \\land \\forall X ( X \\in U \\implies S(X) \\in U) \\right) $$ 空集合と$X$を要素として持ち、$S(X)$も要素として持つ集合$U$が存在する。\n集合$X$に対して、$S(X)$は$S(X):= X \\cup \\left\\{ X \\right\\}$と同じように定義される集合である。 説明 なぜこれが無限公理と呼ばれるのかを長々と説明するより、自然数集合$\\mathbb{N}$の存在性を証明することが良いだろう。\n定理：自然数集合の存在性 $\\mathbb{N}$は存在する。\n証明 戦略：フォン・ノイマンが提案した構築法を用い、自然数自体を集合と対応させて自然数の集合$\\mathbb{N}$を直接構築する。これにより、$\\mathbb{N}$は存在し、同時に自然数の性質も即座に有する。\n空集合$\\emptyset$とその$S(n)$について、次のように定義しよう。 $$ 0 : = \\emptyset \\\\ ( n + 1 ):= S(n) = n \\cup \\left\\{ n \\right\\} $$ すると $$ 1 = 0+1 = S ( 0 ) = \\left\\{ 0 \\right\\} \\\\ 2 = 1+1 = S ( 1 ) = \\left\\{ 0, \\left\\{ 0 \\right\\} \\right\\} = \\left\\{ 0, 1 \\right\\} \\\\ 3 = 2+1 = S ( 2 ) = \\left\\{ 0, \\left\\{ 0 \\right\\}, \\left\\{ 0, \\left\\{ 0 \\right\\} \\right\\} \\right\\} = \\left\\{ 0, 1, 2 \\right\\} \\\\ \\vdots $$ 無限公理により、$\\mathbb{N} = \\left\\{ 1, 2, 3, \\cdots \\right\\}$は次の性質を満たしながら存在する。 $$ n_{1} \\in n_{2} \\iff n_{1} \u0026lt; n_{2} \\\\ n_{1} \\subset n_{2} \\iff n_{1} \\le n_{2} $$\n■\n自然数が無限に多いという主張はどうあれ真実だろうが、実際にこの宇宙の誰もが無限に多い自然数を見たことはない。いくら長く、一貫して、多くの自然数を探しても、無限集合が存在することを帰納的に証明することは不可能だ。無限公理はこのような無限を説明するために導入されたものであり、直感的にこれを拒否する理由は全くないだろう。\n","id":1348,"permalink":"https://freshrimpsushi.github.io/jp/posts/1348/","tags":null,"title":"無限公理"},{"categories":"집합론","contents":"公理 1 $$ \\forall X \\exists P \\forall A ( A \\subset X \\implies A \\in P) $$ 任意の集合 $X$ に対し、$X$ のすべての部分集合を元に持つ集合 $P$ が存在する。\n説明 $X$ の冪集合は一般的に $\\mathcal{P} (X)$ と表記されたり、$2^{X}$ と書かれるが、それは有限集合 $X$ の要素の数を $|X|$ としたとき、$\\left| \\mathcal{P} (X) \\right| =2^{|X|}$ であるからだ。必ずしも数が重要なわけではないから、集合論を多く使用するほど、$2^{X}$ のような表現を好む。\n要素の数が指数関数的に増加することからある程度推測できるが、冪集合 $2^{X}$ というのは $X$ と比較した時にかなり、非常に大きい。また、冪集合の公理により、冪集合の冪集合も存在するので、このようにして大きさを増していく集合をどんなにでも考えることができる。\n冪集合の例として、次のものを考えてみよう：\n$$ X = \\left\\{ 1,2 \\right\\} $$\n$$ 2^{X} = \\left\\{ \\emptyset , \\left\\{ 1 \\right\\} , \\left\\{ 2 \\right\\}, \\left\\{ 1,2 \\right\\} \\right\\} $$ ここで、空集合 $\\emptyset$ と元の集合 $X$ が $2^{X}$ に属していることに注目されたい。以下はいくつかの冪集合の基本的な性質だ。最初は不自然に感じられるかもしれないが、空集合もしっかりとした集合であり、集合が集合の元になりうるので、自然に受け入れるべきである。\n基礎性質 [1]: $A \\subset X \\iff A \\in 2^{X}$ [2]: $\\emptyset \\in 2^{X}$ [3]: $X \\in 2^{X}$ 集合の包含関係の概念が不十分なら、当然混乱するだろう。残念ながら、集合論を初めて触れるとき、冪集合はあまり多く使われず、慣れるための良い練習問題も特にない。現実的には、時間が解決策だと思い、ゆっくりと慣れていくしかない。\n이흥천 역, You-Feng Lin. (2011). 집합론(Set Theory: An Intuitive Approach): p83.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1346,"permalink":"https://freshrimpsushi.github.io/jp/posts/1346/","tags":null,"title":"冪集合公理"},{"categories":"집합론","contents":"公理 $$ \\forall X \\left( \\exists U \\left( \\forall a \\left( a \\in x \\land x \\in X \\implies a \\in U \\right) \\right) \\right) $$ 任意の集合$X$に対して、$X$の全ての要素の要素を含む集合$U$が存在する。\n和集合の定義 1 和集合公理は、以下のように定義される和集合の存在を保証する。 $$ x \\in A \\lor x \\in B \\iff x \\in A \\cup B $$ 任意の二つの集合$A$、$B$に対して、少なくともどちらか一方に属する要素の集合を$A$と$B$の和集合と呼び、$A \\cup B$のように示す。\n説明 和集合公理と和集合の定義は、明確に異なる。もちろん、定義は単にある概念を言うだけであり、公理がその存在を保証するという違いもあるが、「要素の要素を含む」という表現は異なる。要素1の要素2と言えば、要素1は当然集合であり、要素1の形状は対公理を通じて存在が保証され、$\\left\\{ A, B \\right\\}$の$A$、$B$のように見える。言い換えれば、ただの和集合は、$A$と$B$の間の操作$\\cup$で作られるものと見ることができ、和集合公理が述べようとしている和集合の概念は、$X = \\left\\{ A, B \\right\\}$のような集合の集合が与えられたとき、$U(X) := \\left\\{ a \\in x : x \\in X \\right\\}$のようなものを指す。\n実際、学部レベル以下の数学を扱う際にこの区別が大きな意味を持つことはないが、好奇心からでも公理を理解したい、または珍しく必要な場合は、正確に理解して進むべきだ。\n基本的性質 集合$X$の部分集合$A$、$B$、$C$に対して、次が成り立つ。\n[1] 同一律: $$ A \\cup \\emptyset = A \\\\ A \\cap X = A $$ [2] 冪等律: $$ A \\cup A = A \\\\ A \\cap A = A $$ [3] 交換律: $$ A \\cup B = B \\cup A \\\\ A \\cap B = B \\cap A $$ [4] 結合律: $$ A \\cup ( B \\cup C) = (A \\cup B) \\cup C \\\\ A \\cap (B \\cap C) = ( A \\cap B ) \\cap C $$ [5] 分配律: $$ A \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C) \\\\ A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C) $$ [6] ド・モルガンの定理: $$ (A \\cup B)^{c} = A^{c} \\cap B^{c} \\\\ (A \\cap B)^{c} = A^{c} \\cup B^{c} $$ [7] $$ (A \\setminus B)^{c} = A^{c} \\cup B $$ 証明 [7] $$ \\begin{align*} x \\in (A \\setminus B)^{c} \u0026amp;\\iff x \\notin A \\setminus B \\\\ \u0026amp;\\iff x \\notin A \\text{ or } x \\in B \\\\ \u0026amp;\\iff x \\in A^{c} \\text{ or } x \\in B \\\\ \u0026amp;\\iff x \\in A^{c} \\cup B \\end{align*} $$\n■\n李興天 訳, You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p87.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1344,"permalink":"https://freshrimpsushi.github.io/jp/posts/1344/","tags":null,"title":"和集合の公理"},{"categories":"집합론","contents":"公理 1 $$ \\forall X \\exists A \\forall a \\left( a \\in A \\iff ( a \\in X \\land p(a)) \\right) $$ 任意の集合 $X$ に対して、性質 $p$ を持つ要素で構成された部分集合 $A$ が存在する。\n$p(x)$ は $X$ 内の命題関数だ。 説明 $A$ を $X$ の部分集合として限定する理由は、ラッセルの逆理のような問題が起こるのを防ぐためだ。公理ではなく公理形である理由は、この公理が無限に多くの $p(x)$ に基づいて無限に存在するからだ。異なる二つの命題関数 $p_{1}(x)$ と $p_{2}(x)$ があるとすれば、$\\left\\{ a \\in X : p_{2}(a) \\text{ is truth} \\right\\} \\subset X$ の存在を保証するのは「$p_{1}(x)$ に関する分類公理」ではなく「$p_{2}(x)$ に関する分類公理」だ。\n交わりと差集合の定義 2 分類公理形は、以下のように定義される交わりの存在を保証する。\n$$ x \\in A \\land x \\in B \\iff x \\in A \\cap B $$ 任意の二つの集合 $A$、$B$ に対して、両方に属する要素の集合を $A$ と $B$ の交わりといい、$A \\cap B$ と表示する。\nここで、集合 $A$ に対して与えられた命題関数は $p(x): x \\in B$ で、具体的に $A \\cap B= \\left\\{ x \\in A : x \\in B \\right\\}$ のように書ける。もし $A \\cap B = \\emptyset$ ならば $A$ と $B$ は互いに素Disjointという。\n勿論、分類公理形は交わりの存在だけでなく、特定の条件を満たす全ての部分集合の存在も保証する。これは集合を表現する方法の一つである条件提示法そのものと見ることもできる。\n$$ x \\in A \\land x \\notin B \\iff x \\in A \\setminus B $$ 任意の二つの集合 $A$、$B$ に対して、$A$ には属するが $B$ には属さない要素の集合を $A$ に対する $B$ の差集合といい、$A \\setminus B$ と表示する。\n集合 $U$ に対して $U \\setminus A$ を $A$ の補集合といい、$A^{c}$ と表示する。このように補集合を考えるとき、集合 $U$ を全集合とも呼ぶ。\n集合論は無限だが、数学の全ての分野が抽象的な世界全体を探求する必要はない。通常、必要に応じてある全集合を設定し、位相数学のような分野はこれらの概念を特に多く使用する。補集合と全集合に関して、以下のいくつかの性質を紹介する。\n基本性質 集合 $A$、$B$ が全集合$U$ の任意の部分集合であるとする。\n[1] $$ \\left(A^{c} \\right)^{c} = A $$ [2] $$ \\emptyset^{c} = U \\\\ U^{c} = \\emptyset $$ [3] $$ A \\cap A^{c} = \\emptyset \\\\ A \\cup A^{c} = U $$ [4] $$ A \\subset B \\implies B^{c} \\subset A^{c} $$ [5] $$ A \\setminus B = A \\cap B^{c} $$ 証明 [5] $$ \\begin{align*} x \\in A \\setminus B \u0026amp;\\iff x \\in A \\text{ and } x \\notin B \\\\ \u0026amp;\\iff x \\in A \\text{ and } x \\in B^{c} \\\\ \u0026amp;\\iff x \\in A \\cap B^{c} \\end{align*} $$\n■\n李興天 訳, You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p81.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n李興天 訳, You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p87, 95.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1341,"permalink":"https://freshrimpsushi.github.io/jp/posts/1341/","tags":null,"title":"分類 公理形"},{"categories":"집합론","contents":"公理 1 $$ \\exists X \\forall x \\left( \\lnot \\left( x \\in X \\right) \\right) $$ 何の要素も持たない集合 $X$ が存在し、この集合 $X$ を空集合と定義する。\n説明 空集合は一般に $\\emptyset$ のように表記される。一方で空集合は、要素の数が $0$ の集合としても見ることができ、このように要素の数で定義できる集合には以下のようなものがある：\n単元素集合：要素の数がちょうど一つの集合を単元素集合という。 有限集合：集合の要素の数が $\\mathbb{N}$ に属している場合、有限集合という。 無限集合：空集合でも有限集合でもない場合、無限集合という。 ここで有限集合、無限集合の定義は少し雑だが、後で厳密に再定義される。\n注意すべき点は、単元素集合 $\\left\\{ x \\right\\}$ はやはり集合であり、$x$ は $\\left\\{ x \\right\\}$ の要素として明らかに異なるものであるということだ。さらに言えば、実際の現代数学では$x := \\left\\{ x \\right\\}$ のような定義さえ許されない。\n空集合の公理と空集合の定義を区別することは言葉通り、それら二つが異なるからである。空集合自体は空集合の公理にかかわらず定義はできる。しかし、実際に存在しているかは別問題である。空集合が存在することは直感的に理解しているが、単なる定義ではそのことを保証できない。これは解析学での完備性公理に似ている。\n空集合の存在が当たり前でない理由は集合の定義を考えればわかるかもしれない。私たちは集合を直感または思考の対象として、互いに明確に区別されるオブジェクトの集まりとしたし、集合に属するオブジェクトを要素とした。しかし、この定義によれば空集合は「区別される個体」をまったく持たないべきなのに、集めるオブジェクトが一切ないにもかかわらず集まりがあるということは明らかに奇妙である。それにもかかわらず、私たち人間は「不存在の存在」についてあまりにもよく知っているため、空集合を扱うためにこうした公理を追加することになる。\nこのような当たり前でないことを理解したり、共感したりすることとは無関係に、空集合の存在は他の公理から導出されることがある。公理は少なければ少ないほど良いため、通常は教科書で空集合の公理は省略される傾向にある。\n李興天 訳, You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p75.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1337,"permalink":"https://freshrimpsushi.github.io/jp/posts/1337/","tags":null,"title":"空集合の公理"},{"categories":"집합론","contents":"定義 1 $$ A \\subset B \\iff \\forall x (x\\in A \\implies x \\in B) $$ 任意の集合 $A$、 $B$ について、$A$ の全ての要素が $B$ の要素である場合、$A$ は $B$ の部分集合Subset、 $B$ は $A$ の上位集合Supersetと言い、$A \\subset B$ として表される。\n解説 $A \\subset B$ であり $B \\not\\subset A$ である場合、$A$ を $B$ の真部分集合Proper Subsetと言い、$A \\subsetneq B$ として表される。\n細かい注意点として、$A \\subset B$ は $A$ が $B$ に含まれると言い、$a \\in A$ は $a$ が $A$ に属すると言うことである。これが同じに見えるかもしれないが、実際の言語の習慣では混乱が生じやすいし、意味が通じていれば無理に突っ込む人はほとんどいない。しかし、包含関係は集合同士で定義されたものであり、$a \\in A$ を「集合と要素の所属関係」とは言わない点は、知っておくべき違いである。\nまとめ: 包含関係の推移性Transitivity 任意の集合 $A$、 $B$、 $C$ に対して $$A \\subset B \\land B \\subset C \\implies A \\subset C$$\n証明 仮定により、 $$ A \\subset B \\iff \\forall x (x\\in A \\implies x \\in B) \\\\ B \\subset C \\iff \\forall x (x\\in B \\implies x \\in C) $$ 三段論法によると、 $$ \\forall x (x\\in A \\implies x \\in C) \\iff A \\subset C $$\n■\n金星町 訳、You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p77.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1329,"permalink":"https://freshrimpsushi.github.io/jp/posts/1329/","tags":null,"title":"集合の包含関係"},{"categories":"매트랩","contents":"方法 linspace(a,b,n): 区間$[a,b]$を$n$個の等間隔に分けた行ベクトルを返す。 要素数を入力しなければ、$1\\times 100$ベクトルを返す。間隔の数ではなく、間隔の長さが重要な時に使われる。\na: m :b : 区間$[a,b]$を等間隔$m$で分けた行ベクトルを返す。 間隔を入力しなければ、間隔は$1$に設定される。間隔の数ではなく、間隔の長さが主要な時に使われる。$b=a+n\\cdot m$を満たす自然数$n$が存在しないこともある。この場合、終点は$b$ではなく、$a+n\\cdot m$を満たす最大の数になる。\nx1=linspace(1,10,10)\rx2=linspace(1,10)\rx3=linspace(5,55,2^5)\ry1=1:3\ry2=1:1/13:2\ry3=3:1/7:7 他の言語で Juliaで ","id":1376,"permalink":"https://freshrimpsushi.github.io/jp/posts/1376/","tags":null,"title":"MATLABで等間隔の行ベクトルを生成する方法"},{"categories":"확률론","contents":"定理 確率空間 $( \\Omega , \\mathcal{F} , P)$ が与えられているとする。\n[1] 測度論での定理: 可測関数 $f$, $g$ が $\\mathcal{F}$-可測であれば、$g = h (f)$ を満たすボレル関数 $h : \\mathbb{R} \\to \\mathbb{R}$ が存在する。 [2] 確率論での応用: 確率変数 $X$, $Y$ が $\\sigma (X)$-可測であれば、$E(Y | X) = h(X)$ を満たすボレル関数 $h : \\mathbb{R} \\to \\mathbb{R}$ が存在する。 [3]: $X$ が $\\mathcal{F}$-可測であれば $$E(X|\\mathcal{F}) =X \\text{ a.s.}$$ [4]: シグマ場 $\\mathcal{G} = \\left\\{ \\emptyset , \\Omega \\right\\}$ に対して $$E(X|\\mathcal{G}) = E(X) \\text{ a.s.}$$ [5]: 定数 $c$ と全てのシグマ場 $\\mathcal{G}$ に対して $$E(c|\\mathcal{F}) = c \\text{ a.s.}$$ [6]: 定数 $c$ に対して $$E(cX | \\mathcal{G}) = c E(X | \\mathcal{G}) \\text{ a.s.}$$ [7]: $$E(X+Y | \\mathcal{G}) = E(X | \\mathcal{G}) + E(Y| \\mathcal{G}) \\text{ a.s.}$$ [8]: $X \\ge 0 \\text{ a.s.}$ であれば $$E(X | \\mathcal{G}) \\ge 0 \\text{ a.s.}$$ [9]: $X \\ge Y \\text{ a.s.}$ であれば $$E(X | \\mathcal{G}) \\ge E(Y | \\mathcal{G}) \\text{ a.s.}$$ [10]: $$\\left| E( X | \\mathcal{G} ) \\right| \\le E ( | X | | \\mathcal{G} ) \\text{ a.s.}$$ [11]: 全てのシグマ場 $\\mathcal{G}$ に対して $$E \\left[ E ( X | \\mathcal{G} ) \\right] = E(X)$$ $\\sigma (X) = \\left\\{ X^{-1} (B) : B \\in \\mathcal{B}(\\mathbb{R}) \\right\\}$ は確率変数 $X$ によって生成される $\\Omega$ の最小のシグマ場を表す。これについて $E(Y|\\sigma (X)) = E(Y|X)$ のように表記できる。 $Z$ が $\\mathcal{F}$-可測関数であることは、全てのボレル集合 $B \\in \\mathcal{B}(\\mathbb{R})$ に対して $Z^{-1} (B) \\in \\mathcal{F}$ という意味である。 ボレル関数とは、全てのボレル集合 $B \\in \\mathcal{B}(\\mathbb{R})$ に対して $f^{-1} (B)$ もボレル集合である関数 $f : \\mathbb{R} \\to \\mathbb{R}$ を指す。 説明 [1],[2]: これらの二つの定理は、$X$ に関する $Y$ の条件付き期待値が $X$ に依存するある関数として表されることを示している。特に、$X$ の値が与えられた場合は、$E(Y | X = a) = h(a)$ のように表される。[2]は[1]の系として、これにより、基本的な確率論でも日常的に使用される期待値の性質がほとんど確実に保証される。 線形性 [5]～[7]: $E(aX + b | \\mathcal{G}) = aE(X | \\mathcal{G}) + b$: 期待値の線形性は、条件付きであっても保持される。 シグマ場は情報である [3] $E(X | \\mathcal{F}) = X$: 式の意味を考えると、確率変数 $X$ が $\\mathcal{F}$-可測であることは、シグマ場 $\\mathcal{F}$ が $X$ の全ての情報を持っていることを意味する。逆に考えると、そのために可測と呼ぶのである。したがって、$E(X|\\mathcal{F})$ はいかなる妨害もなく $X$ をそのまま把握できる。$\\mathcal{F}$ 上で全ての情報が与えられた $X$ は、わざわざ $E$ で計算する必要はない。次の例を考えてみよう： 6面のサイコロを振って、目ごとに1ドルもらうゲームをするとき、もらえるお金の期待値は3.5ドルである。これを計算する理由は、実際にサイコロの目が何になるかわからないからである。しかし、サイコロを振る前に私の頭の中にシグマ場 $\\mathcal{F}$ が正確に与えられるならば、サイコロの目 $X$ を正確に測定できるため、正確に何ドルもらえるかがわかる。毎回3.5ドルを支払うとしても、勝つゲームはして、負けるゲームはしなければそれでよい。この意味で、乱数ハッキングは、シグマ場（乱数表）を盗んで、本来ランダムであるべきものを決定的にする攻撃技術に相当する。これが成功すれば、銀行のセキュリティカードやOTPなど、乱数に依存する暗号システムが破られる。 一方、$\\sigma (X)$ は $X$ の全ての情報を持ちながら最小のシグマ場として定義されているので、当\n然 $E(X| \\sigma (X)) = X$ である。これは上で紹介した表記に従って、$E(X|X) = X$ のようである。\n[4] $E(X|\\mathcal{G}) = E(X)$: 式の意味を考えると、トリビアルなシグマ場 $\\mathcal{G} = \\left\\{ \\emptyset , \\Omega \\right\\}$ は $X$ に関してどのような情報も与えないため、途方に暮れて確率空間 $\\Omega$ 全体を探して $\\displaystyle \\int_{\\Omega} X d P$ を計算するしかない。 [10] $\\left| E( X | \\mathcal{G} ) \\right| \\le E ( | X | | \\mathcal{G} )$: 絶対値の性質により $$ - E ( | X | | \\mathcal{G} ) \\le E( X | \\mathcal{G} ) \\le E ( | X | | \\mathcal{G} ) $$ [11] $E \\left[ E ( X | \\mathcal{G} ) \\right] = E(X)$: 確率論の様々な証明で有用に使用される等式で、主に $E(X)$ は直接計算が難しいが、何らかの $\\mathcal{G}$ が与えられると $E(X|\\mathcal{G})$ が計算しやすくなる場合にトリックとして使用される。 証明 [1] $h : \\mathbb{R} \\to \\mathbb{R}$ を $z \\in \\mathbb{R}$ に対して $h(z) := \\left( g \\circ f^{-1} ( \\left\\{ z \\right\\} ) \\right)$ のように定義する。\n$\\left\\{ z \\right\\} \\in \\mathcal{B}(\\mathbb{R})$ の場合、$f$ は $\\mathcal{F}$-可測なので、$f^{-1}(\\left\\{ z \\right\\}) \\in \\mathcal{F}$ であり、$g$ も $\\mathcal{F}$-可測なので、$h$ はよく定義され、$g (\\omega) = ( h \\circ f ) ( \\omega )$ を満たす。\n全てのボレル集合 $B \\in \\mathcal{B}(\\mathbb{R})$ に対して $$ h^{-1}(B) = (f \\circ g^{-1})(B) = f \\left( g^{-1} (B) \\right) $$ を考えると、$g^{-1} (B) \\in \\mathcal{F}$ なので $f(g^{-1} (B) ) \\in \\mathcal{B}(\\mathbb{R})$ である。全ての $B \\in \\mathcal{B}(\\mathbb{R})$ に対して $h^{-1}(B) \\in \\mathcal{B}(\\mathbb{R})$ なので、$h$ はボレル関数である。\n■\n[2] $E ( Y | X ) = E ( Y | \\sigma (X) )$ は条件付き期待値の定義により $\\sigma (X)$-可測な確率変数であり、$X$ も $\\sigma (X)$ の定義に従って明らかに $\\sigma (X)$-可測な確率変数である。したがって、[1]により $\\mathcal{F} = \\sigma (X)$ とし、 $$ f = X \\\\ g = E ( Y | X ) $$ とすると、$E(Y|X) = h(X)$ を満たすボレル関数 $h : \\mathbb{R} \\to \\mathbb{R}$ が存在する。\n■\n戦略 [3]～[7]: 積分形に変換して展開し、定積分が同じであることを示した後、次の定理を適用する。元々特別な名前はないが、この投稿でのみルベーグ積分の補題と命名することにする。\nルベーグ積分の性質 $$ \\forall A \\in \\mathcal{F}, \\int_{A} f dm = 0 \\iff f = 0 \\text{ a.e.} $$\n[3] 全ての $A \\in \\mathcal{F}$ に対して $\\displaystyle \\int_{A} X dP = \\int_{A} X dP$ を満たす $X$ が一意に存在するので、条件付き期待値の定義により、$X = E(X| \\mathcal{F})$ は $\\mathcal{F}$ に対する $X$ の条件付き期待値である。したがって、全ての $A \\in \\mathcal{F}$ に対して $$ \\int_{A} E(X |\\mathcal{F}) dP = \\int_{A} X dP $$ となり、ルベーグ積分の補題により $X = E(X |\\mathcal{F}) \\text{ a.s.}$\n■\n[4] 条件付き期待値の定義により、$\\displaystyle \\int_{A} E(X |\\mathcal{G}) dP = \\int_{A} X dP$ である。\nケース 1. $A = \\emptyset$\n$$ 0 = \\int_{\\emptyset} E(X |\\mathcal{G}) dP = \\int_{\\emptyset} X dP = 0 $$\nケース 2. $A = \\Omega$\n$$ \\int_{\\Omega} E(X |\\mathcal{G}) dP = \\int_{\\Omega} X dP = E(X) = E(X) P(\\Omega) = E(X) \\int_{\\Omega} 1 dP = \\int_{\\Omega} E(X) dP $$\nしたがって、どちらの場合も、ルベーグ積分の補題により $X = E(X |\\mathcal{G}) \\text{ a.s.}$\n■\n[5] $c \\in \\mathcal{G}$ であり、$E(c | \\mathcal{G}) \\in \\mathcal{G}$ なので、条件付き期待値の定義により、全ての $A \\in \\mathcal{G}$ に対して $$ \\int_{A} E(c |\\mathcal{G}) dP = \\int_{A} X dP $$ となり、したがってルベーグ積分の補題により $c = E(c | \\mathcal{G}) \\text{ a.s.}$\n■\n[6] 条件付き期待値の定義とルベーグ積分の線形性により、全ての $A \\in \\mathcal{G}$ に対して $$ \\begin{align*} \\int_{A} E( cX |\\mathcal{G}) dP =\u0026amp; \\int_{A} cX dP \\\\ =\u0026amp; c \\int_{A} X dP \\\\ =\u0026amp; c \\int_{A} E(X|\\mathcal{G}) dP \\\\ =\u0026amp; \\int_{A} c E(X|\\mathcal{G}) dP \\end{align*} $$ となり、ルベーグ積分の補題により $E( cX |\\mathcal{G}) = c E(X|\\mathcal{G}) dP \\text{ a.s.}$\n■\n[7] 条件付き期待値の定義とルベーグ積分の線形性により、全ての $A \\in \\mathcal{G}$ に対して $$ \\begin{align*} \\int_{A} E( X+Y |\\mathcal{G}) dP =\u0026amp; \\int_{A} (X+Y) dP \\\\ =\u0026amp; \\int_{A} X dP +\\int_{A} Y dP \\\\ =\u0026amp; \\int_{A} E(X|\\mathcal{G}) dP + \\int_{A} E(Y|\\mathcal{G}) dP \\\\ =\u0026amp; \\int_{A} \\left[ E(X|\\mathcal{G}) + E(Y|\\mathcal{G}) \\right] dP \\end{align*} $$ となり、ルベーグ積分の補題により $$ E( X +Y |\\mathcal{G}) = E(X|\\mathcal{G}) + E(Y|\\mathcal{G}) dP \\text{ a.s.} $$\n■\n[8] $E( X |\\mathcal{G}) \u0026lt; 0$ と仮定すると $$ \\begin{align*} \\int_{A} E( X |\\mathcal{G}) dP =\u0026amp; \\int_{A} X dP \\\\ \\ge\u0026amp; \\int_{A} 0 dP \\\\ =\u0026amp; 0 \\end{align*} $$ となるため、矛盾が生じる。したがって、$E( X |\\mathcal{G}) \\ge 0 \\text{ a.s.}$ でなければならない。\n■\n[9] $Z := X - Y \\ge 0$ とすると、[8] により $$ E(X-Y | \\mathcal{G}) \\ge 0 $$ となり、条件付き期待値の線形性により $$ E(X| \\mathcal{G}) - E(Y | \\mathcal{G}) \\ge 0 \\text{ a.s.} $$\n■\n[10] パート 1. $X \\ge 0$\n$X \\ge 0$ の場合、$|X| = X$ となるため $$ E( |X| |\\mathcal{G}) = E(X|\\mathcal{G}) $$\n[8]により $E(X|\\mathcal{G}) \\ge 0$ となるため、同様に $E(X|\\mathcal{G}) = \\left| E(X|\\mathcal{G}) \\right|$ となり $$ E( |X| |\\mathcal{G}) = E(X|\\mathcal{G}) = \\left| E(X|\\mathcal{G}) \\right| $$\nパート 2. $X \u0026lt; 0$\n[6]により $$ E( |X| |\\mathcal{G}) = E( -X |\\mathcal{G}) = - E(X |\\mathcal{G}) = \\left| E(X|\\mathcal{G}) \\right| $$\nパート 3. $X = X^{+} - X^{-}$\n三角不等式により $$ \\left| E(X|\\mathcal{G}) \\right| \\le \\left| E( X^{+} |\\mathcal{G}) \\right| + \\left| E( X^{-} |\\mathcal{G}) \\right| $$ $X^{+} , X^{-} \\ge 0$ であるため、パート 1により $$ \\left| E(X|\\mathcal{G}) \\right| \\le E( \\left| X^{+} \\right| |\\mathcal{G}) + E( \\left| X^{-} \\right| | \\mathcal{G}) $$\n[7]と絶対値の表示 $|f| = |f^{+}| + |f^{-}|$ により $$ \\begin{align*} \\left| E(X|\\mathcal{G}) \\right| \\le \u0026amp; E( \\left| X^{+} \\right| + \\left| X^{-} \\right| | \\mathcal{G}) \\\\ =\u0026amp; E( \\left| X \\right| | \\mathcal{G}) \\text{ a.s.} \\end{align*} $$\n■\n[11] $$ \\begin{align*} E \\left[ E( X | \\mathcal{G} ) \\right] =\u0026amp; \\int_{\\Omega} E ( X | \\mathcal{G} ) d P \\\\ =\u0026amp; \\int_{\\Omega} X d P \\\\ =\u0026amp; E(X) \\end{align*} $$\n■\n","id":1322,"permalink":"https://freshrimpsushi.github.io/jp/posts/1322/","tags":null,"title":"条件付き期待値の性質"},{"categories":"알고리즘","contents":"ビルドアップ 問題を解く時、大きな問題の解答がそれより小さい問題の解答を含んでいれば、最適部分構造Optimal Substructureを持つと言われる。最適部分構造を持つ問題の例として一番簡単なのが フィボナッチ数を求めることである。$n$ 番目のフィボナッチ数は$a_{n} = a_{n-1} + a_{n-2}$ のように求められるため、大きな問題 $a_{n}$ が小さな問題 $a_{n-1}$、$a_{n-2}$ を含んでいるからである。\nこれを簡単に解く方法は、再帰関数を使うことである。実際、最適部分構造を持っていればどんな問題も再帰関数で解けそうだが、実装してみると、重複呼び出しのために非常に非効率的である。\n定義 動的プログラミングDynamic Programmingは、これらの困難を回避するための技術であり、次の条件を満たす時に使ってみる価値がある：\n(i): 与えられた問題が最適部分構造を持っている。 (ii): 再帰関数で実装するには非効率的である。 例 動的プログラミングの概念は単純だ。小さな問題の解答を保存しながら、重複呼び出しを省略することである。たとえば、6番目のフィボナッチ数$a_{6}$を求めるためには $$ a_{1} = 1 \\\\ a_{2} = 1 \\\\ a_{3} = 2 \\\\ a_{4} = 3 \\\\ a_{5} = 5 $$ を何度も計算するのではなく、メモリに保存しておき、最後に$a_{4}$と$a_{5}$だけを呼び出して$a_{6} = a_{5} + a_{4} = 8$を計算する。これをメモイゼーションMemoizationと言う1。\n説明を読めば、特にダイナミックではないように思えるが、元々は制御分野で、解答をテーブルに保存しながら解いていくことから由来した言葉だという説があった。しかし、動的プログラミングの考案者であるベルマンBellmanが自著で明かしたところによると、ダイナミックという言葉が格好良く、資金獲得に適しているから選んだとのことである。\n以下は、$a_{36}=14930352$を二つの方法で計算し、その時間を計測したGIFと、そのPythonコードである。動的プログラミングでは0.01秒ちょっとの時間しかかからなかったが、再帰関数では5秒以上かかった。この差は$n$が大きくなるほど大きくなる。理論的にも、与えられた$n$に対して再帰関数は指数的に、動的プログラミングは線形に時間を使うことが簡単に証明できる。\nコード import Time\rdef fibo1(n) :\rif n==1 or n==2 :\rreturn 1\relse :\rreturn fibo1(n-1) + fibo1(n-2)\rdef fibo2(n) :\rmemoization = [1,1]\rfor i in range(n-2) :\rmemoization.append(memoization[-1]+memoization[-2])\rreturn memoization[-1]\rstart = Time.time() print(fibo2(36))\rprint(\u0026#34;동적 프로그래밍으로 계산할 때 걸린 시간 : %5.3f\u0026#34; % (time.time() - start))\rstart = Time.time() print(fibo1(36))\rprint(\u0026#34;재귀함수로 계산할 때 걸린 시간 : %5.3f\u0026#34; % (time.time() - start)) メモMemoから来た言葉で、メモライゼーションMemorizationではない。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1262,"permalink":"https://freshrimpsushi.github.io/jp/posts/1262/","tags":null,"title":"ダイナミックプログラミング"},{"categories":"집합론","contents":"定義 1 集合: 我々の直感や思考の対象として互いに明確に区別される対象の集まりを集合という。 要素: 集合に含まれる対象を要素という。 命題関数: 集合$U$の要素$x$に対して真または偽のいずれかである命題$p(x)$を$U$の命題関数という。 説明 数学で集合はほとんど母国言語に匹敵するほど重要な概念だ。自然言語よりも優れているかもしれない。なぜなら、必然的についてくる曖昧さを排除して、その定義と形式だけで論理を展開できるからだ。 通常、要素は小文字で、集合は大文字で表される。$a$が$A$に属している場合、$a \\in A$と表され**$a$は$A$の要素**と言われる。もちろん、要素と集合を必ずアルファベットの大文字と小文字で表示する必要はない。全ての自然数の集合は通常$\\mathbb{N}$と表示され、$N \\in \\mathbb{N}$のように表示しても何の問題もない。 列挙: 自然数の集合$\\mathbb{N}$は$\\left\\{ 1 , 2, 3, \\cdots \\right\\}$のように表すことができる。こうして集合の要素を直接書き出して表示する表記法を列挙法という。 条件表示法: 列挙法とは異なり、特定の条件を満たす要素のみを集めた集合として表現することもできる。例えば、$5$より大きい自然数のみを持つ集合を表現したい場合には、命題関数$p(x): x \u0026gt; 5$に対して$\\left\\{ x \\in \\mathbb{N} : p(x) \\text{ is truth} \\right\\}$のように表現される。これをさらに簡単に、命題関数を別に定義せずに$\\left\\{ x \\in \\mathbb{N} : x \u0026gt; 5 \\right\\}$のように表現する。この表記法を条件表示法という。 命題関数は命題関数そのものとして定義される点に注意する必要がある。集合論で語る関数の定義に合致しているが、命題のみで定義できるというのが重要だ。この点が不明確な場合、関数が関数として循環定義される事態になりかねず、条件表示法の自由な使用が困難になる。一方、命題関数は論理式とも呼ばれる。 李興天 訳, You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p47, 73, 81, 85.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1316,"permalink":"https://freshrimpsushi.github.io/jp/posts/1316/","tags":null,"title":"集合と命題関数の定義"},{"categories":"확률론","contents":"定義 確率空間 $( \\Omega , \\mathcal{F} , P)$ が与えられているとしよう。\n$\\mathcal{G}$ が$\\mathcal{F}$ の部分シグマ場であり、確率変数 $X \\in \\mathcal{L}^{1} ( \\Omega )$ が可積分であるとする。全ての $A \\in \\mathcal{G}$ において $$ \\int_{A} Y d P = \\int_{A} X d P $$ を満たす$\\mathcal{G}$-可測確率変数 $Y$ が一意に存在する場合、$Y := E ( X | \\mathcal{G} )$ を**$\\mathcal{G}$ に関する $X$ の条件付き期待値**と定義する。\n測度論に触れたことがなければ、確率空間という言葉を無視してもいいと言いたいが、測度論を全く知らないでこのポストの内容をきちんと理解することはほとんど不可能だ。 $\\mathcal{G}$ が$\\mathcal{F}$ の部分シグマ場であるとは、二つとも$\\Omega$ のシグマ場であり、$\\mathcal{G} \\subset \\mathcal{F}$ であることを意味する。$Y$ が$\\mathcal{G}$-可測関数であるとは、全てのボレル集合 $B \\in \\mathcal{B}(\\mathbb{R})$ に対して$Y^{-1} (B) \\in \\mathcal{G}$ であるという意味だ。 説明 数式的定義から$\\mathcal{G}$は元の確率空間$( \\Omega , \\mathcal{F} , P)$ほど広くなく、もう少し情報が与えられた確率空間$( \\Omega , \\mathcal{G} , P)$となる。従って $$ \\int_{A} X d P = \\int_{A} Y d P = \\int_{A} E ( X | \\mathcal{G} ) d P $$ はその縮小された空間内では計算が同じであることを意味し、従って確率$P$を$\\mathcal{F}$から$\\mathcal{G}$へうまく引き下ろし、その性質を保持したと言える。\nまた、定義の形式から、$E ( X | \\mathcal{G} )$は$\\mathcal{G}$-可測確率変数として存在するので、期待値が確率変数であり、与えられたシグマ場に対して可測であることを当然受け入れるべきだ。\n直感的に受け入れがたい定義ではないが、その表現が多少見慣れないものである。確率変数 $X$ について、$\\sigma (X) := \\left\\{ X^{-1} (B) : B \\in \\mathcal{B}(\\mathbb{R}) \\right\\}$ は $X$ によって生成される$\\Omega$ の最小のシグマ場 $\\sigma (X) \\subset \\mathcal{F}$ となり、以下のように慣れ親しんだ表現で述べられる。 $$ E(Y|X) = E \\left( Y | \\sigma (X) \\right) $$ もちろんこれで表現することは可能だが、測度に基づく確率論をこれからも学び続けるつもりであれば、この方向に慣れる方がずっと楽だ。考えてみれば、$E(Y|X)$ は概念的には直感的だが、数式を扱ったり直接計算をする際にはとても面倒な記法でもあった。惜しみなく手放そう。\n一方、条件付き期待値の存在はラドン=ニコディム定理によって保証される。定理を理解することが鍵であり、証明自体は難しくない。\n証明 ケース 1. $X \\ge 0$\n$$ P_{\\mathcal{G}}(A) := \\int_{A} X d P $$ 全ての $A \\in \\mathcal{G}$ に対して$P_{\\mathcal{G}}$を上記のように定義すると、$P_{\\mathcal{G}}$は$\\mathcal{G}$上の測度となり、$P_{G} \\ll P$が成り立つ。\nラドン=ニコディム定理によれば、測度空間$( \\Omega , \\mathcal{F} )$の二つのシグマ有限測度$\\nu$、$\\mu$が$\\nu \\ll \\mu$を満たす場合、全ての$A \\in \\mathcal{F}$に対して$\\mu$-ほとんど至る所で$h \\ge 0$であり、 $$ \\nu (A) = \\int_{A} h d \\mu $$ を満たす$\\mathcal{F}$-可測関数$f$が$\\mu$に従って一意に存在する。\n定理に従って、$\\nu = P_{\\mathcal{G}}$、$\\mu = P$とすると、全ての $A \\in \\mathcal{G}$ に対して $$ P_{\\mathcal{G}} (A) = \\int_{A} Y d P $$ を満たす$Y \\ge 0$が一意に存在する。はじめに$P_{\\mathcal{G}}$の定義に従って、$Y$は$X$の$\\mathcal{G}$に対する条件付き期待値となる。\nケース 2. 一般の場合\n$X$ を二つの $X^{+} , X^{-} \\ge 0$ に分解して、**ケース 1.**と同じ方法を使えばいい。\n■\n","id":1315,"permalink":"https://freshrimpsushi.github.io/jp/posts/1315/","tags":null,"title":"測度論で定義される確率変数の条件付き期待値"},{"categories":"매트랩","contents":"方法 $m \\times n$ データが行列で与えられているとし、これを$A$とする。行列$A$の特定の部分のみを使用したい場合は、以下の方法を使えばいい。\nB=A(a:b, c:d) 上のようなコードを実行すると、$B$は行列$A$の$a$行目から$b$行目、$c$列目から$d$列目のデータを持つ$(b-a) \\times (d-c)$行列になる。以下は例のコードと実行結果だ。\nfor k=1:9\rfor l=1:9\rA(k,l)=10*k+l;\rend\rend\rA\ra1=A(3:7,4:9)\ra2=A(2:5,1:6) :：行や列全体を引っ張り出したいときはコロンを使えばいい。$a3$は列全体を、$a4$は行全体を引っ張り出したものだ。\na3=A(3:7,:)\ra4=A(:,4:9) 特定の行や列を引っ張り出すとき、コロンを使うと便利だ。\na5=A(3,:)\ra6=A(:,9) ","id":1362,"permalink":"https://freshrimpsushi.github.io/jp/posts/1362/","tags":null,"title":"MATLABで行列の特定の行、列を選択する方法"},{"categories":"측도론","contents":"定義 1 可測空間 $( X , \\mathcal{E} )$が与えられたとする。\n$\\mu (X) \u0026lt; \\infty$のとき、$\\mu$を有限測度と呼ぶ。 $$\\displaystyle X = \\bigcup_{i=1}^{\\infty} E_{i} \\qquad , E_{i} \\in \\mathcal{E}$$ となる全ての$i \\in \\mathbb{N}$に対して$\\mu ( E_{i} ) \u0026lt; \\infty$となるとき、シグマ有限測度と呼び、また、順序対$(X, \\mathcal{E}, \\mu)$をシグマ有限測度空間と呼ぶ。 $\\mu ( E ) = \\infty$である全ての$E \\in \\mathcal{E}$に対して、$0 \u0026lt; \\mu (F) \u0026lt; \\infty$を満たす$E$の部分集合$F \\in \\mathcal{E}$が存在するならば、$\\mu$を準有限Semifinite測度と呼ぶ。 $\\nu$が与えられた可測空間上の符号付き測度であり、全変動$| \\nu |$が有限（シグマ有限）測度である場合、$\\nu$を有限（シグマ有限）測度と呼ぶ。 説明 有限測度の代表的な例として確率がある。 シグマ有限測度は、全集合$X$に対する有限測度の条件が緩和されたものである。全集合を構成する$E_{i}$は有限でなければならないが、その和$\\displaystyle \\sum_{i \\in \\mathbb{N}} \\mu ( E_{i} )$まで有限である必要はない。つまり、$\\mu (X)=\\infty$でも$\\mu (X) \u0026lt;\\infty$でも構わない。定義により、$\\mu (X)\u0026lt;\\infty$であるシグマ有限測度は有限測度となる。 準有限測度の定義で重要なポイントは、$F$が$0 \u0026lt; \\mu (F)$を満たすという点である。この条件がなければ、シグマ代数には空集合が含まれるため、全ての測度がこの条件を満たすことができる。すべてのシグマ有限測度は準有限測度であるが、逆は成り立たない。 下記の条件が等価であることは容易に確認できる。 $(a)$ $\\nu$がシグマ有限である。 $(b)$ $\\nu^+$、$\\nu^-$がシグマ有限である。 $(c)$ $| \\nu |=\\nu^+ + \\nu^-$がシグマ有限である。 Bartle. (1995). The Elements of Integration and Lebesgue Measure: p19~20.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1314,"permalink":"https://freshrimpsushi.github.io/jp/posts/1314/","tags":null,"title":"有限シグマ測度"},{"categories":"집합론","contents":"定理 1 $$ \\left[ p(1) \\land \\left( p(n) \\implies p(n+1) \\right) \\right] \\implies \\forall n \\in \\mathbb{N} : p(n) $$ 命題 $p(n) (n=1,2,3, \\cdots )$ について、$p(1)$ が真であり、$p(n)$ を仮定した時、$p(n+1)$ が成り立てば、$p(n)$ も真である。\n説明 ある式が自然数に対して成り立つ時、特に強力な証明法として、「ペアノの第5公理」とも呼ばれるか、あるいは、単に\u0026rsquo;数学的\u0026rsquo;という言葉を取り除いて、ただの帰納法とも言われる。もともと帰納法は、現象や実体を経験的に集めて何かの結論を出すことだが、数学では、このような解説がなくとも、ただの帰納法と言われても、自然に数学的帰納法と理解される。\n数学的帰納法は、理解する前は難しく、理解するととても簡単な証明法である。ペアノの第5公理という別名からもわかるように、「公理」と呼べるほど自明な論理だからである。 数学的帰納法 数学的帰納法は、よくドミノに例えられる：\n(1) ドミノの最初のブロックが倒れる。 通常、最初のブロックが倒れるのは、自明な事実である。なぜなら、自分で倒せばいいからである。同じように、証明をする時も、普通は命題に$1$ を代入することで、簡単に示すことができる。 (2) 一つ前のブロックが倒れることで、次のブロックを倒す。 $n$ 番目のブロックが倒れるとするならば、このブロックが倒れることで、$(n+1)$ 番目のブロックも倒す。ドミノでは、ブロックを一列に並べて、前のブロックが次のブロックを倒すから、事実であろう。これを示すのが、数学的帰納法の本質であり、最も難しい部分である。 **(3) 上記の通りならば、最初のブロックを倒すと、すべてのブロックが倒れる。 最初のブロックを倒したとしよう。 (2)により、$1$ 番目のブロックが、$2$ 番目のブロックを倒す。 更に(2)により、$2$ 番目のブロックが倒れることで、$3$ 番目のブロックを倒す。 更に(2)により、$n$ 番目のブロックが倒れることで、$(n+1)$ 番目のブロックを倒す。 このように、すべてのブロックが次のブロックを倒すので、ブロックの数がどれだけ多くても、すべて倒れる。 数学的帰納法に戻ると、$p(1)$ が成り立つ時、$p(1+1)$、すなわち$p(2)$ が成り立つ。$p(2)$ が成り立つ時、$p(2+1)$ が成り立ち、$p(n)$ が成り立つ時、$p(n+1)$ が成り立つ。\nでも、$p(n)$ が成り立つことを仮定するのは、循環論法じゃないの？ 最初に数学的帰納法に出会った時に、なぜ$p(n)$ が成り立つことを仮定してもいいのか、わからなくなることがあり、私も高校時代にこの部分が紛らわしかった。しかし、実際に示すのは、$p(n)$ が自体が成り立つことではなく、$p(n)$ が成り立つ時に、$p(n+1)$ も成り立つことを示すことである。だから、数学的帰納法は間接証明法の一つとされる。\n率直に言って、全ての$n$ において$p(n)$ が成り立たなくても、どうでもいい。偽の仮定から偽の結論が導かれても、その間の論理に問題がなければいい。 もし、それが成り立つなら、$p(1)$ も成り立つので、すべての自然数に対して成り立つということになる。これが、証明の真の結論である。少なくとも一つ真の命題 $p(1)$ が存在して、すべての自然数に対する$p(n)$ を正当化できる。 李興天 訳、林有豊 (2011). 集合論(Set Theory: An Intuitive Approach): p63, 367.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":118,"permalink":"https://freshrimpsushi.github.io/jp/posts/118/","tags":null,"title":"数学的帰納法"},{"categories":"측도론","contents":"定義 1 可測空間$( \\Omega , \\mathcal{F} )$が与えられているとする。測度$\\nu$、$\\mu$がすべての$A \\in \\mathcal{F}$に対して $$ \\mu (A) = 0 \\implies \\nu (A) = 0 $$ を満たす場合、$\\nu$は$\\mu$に対して絶対連続であると言い、$\\nu \\ll \\mu$と表記する。\n説明 $\\nu \\ll \\mu$の表記から分かるように、$\\mu$は$\\nu$を「支配」する感じが強い。問題はこれをなぜ「絶対連続」と呼ぶのかということだ。長い間、良い説明を探してきたが、実解析を学ぶレベルの学習者にとって、以下の同値条件の証明することほど理解しやすい方法はなかった。\n定理 $\\nu \\ll \\mu$ $\\iff$ $\\forall \\varepsilon \u0026gt; 0$、$\\exists \\delta \u0026gt; 0 : F \\in \\mathcal{F}, \\mu ( F ) \u0026lt; \\delta \\implies \\nu (F) \u0026lt; \\varepsilon $\n証明 すべての$n \\in \\mathbb{N}$に対して$\\displaystyle \\mu ( F_{n} ) \u0026lt; {{1} \\over {2^n}}$と$\\nu (F_{n}) \u0026gt; \\varepsilon$を満たすシーケンス$\\left\\{ F_{n} \\right\\}_{n \\in \\mathbb{N}} \\subset \\mathcal{F}$が存在すると仮定する。\n$\\displaystyle A : = \\bigcap_{n \\in \\mathbb{N}} F_{n}$とすると$\\mu (A) = 0$だが、$\\nu (A) \\ne 0$であるので$\\mu (A) = 0 \\implies \\nu (A) = 0$と矛盾する。\n$(\\Leftarrow)$\n$\\forall \\varepsilon \u0026gt; 0$について$\\delta = \\varepsilon$とすると、 $$\\mu (A) = 0 \\implies \\nu (A) = 0$$\n■\n参照 実関数の絶対連続性 測度の絶対連続性 符号付き測度の絶対連続性 Bartle. (1995). The Elements of Integration and Lebesgue Measure: p84.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1309,"permalink":"https://freshrimpsushi.github.io/jp/posts/1309/","tags":null,"title":"測度の絶対連続"},{"categories":"집합론","contents":"法則 1 $$ (p \\land \\lnot q) \\to c \\iff p \\to q $$\n$c$ は矛盾を意味する。 説明 背理法 あるいは 帰謬法 は、数学全般で本当によく使われる証明法だ。しかし、初めて背理法に接する人は、この言葉自体が生見で拒絶感があるかもしれない。または、ただ慣れただけで、どうして背理法が機能するのか理解していない人もいるだろう。\n下の文章を読んで、背理法を理解してみよう：\n(1) 結論 $q$ は真か偽かのどちらかだ。排中律2によって当然だ。 (2) しかし、$\\lnot q$ が真である場合に矛盾が発生するなら、少なくとも $\\lnot q$ が真ではないことがわかる。 (3) (1)では、$q$ は真か偽かのどちらかと言ったけど、$\\lnot q$ が偽ならば $q$ は真でなければならない。 (4) よって、$p \\to q$ だ。 理解が難しいなら、数学を抜きにした例で確認してみよう：\n$p$：俺は腕が二本ある。 $q$：俺は少なくとも腕が一本はある。 $p \\to q$：俺の腕が二本なら、少なくとも一本はある。 まず、言葉だけ見れば背理法も何も、あまりにも当然の事実だ。腕が少なくとも一本ないとどうやって腕が二本あるだろうか？でも、この当たり前の論理がまさに背理法そのものなんだ：\nまず、結論 $q$ を否定して、自分には腕が一本もないと仮定してみよう。 しかし、仮定 $p$ で、自分の腕は二本と言ったが、これは矛盾だ。 それならば、自分に腕が一本もないという $\\lnot q$ が間違っていると考えるしかない。もちろんこれは、自分に腕が二本あるという仮定 $p$ が成り立つときの話だが、$p \\to q$ だ。 証明 $$ \\begin{align*} (p \\land \\lnot q) \\to c \\iff \u0026amp; \\lnot ( p \\land \\lnot q ) \\lor c \\\\ \\iff \u0026amp; \\lnot (p \\land \\lnot q) \\\\ \\iff \u0026amp; \\lnot p \\lor q \\\\ \\iff \u0026amp; p \\to q \\end{align*} $$\n■\n李興天 翻訳, You-Feng Lin. (2011). 集合論(Set Theory: An Intuitive Approach): p39.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n同時に真でありながら偽であることはできない。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":117,"permalink":"https://freshrimpsushi.github.io/jp/posts/117/","tags":null,"title":"背理法の数理論理的証明"},{"categories":"집합론","contents":"法則 1 $$ p \\to q \\iff \\lnot q \\to \\lnot p $$\n説明 ある命題が真であれば、その対偶も真である。ある命題が偽であれば、その対偶も偽である。もちろん、逆Converseが成り立つなら、対偶によって元の命題の逆Reverseも成り立つ。\nこれらの表現は、数学に慣れていない人にとって難しいかもしれない。直感的な例を挙げて理解してみよう：\n$p$ : 天気が暑い $q$ : 汗をかく $p \\to q$ : 天気が暑ければ、汗をかく 天気が暑ければ、汗をかくということが真であれば、汗をかいていないなら、他のことは分からなくても、天気が暑いわけではないということが分かる。\n証明 $$ \\begin{align*} p \\to q \\iff \u0026amp; \\lnot p \\lor q \\\\ \\iff \u0026amp; \\lnot p \\lor \\lnot (\\lnot q) \\\\ \\iff \u0026amp; \\lnot (\\lnot q) \\lor \\lnot p \\\\ \\iff \u0026amp; \\lnot q \\to \\lnot p \\end{align*} $$\n■\n『集合論(Set Theory: An Intuitive Approach)』、林幼烽、翻訳: 李興天、2011年、p29。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":116,"permalink":"https://freshrimpsushi.github.io/jp/posts/116/","tags":null,"title":"逆説の数理論理的証明"},{"categories":"매트랩","contents":"属性 グラフの属性は以下のように指定できる。\nグラフ色 マーカー 線の形状 赤 r 点 . 実線 - 緑 g 星 * 点線 : 青 b x x 一点鎖線 -. 黒 k 円 o（アルファベットのo） 破線 -- 黄 y プラス + 紫 m 四角 s 白 w ダイア d シアン c 星 p 下向き三角 v 上向き三角 ^ 左向き三角 \u0026lt; 右向き三角 \u0026gt; 六角星 h 例 x=linspace(0,1,20);\ry=x.^3+3.*x.^2+3.*x+1;\rfigure()\rplot(x,y,\u0026#39;ro\u0026#39;)\rhold on\rplot(x,y+1,\u0026#39;g-\u0026#39;)\rplot(x,y+2,\u0026#39;c:\u0026#39;)\rplot(x,y+3,\u0026#39;k--\u0026#39;)\rlegend({\u0026#39;ro\u0026#39;, \u0026#39;g-\u0026#39;, \u0026#39;c:\u0026#39;, \u0026#39;k--\u0026#39;}) ","id":1330,"permalink":"https://freshrimpsushi.github.io/jp/posts/1330/","tags":null,"title":"MATLABグラフでの色、線の種類、マーカーの種類の指定方法"},{"categories":"집합론","contents":"要約 1 [1] ド・モルガンの法則: $$ \\lnot (p \\land q) \\iff \\lnot p \\lor \\lnot q \\\\ \\lnot(p \\lor q) \\iff \\lnot p \\land \\lnot q $$ [2] ド・モルガンの定理: $$ (A \\cup B)^{c} = A^{c} \\cap B^{c} \\\\ (A \\cap B)^{c} = A^{c} \\cup B^{c} $$ 説明 ド・モルガンの法則とド・モルガンの定理は、それぞれ命題と集合に関するものだが、実際の会話ではあまり区別されない。法則でも定理でも ド・モルガン- とつけば、否定や補集合を取ると、括弧内の命題や集合と記号が「反転する」と理解されるためだ。\n一方で、数学的帰納法によると、以下の一般化はもちろん、インデックスファミリーにまで拡張可能である。 $$ \\begin{align*} \\begin{matrix} \\displaystyle \\left( \\bigcup_{i=1}^{\\infty} X_{i} \\right)^{c} = \\bigcap_{i=1}^{\\infty} (X_{i})^{c} \\\\ \\displaystyle \\left( \\bigcap_{i=1}^{\\infty} X_{i} \\right)^{c} = \\bigcup_{i=1}^{\\infty} (X_{i})^{c} \\end{matrix} \u0026amp;\\qquad \\\u0026amp; \\begin{matrix} \\displaystyle\\left( \\bigcup_{\\alpha \\in \\forall } X_{\\alpha} \\right)^{c} = \\bigcap_{\\alpha \\in \\forall} (X_{\\alpha})^{c} \\\\ \\displaystyle \\left( \\bigcap_{\\alpha \\in \\forall} X_{\\alpha} \\right)^{c} = \\bigcup_{\\alpha \\in \\forall } (X_{\\alpha})^{c} \\end{matrix} \\end{align*} $$\n証明 [1] 真理表で証明する。\nパート1. $\\lnot (p \\land q) \\iff \\lnot p \\lor \\lnot q$\nパート2. $\\lnot(p \\lor q) \\iff \\lnot p \\land \\lnot q$\n■\n[2] パート1. $(A \\cup B)^{c} = A^{c} \\cap B^{c}$\n$$\\begin{align*} x \\in (A \\cup B)^{c} \\iff \u0026amp; \\lnot (x \\in A \\cup B) \\\\ \\iff \u0026amp; \\lnot ( x \\in A \\lor x \\in B ) \\\\ \\iff \u0026amp; \\lnot ( x \\in A ) \\land \\lnot ( x \\in B ) \\\\ \\iff \u0026amp; x \\in A^{c} \\land x \\in B^{c} \\\\ \\iff \u0026amp; x \\in ( A^{c} \\cap B^{c} ) \\end{align*} $$\nパート2. $(A \\cap B)^{c} = A^{c} \\cup B^{c}$\n$$\\begin{align*} x \\in (A \\cap B)^{c} \\iff \u0026amp; \\lnot (x \\in A \\cap B) \\\\ \\iff \u0026amp; \\lnot ( x \\in A \\land x \\in B ) \\\\ \\iff \u0026amp; \\lnot ( x \\in A ) \\lor \\lnot ( x \\in B ) \\\\ \\iff \u0026amp; x \\in A^{c} \\lor x \\in B^{c} \\\\ \\iff \u0026amp; x \\in ( A^{c} \\cup B^{c} ) \\end{align*} $$\n■\n이흥천 역, You-Feng Lin. (2011). 집합론(Set Theory: An Intuitive Approach): p29, 115.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1306,"permalink":"https://freshrimpsushi.github.io/jp/posts/1306/","tags":null,"title":"ド・モルガンの法則の証明"},{"categories":"매트랩","contents":"方法 imrotate(I,angle,method,bbox)\nI: 回転する画像だ。 angle: 回転角度で、単位は度だ。 method: 補間方法だ。\u0026rsquo;nearest\u0026rsquo;, \u0026lsquo;bilinear\u0026rsquo;, \u0026lsquo;bicubic\u0026rsquo;がある。何も入力しない場合は\u0026rsquo;nearet\u0026rsquo;が適用される。 X = phantom(\u0026#39;Modified Shepp-Logan\u0026#39;,64);\rfigure()\rimagesc(X)\rtitle(\u0026#39;X\u0026#39;)\rY1=imrotate(X,30,\u0026#39;nearest\u0026#39;,\u0026#39;crop\u0026#39;);\rY2=imrotate(X,30,\u0026#39;bilinear\u0026#39;,\u0026#39;crop\u0026#39;);\rY3=imrotate(X,30,\u0026#39;bicubic\u0026#39;,\u0026#39;crop\u0026#39;);\rfigure()\rsubplot(1,3,1)\rimagesc(Y1)\rtitle(\u0026#39;Y1 - nearest\u0026#39;)\rsubplot(1,3,2)\rimagesc(Y2)\rtitle(\u0026#39;Y2 - bilinear\u0026#39;)\rsubplot(1,3,3)\rimagesc(Y3)\rtitle(\u0026#39;Y3 - bicubic\u0026#39;) bbox: 出力画像のサイズを指定する。\u0026rsquo;loose\u0026rsquo;は、回転した画像で元のサイズを超える部分まで出力されるように出力画像のサイズを大きくする。\u0026lsquo;crop\u0026rsquo;は、最初の画像のサイズに合わせて回転した画像を切り取って出力する。何も入力しない場合は\u0026rsquo;loose\u0026rsquo;が適用される。 X = phantom(\u0026#39;Modified Shepp-Logan\u0026#39;,64);\rfigure()\rimagesc(X)\rtitle(\u0026#39;X - 64*64\u0026#39;)\rY1=imrotate(X,30,\u0026#39;nearest\u0026#39;,\u0026#39;loose\u0026#39;);\rY2=imrotate(X,30,\u0026#39;nearest\u0026#39;,\u0026#39;crop\u0026#39;);\rfigure()\rsubplot(1,2,1)\rimagesc(Y1)\rtitle(\u0026#39;Y1 - loose 88*88\u0026#39;)\rsubplot(1,2,2)\rimagesc(Y2)\rtitle(\u0026#39;Y2 - crop 64*64\u0026#39;) 他の言語で ジュリアで ","id":1328,"permalink":"https://freshrimpsushi.github.io/jp/posts/1328/","tags":null,"title":"MATLABで画像を回転する方法"},{"categories":"매트랩","contents":"ゼロ行列 zeros(): ゼロ行列を返す。 zeros(n): $n\\times n$ ゼロ行列を返す。 zeros(m,n): $n\\times m$ ゼロ行列を返す。 zeros(size(A)): 行列Aと同じ大きさのゼロ行列を返す。 全要素が1の行列 ones(): 全要素が1の行列を返す。ただし、二つの行列の間の演算のためには、そのまま1を使う方が便利だ。誰が見ても下のコードの方がずっとシンプルだ。 ones(n): 全要素が1の$n\\times n$ 行列を返す。 ones(m,n): 全要素が1の$n\\times m$ 行列を返す。 ones(size(A)): 行列Aと同じ大きさの全要素が1の行列を返す。 A=[1 2 3; 4 -2 3; 5 3 7]\rones(size(A))./A\r1./A 単位行列 eye(): 単位行列を返す。 eye(n): $n\\times n$ 単位行列を返す。 eye(m,n): $n\\times m$ 単位行列を返す。 eye([m,n]): 主対角線の成分が1で、その他の成分は0の$n\\times m$ 単位行列を返す。 eye(n,'like',A): 行列Aと同じデータタイプの$n\\times n$ 単位行列を返す。つまり、Aが複素行列であれば、複素単位行列を返す。サイズを指定しなければ、Aと同じ大きさの行列を返す。 eye([2,3])\reye(3,6)\rA=[1+i 3-i]\reye(3, \u0026#39;like\u0026#39;, A)\reye(3,4 \u0026#39;like\u0026#39;, A) 乱数 rand(): 0から1の間でランダムに1つの乱数を返す。各数が引かれる確率はすべて同じだ。マトラブ公式ホームページで「一様に分布した乱数」という説明は、これを意味する。 rand(n): 0から1の間の乱数で構成された$n\\times n$ 行列を返す。 rand(m,n): 0から1の間の乱数で構成された$m\\times n$ 行列を返す。 rand(n,'like',A): 行列Aと同じデータタイプの乱数で構成された$n\\times n$ 行列を返す。つまり、Aが複素行列であれば、複素行列を返す。サイズを指定しなければ、Aと同じ大きさの行列を返す。 ","id":1327,"permalink":"https://freshrimpsushi.github.io/jp/posts/1327/","tags":null,"title":"MATLABで特別な行列を作成する関数"},{"categories":"집합론","contents":"定義 1 全ての論理的可能性に対して真である命題を恒真命題Tautologyという。全ての論理的可能性に対して偽である命題を矛盾命題Contradictionという。\n$p$, $q$ に対して条件文 $p \\to q$ が恒真命題なら 含意命題Implicationといい、次のように表される。 $$ p \\implies q $$ $p$, $q$ に対して二重条件文 $p \\to q$ が恒真命題なら 同値Equivalenceといい、次のように表される。 $$ p \\iff q $$ 説明 実際、矛盾命題という言葉はほとんど使われず、代わりに矛盾という言葉がよく使用される。記号では、恒真Tautologyと矛盾Contradictionの先頭の文字を取って恒真 $t$、矛盾 $c$ と表される。\n上の真理表によると$p$が真であっても偽であっても$p \\lor \\lnot p$は真である。命題の定義を考えると$p$が真か偽かで真ということは、当然のことである。恒真命題とは、このように「事実」と関係なく、その形式だけで見ても真であるしかない命題をいう。\n数学に興味がない人は数学が複雑で難しいと思うかもしれないが、数学を学べば学ぶほど気づくことは、数学者たちは「考えること」を避けるために必死にもがいているということである。人類にはあまりにも多くの「言葉」の組み合わせがあり、それを一つ一つ検討しながら正しいかどうかを判断するのは気が遠くなる。だから、少なくとも「形式だけで判断できるもの」については、その程度の努力だけで判断したいというのが本音である。\n含意とは意味意を含む含という意味であるので、Implyの簡略としてピッタリだが、実際には韓国語話者がImplyを含意と解釈することは非常に稀である。\n法則 1 任意の命題 $p$, $q$, $r$ に対して、以下が成り立つ。\n[1] 加法法則: $$ p \\implies p \\lor q $$ [2] 単純化法則: $$ p \\land q \\implies p \\\\ p \\land q \\implies q $$ [3] 吸収法則: $$ p \\land ( p \\lor q) \\iff p \\\\ p \\lor ( p \\land q ) \\iff p $$ [4] 二重否定法則: $$ \\lnot ( \\lnot p) \\iff p $$ [5] 交換法則: $$ p \\land q \\iff q \\land p \\\\ p \\lor q \\iff q \\lor p $$ [6] 冪等法則: $$ p \\land p \\iff p \\\\ p \\lor p \\iff p $$ [7] 結合法則: $$ (p \\land q) \\land r \\iff p \\land (q \\land r) \\\\ (p \\lor q) \\lor r \\iff p \\lor (q \\lor r) $$ [8] 分配法則: $$ p \\land (q \\lor r) \\iff (p \\land q) \\lor (p \\land r) \\\\ p \\lor (q \\land r) \\iff (p \\lor q) \\land (p \\lor r) $$ これらの法則は論理的に考える人間には自然なものであり、理工系の人なら特に以下の法則も自由自在に使えるべきである。専攻が形式科学―コンピュータ科学、統計学、数学に近いほど、早く慣れることが望ましい。\n[9] ド・モルガンの法則: $$ \\lnot (p \\land q) \\iff \\lnot p \\lor \\lnot q \\\\ \\lnot(p \\lor q) \\iff \\lnot p \\land \\lnot q $$ [10] 対偶法: $$ (p \\to q) \\iff (\\lnot q \\to \\lnot p) $$ [11] 背理法: $$ (p \\land \\lnot q) \\to c \\iff p \\to q $$ [12] 三段論法: $$ (p \\to q) \\land (q \\to r) \\implies (p \\to r) $$ 英文表記 このポストで紹介された定理の英文表記は以下の通りである:\n[1] 加法法則Law of Addition [2] 単純化法則Laws of Simplification [3] 吸収法則Laws of Absorption [4] 二重否定法則Law of Double Negation [5] 交換法則Laws of Commutativity [6] 冪等法則Laws of Idempotency [7] 結合法則Laws of Associativity [8] 分配法則Laws of Distributivity [9] ド・モルガンの法則De Morgan\u0026rsquo;s Laws [10] 対偶法Contrapositive Law [11] 背理法Reductio ad Absurdum [12] 三段論法Syllogism 特に[11] 背理法は、馬鹿法とも呼ばれ、[12] 三段論法は推移法則Law of Transitivityとも言われる。\n이흥천 역, You-Feng Lin. (2011). 집합론(Set Theory: An Intuitive Approach): p25.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1304,"permalink":"https://freshrimpsushi.github.io/jp/posts/1304/","tags":null,"title":"逆対偶命題と逆命題"},{"categories":"매트랩","contents":"掛け算 times(), .*: 二つの行列の各成分を掛け合わせ、その結果を返します。 二つの行列の大きさが完全に同じ、または一方がスカラー、もしくは行の大きさが同じ行ベクトル、列の大きさが同じ列ベクトルである場合のみ、演算が可能です。大きさが異なる場合、小さい行列が大きい行列と同じ大きさの行列であるかのように計算され、空いている場所は同じ値で埋められます。例えば、スカラーは全ての成分が同じ値を持つようになり、行ベクトルの場合、全ての行が同じ行列になります。よくわからない場合は、下の式を参照してください。.*は点と掛け算記号が合わさっているので、要素ごとの掛け算と理解すれば良いです。他の成分ごとの演算の記号もこのように作られています。\n$$ A=\\begin{pmatrix} a_{1} \u0026amp; a_2 \u0026amp; a_{3} \\end{pmatrix},\\quad B=\\begin{pmatrix} b_{1} \\\\ b_2 \\\\ b_{3} \\\\ b_{4} \\end{pmatrix} \\quad \\implies \\quad \\begin{align*} A.B\u0026amp;=\\begin{pmatrix} a_{1} \u0026amp; a_2 \u0026amp; a_{3} \\\\ a_{1} \u0026amp; a_2 \u0026amp; a_{3} \\\\ a_{1} \u0026amp; a_2 \u0026amp; a_{3} \\\\ a_{1} \u0026amp; a_2 \u0026amp; a_{3} \\end{pmatrix} \\begin{pmatrix} b_{1} \u0026amp; b_{1} \u0026amp; b_{1} \\\\ b_2 \u0026amp; b_2 \u0026amp; b_2 \\\\ b_{3} \u0026amp; b_{3} \u0026amp; b_{3} \\\\ b_{4} \u0026amp; b_{4} \u0026amp; b_{4} \\end{pmatrix} \\\\ \u0026amp;= \\begin{pmatrix} a_{1}b_{1} \u0026amp; a_2b_{1} \u0026amp; a_{3}b_{1} \\\\ a_{1}b_2 \u0026amp; a_2b_2 \u0026amp; a_{3} b_2 \\\\ a_{1}b_{3} \u0026amp; a_2 b_{3} \u0026amp; a_{3} b_{3} \\\\ a_{1}b_{4} \u0026amp; a_2 b_{4} \u0026amp; a_{3} b_{4} \\end{pmatrix} \\end{align} $$\n例のコードと出力結果は以下の通りです。\nA=[2 1 -3; 4 0 3]\rB=[1 2 3]\rC=[3; 1]\ra=A.*B\rb=A.*C\rc=B.*C\rd=3.*A 割り算 rdivide(), ./: 二つの行列の各成分を割り、その結果を返します。 行列の大きさについての注意事項は.*と同じです。これを使うと、行列Aの各成分の逆数を成分とする行列を簡単に計算できます。1./Aで求めることができます。\n例のコードと出力結果は以下の通りです。\nA=[2 1 -3; 4 0 3]\rB=[1 2 3]\rC=[3; 1]\ra=rdivide(A,B)\rb=A./C\rc=B./C\rd=1./A 累乗 power(), .^: A.^Bの場合、Aの各成分を底とし、Bの各成分を指数として計算した結果を返します。 例のコードと出力結果は以下の通りです。\nA=[2 1 -3; 4 0 3]\rB=[1 2 3]\rC=[3; 1]\ra=power(A,B)\rb=A.^C\rc=B.^C\rd=3.^A ","id":1326,"permalink":"https://freshrimpsushi.github.io/jp/posts/1326/","tags":null,"title":"MATLABで二つの行列に対して要素ごとの演算を行う方法"},{"categories":"매트랩","contents":"関数 size(): 行列の行と列の長さを要素として持つ行ベクトルを返す。\n扱っている行列と同じサイズの零行列を作る時に便利だ。\nzeros(size(A)): Aと同じサイズの零行列を返す。\nlength(): 行と列の中でより大きい数字を返す。\n行ベクトル、列ベクトルの場合には要素の数と同じなのでnumel()と同じだ。また、size()は行と列のサイズを返すので length(A)=max(size(A))となる。\nnumel(): 行列の要素の数を返す。\n例のコードと出力結果は以下の通り。\nA=[1 2 3];\rB=[1 2 3 4 ; 2 3 4 1 ; 3 4 1 2];\rC=zeros(3,8);\ra=size(A)\rb=size(B)\rc=size(C)\rd=length(A)\re=length(B)\rf=length(C)\rg=numel(A)\rh=numel(B)\ri=numel(C) ","id":1323,"permalink":"https://freshrimpsushi.github.io/jp/posts/1323/","tags":null,"title":"MATLABにおける行列のサイズと関連する関数"},{"categories":"거리공간","contents":"定義 開集合 $\\Omega \\subset \\mathbb{R}^n$が与えられたとする。すると、$\\Omega_{\u0026lt;\\delta}$と$\\Omega_{\u0026gt;\\delta}$を下記のように定義する。\n$$ \\begin{align*} \\Omega_{\u0026lt;\\delta} :=\u0026amp; \\left\\{ x\\in\\Omega : \\mathrm{dist}(x, \\mathrm{bdry}\\Omega)\u0026lt;\\delta \\right\\} \\\\ \\Omega_{\u0026gt;\\delta} :=\u0026amp; \\left\\{ x\\in\\Omega : \\mathrm{dist}(x, \\mathrm{bdry}\\Omega)\u0026gt;\\delta \\right\\} \\end{align*} $$\n説明 このような集合は偏微分方程式、関数解析などで有用に使われる。教科書によっては、$\\Omega_\\delta=\\Omega_{\u0026lt;\\delta}$の場合1もあれば、$\\Omega_\\delta=\\Omega_{\u0026gt;\\delta}$の場合2もある。そんな場合は、授業や教科書の記法に忠実に従えばいい。生えび寿司屋では、二つの定義を使うので、上記のような記法で定義した。\nRobert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003), p82\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLawrence C. Evans, Partial Differential Equations (2nd Edition, 2010), p713\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1317,"permalink":"https://freshrimpsushi.github.io/jp/posts/1317/","tags":null,"title":"集合の境界から一定距離外/内の集合"},{"categories":"측도론","contents":"定義1 二つの符号測度$\\nu$、$\\mu$が与えられたとする。以下の三つの条件を$\\nu$、$\\mu$に対して満たす$E,F\\ \\in \\mathcal{E}$が存在する場合、二つの符号測度$\\nu$、$\\mu$はmutually singularと言われ、$\\nu \\perp \\mu$または$\\mu \\perp \\nu$と表示される。\n$E \\cup F=X$ $E \\cap F=\\varnothing$ $E$は$\\nu$に対する零集合であり、$F$は$\\mu$に対する零集合である。 また、「$\\nu$が$\\mu$に対してsingularである」、「$\\mu$が$\\nu$に対してsingularである」という表現も全て同じ意味である。\n説明 $\\mu_{n}$を$\\mathbb{R}^n$のルベーグ測度としよう。そして、以下のように定義されたディラック測度とする$\\delta_{x_{0}}$。\n$$ \\delta_{x_{0}} (E) := \\begin{cases} 1 \u0026amp; x_{0} \\in E \\\\ 0 \u0026amp; x_{0} \\notin E \\end{cases} $$\n$E=\\left\\{ x_{0} \\right\\}$、$F=\\mathbb{R}^n-E$とする。すると$E\\cup F=\\mathbb{R}^n$であり$E \\cap F=\\varnothing$である。さらに、$F$は$\\delta_{x_{0}}-\\mathrm{null}$であり、$E$は$\\mu_{n} -\\mathrm{null}$であるため、ルベーグ測度とディラック測度は互いにsingularである。\n$$ \\delta_{x_{0}} \\perp \\mu_{n} $$\nGerald B. Folland, Real Analysis: Modern Techniques and Their Applications (2nd Edition, 1999), p87\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1310,"permalink":"https://freshrimpsushi.github.io/jp/posts/1310/","tags":null,"title":"相互に特異的"},{"categories":"정수론","contents":"定義 $\\mathbb{Z} [ \\omega ] := \\left\\{ a + \\omega b : a, b \\in \\mathbb{Z} \\right\\}$ をアイゼンシュタイン環Eisenstein Ringと呼び、その要素をアイゼンシュタイン整数という。\n定理 [1]: $\\overline{ \\omega } = \\omega^{2} = - (1 + \\omega)$ [2]: $( a \\pm \\omega b ) + ( c \\pm \\omega d) = (a \\pm c) + \\omega (b \\pm d)$ [3]: $( a + \\omega b )( c + \\omega d) = (ac - bd) + \\omega (ad - bd + bc)$ 説明 $\\omega$ は三次方程式 $x^3 +1 = 0$ の複素根 $\\displaystyle \\omega := {{-1 + \\sqrt{-3}} \\over {2}} = e^{2 \\pi i/3 }$ であり、$\\mathbb{Z} [\\omega]$ は整数環 $\\mathbb{Z}$ のシンプル拡張になる。ガウス整数と同様に興味深い性質を持ち、計算は少し複雑だが、本質的にはガウス整数に似ているため、特に奇妙ではない。複素平面で見ると、$i$ は$1,i,-1,-i$ で四角形の格子を形成し、$\\omega$ は$1, - \\omega^2, \\omega, -1, \\omega^2, -\\omega$ で三角形の格子を形成する。\nもちろん、ガウス整数にガウス素数があるように、アイゼンシュタイン整数にもアイゼンシュタイン素数がある。\n$\\mathbb{Z} [i]$ 上では、次のような通常の数式展開が可能である: $$ \\begin{align*} (7 + \\omega 2)(4 - \\omega 2) =\u0026amp; (28 + 4) + \\omega (- 14 +4 +8 ) \\\\ =\u0026amp; 32 - \\omega 2 \\end{align*} $$ また、ある自然数 $n \\in \\mathbb{N}$ が与えられた場合、有限環 $\\mathbb{Z}_{n}$ に対しても $\\mathbb{Z}_{n}[i]$ を考えることができる。例えば $n = 7$とする時、上記の展開は以下のように変わる: $$ \\begin{align*} (7 + i2)(4 -i 2) =\u0026amp; (28 + 4) + \\omega (- 14 +4 +8 ) \\\\ =\u0026amp; 32 - \\omega 2 \\\\ \u0026amp; \\equiv 4 - \\omega 2 \\pmod{7} \\\\ \u0026amp; \\equiv 4 + \\omega 5 \\pmod{7} \\end{align*} $$ 自然に合同式の使用に注目せよ。$\\mathbb{Z} [i]$ でうまくいくなら、$\\mathbb{Z} [\\omega]$ でもうまくいくのは当然のように見える。$i$ を繰り返し乗算しても高次項が生じないように、$\\omega$ も$\\omega^2 = -(1+\\omega)$ のように次数を下げることができる。もちろん、これは単純な計算ではなく、シンプル拡張の性質によって数学的に保証された事実でもある。\n一方、$2$ と$3$ は、最小の偶数素数と最小の奇数素数である。実際に研究すると、ガウス整数の性質を深く掘り下げるほど$i$ に、アイゼンシュタイン整数の性質を深く掘り下げるほど$3$ にこだわりを感じる。面白いことに$\\overline{i} = i^3$ であり、$\\overline{\\omega} = \\omega^2$ を見るほど、純粋数学の美しさを感じることができる。\nアイゼンシュタイン環の零因子グラフは アルカムによって研究された。\n証明 [1] $\\omega$ の定義と共役の性質により $$ \\begin{align*} \\omega^2 =\u0026amp; \\left( e^{2 \\pi i/3 } \\right)^2 \\\\ =\u0026amp; - e^{ \\pi i/3 } \\\\ =\u0026amp; - {{ 1 + i \\sqrt{3} } \\over { 2 }} \\\\ =\u0026amp; \\overline{ \\left( { -1 + i \\sqrt{3} } \\over { 2 } \\right) } \\\\ =\u0026amp; \\overline{ \\omega } \\\\ =\u0026amp; - (1 + \\omega) \\end{align*} $$\n■\n[2] $\\mathbb{Z} [ \\omega ]$ が環であり、加算に対して結合法則と交換法則が成り立つため $$ \\begin{align*} ( a \\pm \\omega b ) + ( c \\pm \\omega d) =\u0026amp; a \\pm \\omega b + c \\pm \\omega d \\\\ =\u0026amp; a \\pm c + \\omega b \\pm \\omega d \\\\ =\u0026amp; (a \\pm c) + \\omega (b \\pm d) \\end{align*} $$\n■\n[3] 定理 [1] と [2] により $$ \\begin{align*} ( a + \\omega b )( c + \\omega d) =\u0026amp; ac + \\omega ad + \\omega bc -(1 + \\omega) bd \\\\ =\u0026amp; (ac - bd) + \\omega (ad - bd + bc) \\end{align*} $$\n■\n","id":1289,"permalink":"https://freshrimpsushi.github.io/jp/posts/1289/","tags":null,"title":"アイゼンシュタイン整数"},{"categories":"측도론","contents":"定理1 (a) $\\nu$を可測空間 $(X, \\mathcal{E})$上で定義された符号測度とする。すると、以下を満たす $\\nu$の正集合 $P$と負集合 $N$が存在する。\n$$ P \\cup N=X \\quad \\text{and} \\quad P \\cap N =\\varnothing $$\nこのような $X=P \\cup N$を $\\nu$に対するハーン分解Hahn decompositionという。\n(b) $P^{\\prime}, N^{\\prime}$が (a) を満たす別の集合であるとする。その場合、以下の集合は $\\nu$に対する零集合である。\n$$ (P-P^{\\prime}) \\cup (P^{\\prime}-P)=(N-N^{\\prime}) \\cup (N^{\\prime}-N) $$\n対称差symmetric difference記号を使用して以下のように表記される。\n$$ P\\Delta P^{\\prime}=N\\Delta N^{\\prime} $$\n説明 (a) 任意の可測空間が与えられた時、集合 $X$を $\\nu$に対して正の集合と負の集合に分けることができるということである。\n(b) 上述のように集合 $X$を分ける方法が複数存在しても、実質的な違いはないということである。$P$と$P^{\\prime}$、$N$と$N^{\\prime}$は常に互いに零集合だけの差があるため、集合の観点では異なるかもしれないが、測度の観点では同じである。\n証明 この定理の証明自体はそれほど難しくないが、証明の流れが単純ではないため、これを事前に具体的に説明し、始める。まず、ある正の集合 $P$を定義する。そして $N$を $N:=X-P$と定義する。この時、$N$が負の集合であれば、(a) に対する証明が完了する。$N$が負の集合であることを証明する前に、上述のように定義された $N$が持つ2つの性質を確認することにする。そして、最終的な証明では背理法を使用する。$N$が負の集合でないと仮定し、2つの性質を使用して矛盾が生じることを示す。\n一般性を失わずに、$\\nu$が$+\\infty$の値を持たないと仮定する。他の場合は $-\\nu$に対して同じ方法で証明すればよい。$C$を $\\mathcal{E}$のすべてのポジティブセットのコレクションとする。すると、仮定により $\\nu$は $+\\infty$の値を持たないため、以下のように定義される $M$が存在する。\n$$ M:=\\sup \\limits_{P \\in C } \\nu (P) \u0026lt; \\infty $$\nここで、$\\nu (P)=M$を満たすマキシマイザー $P$の存在を示すことができる。以下のようなマキシマイジングシーケンス $\\left\\{ P_{j} \\right\\}$を考える。\n$$ \\lim \\limits_{j \\rightarrow \\infty} \\nu (P_{j})=M $$\nこの時、$P_{j}$同士には含まれる関係がないため、以下のような $\\tilde{P_{j}}$を考える。\n$$ \\tilde{P_{j}} :=\\bigcup \\limits_{k=1}^j P_{k} $$\nすると、$\\nu (P_{j}) \\le \\nu (\\tilde{P_{j}}) \\le M$であるため、$\\left\\{ \\tilde{P_{j}} \\right\\}$はマキシマイジングシーケンスである。また、$\\tilde{P_{1}} \\subset \\tilde{P_2}\\subset \\cdots $であることは定義によって明らかである。ここで、$P$を以下のように定義する。\n$$ P := \\bigcup \\limits_{j=1}^\\infty \\tilde{P_{j}} $$\nすると、次が成り立つ。\n$$ \\nu (P)=\\lim \\limits_{j\\rightarrow \\infty} \\nu (\\tilde{P_{j}})=M $$\nしたがって、$\\nu (P)=M$を満たすマキシマイザーが存在することを示した。また、$P$は正の集合の可算和であるため、正の集合である。実際にこのように作り出された $P$と $N:=X-P$は、定理で述べられているような一つの分解である。$N$がそのような負の集合であることを示すプロセスが残されている。ここで、$N:=X \\setminus P$とする。上述のように、$N$が負の集合であることを示せば証明が完了する。まず、このような $N$が以下の2つの性質を持つことを証明する。\n主張 1 $N$は測度値が0より大きい正の集合を含まない。つまり、0ではない正の集合を含まない。すなわち $\\nu (E)\u0026gt;0$であり、$E$が正の集合であれば、$E \\not \\subset N$である。\nこの時、注意すべき点は、正の集合でも、負の集合でもない $E \\subset N$が存在する可能性があることである。つまり、$N$の部分集合になり得るのは、1. 空集合、2. 負の集合、3. 正の集合でも負の集合でもない集合である。\n証明\n$E\\subset N$が正の集合で $\\nu (E) \u0026gt;0$であるとする。すると、$N$の定義により、$E$と $P$は互いに素な集合である。したがって、次が成り立つ。\n$$ \\nu (P \\cup E)=\\nu (P)+\\nu (E) $$\nしかし、$\\nu (P)=M$であるため、次が成り立つ。\n$$ \\nu (P \\cup E)=\\nu (P)+\\nu (E)\u0026gt;M $$\nしかし、これは $M=\\sup \\nu (F)\\ \\forall F\\in \\mathcal{E}$という仮定に矛盾する。したがって、$\\nu (E)\u0026gt;0$である正の集合 $E \\subset N$は存在しない。\n主張 2 もし $A \\subset N$で $\\nu (A)\u0026gt;0$であれば、$\\nu (B) \u0026gt; \\nu (A)$を満たす $B \\subset A$が存在する。\n証明\n$A \\subset N$で $\\nu (A)\u0026gt;0$であるとする。すると、主張 1 により、$A$は正の集合ではない。したがって、$A$は空集合でもなく、正の集合でもない。従って、次を満たす $C$が存在する2。\n$$ C \\subset A,\\ \\nu (C) \u0026lt;0 $$\nここで、$B:=A-C$とする。すると、次が成り立つ。\n$$ \\nu (A)=\\nu (B)+\\nu (C) \u0026lt; \\nu (B) $$\nここで、$N$が負の集合でないと仮定する。 上の2つの性質を利用して矛盾が生じることを示せば、$N$が負の集合であることが証明される。\nパート 1.\n$\\left\\{ A_{j} \\right\\}$を $N$の部分集合の列とし、$\\left\\{ n_{j} \\right\\}$を自然数の列とする。$N$が負の集合でないと仮定したので、$\\nu (B) \u0026gt;0$となるある $B \\subset N$が存在する。そして、$\\nu (B) \u0026gt; \\frac{1}{n_{j}}$を満たす最小の $n_{j}$を $n_{1}$とし、$n_{1}$に対してこれを満たす $B$を $A_{1}$とする。$\\nu (B)=\\nu (A_{1})\u0026gt;0$であるため、上で $N$に対して行ったプロセスを $A_{1}$に対して同じように適用することができる。\nパート 2\n再び $\\nu (B)\u0026gt;0$となるある $B\\subset A_{1}$が存在し、主張 2 により $\\nu (B) \u0026gt; \\nu (A_{1})$である。したがって、$\\nu (B) \u0026gt; \\nu (A_{1})+\\frac{1}{n}$を満たす自然数 $n$が存在する。この中で最も小さい自然数を $n_2$とし、そのような $B$を $A_2$とする。\nパート 3\n同じプロセスを繰り返すと、$n_{j}$は $\\nu (B)\u0026gt;0$となるある $B \\subset A_{j-1}$に対して $\\nu (B)\u0026gt;\\nu (A_{j-1}) + \\dfrac{1}{n_{j}}$を満たす最も小さい自然数である。また、そのような $B$を $A_{j}$とする。ここで、$A=\\bigcap \\nolimits_{1}^\\infty A_{j}$とする。$\\nu$が $+\\infty$の値を持たないと仮定した上で、符号測度の性質 $(B)$により、次が成り立つ。\n$$ \\begin{align*} +\\infty \\gt \\nu (A) \u0026amp;= \\nu \\left(\\bigcap \\nolimits_{1}^\\infty A_{j} \\right) \\\\ \u0026amp;= \\lim \\limits_{j \\rightarrow \\infty} \\nu (A_{j}) \\\\ \u0026amp;\\ge \\lim \\limits_{j\\rightarrow \\infty} \\left( \\nu (A_{j-1}) +\\frac{1}{n_{j}} \\right) \\\\ \u0026amp;\\ge \\lim \\limits_{j\\rightarrow \\infty} \\left( \\nu (A_{j-2}) + \\frac{1}{n_{j-1}} +\\frac{1}{n_{j}} \\right) \\\\ \u0026amp;\\vdots \\\\ \u0026amp;\\ge \\lim \\limits_{j\\rightarrow \\infty} \\left( \\nu (A_{1}) + \\frac{1}{n_{2}}+\\cdots +\\frac{1}{n_{j}} \\right) \\\\ \u0026amp;\\ge \\lim \\limits_{j\\rightarrow \\infty} \\left( \\frac{1}{n_{1}}+ \\frac{1}{n_{2}}+\\cdots +\\frac{1}{n_{j}} \\right) \\\\ \u0026amp;= \\sum \\limits_{j=1}^\\infty \\frac{1}{n_{j}} \\end{align*} $$\n数列が有限であるため、極限は0である。\n$$ \\lim \\limits_{j\\rightarrow \\infty} \\frac{1}{n_{j}} =0 $$\nしたがって、次を得る。\n$$ \\begin{equation} \\lim \\limits_{j\\rightarrow \\infty} n_{j} =\\infty \\label{eq1} \\end{equation} $$\nしかし、パート 1 で見たように、主張 2 により、ある自然数 $n$に対して $\\nu (B) \u0026gt; \\nu (A) +\\dfrac{1}{n}$を満たす $B \\subset A$が存在する。すると、$A$の定義により $A \\subset A_{j-1}$であり、主張 2 により $\\left\\{ \\nu (A_{j}) \\right\\}$は増加列であることが分かる。したがって、$\\nu (A) =\\lim \\limits_{j \\rightarrow \\infty} \\nu (A_{j})$であるため、$\\nu (A) \u0026gt; \\nu (A_{j-1})$である。\nまた、$(1)$により、十分に大きな $j$に対して $n_{j} \u0026gt;n$である。したがって、次が成り立つ。\n$$ \\nu (B) \u0026gt; \\nu (A) +\\frac{1}{n}\u0026gt;\\nu (A_{j-1}) +\\frac{1}{n} \u0026gt; \\nu (A_{j-1}) +\\frac{1}{n_{j}} $$\nしかし、これは $n_{j}$と $A_{j}$の定義に対する矛盾である。したがって、$N$が負の集合でないという仮定は誤りである。すなわち、$N$は負の集合である。\n$P^{\\prime}$, $N^{\\prime}$を上記の定理を満たす別の一つの分解とする。すると、次が成り立つ。\n$$ P^{\\prime} \\cup N^{\\prime} =X \\quad \\text{and} \\quad P^{\\prime}\\cap N^{\\prime} =\\varnothing $$\nしたがって、$P-P^{\\prime} \\subset P$、$P-P^{\\prime}\\subset N^{\\prime}$であることが分かる。すると、$P-P^{\\prime}$は正の集合でありながら負の集合であるが、これを満たすのは零集合だけであるため、$P-P^{\\prime}$は$\\nu-\\mathrm{null}$である。同様に、$P^{\\prime}-P$、$N-N^{\\prime}$、$N^{\\prime}-N$に対しても同じ方法で $\\nu -\\mathrm{null}$であることを示すことができる。\n■\nGerald B. Folland, Real Analysis: Modern Techniques and Their Applications (第2版, 1999), p86-87\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n存在しなければ、定義により Aは空集合か、あるいは正の集合であるべきである。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1308,"permalink":"https://freshrimpsushi.github.io/jp/posts/1308/","tags":null,"title":"ハーン分解定理"},{"categories":"측도론","contents":"定義1 $\\nu$を$(X,\\mathcal{E})$上の符号測度としよう。そして$E,F \\in \\mathcal{E}$としよう。すると\n$\\nu (F) \\ge 0,\\ \\forall F\\subset E$の時、$E$を$\\nu$に対する正集合positive setまたは単にポジティブpositiveという。\n$\\nu (F) \\le 0,\\ \\forall F\\subset E$の時、$E$を$\\nu$に対する負集合negative setまたは単にネガティブnegativeという。\n$\\nu (F)=0,\\ \\forall F\\subset E$の時、$E$を$\\nu$に対する零集合null setまたは**$\\nu$-ヌル**$\\nu$-nullという。\n説明 定義によると、零集合は同時に正集合であり、負集合である集合だ。正集合、負集合の定義を誤解しやすいので、正しく理解することが重要だ。$\\mu (E)\u0026gt;0$の時、$E$を正の集合と呼ぶわけではない。$E$の全ての可測な部分集合$F\\in\\mathcal{E}$に対して$\\mu (F) \\ge 0$が成り立つ必要があって初めて、$E$を正の集合と呼ぶ。もちろん、この条件を満たすならば自然と$\\nu (E) \u0026gt;0$が成り立つ。要約すると以下の通り。\n$$ E\\ \\mathrm{is\\ positive\\ set\\ for\\ }\\nu \\implies \\nu (E)\u0026gt;0 \\\\ \\nu (E)\u0026gt;0 \\not\\implies E\\ \\mathrm{is\\ positive\\ set\\ for\\ }\\nu $$\nこれは負集合、零集合に対しても同様だ。上述の話は符号測度にのみ適用される。絶対測度については少し話が異なる。$\\mu$を絶対測度とした時、常に$0$以上の関数値を持つので、$\\mu (E)=0$が$E$が$\\nu$-ヌルであることと同値だ。正集合に関する話も同様だ。したがって、絶対測度に対しては、正集合、零集合という言葉を特に使う必要はない。下の図を見よう。\n関数$f$を区間$E_2$でリーマン積分すると、その値は確かに正だが、$E_2$を正の集合とは呼ばない。上の図の例で、関数値が0より小さい部分が一点もない区間が正の集合だ。上の図で、$E_{1}$、$E_{3}$が正の集合で、$E_{5}$が負の集合だ。$E_2$、$E_{4}$は正の集合でも、負の集合でも、零集合でもない。最も重要な点は、ある$E \\in \\mathcal{E}$が必ずしも正集合であるか、または負集合である必要がないことだ。\n要約 (a) 正の集合の可測部分集合も正の集合である。\n(b) 任意の正の集合の可算和も正の集合である。\n証明 (a) 正の集合の定義により自明だ。\n(b) $P_{1},\\ P_2,\\ \\cdots$を正の集合としよう。そして$Q_{n}$を以下のように定義しよう。\n$$ Q_{1}=P_{1},\\quad Q_{n}=P_{n}-\\left( \\bigcup \\nolimits_{j=1}^{n-1}P_{j} \\right)\\ \\forall\\ n\u0026gt;1 $$\nすると$Q_{n} \\subset P_{n}$であり、それぞれの$Q_{n}$は互いに素である。したがって$Q_{n}$は**(a)**により正の集合だ。また、以下の式が成立する。\n$$ \\bigcup \\nolimits_{1}^{\\infty} P_{j}=\\bigcup \\nolimits_{1}^{\\infty} Q_{j} $$\n今、$E$を$\\bigcup \\nolimits _{1}^\\infty P_{n}$の任意の可測な部分集合としよう。\n$$ E \\in \\left( \\bigcup \\nolimits _{1}^\\infty Q_{n} \\right)=\\left( \\bigcup \\nolimits _{1}^\\infty P_{n} \\right) $$\nそれでは、$\\nu (E) \\ge 0$を示せば証明は完了する。$E$の定義により、以下の式が成立することがわかる。 $$ E= \\bigcup \\limits_{j=1}^\\infty \\left( Q_{j} \\cap E \\right) $$ それぞれの$Q_{n}$が互いに素であるので、符号測度の可算加法性により、以下が成立する。\n$$ \\nu (E) = \\nu \\left(\\bigcup \\nolimits_{j=1}^\\infty \\left( Q_{j} \\cap E \\right) \\right) =\\sum \\limits_{j=1}^\\infty \\nu \\left( Q_{j} \\cap E \\right) $$\nこの時、$Q_{n}$が正の集合で、$(Q_{j}\\cap E ) \\subset Q_{j}$であるため、式の右辺は必ず$0$以上である。\n$$ \\nu (E) =\\sum \\limits_{j=1}^\\infty \\nu \\left( Q_{j} \\cap E \\right) \\ge 0 $$\n■\nGerald B. Folland, Real Analysis: Modern Techniques and Their Applications (2nd Edition, 1999), p86\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1303,"permalink":"https://freshrimpsushi.github.io/jp/posts/1303/","tags":null,"title":"正の集合, 負の集合, 零の集合"},{"categories":"확률론","contents":"定義 1 確率空間 $( \\Omega , \\mathcal{F} , P)$ が与えられたとしよう。\nすべてのボレル集合 $B \\in \\mathcal{B} (\\mathbb{R})$ に対して $X^{-1} (B) \\in \\mathcal{F}$ を満たす関数 $X : \\Omega \\to \\mathbb{R}$ を 確率変数Random Variableと呼ぶ。 以下のように定義された$\\mathcal{F}_{X}$ を $X$ によって生成されたシグマ場と呼ぶ。 $$ \\mathcal{F}_{X} := X^{-1} ( \\mathcal{B} ) = \\sigma (X) = \\left\\{ X^{-1} (B) \\in \\Omega : B \\in \\mathcal{B}( \\Omega ) \\right\\} $$ 以下のように定義された測度 $P_{X}$ を$X$ の 確率分布Probability Distributionと呼ぶ。 $$ P_{X} (B) := P ( X^{-1} (B) ) $$ 測度論についてまだ知らないなら、確率空間という言葉を無視してもいい。 説明 確率空間と同じように、確率変数も測度論で厳密に定義することができる。\n$X^{-1} (B) \\in \\mathcal{F}$ という言葉は、$X$ が $\\Omega$ の要素を実数にマッピングして大小関係 $P(a \\le X \\le b)$ のようなものを使えるようにしつつ、ボレル集合の逆像がシグマ場に属させ、理にかなった集合だけを事象として扱うように制約を加えたことを意味している。一見すると過度に抽象的に見えるかもしれないが、逆説的に、その目的は過度な抽象性を失わせることにあるとも考えられる。定義によれば、確率変数$X$ は実数関数であるだけでなく可測関数となり、もし$\\Omega = \\mathbb{R}$ なら $\\mathcal{F} = \\mathcal{B} \\left( \\mathbb{R} \\right)$ で、ただのボレル関数 $X : \\mathbb{R} \\to \\mathbb{R}$ となる。通常、数理統計学の簡単な定理はこのレベルで十分。その先、多変数確率変数への一般化は、すべてのボレル集合 $B \\in \\mathcal{B} (\\mathbb{R}^{p})$ に対して $X^{-1} (B) \\in \\mathcal{F}$ を満たす$X : \\Omega \\to \\mathbb{R}^{p}$ を定義することによって簡単に行うことができる。もちろん、$X$ は各確率変数 $X_{i} : \\Omega \\to \\mathbb{R}$ に対して$X = ( X_{1}, \\cdots , X_{p})$ のようにベクトルとして表すことができ、確率ベクターと呼ばれる。これが確率変数の数列につながれば確率過程Stochastic Process、さらに一般的には確率要素Random Elementと呼ばれる。 シグマ場$\\mathcal{G}$ に対して$Y^{-1} ( \\mathcal{B} ) \\in \\mathcal{G}$ ならば$Y$ が $\\mathcal{G}$-可測であるというけれども、$\\mathcal{F}_{X}$ の定義によれば、当然$X$ は$\\mathcal{F}_{X}$-メジャラブルである。 定義が多くて混乱するかもしれないが、一つ一つ考えてみれば全く難しいことはない。$X^{-1} (B) \\in \\mathcal{F}$ であるため、これを逆関数のように考えると$X^{-1} : \\mathcal{B} (\\mathbb{R}) \\to \\mathcal{F}$ となる。このように、$P_{X} : = ( P \\circ X^{-1} )$ は $$ P_{X} : \\mathcal{B} (\\mathbb{R}) \\to \\mathcal{F} \\to [0,1] $$ と理解でき、ボレル集合$B$ に対して$0$ から$1$ までのどんな値にもマッピングする単なる合成関数にすぎない。例えば、$[-3,-2]$ は自然に$\\mathbb{R}$ のボレル集合で、確率変数$Y$ がどのように定義されているかによって、$P_{Y} ( [-3,-2] ) = 0.7$ のような計算をすることができるようになるわけだ。 参考 数理統計学で定義された確率変数と確率分布 Capinski. (1999). Measure, Integral and Probability: p66~68.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1288,"permalink":"https://freshrimpsushi.github.io/jp/posts/1288/","tags":null,"title":"測度論で定義される確率変数と確率分布"},{"categories":"측도론","contents":"定義1 $(X, \\mathcal{E})$を可測空間だとしよう。以下の条件を満たす拡張実数値関数$\\nu : \\mathcal{E} \\to \\overline{\\mathbb{R}}$を符号付き測度signed measureという。\n$\\nu ( \\varnothing ) = 0$ $\\pm \\infty$の中で高々1つだけが$\\nu$の関数値になれる。つまり、$-\\infty \\in \\nu (\\mathcal{E})$ならば$+\\infty \\notin \\nu (\\mathcal{E})$であり、$+\\infty \\in \\nu (\\mathcal{E})$ならば$-\\infty \\notin \\nu (\\mathcal{E})$である。 $\\left\\{E_{j}\\right\\}$を$\\mathcal{E}$で互いに素な集合の数列としよう。すると$\\nu \\left( \\bigcup \\nolimits_{j=1}^\\infty E_{j} \\right) =\\sum \\limits_{j=1}^\\infty \\nu (E_{j})$を満たす。この時$\\nu (\\cup _{1}^\\infty E_{j})$が有限の場合、右辺の和は絶対収束する。 説明 簡単に言うと、負の値も取れるように一般化された測度である。従って測度だったら符号付き測度でもある。測度と符号付き測度を一緒に言及する際は強調のために測度を正の測度positive measureと呼ぶこともある。符号付き測度の具体的な例にはリーマン積分がある。\n一方で、測度は常に0以上の関数値を持たなければならないため、任意の関数のリーマン積分に絶対値を取ったものと考えることができる。また、全ての符号付き測度は2つの測度の差で表現可能である。\n$$ \\nu = \\mu_{1} -\\mu_2 $$\n性質 $\\nu$を可測空間$(X,\\mathcal{E})$で定義された符号付き測度としよう。\n下からの連続性:\n$\\left\\{ E_{j} \\right\\}_{1}^\\infty \\subset \\mathcal{E}$が単調増加数列だとしよう。つまり$E_{1} \\subset E_2 \\subset \\cdots$。すると以下が成り立つ。 $$ \\mu\\left( \\bigcup \\nolimits _{1}^\\infty E_{j} \\right)= \\lim \\limits_{j\\rightarrow \\infty} \\mu (E_{j}) $$\n上からの連続性:\n$\\left\\{ E_{j} \\right\\}_{1}^\\infty \\subset \\mathcal{E}$が単調減少数列だとしよう。つまり$E_{1} \\supset E_2 \\supset \\cdots$。そして$\\mu (E_{1})\u0026lt;\\infty$としよう。すると以下が成り立つ。 $$ \\mu\\left(\\bigcap \\nolimits _{1}^\\infty E_{j} \\right)= \\lim \\limits_{j\\rightarrow \\infty} \\mu (E_{j}) $$\n基本的には測度での証明方法1と同じである。測度の文脈での証明には可算加法性が必要だったが、符号付き測度にも可算加法性があるため、証明方法は同じである。従って省略する。\n参照 測度 複素測度 性質 (C), (D) を参照\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1301,"permalink":"https://freshrimpsushi.github.io/jp/posts/1301/","tags":null,"title":"符号付き測度"},{"categories":"측도론","contents":"定義 可測空間 $(X,\\mathcal{E})$が与えられたとしよう。以下の三条件を満たす拡張実数値を持つ関数$\\mu : \\mathcal{E} \\to \\overline{\\mathbb{R}}$を測度だという。\n(a) $\\mu ( \\varnothing ) = 0$\n(b) $\\mu (E) \\ge 0,\\quad \\forall E\\in \\mathcal{E}$\n(c) $\\left\\{E_{j}\\right\\}$を$\\mathcal{E}$の中で互いに素な集合の数列としよう。すると、次が成り立つ。\n$$ \\mu \\left( \\bigcup _{j=1}^\\infty E_{j} \\right) =\\sum \\limits_{j=1}^\\infty \\mu (E_{j}) $$\n順序対$(X,\\mathcal{E}, \\mu)$を測度空間という。\n二つの集合 $E_{1}$、$E_2$が$E_{1} \\cap E_2=\\varnothing$を満たすならば、$E_{1}$と$E_2$は互いに素の集合と言う。\n説明 $\\mu$の条件を$\\mu\\ :\\ \\mathcal{E} \\rightarrow [0,\\infty]$に変えると**(b)**を含むので、省略できる。\n条件**(c)**は、簡単に言えば可算加法性だ。注意すべき点は、互いに素な集合に対してのみ成立するということだ。\n符号付き測度と測度を一緒に言及する場合、強調のために測度を正の測度とも呼ぶ。\n性質 $(X,\\mathcal{E},\\mu)$を測度空間としよう。\n(A) 単調性: $E,F\\in \\mathcal{E}$かつ$E\\subset F$ならば、$\\mu (E) \\le \\mu (F)$である。\n(B) 可算準加法性: $\\left\\{ E_{j} \\right\\}_{1}^\\infty$が$\\mathcal{E}$の元の数列ならば、$\\mu \\left( \\bigcup_{1}^\\infty E_{j} \\right) \\le \\sum _{1}^\\infty \\mu (E_{j})$である。\n(C) 下からの連続性: $\\left\\{ E_{j} \\right\\}_{1}^\\infty \\subset \\mathcal{E}$が単調増加数列であるとする。つまり$E_{1} \\subset E_2 \\subset \\cdots$。すると、次が成り立つ。 $$ \\mu\\left( \\bigcup \\nolimits _{1}^\\infty E_{j} \\right)= \\lim \\limits_{j\\rightarrow \\infty} \\mu (E_{j}) $$\n(D) 上からの連続性: $\\left\\{ E_{j} \\right\\}_{1}^\\infty \\subset \\mathcal{E}$が単調減少数列であるとする。つまり$E_{1} \\supset E_2 \\supset \\cdots$。そして$\\mu (E_{1})\u0026lt;\\infty$とする。すると、次が成り立つ。 $$ \\mu\\left(\\bigcap \\nolimits _{1}^\\infty E_{j} \\right)= \\lim \\limits_{j\\rightarrow \\infty} \\mu (E_{j}) $$\n証明 (A) $E \\subset F$とする。すると$F=F\\setminus E+ E$が成り立つ。$E$と$F\\setminus E$は互いに素なので、測度の定義**(c)**により、次が成り立つ。\n$$ \\mu (F) = \\mu (F\\setminus E+ E) = \\mu (F\\setminus E) + \\mu (E) $$\nすると、測度の定義**(b)**によって、次が成り立つ。\n$$ \\mu (F\\setminus E) + \\mu (E) \\ge \\mu (E) $$\nだから、次が得られる。\n$$ \\mu (F) \\ge \\mu (E) $$\n■\n(B) $F_{1}=E_{1}$としよう。そして、$k\u0026gt;1$に対して$F_{k}=E_{k} \\setminus \\left( \\bigcup_{1}^{k-1} E_{j} \\right)$としよう。すると、各々の$F_{k}$は互いに素で、$\\bigcup_{1}^n F_{j}=\\bigcup_{1}^n E_{j},\\ \\forall n$である。また、各々の$j$に対して$F_{j} \\subset E_{j}$である。だから、次が成り立つ。\n$$ \\mu \\left( \\bigcup \\nolimits_{1}^\\infty E_{j}\\right)=\\mu \\left( \\bigcup \\nolimits_{1}^\\infty F_{j}\\right)=\\sum \\limits_{1}^\\infty \\mu (F_{j}) \\le \\sum \\limits_{1}^\\infty\\mu (E_{j}) $$\n二番目の等号は測度の定義**(c)によって成り立つ。最後の不等式は(A)**によって成り立つ。\n■\n(C) $E_{0}:= \\varnothing$としよう。そして、$F_{j}=E_{j}\\setminus E_{j-1}$としよう。すると、各々の$F_{j}$は互いに素である。また、$\\bigcup _{1}^\\infty F_{j} =\\bigcup_{1}^\\infty E_{j}$が成り立つ。だから、次が成り立つ。\n$$ \\begin{align*} \\mu \\left( \\bigcup \\nolimits_{1}^\\infty E_{j}\\right) \u0026amp;= \\mu \\left( \\bigcup \\nolimits_{1}^\\infty F_{j}\\right) \\\\ \u0026amp;= \\sum_{1}^\\infty \\mu (F_{j}) \\\\ \u0026amp;= \\sum \\limits_{1}^\\infty \\mu (E_{j} \\setminus E_{j-1} ) \\\\ \u0026amp;= \\lim \\limits_{n \\rightarrow \\infty} \\sum \\limits_{1} ^n \\mu (E_{j}\\setminus E_{j-1} ) \\\\ \u0026amp;= \\lim \\limits_{n \\rightarrow \\infty} \\mu (E_{n}) \\\\ \u0026amp;= \\lim \\limits_{j \\rightarrow \\infty} \\mu (E_{j}) \\end{align*} $$\n二番目の等号は測度の定義**(c)**によって成り立つ。\n■\n(D) $F_{j}=E_{1} \\setminus E_{j}$としよう。すると、$F_{1} \\subset F_2 \\subset \\cdots$が成り立つ。また、$\\mu (E_{1})=\\mu (F_{j})+\\mu (E_{j})$で、$\\bigcup_{1}^\\infty F_{j}=E_{1} \\setminus \\left( \\bigcap_{1}^\\infty E_{j} \\right)$が成り立つ。$E_{1}= \\bigcup_{1}^\\infty F_{j}+\\bigcap_{1}^\\infty E_{j}$なので、次が成り立つ。\n$$ \\begin{align*} \\mu (E_{1}) \u0026amp;= \\mu \\left( \\bigcap \\nolimits_{1}^\\infty E_{j}\\right) + \\mu \\left( \\bigcup \\nolimits _{1}^\\infty F_{j} \\right) \\\\ \u0026amp;= \\mu \\left( \\bigcap \\nolimits_{1}^\\infty E_{j}\\right) + \\lim \\limits_{j \\rightarrow \\infty} \\mu ( F_{j} ) \\\\ \u0026amp;= \\mu \\left( \\bigcap \\nolimits_{1}^\\infty E_{j}\\right) + \\lim \\limits_{j \\rightarrow \\infty}\\big[ \\mu ( E_{1} )-\\mu (E_{j}) \\big] \\\\ \u0026amp;= \\mu ( E_{1} )+ \\mu \\left( \\bigcap \\nolimits_{1}^\\infty E_{j}\\right) -\\lim \\limits_{j \\rightarrow \\infty}\\mu (E_{j}) \\end{align*} $$\n二番目の等号は**(C)**によって成り立つ。$\\mu (E_{1}) \u0026lt; \\infty$なので、次が得られる。\n$$ \\mu \\left( \\bigcap \\nolimits_{1}^\\infty E_{j}\\right) = \\lim \\limits_{j \\rightarrow \\infty}\\mu (E_{j}) $$\n■\n参照 ルベーグ測度 ボレル測度 符号付き測度 複素測度 ","id":1302,"permalink":"https://freshrimpsushi.github.io/jp/posts/1302/","tags":null,"title":"測度の一般的な定義"},{"categories":"알고리즘","contents":"定義 与えられた問題を解く時の時間を時間複雑度Time Complexity、メモリの要求を空間複雑度Space Complexityと言う。\n例 漸近記法はこれらを表現するのに非常に便利な手段になる。時間複雑度の例を見てみよう。\n定数時間 $O(1)$ $n$ にかかわらず終わることができるアルゴリズムで、実質的に時間がかからないことだ。例えば、$\\mathbb{x} = [4,3,8,-1,-9,0,5,7,2,6]$ の三番目の要素を見つけるアルゴリズムは、$\\mathbb{x}$ がどうなっているかに関心がなく、ただ $8$ を返せばいい。\n線形時間 $O(n)$ $n$ に比例する時間がかかる。例えば、$\\mathbb{x} = [4,3,8,-1,-9,0,5,7,2,6]$ の最大値を見つけるアルゴリズムは、$8$ で見つけられるが、それが本当に最大値であることを保証するためには、他の要素もすべてチェックしなければならない。表現がこんな感じだと少し悪い気がするが、実際、線形時間くらいであればかなり速いアルゴリズムだと言われている。\n二次時間 $O(n^2)$ $n^2$ に比例する時間がかかる。例えば、$\\mathbb{x} = [4,3,8,-1,-9,0,5,7,2,6]$ を大きさ順に並べ替える場合は、最大値を一度見つけて最後に置き、その最大値を抜いた配列でまた最大値を見つけることを繰り返せばいい。最大値を見つけるたびに $n, n-1, \\cdots , 1$ 時間がかかるので、その合計は 等差数列の和の公式により $\\displaystyle \\sum_{k=1}^{n} k = {{ n (n+1)} \\over {2}} = O(n^2)$ になる。もっと賢い方法があると思うかもしれないが、これより愚かな方法は考える必要がない。\n三次時間 $O(n^3)$ $n^3$ に比例する時間がかかる。例えば、$n=2^{k}$ 時に $n \\times n$ 行列の乗算を考えてみると、$A, B \\in \\mathbb{R}^{n \\times n}$ を掛けた$C = AB$ を計算することになるが、次の8つの ${{n} \\over {2}} \\times {{n} \\over {2}}$ 行列の積を計算して解決できる。 $$ AB= \\begin{bmatrix} A_{1} \u0026amp; A_{2} \\\\ A_{3} \u0026amp; A_{4} \\end{bmatrix} \\begin{bmatrix} B_{1} \u0026amp; B_{2} \\\\ B_{3} \u0026amp; B_{4} \\end{bmatrix} = \\begin{bmatrix} C_{1} \u0026amp; C_{2} \\\\ C_{3} \u0026amp; C_{4} \\end{bmatrix} = C $$\n$$ C_{1} = A_{1}B_{1} + A_{2} B_{3} $$\n$$ C_{2} = A_{1}B_{2} + A_{2} B_{4} $$\n$$ C_{3} = A_{3}B_{1} + A_{4} B_{3} $$\n$$ C_{4} = A_{3}B_{2} + A_{4} B_{4} $$ この計算を続けていくわけだが、一回の計算にかかる時間が $T(n)$ で、繰り返し外の実行時間を $c$ とすると $\\displaystyle T(n) = 8 T \\left( {{n} \\over {2}} \\right) + c$ だからだ。 $$ \\begin{align*} T(n) =\u0026amp; 8 T \\left( {{n} \\over {2}} \\right) + c \\\\ =\u0026amp; 8 \\left[ 8 T \\left( {{n} \\over {4}} \\right) + c \\right] + c \\\\ =\u0026amp; 8 \\left[ 64 T \\left( {{n} \\over {8}} \\right) + 8 c + c \\right] + c \\\\ =\u0026amp; 8^3 T \\left( {{n} \\over {8}} \\right) + (1+8+8^2)c \\\\ =\u0026amp; 8^3 T \\left( {{n} \\over {8}} \\right) + {{8^3 - 1} \\over {8 - 1}}c \\\\ \u0026amp; \\vdots \u0026amp; \\\\ \\approx\u0026amp; 8^{\\log_{2} n} ( T(1) + c ) \\\\ =\u0026amp; n^{\\log_{2} 8} ( T(1) + c ) \\\\ =\u0026amp; n^{3} ( T(1) + c ) \\\\ =\u0026amp; \\Theta ( n^3 ) \\end{align*} $$ 行列の乗算は、数学が入った応用分野でほとんど例外なく、それもかなり多く行う。しかし、$n^3$ は少し大きい。$n=100$ だけになると、なんと $n^3 = 10^6$ になる。ここでさらに計算を減らすことはできるだろうか？もともと不要な計算をしていないので、これ以上どう減らせるかは見当たらない。しかし、シュトラッセンアルゴリズムという驚くべき方法を使えば、これをさらに減らすことができる。これがアルゴリズムの醍醐味だ。\n対数時間 $O \\left( \\log (n) \\right)$ 非常に速いという意味だ。例えば、ソートされた配列 $\\text{sort} (\\mathbb{x}) = [-9,-1,0,2,3,4,5,6,7,8]$ で $0$ の位置を見つける問題があるとしよう。直感的に考えられる方法は、真ん中の要素を一つ選んで $0$ より大きいか小さいかを確認し、$0$ より大きければ右側を捨て、小さい場合は左側を捨てて配列を縮めながら探すことだ。これを二分探索Binary Searchと言う。一回の計算にかかる時間が $T(n)$ で、比較にかかる時間を $c$ とすると、 $$ \\begin{align*} T(n) =\u0026amp; T \\left( {{n} \\over {2}} \\right) + c \\\\ =\u0026amp; \\left[ T \\left( {{n} \\over {4}} \\right) + c \\right] + c \\\\ =\u0026amp; T \\left( {{n} \\over {4}} \\right) + 2 c \\\\ =\u0026amp; T \\left( {{n} \\over {8}} \\right) + 3 c \\\\ \u0026amp; \\vdots \u0026amp; \\\\ \\approx\u0026amp; T \\left( 1 \\right) + c \\log_{2} n \\\\ =\u0026amp; O \\left( \\log_{2} (n) \\right) \\end{align*} $$ それだからだ。対数の底の変換公式によれば $O( \\log (n)))$ になってもいいが、元々コンピュータ科学では、特に対数の底に言及がなければ、通常は $e = 2.7182\u0026hellip;$ ではなく$2$ だから、表現に気を使う必要はない。\n指数時間 $O(2^n)$ 非常に長い時間がかかるという意味だ。再帰函数を使ってフィボナッチ数列を計算する場合、非常に非効率的で、一度の計算が $n$ 番目のフィボナッチ数 $a_{n}$ を求めるためにかかる時間が $T(n)$ で、前の二項を足すのにかかる時間を $c$ とすると、黄金比 $\\phi = 1.618\u0026hellip;$ に対して、 $$ \\begin{align*} T(n) =\u0026amp; T(n-1) + T(n-2) + c \\\\ =\u0026amp; [ T(n-2) + T(n-3) + c ] + T(n-2) + c \\\\ =\u0026amp; 2 T(n-2) + T(n-3) + (1+1) c \\\\ =\u0026amp; 3 T(n-3) + 2 T(n-4) + (1+1+2) c \\\\ =\u0026amp; 5 T(n-4) + 3 T(n-5) + (1+1+2 + 3) c \\\\ =\u0026amp; 8 T(n-6) + 5 T(n-4) + (1+1+2+3+5) c \\\\ \u0026amp; \\vdots \u0026amp; \\\\ =\u0026amp; c \\sum_{k=1}^{n} a_{n} \\\\ \\approx\u0026amp; {{\\phi^{n-1}-1} \\over {\\phi-1}} c \\\\ =\u0026amp; \\Omega ( \\phi^n ) \\end{align*} $$ こうなる理由だ。最後の部分は、$\\displaystyle \\phi \\approx {{a_{n}} \\over {a_{n-1}}}$ であるため、等比数列の和の公式を使って近似したものだ。こんなアルゴリズムは現実的には使うことができず、動的プログラミングのような解決法を探すか、または、このような時間複雑度を持つことを利用して、暗号に応用することもできる。\n","id":1283,"permalink":"https://freshrimpsushi.github.io/jp/posts/1283/","tags":null,"title":"時間計算量と空間計算量"},{"categories":"통계적분석","contents":"定義 1 与えられた時系列データ$\\left\\{ p_{t} \\right\\}$。\n$\\left\\{ p_{t} \\right\\}$の分散が$t$に依存しているとき、$\\left\\{ p_{t} \\right\\}$は異分散性Heteroscedasticityを持つと言われる。 異分散性を持つ$\\left\\{ p_{t} \\right\\}$の分散が大きくなったり小さくなったりを繰り返す現象をボラティリティクラスタリングVolatility Clusteringと言う。 次のように定義された$r_{t}$を$t$での**(ログ)リターン**Returnと言う。 $$ r_{t} := \\nabla \\log p_{t} = \\log {{ p_{t} } \\over { p_{t-1} }} $$ 説明 ヘテロスケダスティシティは[heteroscedasticity]と読み、ヴォラティリティクラスタリングは[volatility clustering]と読む。これら2つの言葉を韓国語で説明した文書は少ないが、これを勉強した人々が言葉を簡単にする必要性を感じないからだろう。異分散性やボラティリティクラスタリングを書いて読んでみると不自然ではないが、話すときはその長い言葉をそのまま発音することが多い。\n異分散性 ARIMAモデルは、ほとんどの統計分析と同じように、データの平均に関心がある。実際の値がどのように動くかが重要なのである。しかし、値自体だけでなく、その変動自体も時間の流れと共に変わるという仮定をすることもできる。そのために異分散性という言葉が作られたのである。時系列データの分散が時間に関係なく常に一定であれば、この言葉は必要ない。\n時系列の分散に関心を持つのは、通常経済、金融分野だ。データの変動は価値の変化を示し、ある資産のリスクを論じるとき、すぐに使われることができる。リスクは利益の別の姿であるため、関心の対象となる。時系列データを表すときは通常$\\left\\{ y_{t} \\right\\}$と書くが、異分散性に注目するときは$\\left\\{ p_{t} \\right\\}$と書くのもそのためである。$p_{t}$の$p$はPriceから来たものだ。\nボラティリティクラスタリング 分散が大きくなったり小さくなったりを繰り返すことをボラティリティクラスタリングと呼び、この用語がある理由は単純だ。市場でボラティリティがずっと大きくなったり小さくなったりすることはなく、もしそのような場合があっても、統計分析をする必要があるほど複雑なデータではないからだ。もっと上品に表現することもできるが、まずはこの説明で進もう。\nリターン リターンは、このような異分散性を見つけ、分析するのに役立つ表現だ。ログを取ることで、$p_{t}$が$p_{t-1}$より大きい（価格が上がれば）場合は正の数となり、小さければ（価格が下がれば）負の数となり、パーセンテージで見やすくするために100を掛けることもある。単に値の差が大きいか小さいかではなく、前のデータとの比率を使うので、データの固有の特徴に大きく影響されないのも良い点だ。\n実習 上のグラフは、組み込みデータEuStockMarketsからDAXだけを抽出して描いたもので、1991年から1999年までのドイツDAX指数を示している。\nリターンは上のように描かれており、ハイライトが入っている画像を見て、ボラティリティが高いと低いが交互に現れるように見えたら、ボラティリティクラスタリングが何か理解したと見なしても良い。\nそう見えなくても良い。感覚に頼った主張を堅固にするのが統計学の役割であり、まだ何の統計技術も使われていないからだ。\nコード returnize \u0026lt;- function(data) {return(diff(log(data)))}\rwin.graph(6,4)\rplot(EuStockMarkets[,1],main=\u0026#34;EuStockMarkets\u0026#34;,ylab=\u0026#39;DAX\u0026#39;)\rDAX \u0026lt;- ts(EuStockMarkets[,1],start=1)\rr.DAX \u0026lt;- returnize(DAX)\rwin.graph(6,4)\rplot(r.DAX,type=\u0026#39;h\u0026#39;,main=\u0026#39;DAX의 리턴\u0026#39;) Cryer. (2008). Time Series Analysis: With Applications in R(2nd Edition): p277~279.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1272,"permalink":"https://freshrimpsushi.github.io/jp/posts/1272/","tags":["R"],"title":"時系列分析における異質スケダスティシティとボラティリティクラスタリング"},{"categories":"정수론","contents":"定義 1 $\\mathbb{Z} [i] := \\left\\{ a + i b : a, b \\in \\mathbb{Z} \\right\\}$をガウス環と言い、その要素をガウス整数という。\n定理 [1]: $\\overline{i} = i^{3}$ [2]: $( a \\pm ib ) + ( c \\pm id) = (a \\pm c) + i (b \\pm d)$ [3]: $( a + ib )( c + id) = (ac - bd) + i (ad + bc)$ 説明 $i$は二次方程式$x^2 +1 = 0$の複素根であり、$\\mathbb{Z} [i]$は整数環$\\mathbb{Z}$の拡張である。実数体$\\mathbb{R}$が複素数体$\\mathbb{C} = \\mathbb{R} [i]$に拡張されるのに似ていて、その理由も大差ない。整数を議論する上で無理数でさえもタブーではないのだから、複素数を考えない理由はない。むしろ、無理数よりも簡単だ。\n整数に素数があるように、ガウス整数にもガウス素数がある。 $\\mathbb{Z} [i]$上では次のように通常の数式展開が可能である： $$ \\begin{align*} (7 + i2)(4 -i 2) =\u0026amp; (28 + 4) + i (- 14 +8 ) \\\\ =\u0026amp; 32 - i 6 \\end{align*} $$ また、ある自然数$n \\in \\mathbb{N}$が与えられたとき、有限環$\\mathbb{Z}_{n}$に対しても$\\mathbb{Z}_{n}[i]$を考えることができる。例えば、$n = 7$とすると、上記の展開は次のように変わる： $$ \\begin{align*} (7 + i2)(4 -i 2) =\u0026amp; (28 + 4) + i (- 14 + 8 ) \\\\ =\u0026amp; 32 - i 6 \\\\ \u0026amp; \\equiv 4 - i 6 \\pmod{7} \\\\ \u0026amp; \\equiv 4 + i \\pmod{7} \\end{align*} $$ 自然に合同式を使用したことに注目。$\\mathbb{Z}$を$\\mathbb{Z} [i]$に一般化したいという欲求は、数学者にとって言葉で説明するのも難しいほど自然なものだ。解析学が$i$を許容したことによる革新と比べられるかはわからないが、整数論もまた豊かで美しくなったことは確かだ。ただちに代数学の基本定理を考えても、$\\mathbb{Z} [i]$에서は$d$次の合同方程式が$d$個の解を持つといった汚い話はない。複素数が導入されたことにより、ただきれいに正確に$d$個の解を持つと言えるようになったのだ。\nここから一歩進んだ整数体系としては、アイゼンシュタイン整数がある。\nガウス環の零因子グラフはオスバによって研究された。\n証明 [1] $i$の定義と共役の性質により $$ \\overline{i} = -i = i^{3} $$\n■\n[2] $\\mathbb{Z} [i]$は環であり、加算に対し結合法則と交換法則が成り立つので $$ \\begin{align*} ( a \\pm ib ) + ( c \\pm id) =\u0026amp; a \\pm ib + c \\pm id \\\\ =\u0026amp; a \\pm c + ib \\pm id \\\\ =\u0026amp; (a \\pm c) + i (b \\pm d) \\end{align*} $$\n■\n[3] [2]に基づき $$ \\begin{align*} ( a + ib )( c + id) =\u0026amp; ac + ibc + iad + (- 1)bd \\\\ =\u0026amp; (ac - bd) + i (ad + bc) \\end{align*} $$\n■\nSilverman. (2012). 数論へのフレンドリーな導入 (第4版): p267。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1267,"permalink":"https://freshrimpsushi.github.io/jp/posts/1267/","tags":null,"title":"ガウス整数"},{"categories":"추상대수","contents":"定義 1 整域 $D$において、全ての$\\alpha , \\beta \\in D$に対して以下の条件を満たす関数$N : D \\to \\mathbb{Z}$を乗法的ノルム乗法的ノルムと定義する。\n(i): $N (\\alpha) = 0 \\iff \\alpha = 0$ (ii): $N ( \\alpha \\beta ) = N ( \\alpha ) N ( \\beta )$ 定理 $p \\in \\mathbb{Z}$が素数だとする。\n[1]: $D$に乗法的ノルム$N$が定義された場合、$N(1) = 1$であり、全てのユニット$u \\in D$に対して$| N ( u ) | = 1$ [2]: $| N ( \\alpha )| =1$を満たす全ての$\\alpha \\in D$が$D$のユニットなら、$| N ( \\pi ) | = p$を満たす$\\pi \\in D$は$D$の既約元である。 ユニットは乗算の逆元を持つ要素である。 説明 一般にノルムと言うと、通常$N (\\alpha) \\ge 0$が前提され、$\\alpha \\ne 0$に対して$\\nu ( \\alpha) = N ( \\alpha)$のような条件が加えられるが、同時にユークリッドノルムとなることが多い。一般的に断言はできないが、この程度の常識にも合わない代数構造を研究するモチベーションは珍しいだろう。ノルムが定義されている場合、ほぼ間違いなく$N : D \\to \\mathbb{N}_{0}$と見て良い。\nノルムの定義は、整域 $D$の算術構造を理解する上で大きな助けとなる。代数的整数論では、ドメインに適したさまざまなノルムを定義し、一見数論の領域にないように見える代数構造も、数論の領域に「引き込んで」研究できるようにする。数論に直接適用できることは言うまでもない。興味深い例としては、ガウス整数$\\mathbb{Z} [i]$$i$, $\\omega$やアイゼンシュタイン整数$\\mathbb{Z} [\\omega]$(../1291)を考えることができる。\n定理[2]によると、その$D$の要素$\\pi$についてよくわからなくても、$N ( \\pi )$が素数であることだけで、$\\pi$が$D$の既約元であることが保証される。知られているように、素数$p$は$\\mathbb{Z}$で既約元であり、$N$は条件(ii)を通じて、$D$から$\\mathbb{Z}$への既約元の性質を保存したと見ることができる。\n証明 [1] 戦略: 条件(ii)を通じて$D$の要素を引き裂くことで、自然に演繹される。\n単位元$1 \\in D$に対して$N(1)$を計算すると、乗法性により $$ N(1) = N \\left( 1 \\cdot 1 \\right) = N (1) N (1) $$ ゆえに$N(1)$である。また、$u \\in D$がユニットならば、定義によりその逆元$u^{-1} \\in D$が存在するので、 $$ 1 = N ( 1) = N ( u u^{-1} ) = N (u ) N (u^{-1}) $$ もちろん$N (u)$は整数なので、$| N ( u) | =1$でなければならない。\n■\n[2] $| N(u) | = 1$を満たす全ての$u \\in D$は$D$のユニットであるとする。$\\pi \\in D$が$| N ( \\pi ) | = 1$であり、かつ$\\pi = \\alpha \\beta$である場合、 $$ p = | N ( \\pi ) | = | N ( \\alpha ) N ( \\beta ) | $$ $p$は素数なので、$| N ( \\alpha ) | = 1$か$| N ( \\beta ) | = 1$でなければならない。仮定から、$\\alpha$または$\\beta$のどちらかが$D$のユニットであり、結果として$\\pi$は$D$の既約元となる。\n■\nFraleigh. (2003). A first course in abstract algebra(第7版): p410。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1259,"permalink":"https://freshrimpsushi.github.io/jp/posts/1259/","tags":null,"title":"積分領域のノルム"},{"categories":"해석개론","contents":"定義 以下のように定義される集合を拡張された実数システムと言います。\n$$ \\overline{ \\mathbb{R} } := \\mathbb{R} \\cup \\left\\{ -\\infty, +\\infty\\right\\} $$\n説明 解析学などの分野では、便宜のために実数の集合$\\mathbb{R}$の代わりに$\\overline{ \\mathbb{R} }$を使用することがよくあります。$\\pm \\infty$は数ではありませんが、便宜のために数として扱い、$\\mathbb{R}$に加えて使用すると便利です。拡張された実数システムの中で大小比較と演算のルールは以下の通りです。\nすべての$x \\in \\mathbb{R}$に対して、\n$$ -\\infty \u0026lt; x \u0026lt;+\\infty $$\n$$ (\\pm \\infty) + (\\pm \\infty) = \\pm \\infty $$\n$$ x + (\\pm \\infty)=\\pm \\infty+x=\\pm \\infty $$\n$$ \\dfrac{x}{+\\infty}=0=\\dfrac{x}{-\\infty} $$\n$$ (\\pm \\infty)(\\pm\\infty)=+ \\infty $$\n$$ (\\pm \\infty)(\\mp \\infty)=- \\infty $$\n$$ x(\\pm \\infty)=(\\pm \\infty)x=\\begin{cases} \\pm \\infty \u0026amp; x\u0026gt;0 \\\\ 0 \u0026amp; x=0 \\\\ \\mp \\infty \u0026amp; x\u0026lt;0 \\end{cases} $$\n$(\\pm \\infty)+(\\mp \\infty)$は定義しないことに注意してください。\n定理 $\\overline{ \\mathbb{R} }$は完全順序集合です。\n与えられた$A \\subset \\overline{ \\mathbb{R} }$に対して、$\\sup A$と$\\inf A$が存在します。\n$a \\in \\mathbb{R}$に対して、$(a, +\\infty]$は$+\\infty$の近傍です。\n","id":1252,"permalink":"https://freshrimpsushi.github.io/jp/posts/1252/","tags":null,"title":"拡張実数体系"},{"categories":"측도론","contents":"まとめ $X$を任意の集合とする。そして、空集合でない$A \\subset \\mathcal{P}(X)$が与えられたとする。そうしたら、$A$を含む最小の$\\sigma$-代数、$\\mathcal{E}_{A}$が存在する。\n証明 $\\mathcal{E}_{A}$を定義し、それが$\\sigma$-代数になることを示した後、最小である1ことを示そうと思う。\n$A$を含む全ての$\\sigma$-代数の集合を$S$とする。\n$$ S:= \\left\\{ \\mathcal{E} \\subset \\mathcal{P}(X)\\ :\\ \\mathcal{E}\\ \\mathrm{is\\ } \\sigma \\mathrm{-algebra, \\ } A \\subset \\mathcal{E} \\right\\} $$\nそれで、$\\mathcal{P}(X) \\in S$であることは自明である。従って、$S \\ne \\varnothing$である。今、$\\mathcal{E}_{A} := \\bigcap \\limits_{\\mathcal{E} \\in S} \\mathcal{E}$としよう。そうすると$A \\subset \\mathcal{E}_{A}$である。さらに、$\\mathcal{E}_{A}$が$\\sigma$-代数であることを示すことができる。\n$\\sigma$-代数\n集合$X$が与えられたとする。下記の条件を満たす$X$の部分集合のコレクション $\\mathcal{E} \\subset \\mathcal{P}(X)$を**$\\sigma$-代数**という。\n(D1) $\\varnothing, X \\in \\mathcal{E}$ (D2) $E \\in \\mathcal{E} \\implies E^c \\in \\mathcal{E}$ (D3) $E_{k} \\in \\mathcal{E}\\ (\\forall k \\in \\mathbb{N}) \\implies \\bigcup_{k=1}^\\infty E_{k} \\in \\mathcal{E}$ (D4) $E_{k} \\in \\mathcal{E}\\ (\\forall\\ k \\in \\mathbb{N}) \\implies \\bigcap_{k=1}^\\infty E_{k} \\in \\mathcal{E}$ (D1)\n各$\\mathcal{E}$が$\\sigma$-代数であるので、$\\varnothing$、$X$が含まれていることは明らかである。従って、$\\mathcal{E}_{A}$の定義によれば、$\\varnothing$、$X\\in \\mathcal{E}_{A}$は明らかである。\n(D2)\n$E \\in \\mathcal{E}_{A}$とする。すると、$\\mathcal{E}_{A}$の定義により、各$\\mathcal{E}$に対しても$E \\in \\mathcal{E}$が成立する。各$\\mathcal{E}$は$\\sigma$-代数であるので、$E^c \\in \\mathcal{E}$である。従って、$\\mathcal{E}_{A}$の定義により$E^c \\in \\mathcal{E}_{A}$である。\n(D3)\n条件**（D2）を示したように、$\\mathcal{E}_{A}$の定義と各$\\mathcal{E}$が$\\sigma$-代数である事実を利用すると、簡単に示すことができる。(D4)は（D3）**が成立すれば、デモルガンの法則により自動的に成立する。\nしたがって、$\\mathcal{E}_{A}$は条件**（D1）〜（D4）**を満たすので、$\\sigma$-代数である。今、$A$を含む別の$\\sigma$-代数を$\\mathcal{E}^{\\prime}$としよう。すると、集合$S$の定義により、$\\mathcal{E}^{\\prime} \\in S$であり、明らかに$\\mathcal{E}_{A} \\subset \\mathcal{E}^{\\prime}$である。したがって、$\\mathcal{E}_{A}$は$A$を含む最小の$\\sigma$-代数である。\n■\n定義 この時、$\\mathcal{E}_{A}$を**$A$によって生成された$\\sigma$-代数**$\\sigma$-algebra generated by Aと呼び、$\\mathcal{G}(A)$で表記する。\n対$(X,\\mathcal{T})$を位相空間と言う。位相の定義により、$\\mathcal{T} \\subset \\mathcal{P}(X)$である。従って、上記の定理により、$\\mathcal{T}$を含む最小の$\\sigma$-代数が存在する。これを$\\mathcal{B}_\\sigma (X) :=\\mathcal{G}(\\mathcal{T})$で表記し、位相空間$(X,\\mathcal{T})$上のボレル$\\sigma$-代数あるいは単にボレル代数Borel algebraという。\n$\\mathcal{B}_\\sigma (X)$の要素をボレル集合Borel setと言い、対$(X,\\mathcal{B}_\\sigma (X) )$をボレル可測空間Borel measurable spaceと言う。\n簡単に言えば、ボレル代数とは全ての開集合を要素として持つ最小の$\\sigma$-代数である。特に、ボレル代数で定義される全ての測度をボレル測度Borel measureと呼ぶ。\n参照 実数空間におけるボレル集合 無駄な部分が最小限に抑えられたシグマ場とも言える。この意味で、ボレルシグマ場は特に確率論を議論する際に便利である。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1251,"permalink":"https://freshrimpsushi.github.io/jp/posts/1251/","tags":null,"title":"ボレルσ-代数、ボレル可測空間"},{"categories":"매트랩","contents":"方法 subplot() 関数を使えば、複数の図を一ページに出力することができる。第一、第二の変数はそれぞれ、画像を出力する盤面の行と列を示し、図をどんな形で配置するかを決定する。第三の変数は、その図を何番目に配置するかを決定する。\n以下はコードと実際に出力された結果である。\nX1=Phantom();\rX2=radon(X1);\rX3=fft(X2);\rX4=iradon(X2,0:179);\rsubplot(2,2,1)\rimagesc(X1)\rtitle(\u0026#34;Phantom\u0026#34;);\rsubplot(2,2,2)\rimagesc(X2)\rtitle(\u0026#34;radon\u0026#34;);\rsubplot(2,2,3)\rimagesc(abs(X3))\rtitle(\u0026#34;fft\u0026#34;);\rsubplot(2,2,4)\rimagesc(X4)\rtitle(\u0026#34;iradon\u0026#34;); ","id":1247,"permalink":"https://freshrimpsushi.github.io/jp/posts/1247/","tags":null,"title":"MATLABで1ページに複数の図を出力する方法"},{"categories":"수치해석","contents":"メソッド 1 説明 ルンゲ＝クッタ法は、アダムス法のように、多様な形式を持ち、複雑な代数的操作を通じて $\\gamma_{j}$ と $V_{j}$ を決定する。その中でも特に人気があるのが4次のルンゲ＝クッタ法で、通常 RK4 と略される。\n4次のルンゲ＝クッタ法: $$ \\begin{align*} y_{n+1} =\u0026amp; y_{n} + {{h} \\over {6}} \\left[ V_{1} + 2 V_{2} + 2 V_{3} + V_{4} \\right] \\\\ V_{1} =\u0026amp; f(x_{n} , y_{n}) \\\\ V_{2} =\u0026amp; f \\left( x_{n} + {{h} \\over {2}} , y_{n} + {{h} \\over {2}} V_{1} \\right) \\\\ V_{3} =\u0026amp; f \\left( x_{n} + {{h} \\over {2}} , y_{n} + {{h} \\over {2}} V_{2} \\right) \\\\ V_{4} =\u0026amp; f \\left( x_{n} + h , y_{n} + h V_{3} \\right) \\end{align*} $$\nこの式を見ると、ステップ $h$ を進めるために $\\displaystyle {{h} \\over {2}}$ 地点のデータまで使ったことがわかる。もっと一般化した説明は関連項目を参照してほしい。\n直感的な導入 上記の項を一つずつ解いて、最も単純なオイラー法を使って考えてみよう。 (便宜上 $\\displaystyle x_{n+ {{1} \\over {2}} } : = x_{n} + {{h} \\over {2}}$、$y_{n+ {{1} \\over {2}} } \\approx Y(x_{n + {{h} \\over {2}} } )$ とする。) $$ V_{1} = f(x_{n} , y_{n} ) $$ は、$x_{n}$ 地点での微分係数 $y_{n} ' $ になる。 $$ \\begin{align*} V_{2} =\u0026amp; f \\left( x_{n} + {{h} \\over {2}} , y_{n} + {{h} \\over {2}} f \\left( x_{n} , y_{n} \\right) \\right) \\\\ =\u0026amp; f \\left( x_{n + {{1} \\over {2}} } , y_{n + {{1} \\over {2}} } \\right) \\end{align*} $$ は、$x_{n+{{1} \\over {2}} }$ 地点での微分係数 $\\displaystyle y_{n + {{1} \\over {2}} } ' $ になる。 $$ V_{3} = f \\left( x_{n} + {{h} \\over {2}} , y_{n} + {{h} \\over {2}} f \\left( x_{n + {{1} \\over {2}} } , y_{n + {{1} \\over {2}} } \\right) \\right) $$ また、バックワードオイラー法により $\\displaystyle V_{3} = f \\left( x_{n + {{1} \\over {2}} } , y_{n + {{1} \\over {2}} } \\right) $ で、$x_{n+{{1} \\over {2}} }$ 地点での微分係数 $\\displaystyle y_{n + {{1} \\over {2}} } ' $ になる。 $$ \\begin{align*} V_{4} =\u0026amp; f \\left( x_{n} + h , y_{n} + h V_{3} \\right) \\\\ =\u0026amp; f \\left( x_{n} + {{h} \\over {2}} + {{h} \\over {2}} , y_{n} + {{h} \\over {2}} V_{3} + {{h} \\over {2}} V_{3} \\right) \\\\ =\u0026amp; f \\left( x_{n + {{h} \\over {2}}} + {{h} \\over {2}} , y_{n + {{h} \\over {2}}} + {{h} \\over {2}} V_{3} \\right) \\end{align*} $$ さらに、バックワードオイラー法により、$x_{n+ 1 }$ 地点での微分係数 $\\displaystyle y_{n + 1} ' $ になる。 $$ {{h} \\over {6}} \\left[ V_{1} + 2 V_{2} + 2 V_{3} + V_{4} \\right] $$ 特に、$x_{n+{{1} \\over {2}} }$ 地点のデータまで使いながら、特に $x_{n+{{1} \\over {2}} }$ 地点に重みをつけた加重平均と見ることができるステップ $h$ を進めるためのものと考えられる。\nここで更に計算を加えることは可能だが、RK4からは収束次数が計算量だけで上がってくれない。別にもっと上げなくても、問題がスティフでなければ、RK4だけで十分であり、コーディングも比較的簡単で広く使われている。\n実装 R 以下は、RでRK4を実装し、ローレンツアトラクタを3Dで描画するコードである。\nlibrary(rgl)\rRK4\u0026lt;-function(ODE,v,h=10^(-2)){\rV1 = ODE(v)\rV2 = ODE(v + (h/2)*V1)\rV3 = ODE(v + (h/2)*V2)\rV4 = ODE(v + h*V3)\rv = v + (h/6)*(V1 + 2*V2 + 2*V3 + V4)\rreturn(v)\r}\rlorenz\u0026lt;-function(v,rho=28,sigma=10,beta=8/3){\rdvdt = v\rdvdt[1] = sigma*(v[2]-v[1])\rdvdt[2] = v[1]*(rho-v[3])-v[2]\rdvdt[3] = v[1]*v[2] - beta*v[3]\rreturn(dvdt)\r}\rtraj\u0026lt;-numeric(0)\rv=c(1,1,1)\rfor(i in 1:10000){\rtraj\u0026lt;-rbind(traj,v)\rv=RK4(lorenz,v)\r}\rplot3d(traj,type=\u0026#39;l\u0026#39;)\rplot(traj[,1],traj[,3],type=\u0026#39;l\u0026#39;) このコードを実行すると、次のような3Dプロットを得られる。\nジュリア 以下は、上記と同じ内容のコードをジュリアで実装したものである。\nusing Plots\rfunction RK4(ODE::Function,v::Array{Float64,1},h=10^(-2))\rV1 = ODE(v)\rV2 = ODE(v .+ (h/2)*V1)\rV3 = ODE(v .+ (h/2)*V2)\rV4 = ODE(v .+ h*V3)\rreturn @. v + (h/6)*(V1 + 2*V2 + 2*V3 + V4)\rend\rfunction lorenz(v::Array{Float64,1}; ρ=28.,σ=10.,β=8/3)\rdvdt = deepcopy(v)\rdvdt[1] = σ*(v[2]-v[1])\rdvdt[2] = v[1]*(ρ-v[3])-v[2]\rdvdt[3] = v[1]*v[2] - β*v[3]\rreturn dvdt\rend\rfunction lorenz_attracter(v::Array{Float64,1}, endtime = 10000)\rx,y,z = [v[1]], [v[2]], [v[3]]\rfor t in 1:endtime\rnewx,newy,newz = RK4(lorenz,[x[end], y[end], z[end]])\rpush!(x, newx)\rpush!(y, newy)\rpush!(z, newz)\rend\rreturn x,y,z\rend\rresult = lorenz_attracter([1.,1.,1.])\rplot(result) 関連項目 明示的ルンゲ＝クッタ法 暗示的ルンゲ＝クッタ法 Atkinson. (1989). An Introduction to Numerical Analysis(2nd Edition): p420. $D \\subset \\mathbb{R}^2$ で定義された連続関数 $f$ に対する初期値問題 $\\begin{cases} y ' = f(x,y) \\\\ y( x_{0} ) = Y_{0} \\end{cases}$ が与えられている。区間 $(a,b)$ を $a \\le x_{0} \u0026lt; x_{1} \u0026lt; \\cdots \u0026lt; x_{n} \u0026lt; \\cdots x_{N} \\le b$ のようなノードポイントで分けたとする。特に充分に小さい $h \u0026gt; 0$ に対して $x_{j} = x_{0} + j h$ とすると、初期値 $y_{0} = Y_{0}$ に対して $$ y_{n+1} = y_{n-1} + h \\sum_{j=1}^{p} \\gamma_{j} V_{j} $$\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":796,"permalink":"https://freshrimpsushi.github.io/jp/posts/796/","tags":null,"title":"四次のルンゲ＝クッタ法"},{"categories":"통계적분석","contents":"定義 1 $\\left\\{ X_{t} \\right\\}_{t=1}^{n}$、$\\left\\{ Y_{t} \\right\\}_{t=1}^{n}$を確率過程としよう。\n次のように定義された$\\rho_{k}$をラグ$k$の交差相関関数Crossという。 $$ \\rho_{k} (X,Y) := \\text{cor} \\left( X_{t} , Y_{t-k} \\right) = \\text{cor} \\left( X_{t+k} , Y_{t} \\right) $$ 次のように定義された$r_{k}$をラグ$k$の標本交差相関関数という。 $$ r_{k} := {{ \\sum \\left( X_{t} - \\overline{X} \\right) \\left( Y_{t-k} - \\overline{Y} \\right) } \\over { \\sqrt{ \\sum \\left( X_{t} - \\overline{X} \\right)^2 } \\sqrt{ \\left( Y_{t-k} - \\overline{Y} \\right)^2 } }} $$ 説明 交差相関関数は、二つの時系列データ間の相関関係を理解するための関数だ。時系列に適用される点のみが異なり、式だけ見たらピアソンの相関係数そのものだ。\nsCCF $r_{k}$はCCF $\\rho_{k}$の推定値で、$\\left\\{ X_{t} \\right\\}_{t=1}^{n}$、$\\left\\{ Y_{t} \\right\\}_{t=1}^{n}$が定常性を持ちつつ互いに独立していれば、次のように正規分布に従うとされる。 $$ r_{k} \\sim N \\left( 0 , {{ 1 } \\over { n}} \\left[ 1 + 2 \\sum_{k=1}^{\\infty} \\rho_{k} ( X , Y) \\right] \\right) $$ これを利用して回帰分析のように仮説検定を行うことができる。\nテスト $\\displaystyle Y_{t} = e_{t} + \\sum_{k=0}^{m} \\beta_{k} X_{t-k}$としよう。\n$H_{0}$: $\\beta_{k} = 0$ つまり、$X_{t}$と$Y_{t-k}$は相関関係を持たない。 $H_{1}$: $\\beta_{k} \\ne 0$ つまり、$X_{t}$と$Y_{t-k}$は相関関係を持つ。 解釈 帰無仮説の下では、$\\rho_{k} ( X , Y) = 0$と同時に$\\displaystyle N \\left( 0 , {{ 1 } \\over { n }} \\right)$を仮定し、標準誤差は$\\displaystyle {{1} \\over {\\sqrt{n}}}$となる。したがって、有意水準 $\\alpha$で仮説検定を行いたい場合は、$| r_{k} |$が信頼区間上限$\\displaystyle {{z_{1- \\alpha/2}} \\over {\\sqrt{n} }}$を超えるか確認すればいい。超えた場合は有意なラグの候補となり、超えなければ相関関係がないとみなされる。\n参照 ACF：自己相関関数 PACF：偏自己相関関数 EACF：拡張自己相関関数 Cryer. (2008). 時系列分析：Rでのアプリケーション（第2版）: p261~262.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1227,"permalink":"https://freshrimpsushi.github.io/jp/posts/1227/","tags":null,"title":"相互相関関数"},{"categories":"양자역학","contents":" 🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧\n反射係数と透過係数Reflection coefficient 波動関数の反射係数(反射率)と透過係数(透過率)は次のように表せる。 $$ R=\\left| \\frac{j_{ref}}{j_{inc}} \\right|,\\quad T=\\left| \\frac{j_{trans}}{j_{inc}}\\right| $$ ここで、$j$は確率流だ。$inc$は入射$(\\mathrm{incident})$を意味する。$R$、$ref$は反射$(\\mathrm{reflection})$を表し、$T$、$trans$は透過$(\\mathrm{transmission})$を示す。\nエネルギーが$E$の粒子がそのエネルギーよりも高いポテンシャル障壁に遭遇した時、反射と透過が起こる。古典的な観点からは、粒子は透過せずに純粋に反射するだけだ。しかし、量子の世界では、粒子の波動性のために、確率的に透過が起こる。これを量子トンネリングtunneling})$ 혹은 터널 효과$(\\mathrm{トンネル効果と呼ぶ。つまり、反射する比率と透過する比率が入射波、反射波、透過波の確率流の比率で表されるということだ。フラックス$(\\mathrm{flux}$、フラックス$)$は、単位時間に何らかの点を通過する物理量の量をフラックスという。従って、波動関数が反射および透過をする比率を知りたければ、入射波のフラックスに対する反射波、透過波のフラックスの比率を確認すればいい。すなわち、「反射率=(反射波のフラックス)/(入射波のフラックス)」、「透過率=(透過波のフラックス)/(入射波のフラックス)」と表せる。しかし、量子力学では、粒子の波動関数は、粒子が見つかる確率を意味し、確率の流れを表す物理量として確率流密度 $j(x,t)$がある。実際、$j(x,t)$が波動関数のフラックスを示すことが確認できる。ある波動関数 $\\psi=A e^{ikx}$が与えられたとする。それなら、区間 $\\Delta x$を通過する波動関数は $|\\psi|^2 \\Delta x =|\\psi \\psi^{\\ast}| \\Delta x=P\\Delta x$で表せる。$\\psi$でなく$|\\psi|^2$である理由は、私たちが扱っている物理量が実数だからだ。波動関数 $\\psi$は複素関数であるため、実数値を示すために$|\\psi \\psi^{\\ast}|$で表す。この時、$P=|\\psi \\psi^{\\ast}|=|Ae^{ikx} A^{\\ast}e^{-ikx}|=|AA^{\\ast}|=|A|^2$だ。従って、フラックスは単位時間に流れる量であるので、$\\psi$のフラックスは $$ F=|A|^2\\dfrac{\\Delta x}{\\Delta t} $$ 非常に短い時間について考えると、$\\dfrac{\\Delta x}{\\Delta t}=v$だ。量子力学では、速度ではなく運動量を扱うので、$v=\\frac{p}{m}$が成り立ち、量子力学における運動量は$p=\\hbar k$なので $$ F=|A|^2\\frac{\\hbar k}{m} $$ $\\psi$の確率流を計算すると、正確に上記の値と同じであることが分かる。 $$ \\begin{align*} j =\u0026amp;\\ \\frac{\\hbar }{2mi}\\left( \\psi* \\frac{\\partial \\psi}{\\partial x} -\\psi\\frac{\\partial \\psi^{\\ast}}{\\partial x} \\right) \\\\ =\u0026amp;\\ \\frac{\\hbar}{2mi}\\left( A^{\\ast}e^{-ikx}(ik)Ae^{ikx}-Ae^{ikx}(-ik)A^{\\ast}e^{-ikx}\\right) \\\\ =\u0026amp;\\ \\frac{\\hbar}{2mi} \\left( ikAA^{\\ast}+ikAA^{\\ast}\\right) \\\\ =\u0026amp;\\ \\frac{\\hbar k}{m}|A|^2 \\end{align*} $$ 従って、入射波の確率流と反射波の確率流の比が反射率を、入射波の確率流と透過波の確率流の比が透過率を表す。もし、波動関数が次のように実関数として与えられたとする。 $$ u=Ae^{kx} $$ それなら、波の確率流 $j$は$0$になる。 $$ \\begin{align*} j =\u0026amp;\\ \\frac{\\hbar }{2mi}\\left( u^{\\ast} \\frac{\\partial u}{\\partial x} -u\\frac{\\partial u^{\\ast}}{\\partial x} \\right) \\\\ =\u0026amp;\\ \\frac{\\hbar }{2mi}\\left( u \\frac{\\partial u}{\\partial x} -u\\frac{\\partial u}{\\partial x} \\right) \\\\ =\u0026amp;\\ 0 \\end{align*} $$\n","id":1241,"permalink":"https://freshrimpsushi.github.io/jp/posts/1241/","tags":null,"title":"波動関数の反射と透過"},{"categories":"통계적분석","contents":"ビルドアップ PACFは$AR(p)$の次数を、ACFは$MA(q)$の次数を決める時に大きな助けとなる。しかし、$ARMA(p,q)$モデルを適用する際は、アルマモデルの可逆性のために、$AR(p)$が$MA(\\infty)$のように見えたり、$MA(q)$が$AR(\\infty)$のように見えたりすることがある。そこで、このような問題を回避し、アルマモデルを見つけるためのいくつかの方法が考案された。\n定義 拡張自己相関関数はその中の一つの方法で、次のように定義される$W_{t,k,j}$に対する時差$k$と$j$のEACFと言う。 $$ W_{t,k,j} := Y_{t} - \\tilde{\\phi}_{1} Y_{t-1} - \\cdots - \\tilde{\\phi}_{k} Y_{t-k} $$\n説明 この定義だけではEACFを理解することが不可能なため、数式で理解しよう。与えられた$k$、$j$に対してアルマモデル$ARMA(k,j)$は次のように表現される。 $$ Y_{t} = \\sum_{i = 1}^{k} \\phi_{i} Y_{t-i} + e_{t} - \\sum_{i = 1}^{j} \\theta_{i} e_{t-i} $$ ここで、$Y_{t}$を$Y_{t-1} , \\cdots , Y_{t-k}$で多重回帰分析すると、$\\phi_{1} , \\cdots , \\phi_{k}$の推定値である回帰係数$\\tilde{\\phi}_{1} , \\cdots , \\tilde{\\phi}_{k}$を得ることができる。そして、その残差は次のようにメチャクチャになる。 $$ Y_{t} - \\sum_{i = 1}^{k} \\tilde{\\phi}_{i} Y_{t-i} = e_{t} - \\sum_{i = 1}^{j} \\theta_{i} e_{t-i} $$ しかし、よく考えてみると、左辺を上で定義した$W_{t,k,j}$としておくことで、$MA(j)$モデルを得ることができる。 $$ W_{t,k,j} = e_{t} - \\sum_{i = 1}^{j} \\theta_{i} e_{t-i} $$ すると、$W_{t,k,j}$はACFがそうであったように正規分布$\\displaystyle N \\left( 0 , {{1} \\over {n - k - j}}\\right)$に従い、これを利用して仮説検定を行う。\nこの展開を簡単に言うと、複雑に絡み合ったアルマモデル$ARMA(k,j)$で時差$k$を与えて$AR(k)$を取り除き、そこから時差$j$を与えて$MA(j)$だけを考えて各個撃破することである。最終的にACFを使用しなければならないため、拡張自己相関関数という名前がついたのは適切だと言える。\n実習 時差が二つの軸に列挙されているため、ACFやPACFと違い、コレログラムは描けず1、OとXで帰無仮説を棄却したか否かだけを示すテーブルを使用する。ただし、このテーブルは実際のデータから得られたものではなく、分析がうまくいく$ARMA(1,1)$モデルについて分析した場合に出てくるべき理論的な図示に過ぎない。実際にこんなにキレイに出てくれることはほとんどない。\nテーブルの読み方は次の通りである：\nステップ1. 左上から右へ水平に続く線がある頂点を見つける。 ステップ2. 頂点から右下に落ちて水平線と鋭角をなす線を見つける。 ステップ3. 両線を見つけたら、その頂点に対応する$ARMA(p,q)$で分析を試みる。 この方法に従えば、その図では頂点O*に対応する$ARMA(1,1)$がモデルの候補となる。時系列をある程度まっすぐに勉強し、説明をよく読んだなら推測できるだろうが、実際の分析ではかなり主観的で曖昧なケースが多い。これについては、実際に分析をたくさんやってみて慣れるしかない。\n参照 ACF : 自己相関関数 PACF : 偏自己相関関数 CCF : 交差相関関数 RでEACFによるARMAモデル選択法 実際には3次元で描くことができるが、見にくいため使わない。分析者は、信頼区間を外れるか否かだけを確認すれば十分である。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1213,"permalink":"https://freshrimpsushi.github.io/jp/posts/1213/","tags":null,"title":"拡張自己相関関数"},{"categories":"바나흐공간","contents":"実数に関するハーン・バナッハの定理1 $X$は$\\mathbb{R}$-ベクトル空間であり、$Y \\subset X$とする。$p : X \\to \\mathbb{ R}$を$X$の準線形 線形汎関数とする。今、$y^{\\ast} : Y \\to \\mathbb{ R}$が以下の条件を満たす$Y$の$\\mathbb{R}$-線形汎関数であると仮定する。\n$$ y^{\\ast}(y) \\le p(y)\\quad \\forall y\\in Y $$\nすると、以下の条件を満たす$X$の線形汎関数$x^{\\ast} : X \\to \\mathbb{R}$が存在する。\n(a) $x^{\\ast}(y)=y^{\\ast}(y),\\quad \\forall y \\in Y$\n(b) $x^{\\ast}(x) \\le p(x),\\quad \\forall x \\in X$\n説明 $\\mathbb{R}-$ベクトル空間とは、体$\\mathbb{R}$に関するベクトル空間のことである。つまり、ベクトル空間のスカラー倍に関する条件$(M1)$〜$(M5)$が実数に対して成立するという意味である。同様に、$\\mathbb{R}$-線形とは、線形の二つの性質のうちスカラー倍に関する内容が実数に対して成立するという意味である。\n$X, Y$が$\\mathbb{R}$-ベクトル空間であるため、$y^{\\ast}$、$x^{\\ast}$が線形であることと$\\mathbb{R}$-線形であることは同じ意味である。この部分が混乱する場合は、**$\\mathbb{R}$-、$\\mathbb{C}$-はこの記事では存在しない文字と考えても、証明を理解する上で問題はない。**後にハーン・バナッハの定理をノルム空間に適用する際には、関数$p$がノルムに対応する。この定理の証明は省略し、複素数に関するハーン・バナッハの定理の証明に使用する補助定理として利用する。\n複素数に関するハーン・バナッハの定理2 $X$は$\\mathbb{C}$-ベクトル空間であり、$Y \\subset X$とする。$p : X \\to \\mathbb{ R}$を以下のように定義された準線形汎関数とする。\n$$ p(\\lambda x)=|\\lambda| p(x),\\quad x\\in X, \\lambda \\in \\mathbb{C} $$\nそして、$y^{\\ast} : Y \\to \\mathbb{ C}$が以下の条件を満たす$Y$の線形汎関数であると仮定する。\n$$ \\begin{equation} \\text{Re}\\left( y^{\\ast}(y) \\right) \\le p(y),\\quad \\forall y\\in Y \\end{equation} $$\nすると、以下の条件を満たす$X$の線形汎関数$x^{\\ast} : X \\to \\mathbb{C}$が存在する。\n$x^{\\ast}(y)=y^{\\ast}(y),\\quad \\forall y \\in Y$ $\\text{Re}(x^{\\ast}(x)) \\le p(x),\\quad \\forall x \\in X$ 説明 実数に関する定理と比較した場合、$p$の値域が$\\mathbb{R}$であることは変わらないが、これは上述のように$X$がノルム空間の場合、$p$がノルムに対応するからである。$X$、$Y$は$\\mathbb{C}$-ベクトル空間であり、$\\mathbb{R} \\subset \\mathbb{C}$であるため、$\\mathbb{R}$-ベクトル空間である条件も満たされる。すべての複素数に対してベクトル空間の条件$(M1)$〜$(M5)$が成立する場合、自動的にすべての実数に対しても成立するからである。同様に、$y^{\\ast}$、$x^{\\ast}$は$\\mathbb{C}$-線形であるため、$\\mathbb{R}-$線形である条件も満たされる。\n証明 関数$\\psi : Y \\to \\mathbb{ R}$を以下のように定義する。\n$$ \\psi (y) = \\text{Re} ( y^{\\ast}(y) ) $$\nすると、$\\psi$も$Y$の$\\mathbb{C}$-線形汎関数であることが示される。これは$\\mathrm{ Re}$と$y^{\\ast}$が線形であるために自明な結果であり、示す過程は非常に簡単なので省略する。$\\psi$の定義と$(1)$により、以下の式が成立する。\n$$ \\psi(y)= \\text{Re} \\left( y^{\\ast}(y) \\right) \\le |y^{\\ast}(y)| \\le p(y) $$\nすると、実数に関するハーン・バナッハの定理により、以下の条件を満たす$X$の$\\mathbb{R}$-線形汎関数$\\Psi : X \\to \\mathbb{ R}$が存在する。\n$$ \\Psi (y) = \\psi (y),\\quad \\forall y \\in Y $$\n$$ \\Psi (x) \\le p(x),\\quad \\forall x \\in X $$\nそして、新たに関数$\\Phi : X \\to \\mathbb{ C}$を以下のように定義しよう。最終的な目標は、以下のように定義された$\\Phi$が、定理で存在すると言われていた$x^{\\ast}$であることを示すことである。\n$$ \\Phi (x) := \\Psi (x) -i \\Psi(ix) $$\nすると、$\\Phi$が$X$の線形汎関数であることが確認できる。$\\Psi$が$\\mathbb{R}$-線形であるため、加法と実数乗に関しては線形性が自明であるため、$\\Phi(ix)=i\\Phi(x)$のみを確認すればよい。\n$$ \\begin{align*} \\Phi(ix) =\u0026amp;\\ \\Psi(ix) -i \\Psi( -x) \\\\ =\u0026amp;\\ \\Psi(ix)+i\\Psi(x) \\\\ =\u0026amp;\\ -i^2 \\Psi(ix)+i\\Psi(x) \\\\ =\u0026amp;\\ i \\big( \\Psi(x)-i\\Psi(ix) \\big) \\\\ =\u0026amp;\\ i\\Phi(x) \\end{align*} $$\n$\\Phi$が**(a)**を満たすことは、以下のように示すことができる。$y \\in Y$とすると、\n$$ \\begin{align*} \\Phi(y) =\u0026amp;\\ \\Psi (y) -i \\Psi(iy) \\\\ =\u0026amp;\\ \\psi(y) -i\\psi(iy) \\\\ =\u0026amp;\\ \\text{Re} \\left( y^{\\ast}(y) \\right)-i\\text{Re} \\left( y^{\\ast}(iy) \\right) \\\\ =\u0026amp;\\ \\text{Re} \\left( y^{\\ast}(y) \\right) +\\text{Im} \\left(-iy^{\\ast}(iy) \\right) \\\\ =\u0026amp;\\ \\text{Re} \\left( y^{\\ast}(y) \\right) +\\text{Im} \\left( y^{\\ast}(y) \\right) \\\\ =\u0026amp;\\ y^{\\ast}(y) \\end{align*} $$\n$\\Phi$が**(b)**を満たすことを示すのはさらに簡単である。\n$$ \\mathrm{Re }\\left( \\Phi(x) \\right) = \\Psi(x) \\le p(x) $$\nしたがって、$\\Phi$が$X$の線形汎関数であり、**(a), (b)**を満たすため、$x^{\\ast}=\\Phi$が存在する。\n■\nセミノルムに関するハーン・バナッハの定理 $X$は$\\mathbb{C}$-ベクトル空間であり、$Y \\subset X$とする。$p : X \\to \\mathbb{ R}$を$X$のセミノルムとする。そして、$y^{\\ast} : Y \\to \\mathbb{ C}$が以下の条件を満たす$Y$の線形汎関数であると仮定する。\n$$ | y^{\\ast}(y) | \\le p(y),\\quad \\forall y\\in Y $$\nすると、以下の条件を満たす$X$の線形汎関数$x^{\\ast} : X \\to \\mathbb{C}$が存在する。\n$x^{\\ast}(y)=y^{\\ast}(y),\\quad \\forall y \\in Y$\n$| x^{\\ast}(x) | \\le p(x),\\quad \\forall x \\in X$\n証明 セミノルムと準線形の定義から、$p$がセミノルムであれば準線形の条件も自動的に満たされる。\nまず、以下の式が成立することは自明である\n$$ \\text{Re} \\left( y^{\\ast}(y) \\right) \\le |y^{\\ast}(y) | \\le p(y) $$\nしたがって、複素数に関するハーン・バナッハの定理により、以下の二つの条件を満たす$X$の線形汎関数$x^{\\ast} : X \\to \\mathbb{C}$が存在する。\n$$ x^{\\ast}(y)=y^{\\ast}(y) \\quad \\forall y \\in Y $$\n$$ \\text{Re} \\left( x^{\\ast}(x) \\right) \\le p(x) \\quad \\forall x \\in X $$\n$S = \\left\\{ \\lambda \\in \\mathbb{C} : | \\lambda | =1 \\right\\}$とする。すると、\n$$ \\begin{align*} \\text{Re} \\left( \\lambda x^{\\ast}(x) \\right) =\u0026amp;\\ \\text{Re} \\left( \\lambda x^{\\ast}(\\lambda x) \\right) \\\\ \\le \u0026amp; p(\\lambda x) \\\\ =\u0026amp;\\ |\\lambda| p(x)=p(x) \\quad \\forall x \\in X \\end{align*} $$\nこの時、固定された$x \\in X$に対して$|x^{\\ast}(x)|=\\lambda x^{\\ast}(x)$を満たす$\\lambda \\in S$を常に見つけることができる。したがって、$x$とその特定の$\\lambda$に対して、以下の式が成立する。\n$$ | x^{\\ast}(x) | =\\lambda x^{\\ast}(x) = \\text{Re} \\left( \\lambda x^{\\ast}(x) \\right) \\le p(x), \\quad \\forall x \\in X $$\n$X$の線形汎関수$x^{\\ast}$が二つの条件を満たすため、証明完了。\n■\n付録 固定された$x$に対して$x^{\\ast}(x)=a+ib$とする。$\\lambda=c+id$とする。$\\lambda$の条件により$c^2+d^2 =1$であるため、$\\lambda=c+i\\sqrt{1-c^2}$である。また、$|x^{\\ast}(x)|=\\sqrt{a^2+b^2}$である。$\\lambda x^{\\ast}(x)=(ac-b\\sqrt{1-c^2})+i(a\\sqrt{1-c^2}+bc)$であり、$|x^{\\ast}(x)|$が非負の実数であるため、\n$$ \\begin{align*} \u0026amp;\u0026amp; a\\sqrt{1-c^2}+bc =\u0026amp;\\ 0 \\\\ \\implies\u0026amp;\u0026amp; a^2(1-c^2) =\u0026amp;\\ b^2c^2 \\\\ \\implies\u0026amp;\u0026amp; a^2 =\u0026amp;\\ (a^2+b^2)c^2 \\\\ \\implies\u0026amp;\u0026amp; c^2 =\u0026amp;\\ \\dfrac{a^2}{a^2+b^2} \\tag{2} \\end{align*} $$\n便宜上$c=\\dfrac{a}{\\sqrt{a^2+b^2}}$とし、$d=\\dfrac{-b}{\\sqrt{a^2+b^2}}$とする。すると、$(2)$と$c^2+d^2=1$が成立する。また、$|x^{\\ast}(x)|=ac-bd=\\sqrt{a^2+b^2}$が成立する。したがって、固定された$x$に対して$x^{\\ast}(x)=a+ib$であれば、$\\lambda=\\dfrac{a}{\\sqrt{a^2+b^2}}-i\\dfrac{b}{\\sqrt{a^2+b^2}}\\in S$に対して$|x^{\\ast}(x)|=\\lambda x^{\\ast}(x)$が成立する。\nhttp://mathonline.wikidot.com/the-hahn-banach-theorem-real-version\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttp://mathonline.wikidot.com/the-hahn-banach-theorem-complex-version\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1230,"permalink":"https://freshrimpsushi.github.io/jp/posts/1230/","tags":null,"title":"実数、複素数、セミノルムに対するハーン・バナッハの定理"},{"categories":"통계적분석","contents":"定義 1 $\\left\\{ Y_{t} \\right\\}_{t=1}^{n}$ を確率過程で、時差$k$に対して$Y_{t-1}, \\cdots , Y_{t-(k-1)}$によって$Y_{t}$を回帰分析した残差を$\\widehat{e_{t}}$、$Y_{t-k}$を回帰分析した残差を$\\widehat{e_{t-k}}$としよう。\n次のように定義される$\\phi_{kk}$を時差$k$の偏自己共分散関数という。 $$ \\phi_{kk} := \\text{cor} ( \\widehat{e_{t}} , \\widehat{e_{t-k}} ) $$ 次のように定義される$\\phi_{kk}$を時差$k$の標本偏自己共分散関数という。 $$ \\widehat{ \\phi_{kk} } := {{ r_{k} - \\sum_{j=1}^{k-1} \\phi_{(k-1),j} r_{k-j} } \\over { 1 - \\sum_{j=1}^{k-1} \\phi_{(k-1),j} r_{j} }} \\\\ \\phi_{k,j} := \\phi_{(k-1),j} - \\phi_{kk} \\phi_{(k-1),(k-j)} $$ $r_{k}$は時差$k$の標本自己相関関数だ。 説明 偏自己相関関数とは、自己相関性を把握しながら、$Y_{t}$と$Y_{t-k}$の間にある$Y_{t-1}, \\cdots , Y_{t-(k-1)}$の影響を削除し、純粋に二つの関係性だけを把握しようとするものだ。定義からいきなり回帰分析が飛び出てきて複雑そうに見えるが、実際は単純だ。$\\widehat{e_{t}}$だけを見てみよう。$Y_{t}$を$Y_{t-1}, \\cdots , Y_{t-k+1}$で回帰分析するとは、次の式に入る$\\beta_{1} , \\cdots , \\beta_{k-1}$を見つけることを意味する。 $$ Y_{t} = \\beta_{1} Y_{t-1} + \\cdots \\beta_{k-1} Y_{t-(k-1)} + \\widehat{e_{t}} $$ 再び書くと $$ \\widehat{e_{t}} = Y_{t} - \\left( \\beta_{1} Y_{t-1} + \\cdots \\beta_{k-1} Y_{t-(k-1)} \\right) $$ これは$\\widehat{e_{t}}$が$Y_{t-1}, \\cdots , Y_{t-(k-1)}$で説明できる部分が削除されたという意味だ。同様に、$\\widehat{e_{t-k}}$も$Y_{t-1}, \\cdots , Y_{t-(k-1)}$で説明できそうな部分はすべて削除されているため、$\\text{cor} ( \\widehat{e_{t}} , \\widehat{e_{t-k}} )$を計算するとは$Y_{t-1}, \\cdots , Y_{t-(k-1)}$がない状態での$Y_{t}$と$Y_{t-k}$だけの相関関係を見ようとすることだ。興味のある変数だけに着目するという点で、「偏」自己相関関数という名前は適切であることが分かる。[ 注: 概念が単純であってもsPACFを実際に計算するのはかなり難しいが、レビンソンLevinsonとダービンDurbinが提案したメソッドのおかげで$\\widehat{ \\phi_{kk} }$を再帰的に計算できるようになったこと。 ]\n数式的説明 数式的には、$Y_{t}$が$AR(p)$から出たと考えた場合$\\displaystyle Y_{t} = \\sum_{k=1}^{p} \\phi_{k} Y_{t-k} + e_{t}$であるため$Y_{t-k}$の係数$\\phi_{k}$を計算する際、他の変数を除外することができ、$AR(p)$モデルを見つけるのに役立つ。\nsPACF $\\widehat{\\phi_{kk}}$はPACF $\\phi_{kk}$の推定値であり、$Y_{t}$が$AR(p)$モデルから出ている場合$k\u0026gt;p$で正規分布$\\displaystyle N \\left( 0 , {{ 1 } \\over { n }} \\right)$に従う。数式で表すと $$ \\widehat{\\phi_{kk}} \\sim N \\left( 0 , {{ 1 } \\over { n }} \\right) $$ であり、これを利用して仮説検定を行う。\nテスト $\\displaystyle Y_{t} = \\sum_{k=1}^{p} \\phi_{k} Y_{t-k} + e_{t}$が与えられ、$k = 1 , \\cdots , p$としよう。\n$H_{0}$：$AR(0) \\iff \\theta_{k} = 0$、つまり、$Y_{t}$は自己回帰モデルに従わない。 $H_{1}$：$AR(k) \\iff \\theta_{k} \\ne 0$、つまり、$Y_{t}$は時差$k$の偏自己相関関係を持つ。 解釈 帰無仮説の下では、$p=0$と同時に$\\widehat{\\phi_{kk}} \\sim N \\left( 0 , {{ 1 } \\over { n }} \\right)$を仮定し、標準誤差は$\\displaystyle {{1} \\over {\\sqrt{n}}}$になる。したがって、有意水準$\\alpha$で仮説検定を行いたい場合は、$| \\phi_{k} |$が信頼区間上限$\\displaystyle {{ z_{1 - \\alpha/2} } \\over { \\sqrt{n} }}$を超えるかどうかを確認すれば良い。超えれば有意な時差の候補となり、超えなければ偏自己相関関係がないとみなされる。\n実習 ar1.sデータはTSAパッケージの$AR(1)$モデルからのサンプルデータだ。実際にARIMAモデルで分析する際も、推定値の絶対値が標準誤差の2倍を超えるかどうかを基準に有意な係数かどうかを判断する。\nまた、TSAパッケージのacf()関数を使用すると、上のように様々な$k$に対してコレログラムを描いてくれる。考える必要なく、線を超えれば有意と見なし、超えなければ有意でないと見なしても良い。基本的には有意水準$5 \\%$で計算される。\n偏自己相関関数を利用した仮説検定を正しく理解したかどうかを確認する方法として、上のように実際に線を直接描いてみることをお勧めする。Rではたった一行のコードだが、一度実行してみると、$\\widehat{\\phi}_{kk}$が正規分布に従い、その標準誤差が$\\displaystyle \\text{se} ( r_{k} ) = {{1} \\over {\\sqrt{n}}}$で計算されることを受け入れることができる。\nコード library(TSA)\rdata(ar1.s); win.graph(6,4); pacf(ar1.s)\rarima(ar1.s, order=c(1,0,0))\rabline(h=1.96*1/sqrt(length(ar1.s)),col=\u0026#39;red\u0026#39;) 関連項目 ACF: 自己相関関数 EACF: 拡張自己相関関数 CCF: 交差相関関数 Cryer. (2008). Time Series Analysis: With Applications in R(2nd Edition): p112.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1211,"permalink":"https://freshrimpsushi.github.io/jp/posts/1211/","tags":null,"title":"自己相関関数"},{"categories":"바나흐공간","contents":"定義1 $X$をベクトル空間とする。以下の三つの条件を満たす関数$\\left\\| \\cdot \\right\\| : X \\to \\mathbb{R}$が存在する場合、$\\left\\| \\cdot \\right\\|$を$X$のセミノルムセミノルム、半ノルムという。\n(a) $\\left\\| x \\right\\| \\ge 0,\\quad \\forall\\ x \\in X$\n(b) $|cx|=|c|\\left\\| x \\right\\|,\\quad \\forall\\ x\\in X,\\ \\forall\\ c \\in\\mathbb{C}$\n(c) $\\left\\| x + y \\right\\| \\le \\left\\| x \\right\\| + \\left\\| y \\right\\|,\\quad \\forall\\ x,y\\in X$\n説明 ノルムの定義から$\\left\\| x \\right\\|=0 \\iff x = 0$が抜けているわけだ。\nRobert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003), p101\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1229,"permalink":"https://freshrimpsushi.github.io/jp/posts/1229/","tags":null,"title":"セミノルム"},{"categories":"함수","contents":"定義1 関数 $f : X \\to Y$が下記の二つの条件を満たす場合、準線形sublinearと呼ばれる。$x,x_{1},x_{2}\\in X$と$a \\in \\mathbb{R}$に対して、\n$f(ax) = af(x)$ $f(x_{1} + x_{2}) \\le f(x_{1}) + f(x_{2})$ 説明 二つ目の条件が等式で成立する場合は線形で、不等式で成立する場合は準線形である。\nRobert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003), p54\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1333,"permalink":"https://freshrimpsushi.github.io/jp/posts/1333/","tags":null,"title":"半線形関数"},{"categories":"해석개론","contents":"定義1 $a$を含むある$E$で$f$が定義されていて、限界\n$$ f^{\\prime} (a) := \\lim_{h \\to 0} {{ f (a + h ) - f(a) } \\over { h }}=\\lim \\limits_{x\\rightarrow a}\\frac{f(x)-f(a)}{x-a} $$\nが存在するならば、$f$は$a$で微分可能differentiableであるといい、$f^{\\prime} (a)$を$a$での$f$の微分係数という。\n全ての点$a \\in E$に対して$f$が微分可能なら、$f$は$E$で微分可能であるという。$f$が$E$で微分可能な時、$E$上で定義された$f^{\\prime}$を$f$の導関数derivativeと呼ぶ。\n説明 解析学を学ぶ上で最も歓迎されるのが微分だ。なぜなら、数列であれ積分であれ本来の姿をそのまま持っているだけでなく、複雑になることに比べて、微分だけが比較的簡単で理解しやすいからだ。多重積分や偏微分も登場するが、他の概念に比べれば簡単で分かりやすい。このように微分の定義をあえて「実数空間」に限定し、偏微分が言及されるのは、微分が多次元に拡張されることを示唆しているためだ。\n要約 (a) 連続性: $f$が$a \\in E$で微分可能ならば、$a \\in E$で連続である。\n(b) 連鎖律: $( g \\circ f)' ( a ) = g \u0026rsquo; ( f (a) ) f '(a)$\n(c) 逆関数の定理: 開区間$E$で$f : E \\to \\mathbb{R}$が一対一の連続関数であるとする。(i) ある$a \\in E$に対して$b = f(a)$であり、(ii): $f ' (a) \\ne 0$が存在するならば、$f^{-1}$は$a$で微分可能であり、\n$$ \\left( f^{-1} \\right)' (b) = {{ 1 } \\over { f '(a) }} $$\nWilliam R. Wade, An Introduction to Analysis (4th Edition, 2010), p98-99\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1210,"permalink":"https://freshrimpsushi.github.io/jp/posts/1210/","tags":null,"title":"実数空間で定義された関数の微分"},{"categories":"통계적분석","contents":"定義 1 $\\left\\{ Y_{t} \\right\\}_{t=1}^{n}$ を確率過程とする。\n$\\mu_{t} := E ( Y_{t} )$ を平均関数という。 次のように定義された $\\gamma_{ t , s }$ を自己共分散関数という。 $$ \\gamma_{t , s} : = \\text{cov} ( Y_{t} , Y_{s} ) = E ( Y_{t} - \\mu_{t} ) E ( Y_{s} - \\mu_{s} ) $$ 次のように定義された $\\rho_{ t , s }$ を自己相関関数という。 $$ \\rho_{ t , s } := \\text{cor} ( Y_{t} , Y_{s} ) = {{ \\gamma_{t , s} } \\over { \\sqrt{ \\gamma_{t , t} \\gamma_{s , s} } }} $$ 次のように定義された $\\rho_{ k }$ をラグ$k$の自己相関関数という。 $$ \\rho_{ k } := \\text{cor} ( Y_{t} , Y_{t-k} ) = {{ \\gamma_{t , t - k} } \\over { \\sqrt{ \\gamma_{t , t} \\gamma_{t-k , t-k} } }} $$ 次のように定義された $r_{ k }$ をラグ$k$の標本自己相関関数という。 $$ r_{ k } := {{ \\sum_{t = k+1}^{n} \\left( Y_{t} - \\overline{Y} \\right) \\left( Y_{t-k} - \\overline{Y} \\right) } \\over { \\sum_{t=1}^{n} \\left( Y_{t} - \\overline{Y} \\right)^2 }} $$ 説明 自己相関関数とは、時系列データの自己相関性を把握するための関数で、同じ変数でもあるラグを持って自己とどの程度似ているかに関心を持つ。異なる変数の相関関係に関心を持つ回帰分析のアイデアとは異なり、自己がラグ$k$を持って$Y_{t}$と$Y_{t-k}$に分かれ、二つの変数のように扱われる。\n数式的説明 数式的には、$Y_{t}$が$MA(q)$から出てきたと考えると、$\\displaystyle Y_{t} = e_{t} - \\sum_{k=1}^{q} \\theta_{k} e_{t-k}$であるため$Y_{t}$を複数の正規分布の和と見ることができ、$\\rho_{k}$が$\\theta_{k}$であるため、$MA(q)$モデルを見つけるのに有用である。\nsACF $r_{k}$はACF $\\rho_{k}$の推定値であり、$Y_{t}$が$MA(q)$モデルから出てきた場合、$k \u0026gt; q$の時、正規分布$\\displaystyle N \\left( \\rho_{k} , {{1} \\over {n}} \\left[ 1 + 2 \\sum_{j=1}^{q} \\rho_{j}^{2} \\right]^2 \\right)$に従う。数式で表すと $$ r_{k} \\sim N \\left( \\rho_{k} , {{1} \\over {n}} \\left[ 1 + 2 \\sum_{j=1}^{q} \\rho_{j}^{2} \\right]^2 \\right) $$ であり、これを利用して仮説検定を行う。\nテスト $\\displaystyle Y_{t} = e_{t} - \\sum_{k=1}^{q} \\theta_{k} e_{t-k}$が与えられ、$k = 1 , \\cdots , q$とする。\n$H_{0}$: $MA(0) \\iff \\theta_{k} = 0$、つまり、$Y_{t}$は移動平均モデルに従わない。 $H_{1}$: $MA(k) \\iff \\theta_{k} \\ne 0$、つまり、$Y_{t}$はラグ$k$の自己相関関係を持つ。 解釈 帰無仮説の下では、すべての$k$に対して$\\rho_{k} = \\theta_{k} = 0$であるため、$q = 0$と$\\displaystyle r_{k} \\sim N \\left( 0 , {{1} \\over {N }} \\right)$を仮定し、標準誤差は$\\displaystyle {{1} \\over {\\sqrt{n} }}$となる。したがって、有意水準$\\alpha$に対して仮説検定を行いたい場合は、$| \\theta_{k} |$が信頼区間上限$\\displaystyle {{ z_{1 - \\alpha/2} } \\over { \\sqrt{n} }}$を超えるか確認すればよい。超えれば有意なラグの候補となり、超えなければ自己相関関係がないと見なされる。\n実践 ma1.2.sデータは、$MA(1)$モデルから得られたTSAパッケージのサンプルデータである。実際にARIMAモデルで分析する際も、推定値の絶対値が標準誤差の2倍を超えるかどうかを基準に、有意な係数かどうかを判断する。\nTSAパッケージのacf()関数を使用すると、上のように様々な$k$に対してコレログラムを描いてくれる。頭の中で計算することなく、線を超えれば有意と見てもよく、超えなければ有意でないと見てもよい。基本的に有意水準$5 \\%$で計算される。\n注意すべき点は、$k=6$をわずかに超えたものも統計的に有意ではあるが、実際に自己相関関係があるとは見なされないことである。時系列分析では、この程度の超出は非常に頻繁であり、精神衛生のためにも、柔軟性を持ってそのまま受け入れることをお勧めする。\n上のように実際に線を自分で引いてみることで、自己相関関数を使用した仮説検定を正しく理解したかどうかを確認する方法をお勧めする。Rではたった一行のコードだが、一度でも実行してみることで、$r_{k}$が正規分布に従い、その標準誤差が複雑な式なしで$\\displaystyle \\text{se} ( r_{k} ) = {{1} \\over {\\sqrt{n}}}$として得られることを受け入れることができる。\nコード library(TSA)\rdata(ma1.2.s); win.graph(6,4); acf(ma1.2.s)\rarima(ma1.2.s, order=c(0,0,1))\rabline(h=1.96*1/sqrt(length(ma1.2.s)),col=\u0026#39;red\u0026#39;) 参照 PACF : 部分自己相関関数 EACF : 拡張自己相関関数 CCF : 相互相関関数 Cryer. (2008). 時系列分析：Rによるアプリケーション(第2版): p11, 109。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1209,"permalink":"https://freshrimpsushi.github.io/jp/posts/1209/","tags":["R"],"title":"自己相関関数"},{"categories":"해석개론","contents":"定義1 空集合じゃない$E \\subset \\mathbb{R}$に対して$f : E \\to \\mathbb{R}$とする。全ての$\\varepsilon \u0026gt; 0$に対して\n$$ | x_{1} - x_{2} | \u0026lt; \\delta \\land x_{1} , x_{2} \\in E \\implies | f(x_{1}) - f(x_{2}) | \u0026lt; \\varepsilon $$\nを満たす$\\delta\u0026gt;0$が存在する場合、$f$を$E$上で一様連続uniformly continuousという。\n$\\land$は論理的に「そして」と表す論理積記号だ。 説明 関数の連続性それ自体が$a \\in E$のようにある点での概念であるのに対して、一様連続は集合$E$全体を見る概念だ。例えば連続関数$f (x) := x^2$を考えてみよう。\n空集合じゃない$E \\subset \\mathbb{R}$に対して$f : E \\to \\mathbb{R}$とする。\n(a) コンパクト距離空間: $f$が連続で、$E$が有界閉区間ならば、$f$は一様連続だ。\n(b) 収束性保存: $f$が一様連続で、$\\left\\{ x_{n} \\right\\}_{n=1}^{\\infty}$がコーシーなら、$\\left\\{ f(x_{n}) \\right\\}$もコーシーだ。\n(1) $E = (0,1)$ $\\delta = \\dfrac{\\varepsilon}{2}$をこう取ると、全ての$x_{1} , x_{2} \\in (0,1)$に対して$| x_{1} - x_{2} | \u0026lt; \\delta$と言える時、\n$$ \\begin{align*} | f(x_{1}) - f(x_{2}) | =\u0026amp; | x_{1}^{2} - x_{2}^{2} | \\\\ =\u0026amp; | x_{1} - x_{2} | | x_{1} + x_{2} | \\\\ \\le \u0026amp; 2 | x_{1} - x_{2} | \\\\ \u0026amp; \u0026lt; \u0026amp; 2 \\delta \\\\ =\u0026amp; \\varepsilon \\end{align*} $$\n定義によれば、$f$は$E = ( 0 , 1 )$で一様連続だ。\n(2) $E = \\mathbb{R}$ $f$が$E$で一様連続だと仮定しよう。$\\varepsilon = 1$が与えられている時も、\n$$ | x_{1} - x_{2} | \u0026lt; \\delta \\land x_{1} , x_{2} \\in E \\implies | f(x_{1}) - f(x_{2}) | \u0026lt; 1 $$\n$\\delta$が存在しなければならない。しかし、アルキメデスの原理に従って、$n \\delta \u0026gt; 1$を満たす$n \\in \\mathbb{N}$を取ることができる。すると$x_{1} = n$、$x_{2} = \\left( n + \\dfrac{ \\delta }{2} \\right)$に対して、\n$$ \\begin{align*} 1 \u0026amp; \u0026lt; \u0026amp; n \\delta \\\\ \u0026lt;\u0026amp; n \\delta + {{ \\delta^{2} } \\over { 4 }} \\\\ =\u0026amp; \\left| n^2 - \\left( n + {{ \\delta } \\over {2}} \\right)^2 \\right| \\\\ =\u0026amp; \\left| f( n ) - f \\left( n + {{ \\delta } \\over {2}} \\right) \\right| \\\\ =\u0026amp; | f (x_{1} ) - f ( x_{2} ) | \\\\ \u0026lt;\u0026amp; 1 \\end{align*} $$\n整理すると、$1 \u0026lt; 1$になるが、これは矛盾なので、$f$は$E = \\mathbb{R}$で一様連続ではない。\n一方、$g(x) = x$を考えると、どんな定義域$E$を考えても、$\\delta = \\varepsilon$で取ることによって、一様連続の条件を満たす。こんな例から直感的に、一様連続関数は何らかの「おとなしい」関数だと分かる。$g$も$x$を無限大に送ると発散するはずだ。しかし、$f(x) = x^2$のように荒々しく大きくなるのではなく、ある程度の線を保ちながら大きくなる。世の中のどんなことでも、おとなしいものが荒々しいものより扱いやすいのは当然であり、一様連続関数が連続関数よりも良い条件を持っているのも道理だ。\n**(b)**で、一様連続が仮定されずに連続関数だけの場合を考えてみよう。\n$\\displaystyle h(x) := {{1} \\over {x}}$は連続関数で、$\\displaystyle x_{n} := {{1} \\over {n}}$とすると、$\\left\\{ x_{n} \\right\\}$は$0$に収束するコーシー数列だ。しかし、$\\displaystyle h (x_{n} ) = {{1} \\over { {{1} \\over {n}} }} = n$なので、$\\left\\{ h ( x_{n} ) \\right\\}$がコーシー数列ではないことが分かる。\nWilliam R. Wade, An Introduction to Analysis (4th Edition, 2010), p92\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1207,"permalink":"https://freshrimpsushi.github.io/jp/posts/1207/","tags":null,"title":"関数の一様連続"},{"categories":"바나흐공간","contents":"定義1 $X$をベクター空間としよう。次の三条件を満たす関数$\\left\\| \\cdot \\right\\| : X \\to \\mathbb{R}$が存在すれば、$\\left\\| \\cdot \\right\\|$を$X$のノルムと呼び、$(X,\\left\\| \\cdot \\right\\| )$をノルム空間と呼ぶ。\n(a) $\\left\\| x \\right\\| \\ge 0,\\quad \\forall\\ x \\in X$かつ$\\left\\| x \\right\\|=0 \\iff x = 0$\n(b) $|cx|=|c|\\left\\| x \\right\\|,\\quad \\forall\\ x\\in X,\\ \\forall\\ c \\in\\mathbb{C}$\n(c) $\\left\\| x + y \\right\\| \\le \\left\\| x \\right\\| + \\left\\| y \\right\\|,\\quad \\forall\\ x,y\\in X$\n説明 ノルム空間$X$のノルムは以下のように表される。\n$$ \\left\\| x \\right\\|_{X},\\quad \\left\\| x, X \\right\\|, \\quad \\left\\| x ; X \\right\\| $$\n(a) $\\left\\| x \\right\\|=0 \\iff x = 0$の条件がない場合は、セミノルムになる。\n(b) は$\\left\\| x - y \\right\\| =|y -x|$が成立するという意味である。\n(c) を三角不等式と呼び、以下の不等式を逆三角不等式と呼ぶ。ノルム空間$(X, \\left\\| \\cdot \\right\\| )$と$x, y \\in X$に対して、以下の不等式が成立する。\n$$ \\left| \\left\\| x \\right\\| - \\left\\| y \\right\\|\\ \\right| \\le \\left\\| x- y \\right\\| $$\nノルムは連続写像である。\nノルム空間としての距離空間、位相空間 ノルムが与えられると、以下のように自然に距離を定義できる。したがって、ノルム空間は距離空間になる。\n$$ d(x,y) = d_{X}(x,y) = \\left\\| x - y \\right\\|_{X} $$\n距離が与えられると、以下のようにオープンボールを定義できる。\n$$ B_{d}(x,r)=B_{r}(x):=\\left\\{ y\\in X\\ :\\ \\left\\| x - y \\right\\|_{X} \u0026lt;r \\right\\} $$\n全てのオープンボールの集合は$X$上の(位相数学での)基底になる。つまり、$X$のノルムで定義されたオープンボールによって$X$上の位相を作ることができるということである。このようにして作られた位相を$X$上のノルム位相2と呼ぶ。さらに、位相ベクター空間$X$の位相がノルム位相ならば、$X$をノルマブルと呼ぶ。\n以上の内容をまとめると、$X$がノルム空間であるということは、$X$がベクター空間であり、距離空間であり、位相空間であるという意味を全て含んでいるということである。したがって、関数解析学では与えられたノルム空間を自然に距離空間、位相空間としても扱う。\n証明3 三角不等式により、\n$$ \\left\\| x \\right\\|= | (x-y) +y| \\le |x-y| + \\left\\| y \\right\\| $$\nが成立する。したがって、\n$$ \\begin{equation} \\left\\| x \\right\\| - \\left\\| y \\right\\| \\le \\left\\| x- y \\right\\| \\end{equation} $$\n同様に、\n$$ \\left\\| y \\right\\| = | (y - x) + x| \\le \\left\\| y- x \\right\\| + \\left\\| x \\right\\| $$\nなので、\n$$ \\begin{equation} \\left\\| y \\right\\| - \\left\\| x \\right\\| \\le \\left\\| y- x \\right\\|=\\left\\| x - y \\right\\| \\end{equation} $$\nが成立する。したがって、$(1), (2)$により、\n$$ \\left| \\ \\left\\| x \\right\\| -\\left\\| y \\right\\|\\ \\right| \\le \\left\\| x- y \\right\\| $$\n■\nRobert A. Adams and John J. F. Foutnier、Sobolev Space (第2版、2003)、p4-5\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n距離の観点では、これを距離位相と呼ぶ。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOle Christensen、Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010)、p30\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1225,"permalink":"https://freshrimpsushi.github.io/jp/posts/1225/","tags":null,"title":"ノルム空間とは何か"},{"categories":"해석개론","contents":"定義 空集合じゃない$E \\subset \\mathbb{R}$に対して$f : E \\to \\mathbb{R}$としよう。全ての$\\varepsilon \u0026gt; 0$に対して\n$$ | x - a | \u0026lt; \\delta \\implies | f(x) - f(a) | \u0026lt; \\varepsilon $$\nを満たす$\\delta\u0026gt;0$が存在するなら、$f$を$a \\in E$で連続continuousと言い、$E$の全ての点で連続なら$f$を連続関数continuous functionという。\n説明 高校で連続を定義する時、\n関数値$f(a)$が存在する。 極限$\\lim \\limits_{x \\to a}$が存在する。 $f(a) = \\lim \\limits_{x \\to a}$が成り立つ。 この三条件が成り立つ時、$f$は$x = a$で連続って言った。イプシロン-デルタ論法を受け入れたなら、この定義は実は高校のレベルと変わらないことが分かるだろう。\n$| x - a | \u0026lt; \\delta$の時に$| f(x) - f(a) | \u0026lt; \\varepsilon$っていうのは、$x$が$a$の近くでちょっとだけ動くなら、$f(x)$も$f(a)$からちょっとだけ動くって意味になるだろう。つまり、$x$を変えて$f$に入れてみても、「急激に」、つまり不連続的に関数値が変わらないってことだ。言い換えれば、連続ってグラフで考えるなら「切れてない」ことを言うんだ。\n高校生の中には、こんな直感的にだけ受け入れて「切れてない」関数を連続関数と受け入れたケースが結構ある。そうじゃない例で言うと、$f(x) := {{ 1 } \\over { x }}$は$x=0$で切れてるけど、定義域$\\mathbb{R}^{ \\ast } = \\mathbb{R} \\setminus \\left\\{ 0 \\right\\}$の全ての点で連続だから、連続関数であってる。普通は知らなくても生きていく上で支障はないけど、知らなかったら、この機会にしっかりと概念を捉え直そう。\n定理 $f$が$a \\in E$で連続っていうのは、以下と同値だ。\n$$ \\lim \\limits_{n \\to \\infty} x_{n} = a \\implies \\lim \\limits_{n \\to \\infty} f( x_{n} ) = f(a) $$\nこの定理は、関数の連続性によって$\\lim \\limits_{n \\to \\infty}$が$f$の内外を行き来できることを保証する。数学以外の多くの分野で、ちゃんとチェックせずに当たり前のように使われる場合が多いけど、これもまた、数学者の立場からすると、厳密に問いただすべき点だ。\n","id":1206,"permalink":"https://freshrimpsushi.github.io/jp/posts/1206/","tags":null,"title":"大学数学で新しく定義される連続関数"},{"categories":"해석개론","contents":"定義[^1] $I$が$a \\in \\mathbb{R}$を含む区間であり、$f$は$I \\setminus \\left\\{ a \\right\\}$で定義された関数だとしよう。全ての$\\epsilon \u0026gt; 0$に対して\n$$ 0 \u0026lt; | x - a | \u0026lt; \\delta \\implies | f(x) - L | \u0026lt; \\varepsilon $$\nを満たす$\\delta\u0026gt;0$が存在するならば、$x \\to a$が$f(x)$に近づくとき、$L \\in \\mathbb{R}$に収束するconvergeという。\n説明 イプシロン-デルタの名前は見ての通り、定義に登場するイプシロン$\\varepsilon$とデルタ$\\delta$から来ている。これは「解析学の父」として知られるコーシーが初めて使った表現で、イプシロンとデルタはそれぞれ誤差$\\varepsilon$rrorと距離$\\delta$istanceを意味する。\n見てわかるように、表現は非常に複雑で直感からは遠いため、最初は難しい。数列の極限と同様に、新しく定義する理由もあれば、複雑に定義する理由もあるが、理由を受け入れてイプシロン-デルタを本当に理解することは別物だ。実際、イプシロン-デルタを理解するだけでは不十分で、慣れた後に役立つものとなる。\n感覚をつかむために、射撃ゲームを想像してみよう。このゲームでは、銃$f$を持ち、指定された位置$a$から指定されたターゲット$L$を狙い撃つが、命中したかどうかの判定は許容誤差$\\varepsilon$内で行われる。もちろん、$a$から全く動けなければ、当てることはできないだろう。射手は許容誤差$\\varepsilon$が与えられたとき、どれだけ動けば当てられるかを判断し、許容距離$\\delta$を提案できるとしよう。\n与えられた銃は$f(x) := 2x$であり、これで$x$を狙い撃つと$2x$に命中する。この銃がちゃんとした銃かどうかを確かめる方法として、$a=0$から$L=0$を狙ったテストをしたい。しかし、このように命中点が散らばる銃で本当にターゲットを狙い撃てるだろうか？実際にいくつかの場合を見てみよう。\n**ケース1. $\\varepsilon = 12$\n最初の許容誤差はたっぷりと$\\varepsilon = 12$で与えられた。$| f(x) - L | \u0026lt; \\varepsilon$だけを満たせば良いので、$| f(x) | \u0026lt; 12$になるように$x$から撃てば命中と判断されるだろう。つまり、$x$は$6$の絶対値を超えなければ良い。即ち、$| x | \u0026lt; 6$であれば$| f(x) | \u0026lt; 12$になる。これを式に再記述すると\n$$ | x | \u0026lt; 6 \\implies | f(x) | \u0026lt; 12 $$\nこれは、許容誤差を$\\varepsilon = 12$にしたとき、銃$f$で$a = 0$からターゲット$L = 0$を狙い撃ちできる許容距離$\\delta = 6$を提示できることを意味する。もちろん、これより小さくても良いが、わざわざ難しくする必要はない。\n**ケース2. $\\varepsilon = 6$\n二番目の許容誤差は$\\varepsilon = 6$で与えられた。先ほどと同様に、$| f(x) | \u0026lt; 6$を満たせば良いので、必要な許容距離$\\delta = 3$を提示できる。\n**ケース3. $\\varepsilon \u0026gt; 0$\nこれまで見たように、許容誤差$\\varepsilon \u0026gt; 0$がどのように与えられても、ターゲットを狙い撃つための許容距離$\\delta = \\varepsilon / 2$を提示できる。全ての$\\varepsilon\u0026gt; 0$に対して$\\delta$を提示できるということは、次のようになる。\n$$ \\forall \\varepsilon \u0026gt; 0 , \\exists \\delta : | x - 0 | \u0026lt; \\delta \\implies | f(x) - 0 | \u0026lt; \\varepsilon $$\n親しまれた表現で書き直すと、$\\lim_{x \\to 0} 2x = 0$になる。これまでに$x \\to 0$のとき、$2x \\to 0$であることを示した。もはや射撃の例えは必要ないが、もし書き直すなら、銃$f$で$a = 0$からターゲット$L = 0$を狙い撃てるということだ。\n例えば、$\\delta (12) =6$が存在し、$\\delta (6) = 3$が存在し…と説明するとき、理解できる気がしただろう。説明を読んでいると、ふと$\\lim_{x \\to 0} 2x = 0$を証明したが、このような例えは一貫性がないため、振り返ると忘れがちだ。イプシロン-デルタ法が難しい理由を考えてみよう。\n直感: イプシロン-デルタ法の感じと、$x \\to a$や$f(x) \\to L$のように「限りなく近づく」感じが違う\n実際、これがイプシロン-デルタ法を使う本当の理由だが、当初は$\\delta$の存在がなぜ$\\lim_{x \\to a} f(x) = L$のようなものと同じ言葉なのか「納得」できないかもしれない。これだけで行き詰まっているなら、イプシロン-デルタ法を理解していないわけではない。慣れていないだけだ。$| x - a | \u0026lt; \\delta$でも$| f(x) - L | \u0026lt; \\varepsilon$でも、$\\delta$は「大きな数」とは思わない。十分に小さな正の数として、$| x - a |$や$| f(x) - L |$を「抑える何か」として受け入れ、結局次のような考え方を持つべきだ。\n$$ | x - a | \u0026lt; \\delta \\implies \\lim_{\\delta \\to 0} | x - a | = 0 \\implies x \\to a $$\n$$ | f(x) - L | \u0026lt; \\varepsilon \\implies \\lim_{\\varepsilon \\to 0} | f(x) - L | = 0 \\implies f(x) \\to L $$\n言葉: $\\delta$の存在という言葉が響かない\n実際、これは文字通り$\\delta$を作り出すというより、$\\varepsilon$に対して示すという意味だが、「存在する」という表現のために分かりづらく感じるかもしれない。$\\delta$を$\\varepsilon$に対する関数$\\delta = \\delta ( \\varepsilon )$として表現することに成功したなら、$\\varepsilon \u0026gt; 0$の存在は既に仮定されているため、$\\delta$も存在する。\n順序: 条件は$|x - a| \u0026lt; \\delta \\implies | f(x) - L | \u0026lt; \\varepsilon$だが、考える順序は逆だ\nこれが本当に混乱する部分だが、$\\implies$の形のために、どこかで前から後ろに進まなければならないと誤解しやすい。しかし、「全ての$\\varepsilon \u0026gt; 0$に対して」とはっきりしているように、$| f(x) - L | \u0026lt; \\varepsilon$を先に考え、その後で$| x - a | \u0026lt; \\delta$を考えなければならない。$\\varepsilon$が何かも分からなければ、悩む価値もないだろう。\nこれら三つの理由を考えながら、もう一度説明を読めば役に立つだろう。理解できたなら、今度はいくつか奇妙な点が見えてくるかもしれない。例えば、\n","id":1204,"permalink":"https://freshrimpsushi.github.io/jp/posts/1204/","tags":null,"title":"イプシロン-デルタ論法"},{"categories":"해석개론","contents":"定義 $\\left\\{ x_{n} \\right\\}_{n \\in \\mathbb{N}}$、$\\left\\{ y_{n} \\right\\}_{n \\in \\mathbb{N}}$が実数列だとしよう。\n$\\displaystyle \\limsup_{n \\to \\infty} x_{n} := \\lim_{n \\to \\infty} \\left( \\sup_{k \\ge n} x_{k} \\right)$を$\\left\\{ x_{n} \\right\\}$のリミットスプレマムlimit supremumと言う。 $\\displaystyle \\liminf_{n \\to \\infty} y_{n} := \\lim_{n \\to \\infty} \\left( \\inf_{k \\ge n} y_{k} \\right)$を$\\left\\{ y_{n} \\right\\}$のリミットインフィマムlimit infimumと言う。 ここで、$\\displaystyle \\sup_{k \\ge n} x_{k} := \\sup \\left\\{ x_{k} : k \\ge n \\right\\}$と$\\displaystyle \\inf_{k \\ge n} x_{k} := \\inf \\left\\{ x_{k} : k \\ge n \\right\\}$である。\n性質 (a)： $$ \\liminf_{n \\to \\infty} x_{n} \\le x \\le \\limsup_{n \\to \\infty} x_{n} $$ (b)： $$ \\liminf_{n \\to \\infty} x_{n} = x = \\limsup_{n \\to \\infty} x_{n} \\iff \\lim_{n \\to \\infty} x_{n} = x $$ (c)： $$ \\begin{align*} - \\liminf_{n \\to \\infty} x_{n} =\u0026amp; \\limsup_{n \\to \\infty} ( - x_{n} ) \\\\ - \\limsup_{n \\to \\infty} x_{n} =\u0026amp; \\liminf_{n \\to \\infty} ( - x_{n} ) \\end{align*} $$ (d)：もし$x_{n} \\le y_{n}$ならば $$ \\begin{align*} \\limsup_{n \\to \\infty} x_{n} \\le\u0026amp; \\limsup_{n \\to \\infty} y_{n} \\\\ \\liminf_{n \\to \\infty} x_{n} \\le\u0026amp; \\liminf_{n \\to \\infty} y_{n} \\end{align*} $$ 説明 解析学全般で役立つ表現としてリミットスプレマムは導入されたもので、すぐに賛成するかどうかにかかわらず便利なためである。直感的には、数列の前部を捨てながらスプレマムとインフィマムに関心を持つと考えると分かりやすい。\n例えば、$\\displaystyle x_{k} = {{ 1 } \\over { k }}$のとき、$\\displaystyle \\sup_{k \\ge n} \\left\\{ x_{k} \\right\\}$の実際の計算過程を見てみよう。\n$$ n=3 : \\sup \\left\\{ {{ 1 } \\over { 3 }} , {{ 1 } \\over { 4 }} , {{ 1 } \\over { 5 }} , \\cdots \\right\\} = {{ 1 } \\over { 3 }} $$\n$$ n=4 : \\sup \\left\\{ \\quad {{ 1 } \\over { 4 }} , {{ 1 } \\over { 5 }} , \\cdots \\right\\} = {{ 1 } \\over { 4 }} $$\n$$ n=5 : \\sup \\left\\{ \\quad \\quad {{ 1 } \\over { 5 }} , \\cdots \\right\\} = {{ 1 } \\over { 5 }} $$\n$$ n \\to \\infty : \\sup_{k \\ge n} \\left\\{ {{ 1 } \\over { k }} : k \\in \\mathbb{N} \\right\\} = 0 $$\nこのように、前部を捨てて計算するということは、十分に大きな$n$について話したいという意味であり、結局、$\\lim$との関連があることがわかる。\n$\\displaystyle \\lim_{n \\to \\infty} \\left\\{ {{1} \\over {k}} : k \\ge n \\right\\} = \\emptyset$であるから、$\\displaystyle \\sup \\lim_{n \\to \\infty} \\left\\{ x_{n} \\right\\}$が存在しないことを考えると、なぜ$\\displaystyle \\limsup_{n \\to \\infty} = \\lim_{n \\to \\infty} \\sup_{k \\ge n}$のような表現が必要であるかについて少し理解する助けになるかもしれない。極限は考えられるが、少なくとも$n$が与えられているので、$s_{n}$という別の数列の極限だけを考えても差し支えない。\n一方で、$\\displaystyle y_{n} = {{1} \\over {(-2)^{n-1}}}$を考えると、$\\sup \\left\\{ y_{n} \\right\\} = 1$であり$\\displaystyle \\inf \\left\\{ y_{n} \\right\\} = - {{1} \\over {2}}$だが、\n$$ \\limsup_{n \\to \\infty} y_{n} = \\liminf_{n \\to \\infty} y_{n} = 0 $$\nこれは性質 (b) に関する一例でもある。\n","id":1198,"permalink":"https://freshrimpsushi.github.io/jp/posts/1198/","tags":null,"title":"リミット・スプレムとリミット・インフィマム"},{"categories":"바나흐공간","contents":" imbeddingとembeddingは同じ意味だ。 埋め込みは、挿入、埋め込み、組み込み、埋めるなどと訳される。 定義1 $(X, \\left\\| \\cdot \\right\\|_{X}), (Y, \\left\\| \\cdot \\right\\|_{Y})$がノルム空間だとしよう。$X$と$Y$に対して、以下の二つの条件が成り立つ場合、$X$が$Y$に埋め込まれているimbeddedと言い、$I : X \\to Y$を埋め込みimbeddingという。\n$X$が$Y$の部分空間である。\nすべての$x \\in X$に対して、$Ix = x$で定義された恒等作用素$I : X \\to Y$が連続である。\n説明 恒等作用素は線形なので、二番目の条件は$I$が有界であることと同値である。したがって、次のように書き換えることができる。\n$$ \\exists M \\gt 0 \\text{ such that } \\left\\| Ix \\right\\|_{{Y}} \\le M \\left\\| x \\right\\|_{X},\\quad x \\in X $$\n埋め込み演算子$I$がコンパクトである場合、$X$が$Y$内にコンパクトに埋め込まれているcompactly imbeddedと言う。\n$f : X \\to Y$が等距離埋め込みであるということは、$f : X \\to f(X)$が等距離写像であるということである。定理2によれば、すべての距離空間は完備距離空間に等距離埋め込みが可能であることがわかる。つまり、すべての距離空間は完備距離空間の部分集合として扱うことができる。\n定理 定理1 $X, Y$を距離空間としよう。$f : X \\to Y$を等距離写像としよう。すると$f$は埋め込みである。\n定理2 $(X, d_{X})$を距離空間としよう。$(Y,d_{Y})$を完備距離空間としよう。すると、等距離埋め込み$f : X \\to Y$が存在する。\n参照 位相数学における埋め込み 微分多様体における埋め込み Robert A. Adams and John J. F. Foutnier, Sobolev Space (第2版, 2003), p9\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1214,"permalink":"https://freshrimpsushi.github.io/jp/posts/1214/","tags":null,"title":"数学における埋め込み、挿入写像"},{"categories":"수치해석","contents":"定義 1 $D \\subset \\mathbb{R}^2$で定義された連続関数に対して、初期値問題$\\begin{cases} y ' = f(x,y) \\\\ ( y( x_{0} ) , \\cdots , y(x_{p}) ) = (Y_{0}, \\cdots , Y_{p} ) \\end{cases}$が与えられているとする。区間$(a,b)$を$a \\le x_{0} \u0026lt; x_{1} \u0026lt; \\cdots \u0026lt; x_{n} \u0026lt; \\cdots x_{N} \\le b$のようなノードポイントに分割したとしよう。特に、十分に小さい$h \u0026gt; 0$に対して$x_{j} = x_{0} + j h$とすると、初期値と$0 \\le p \\le m$に対して、$a_{p} \\ne 0$または$b_{p} \\ne 0$ならば、以下を**$(p+1)$ステップメソッド**と呼ぶ。 $$ y_{n+1} = \\sum_{j=0}^{p} a_{j} y_{n-j} + h \\sum_{j = -1}^{p} b_{j} f (x_{n-j} , y_{n-j} ) $$\n説明 もちろん、十分に大きな$q \\ge 1$と$D \\subset \\mathbb{R}^2$で定義された$f \\in C^{q}(D)$を考えても構わない。この一般的な形で、特に$p=0$であり、$a_{0} = 1 , b_{0} = 1 , b_{-1} = 0$であるならば、オイラーメソッドになる。\nマルチステップメソッドは、ワンステップメソッドに比べてより多くのデータ情報を使用するため、通常は精度が高くなる。初期値問題$\\begin{cases} y ' = f(x,y) \\\\ ( y( x_{0} ) , \\cdots , y(x_{p}) ) = (Y_{0}, \\cdots , Y_{p} ) \\end{cases}$に対して、切り捨て誤差Truncated Errorを $$ T_{n} (Y) := Y_{n+1} - \\sum_{j=0}^{p} a_{j} Y_{n-j} + h \\sum_{j = -1}^{p} b_{j} Y\u0026rsquo;_{n-j} $$ としよう。これについて、$\\displaystyle \\tau_{n} (Y) := {{1} \\over {h}} T_{n} (Y) $と書き、$\\displaystyle \\lim_{h \\to 0} \\max_{x_{p} \\le x_{n} \\le b} | \\tau_{n} (Y) | = 0$を満たす場合、メソッドが一貫性Consistency Conditionを持つと言われる。式で書くと複雑に見えるが、簡単に言えば、$h$が小さくなる速さより切り捨て誤差が小さくなる速さの方が速いことを指す。ここで $$ \\tau (h) : = \\max_{x_{p} \\le x_{n} \\le b} | \\tau_{n} (Y) | = O (h^m) $$ を満たす$m$の中で最も大きい数をメソッドの収束次数Order of Convergenceと呼ぶ。\n特に$b_{-1} = 0$の場合、$y_{n+1}$は左辺にのみ現れるので、明示的メソッドExplicit Methodと呼ばれる。$b_{-1} \\ne 0$であれば、$y_{n+1}$が両辺に現れるので、暗黙的メソッドImplicit Methodと呼ばれる。計算の際には明示的メソッドが便利だが、一般的に知られている暗黙的メソッドは性能が良いが追加的な計算が必要だ。\nAtkinson. (1989). 数値解析入門(第2版): p357.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":693,"permalink":"https://freshrimpsushi.github.io/jp/posts/693/","tags":null,"title":"マルチステップ法"},{"categories":"수치해석","contents":"要約 $[x_{0} , b] \\times \\mathbb{R}$ で定義された$f$ に関して、初期値問題 $\\begin{cases} y ' = f(x,y) \\\\ y( x_{0} ) = Y_{0} \\end{cases}$ の解 $Y(x)$ が $[x_{0} , b]$ で二回微分可能だとする。$f$ が全ての $x_{0} \\le x \\le b$ と $ y_{1} , y_{2} \\in \\mathbb{R}$、そして $K \\ge 0$ に対して強いリプシッツ条件 $$ |f(x,y_{1} ) - f(x,y_{2}) | \\le K | y_{1} - y_{2} | $$ を満たすなら、オイラーメソッドで得られた解 $\\left\\{ y_{n} ( x_{ n } ) \\ : \\ x_{0} \\le x_{n} \\le b \\right\\} $ に対して、 $$ \\max_{ x_{0 } \\le x_{n} \\le b } | Y_{x_{n}} - y_{h} (x_{n}) | \\le \\epsilon^{( b - x_{0} ) K} | \\epsilon_{0} | + \\left[ {{ \\epsilon^{(b- x_{0}) K } - 1 } \\over {K}} \\right] \\tau (h) $$ ここで、$\\tau (h) = {{h} \\over {2}} \\left\\| Y\u0026rsquo;\u0026rsquo; \\right\\|_{\\infty}$ かつ $\\epsilon_{0} = Y_{0} - y_{h} (x_{0 } )$ である。\n参照 強いリプシッツ条件 $\\implies$ リプシッツ条件 $\\implies$ 局所リプシッツ条件\n説明 非常に長い説明だったが、要約すると、オイラーメソッドの解の正確性について話す少し強い条件が リプシッツ条件よりあるということだ。リプシッツ条件の場合は連続性のみを仮定していたが、強いリプシッツ条件を使う場合は微分可能性まで考慮する点が異なる。\n証明 1 $$ x_{n+1} - x_{n} = h $$\n$$ Y_{n} := Y(x_{n} ) $$\n$$ y_{n} := y(x_{n}) $$ ここで、$n$ 回目の誤差を $\\epsilon_{n} : = Y(x_{n}) - y (x_{n} )$ と表す。\n$Y(x_{n+1} )$ を$ x_{n}$ に関して $2$ 次のテイラー展開すると、何らかの $x_{n} \\le \\xi_{n} \\le x_{n+1}$ に対して、 $$ Y_{n+1} = Y_{n} + h Y\u0026rsquo;_{n} + {{h^2} \\over {2}} Y\u0026rsquo;\u0026rsquo; ( \\xi_{n} ) $$ 便宜上 $\\displaystyle \\tau_{n} := {{h} \\over {2}} Y\u0026rsquo;\u0026rsquo; ( \\xi_{n} )$ とすると、 $$ Y_{n+1} = Y_{n} + h Y\u0026rsquo;_{n} + h \\tau_{n} $$\n$$ \\max_{n} | \\tau_{n} | \\le \\tau (h) $$ 両辺から オイラーメソッドで得られた式 $y_{n+1} = y_{n} + h f ( x_{n} , y_{n} )$ を引くと、 $$ Y_{n+1} - y_{n+1} = Y_{n} - y_{n} + h ( f( x_{n} , Y_{n} ) - f (x_{n} , y_{n}) ) + h \\tau_{n} $$ $\\epsilon_{n}$ に関して表すと、 $$ \\epsilon_{n+1} = \\epsilon_{n} + h ( f( x_{n} , Y_{n} ) - f (x_{n} , y_{n}) ) + h \\tau_{n} $$ 両辺の絶対値を取ると、 $$ | \\epsilon_{n+1} | \\le | \\epsilon_{n} | + h | f( x_{n} , Y_{n} ) - f (x_{n} , y_{n}) | + h | \\tau (h) | $$ リプシッツ条件によって、 $$ | \\epsilon_{n+1} | \\le | \\epsilon_{n} | + h K | Y_{n} - y_{n} | + h | \\tau (h) | $$ まとめると、 $$ | \\epsilon_{n+1} | \\le (1 + hK ) | \\epsilon_{n} | + h | \\tau (h) | $$ 再帰的に解くと、 $$ | \\epsilon_{n+1} | \\le (1 + hK )^n | \\epsilon_{0} | + [ 1 + (1 + hK) + \\cdots + (1 + hK)^{n-1} ] h | \\tau (h) | $$ 有限等比級数の和の公式によって、 $$ | \\epsilon_{n+1} | \\le (1 + hK )^n | \\epsilon_{0} | + \\left[ {{ (1+ hK)^{n} - 1} \\over {hK }} \\right] h | \\tau (h) | $$\nベルヌーイの不等式の系: $(1 + x )^{ \\alpha } \\le e^{ x \\alpha }$\nベルヌーイの不等式の系によって、$ (1 + hK )^n \\le e^{hKn}$ だから、 $$ | \\epsilon_{n} | \\le e^{( b - x_{0} ) K} | \\epsilon_{0} | + \\left[ {{ e^{(b- x_{0}) K } - 1 } \\over {K}} \\right] \\tau (h) $$\n■\n追加条件 一方、リプシッツ条件によって $\\displaystyle | \\epsilon_{n+1} | \\le | \\epsilon_{n} | + h K | Y_{n} - y_{n} | + h | \\tau (h) | $ から $\\displaystyle {{\\partial f (x,y) } \\over { \\partial y }} \\le 0$ までの条件が追加されると考える。表現をきれいにすると、 $$ | \\epsilon_{n+1} | \\le (1 + hK) | \\epsilon_{n} | + {{h^2} \\over {2}} | Y\u0026rsquo;\u0026rsquo; ( \\xi_{n } ) | $$ 平均値定理によって、 $$ K = \\left| {{f(x,y_{1} ) - f(x,y_{2}) } \\over {y_{1} - y_{2}}} \\right| = \\left| {{ \\partial f ( x_{n} , \\zeta_{n} ) } \\over { \\partial y }} \\right| $$ $\\zeta_{n} \\in \\mathscr{H} \\left\\{ y_{h} (x_{n} ) , Y ( x_{n} ) \\right\\}$ が存在する。この時、$h$ が十分小さければ $\\displaystyle 1+ h {{ \\partial f ( x_{n} , \\zeta_{n} ) } \\over { \\partial y }} \\ge -1$ なら、 $$ | \\epsilon_{n+1} | \\le | \\epsilon_{n} | + {{h^2} \\over {2}} | Y\u0026rsquo;\u0026rsquo; ( \\xi_{n } ) | $$ この不等式も再帰的に解けば、 $$ | \\epsilon_{n} | \\le | \\epsilon_{0} | + {{h^2} \\over {2}} [ |Y\u0026rsquo;\u0026rsquo; ( \\xi_{0 } ) | + \\cdots + |Y\u0026rsquo;\u0026rsquo; ( \\xi_{n-1 } ) | ] $$ したがって、 $$ | \\epsilon_{n} | \\le | \\epsilon_{0} | + {{h^2} \\over {2}} n \\left\\| Y\u0026rsquo;\u0026rsquo; \\right\\|_{\\infty} $$ $nh = b - x_{0}$ だから、 $$ | \\epsilon_{n} | \\le | \\epsilon_{0} | + {{h} \\over {2}} \\left\\| Y\u0026rsquo;\u0026rsquo; \\right\\|_{\\infty} ( b - x_{0}) $$ つまり、$\\displaystyle {{\\partial f (x,y) } \\over { \\partial y }} \\le 0$ の条件は元々 $(b - x_{0})$ に関して指数関数的に増加する誤差の上限を線形に減少させたということだ。幸いにも自然界に存在する多くの問題がこの仮定を満たしており、そのおかげで誤差が大幅に減少することが保証される。\nAtkinson. (1989). An Introduction to Numerical Analysis(2nd Edition): p347. $$ x_{i} : = x_{0} + ih $$\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":689,"permalink":"https://freshrimpsushi.github.io/jp/posts/689/","tags":null,"title":"強いリプシッツ条件とオイラーメソッドの誤差"},{"categories":"정수론","contents":"アルゴリズム 1 準素数 $N$ が与えられたとする。$p$ が スムーズ 素数であれば、$N$ の素因数分解 $N = pq$ は次のように求めることができる。\nステップ 1.\n$a := 2$ と $L := 1$ を設定する。\nステップ 2.\n$d := \\gcd ( a^{L} - 1 , N )$ を計算する。\nステップ 3.\n$1\u0026lt; d \u0026lt; N$ ならば、$N$ の 約数 $d = p$ を見つけたことになり、終了する。それ以外の場合は、$L := (L+1)!$ のように更新する。$L$ があまりに大きくなった場合は、$L := 1$ に戻り、$a : = a+ 1$ のように更新する。その後、ステップ 2 に戻る。\n説明 ポラードの $p-1$ 素因数分解アルゴリズムは、ポリグ-ヘルマンアルゴリズムのように スムーズ素数 の弱点を利用する。一つの素数が弱点になって、素因数分解問題が簡単に解けてしまう。この攻撃方法のため、素因数分解問題の難しさを利用した暗号システムは、秘密鍵としてスムーズ素数を使用できなくなる。\n原則として、ステップ 1のように $a=2$ から始める必要も、ステップ 2のように $L = n!$ を使用する必要もない。ただ、$p$ がスムーズであれば、$d := \\gcd ( a^{L} - 1 , N )$ を成功させる確率が高いため、そのように使用されるだけだ。\n証明 $$ (p-1) \\mid L \\\\ (q-1) \\nmid L $$ $p$ は スムーズ であるから、$L = n!$ に対して上記の条件を満たしやすいと仮定する。これは、ある $i,j \\in \\mathbb{Z}$ と $k \\in \\mathbb{N}$ に対して、次のことが成立するということである。 $$ \\begin{align*} L =\u0026amp; i ( p -1 ) \\\\ L =\u0026amp; j ( q -1 ) + k \\end{align*} $$\nフェルマーの小定理: 素数 $p$ と $p$ と互いに素な整数 $a$ に対して、$a^{p-1} \\equiv 1 \\pmod{p}$\n$\\pmod{p}$ では $$ \\begin{align*} a^{L} \u0026amp; \\equiv a^{( p - 1 ) i } \\\\ \u0026amp; \\equiv 1^{i} \\\\ \u0026amp; \\equiv 1 \\pmod{p} \\end{align*} $$ $\\pmod{q}$ では $$ \\begin{align*} a^{L} \u0026amp; \\equiv a^{( q - 1 ) j + k } \\\\ \u0026amp; \\equiv 1^{j} a^{k} \\\\ \u0026amp; \\equiv a^{k} \\pmod{q} \\end{align*} $$ ここで、$k \\ne 0$ より、通常の場合は $a^{k} \\ne 1 \\pmod{q}$ である可能性が高い。これは、再び言えば $$ p \\mid a^{L} -1 $$\n$$ q \\nmid a^{L} -1 $$ これは、$p$ が $a^{L} -1$ の 約数 であることを意味しており、$p$ が $N$ の 約数 でもあるため、二つの数の最大公約数を計算すると $$ p = \\gcd \\left( a^{L} - 1 , N \\right) $$\n■\n関連項目 素因数分解 素因数分解問題の難しさを利用したセキュリティアルゴリズム RSA 公開鍵暗号システム ゴールドワッサー-ミカリ確率鍵暗号システム 素因数分解問題に対する攻撃アルゴリズム ポラードの p-1素因数分解アルゴリズム 準素数の素因数分解問題が容易に解ける条件 Hoffstein. (2008). An Introduction to Mathematical Cryptography: p133~135.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1187,"permalink":"https://freshrimpsushi.github.io/jp/posts/1187/","tags":null,"title":"ポラードのp-1素因数分解アルゴリズムの証明"},{"categories":"수치해석","contents":"メソッド 1 $D \\subset \\mathbb{R}^2$ で定義された連続関数について、初期値問題が$\\begin{cases} y ' = f(x,y) \\\\ y( x_{0} ) = Y_{0} \\end{cases}$で与えられている。区間$(a,b)$を$a \\le x_{0} \u0026lt; x_{1} \u0026lt; \\cdots \u0026lt; x_{n} \u0026lt; \\cdots x_{N} \\le b$のようなノードポイントで分けたとする。特に十分小さい$h \u0026gt; 0$に対して$x_{j} = x_{0} + j h$としたとき、初期値$y_{0} \\simeq Y_{0}$に対して $$ y_{n+1} = y_{n} + h f ( x_{n} , y_{n} ) $$\n説明 オイラーメソッドは概念的に非常にシンプルな方法だが、数値解析の核心的なアイデアを示している。もちろん今では様々な問題が多いが、逆に言えば改善の余地も多い手法である。だからといって、オイラーメソッド自体が重要というわけではなく、導出過程をしっかりと理解することが必要だ。\n導出 テイラー展開 $Y(x_{n+1} )$を$x_{n}$に対して$2$項までテイラー展開すると $$ Y ( x_{n+1} ) = Y ( x_{n} ) + ( x_{n+1} - x_{n}) y ' ( x_{n} ) + {{( x_{n+1} - x_{n})^2} \\over {2}} Y\u0026rsquo;\u0026rsquo; ( \\xi_{n} ) $$ 整理すると $$ Y ( x_{n+1} ) = Y ( x_{n} ) + h y ' ( x_{n} ) + {{h^2} \\over {2}} Y\u0026rsquo;\u0026rsquo; ( \\xi_{n} ) $$ ここで、誤差項$\\displaystyle {{h^2} \\over {2}} Y\u0026rsquo;\u0026rsquo; ( \\xi_{n} )$を無視すると $$ y_{n+1} = y_{n} + h f ( x_{n} , y_{n} ) $$\n■\nテイラー展開を用いた導出から直ちに分かることは、$h$が小さくなると誤差も減少するということだ。また、テイラー近似がどうして$2$次でなければならないという特別な理由はないため、次数を上げれば誤差が減少するということだ。\n数値積分 $[x_{n} , x_{n+1}]$から$Y\u0026rsquo;(t) = f (t, Y(t))$の定積分は $$ \\int_{x_{n}}^{x_{n+1}} f(t,Y(t)) dt = Y(x_{n+1}) - Y(x_{n}) $$ 少しの誤差はあるが、数値積分によって$\\displaystyle \\int_{x_{n}}^{x_{n+1}} f(t,Y(t)) dt \\simeq h f(x_{n} , Y(x_{n}) ) $であるため、 $$ y_{n+1} - y_{n} = h f ( x_{n} , y_{n} ) $$\n■\n分割積分の考え方からして、$h$が小さくなればなるほど誤差も小さくなると推測できる。図からわかるように、実際の積分値と数値的に計算された値の差はかなり大きいが、台形公式やシンプソン法を使えば誤差が減少するだろう。\n実装 以下はRで書かれたコードだ。\nEuler\u0026lt;-function(f,Y\\_0,a,b,h=10^(-3))\r{\rY \u0026lt;- t(Y\\_0)\rnode \u0026lt;- seq(a,b,by=h)\rfor(x in node)\r{\rY\u0026lt;-rbind(Y,Y[length(Y[,1]),]+h*f(x,Y[length(Y[,1]),]))\r}\rreturn(Y)\r}\rf\u0026lt;-function(x,y) {y}\rout\u0026lt;-Euler(f,seq(1,1,len=100),0,2)\rout[,1]\rg\u0026lt;-function(x,y) {1/(1+x^2) + - 2*(y^(2))}\rout\u0026lt;-Euler(g,seq(0,0,len=100),0,2,h=0.2)\rout[,1] Atkinson. (1989). 数値解析入門(第2版): p341.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":687,"permalink":"https://freshrimpsushi.github.io/jp/posts/687/","tags":null,"title":"数値解析におけるオイラー法"},{"categories":"수리물리","contents":"定義 以下の微分方程式を**$\\nu$次ベッセル方程式**Bessel\u0026rsquo;s equation of order $\\nu$と呼ぶ。\n$$ \\begin{align*} x^2 y^{\\prime \\prime} +xy^{\\prime} +(x^2-\\nu^2)y =\u0026amp;\\ 0 \\\\ x(xy^{\\prime})^{\\prime} + (x^2- \\nu ^2) y =\u0026amp;\\ 0 \\\\ y^{\\prime \\prime}+\\frac{1}{x} y^{\\prime} + \\left( 1-\\frac{\\nu^{2}}{x^{2}} \\right)y =\u0026amp;\\ 0 \\end{align*} $$\n説明 ベッセル方程式の解をベッセル関数Bessel functionと呼ぶ。\nベッセル関数は、物理学や工学などでよく見られ、特に円筒対称性を持つ問題で使用される。この理由から、ベッセル関数はシリンダー関数cylinder functionとも呼ばれるが、あまり一般的ではない。\n導出 2次元極座標での波動方程式は以下のように与えられる。\n$$ \\begin{equation} \\dfrac{\\partial ^2 u}{\\partial t^2} = c^2 \\left( \\dfrac{\\partial ^2 u}{\\partial r^2}+\\frac{1}{r}\\dfrac{\\partial u}{\\partial r}+\\frac{1}{r^2} \\dfrac{\\partial ^2 u}{\\partial \\theta ^2}\\right) \\end{equation} $$\n$c$は定数である。上記の方程式の解$u$が変数分離可能な関数だとしよう。\n$$ u(t, r, \\theta)=T(t)R(r)\\Theta (\\theta) $$\n$(1)$を代入すると\n$$ T^{\\prime \\prime}R\\Theta=c^2\\left( TR^{\\prime \\prime}\\Theta + \\dfrac{1}{r}TR^{\\prime}\\Theta + \\frac{1}{r^2}TR\\Theta^{\\prime \\prime} \\right) $$\n両辺を$c^2TR\\Theta$で割ると\n$$ \\dfrac{T^{\\prime \\prime}}{c^2T}=\\dfrac{R^{\\prime \\prime}}{R}+\\dfrac{R^{\\prime}}{rR}+\\dfrac{\\Theta^{\\prime \\prime}}{r^2\\Theta} $$\n左辺は純粋に$t$の関数であり、右辺は$r$、$\\theta$の関数なので、式の両辺は定数でなければならない。左辺が$t$に対して定数ではない場合、$t$の値が変わると左辺の値が変わり、右辺の値が変わらないため等式が成り立たなくなる。したがって、すべての$t$、$r$、$\\theta$において、両辺は定数である。この定数を$-\\mu ^2$としよう。すると、\n$$ \\begin{equation} \\dfrac{T^{\\prime \\prime}}{c^2T}=\\dfrac{R^{\\prime \\prime}}{R}+\\dfrac{R^{\\prime}}{rR}+\\dfrac{\\Theta^{\\prime \\prime}}{r^2\\Theta}=-\\mu^2 \\end{equation} $$\nまず、$r$、$\\theta$の式を見てみよう。\n$$ \\dfrac{R^{\\prime \\prime}}{R}+\\dfrac{R^{\\prime}}{rR}+\\dfrac{\\Theta^{\\prime \\prime}}{r^2\\Theta}=-\\mu^2 $$\n両辺に$r^2$を掛け、$r$と$\\theta$の式を分けてみると\n$$ \\dfrac{r^2R^{\\prime \\prime}}{R}+\\dfrac{rR^{\\prime}}{R}+r^2\\mu^2=-\\dfrac{\\Theta^{\\prime \\prime}}{\\Theta} $$\n先に述べた理由と同じで、この式の両辺もまた定数である。この定数を$\\nu^2$としよう。すると、以下のような式を得る。\n$$ \\begin{equation} -\\dfrac{\\Theta^{\\prime \\prime}}{\\Theta}=\\nu^2 \\quad \\implies \\quad \\Theta^{\\prime \\prime} =-\\nu^2 \\Theta \\quad \\end{equation} $$\nそして再び$(2)$に戻り、$t$に関する式を整理すると\n$$ \\begin{equation} T^{\\prime \\prime}=-c^2\\mu^2T \\end{equation} $$\n$(3)$と$(4)$を$(2)$に代入し、適切に式を整理すると以下のようになる。\n$$ \\begin{align*} \u0026amp;\u0026amp;\\dfrac{-c^2 \\mu^2 T}{c^2T}=\\dfrac{R^{\\prime \\prime}}{R}+\\frac{R^{\\prime}}{rR}+\\dfrac{-\\nu^2\\Theta}{r^2\\Theta} \\\\ \\implies \u0026amp;\u0026amp;\\frac{1}{R}R^{\\prime \\prime}+\\dfrac{1}{rR}R^{\\prime}+\\left(\\mu^2-\\frac{\\nu^2}{r^2}\\right) =0 \\\\ \\implies \u0026amp;\u0026amp; r^2R^{\\prime \\prime}(r)+rR^{\\prime}(r)+(\\mu^2r^2-\\nu^2)R(r)=0 \\end{align*} $$\nこの時、$\\mu r=x$と置換しよう。そして次のようにする。\n$$ R(r)=f(\\mu r)=f(x),\\quad R^{\\prime}(r)=\\mu f^{\\prime}(\\mu r)=\\mu f^{\\prime}(x),\\quad R^{\\prime \\prime}(r)=\\mu^2 f^{\\prime \\prime}(\\mu r)=\\mu^2 f^{\\prime \\prime}(x) $$\nこれらの式を先に得た式に代入すると\n$$ \\begin{align*} \u0026amp;\u0026amp; \\frac{x^2}{\\mu^2}\\mu^2f^{\\prime \\prime}(x) + \\dfrac{x}{\\mu}\\mu f(x)+(x^2-\\nu^2)f(x)=\u0026amp;\\0 \\\\ \\implies \u0026amp;\u0026amp; x^2f^{\\prime \\prime}(x) + x f(x)+(x^2-\\nu^2)f(x)=\u0026amp;\\0 \\end{align*} $$\n上記の式を$\\nu$次ベッセル方程式と呼ぶ。通常、以下の形で見ることができる。\n$$ \\begin{align*} x^2 y^{\\prime \\prime} +xy^{\\prime} +(x^2-\\nu^2)y=\u0026amp;\\0 \\\\ x(xy^{\\prime})^{\\prime}+(x^2 \\nu ^2) y=\u0026amp;\\0 \\end{align*} $$\nこの方程式の最初の解は以下の通りで、第1種ベッセル関数と呼ばれる。\n$$ J_{\\nu}(x)=\\sum \\limits_{n=0}^{\\infty}\\frac{(-1)^{n}}{\\Gamma (n+1)\\Gamma (n+\\nu+1)} \\left( \\frac{x}{2} \\right)^{2n+\\nu} $$\n2つ目の解は以下の通りで、第2種ベッセル関数と呼ばれる。​​​\n$$ N_{\\nu}(x)=Y_{\\nu}(x)=\\frac{\\cos (\\nu \\pi)J_{\\nu}(x)-J_{-\\nu}(x)}{\\sin (\\nu\\pi)} $$\nしたがって、ベッセル方程式の一般解は以下の通りである。\n$$ y(x)=AJ_{\\nu}(x)+BN_{\\nu}(x) $$\nこの時、$A$と$B$は定数である。\n","id":1195,"permalink":"https://freshrimpsushi.github.io/jp/posts/1195/","tags":null,"title":"ベッセル方程式の導出"},{"categories":"해석개론","contents":"定義1 2 $\\mathbb{N}$ は自然数の集合を、$\\mathbb{R}$ は実数の集合を意味する。\n定義域が $\\mathbb{N}$ である関数を数列と言う。\n自然数の数列 $\\left\\{ n_{k} \\right\\}_{ k \\in \\mathbb{N}}$ に対して、$\\left\\{ x_{n_{k}} \\right\\}_{ k \\in \\mathbb{N}}$ を $\\left\\{ x_{n} \\right\\}_{ n \\in \\mathbb{N}}$ の部分数列Subsequenceという。\n全ての $x \\in \\left\\{ x_{n} \\right\\}_{ n \\in \\mathbb{N}}$ に対して $x \\le M$ を満たす $M \\in \\mathbb{R}$ が存在するならば、$\\left\\{ x_{n} \\right\\}_{ n \\in \\mathbb{N}}$ は 上に有界、$m \\le x$ を満たす $m \\in \\mathbb{R}$ が存在するならば 下に有界、上にも下にも有界ならば 有界Boundedという。\n$\\left\\{ x_{n } \\right\\}_{n = 1}^{\\infty}$ が実数列だとしよう。全ての $\\varepsilon \u0026gt; 0$ に対して $$n \\ge N \\implies | x_{n} - a | \u0026lt; \\varepsilon$$ を満たす $N \\in \\mathbb{N}$ が存在するなら、$\\left\\{ x_{n } \\right\\}$ は $a \\in \\mathbb{R}$ に収束するConvergeといい、$\\lim \\limits_{n\\to \\ \\infty}x_{n}=a$と表記される。\n$\\left\\{ x_{n } \\right\\}$が収束しない場合、発散するDivergeという。\n全ての $M \\in \\mathbb{R}$ に対して $n \\ge N \\implies x_{n} \u0026gt; M$を満たす $N \\in \\mathbb{N}$ が存在するなら $$ \\lim \\limits_{n\\to \\infty} x_{n} = +\\infty \\quad \\text{ or } \\quad x_{n} \\to +\\infty $$ と表記される。\n全ての $M \\in \\mathbb{R}$ に対して $n \\ge N \\implies x_{n} \u0026lt; M$を満たす $N \\in \\mathbb{N}$ が存在するなら $$ \\lim \\limits_{n\\to \\infty} x_{n} = -\\infty \\quad \\text{ or } \\quad x_{n} \\to -\\infty $$ と表記される。\n説明 大学で初めて収束と発散の定義に触れると、一体矢印はどこへ行って、見えにくい$\\varepsilon$、$M$、$N$が現れるのだろう。正直に言って、学びたくないだろう。学生の立場では、新しい極限の定義を知らないということが、極限の概念自体を知らないということではなく、中間試験さえ何とか乗り切れば、二度と見ることがないような気がするからだ。もちろん、愚かな考えだ。\n高校時代を振り返ると、先生方も$n \\to \\infty$について述べるときには「無限に大きくなる」とか「無限大に送る」という表現を使っていたが、何となく数列を「動く何か」として扱うことに過剰に慎重だったような気がする。それは、先生方が学んだ人々だからだ。\n直感ではなく厳密な定義を使用する理由は、実は厳密な定義の方が簡単だからである。SATなどで登場する「簡単な数列」では直感が速いが、「複雑な数列」を扱うには不十分であるため、厳密な定義が導入された。歴史的にも、イギリスの数学はニュートンを軸に大陸の数学を大きく先んじていたにもかかわらず、直感主義に固執したために学界の主導権を大陸に奪われた。\n数列の収束について学ぶ際の最大の障害は、その本質的な難しさではなく、「わざわざ」「難しく」「再度」学ぶことから来る嫌悪感が大きい。たとえば、$\\displaystyle \\lim_{n \\to \\infty} {{n + 3} \\over {2n}} = {{1} \\over {2}}$のような簡単な問題を無理やり回りくどく解く時がそうである。\n学ぶことが難しいというよりも、学びたくないというのが問題で、残念ながら、理解するのに苦労する学生たちにそれだけの価値がある数列を教えることは非常に困難な仕事だ。理解と共感を助けるために、次の2つの定理を紹介する。\n定理 $\\left\\{ w_{n} \\right\\}, \\left\\{ x_{n} \\right\\}, \\left\\{ y_{n} \\right\\}$が実数列であり、$a \\in \\mathbb{R}$だとしよう。\n(a) サンドイッチ定理:\n$$ \\displaystyle \\lim_{n \\to \\infty} x_{n} = \\lim_{n \\to \\infty} y_{n} = a $$\nが成立し、\n$$ n \\ge N_{0} \\implies x_{n} \\le w_{n} \\le y_{n} $$\nを満たす$N_{0} \\in \\mathbb{N}$が存在するならば、\n$$ \\displaystyle \\lim_{n \\to \\infty} w_{n} = a $$\n(b) 比較定理:\n$$ n \\ge N_{0} \\implies x_{n} \\le y_{n} $$\nを満たす$N_{0} \\in \\mathbb{N}$が存在するならば、\n$$ \\displaystyle \\lim_{n \\to \\infty} x_{n} \\le \\lim_{n \\to \\infty} y_{n} $$\nもちろん、サンドイッチ定理や比較定理は直観的に見ても明らかに成り立つものである。別に難しい事実ではない。でも、収束の新しい定義を拒否するあなたは、一体どうやってそれらを証明するのか？\nこれらの2つの定理は、高校レベルですでに証明なしに堂々と使用されていたが、実際には論理的な推論ではなく、常識的な推測を通じて受け入れられた仮説に過ぎなかった。人間の常識がどれほど頻繁に間違っているかを考えると、厳密な証明がなぜ必要か、少なくとも理工学の学生であれば納得できるだろう。\n収束の定義に従えば、これらの定理の証明は難しくはないが、読者がこのような議論に初めて触れるという前提で、できるだけ詳細に示そうとする。証明を読むと、$N$の存在に一貫して執着していると感じられるかもしれないし、実際そうである。収束性を示す際には、$| x_{n} - a |$が$\\varepsilon$より小さくなるような不等式を立てることが重要ではなく、式を満たす$N$の存在を示すことが優先される。\n率直に言って、数列の収束性を示す際に、$\\varepsilon$をどのように導いたかは関係ない。定義に従えば、$N$が存在するだけで収束するので、まずは$N$の存在に着目するべきである。これを理解していないと、問題で明らかに与えられている$N_{1}$や$N_{2}$を使わずに、もっともらしい不等式を並べて結局は論理的に崩壊した主張を提示してしまう。\n証明 (a) 戦略: ぼんやりと無限大に送るのではなく、不等式で具体的に$n \\ge N \\implies| w_{n} - a | \u0026lt; \\varepsilon$を満たす$N$の存在を示す。\n$\\varepsilon \u0026gt; 0$とする。\n$\\displaystyle \\lim_{n \\to \\infty} x_{n} = \\lim_{n \\to \\infty} y_{n} = a$より、\n$$ n \\ge N_{1} \\implies | x_{n} - a | \u0026lt; \\varepsilon $$\n$$ n \\ge N_{2} \\implies | y_{n} - a | \u0026lt; \\varepsilon $$\nを満たす$N_{1} , N_{2} \\in \\mathbb{N}$が存在する。必要な部分を要約すると、\n$$ n \\ge N_{1} \\implies a - \\varepsilon \u0026lt; x_{n} $$\n$$ n \\ge N_{2} \\implies y_{n} \u0026lt; a + \\varepsilon $$\n一方で、▷eq59\nWilliam R. Wade, An Introduction to Analysis (4th Edition, 2010), Chapter 2.1-2.2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWalter Rudin, Principles of Mathematical Analysis (3rd Edition, 1976), Chapter 3.1-3.4\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1184,"permalink":"https://freshrimpsushi.github.io/jp/posts/1184/","tags":null,"title":"大学数学における数列の極限を新たに定義する理由"},{"categories":"정수론","contents":"ビルドアップ 左から順に アリス、ボブ、イブとしよう。アリスとボブはメッセージを交換する当事者で、イブはメッセージに興味がある受動的攻撃者だ。オレンジ色の箱はアリスだけが知っている情報を、空色の箱はボブだけが知っている情報を、黒色の箱は公開されている（イブも知っている）情報を表す。\nアリスはボブから受信しなければならないメッセージ $m \\in \\mathbb{N}$ がある。\nアルゴリズム 1 鍵設定: アリスが大きな素数 $p$, $q$の積 $N=pq$ を選択し、$\\gcd \\left( e , (p-1)(q-1) \\right) = 1$、$e$ を満たす $e$ を選択して公開する。\n暗号化: ボブが $c \\equiv m^{e} \\pmod{N}$ を計算して公開する。\n復号: アリスが $de \\equiv 1 \\pmod{(p-1)(q-1)}$ を満たす $d$ を見つけて $x \\equiv c^{d} \\equiv m \\pmod{N}$ を計算すれば $x=m$ を得る。イブは現実的に $d$ を知ることができないため、$m$ を知ることができない。\n説明 RSA公開鍵暗号システムは、ロナルド・ライベストRonald Rivest、アディ・シャミアAdi Shamir、レナード・アドルマンLeonard Adlemanによって開発され、彼らの名前の頭文字からRSAと呼ばれる。3人はこの功績で2002年にチューリング賞を受賞している。\n離散対数問題の難しさにのみ基づいたエルガマル公開鍵暗号システムとは異なり、素因数分解問題の難しさにも基づいており、その堅牢さだけで大きな素数を探す価値があるようになる。さらに、理論的なものだけでなく、現実に今、世界中で使われている点から学ぶ楽しみがあるシステムだ。致命的な攻撃方法としては量子コンピューターを基盤としたショアアルゴリズムがあるが、量子コンピューターの商用化は遠いため、しばらくは現役でいると見られる。\n証明 復号 アリスは$N$の素因数分解$N = pq$を知っているため、$\\gcd \\left( e , (p-1)(q-1) \\right) = 1$を満たす$e$を見つけることができる。\n拡張ユークリッドの定理: 2つの整数$a,b$に対し、必ず$aX + bY = \\gcd (a,b)$の整数解が存在する。\n2つの整数$e$、$(p-1)(q-1)$に対し $$ e X + (p-1)(q-1) Y = 1 $$ 整数解$\\begin{cases} X=d \\\\ Y = - k \\end{cases}$が存在するため $$ de = 1 + k (p-1)(q-1) $$ 言い換えると$d$、つまり$e$のリング$\\mathbb{Z}_{(p-1)(q-1)}$での逆元であり、アリスは$p$、$q$を知っているため、$d$を簡単に見つけることができる。[ 注: リングでの乗法逆元の存在は決して自明でない点を考慮すると、$\\gcd \\left( e , (p-1)(q-1) \\right) = 1$という条件が必須であることがわかる。]\nオイラーのφ関数定理: $$ \\gcd(a,m) = 1 \\implies a^{ \\phi (m) } \\equiv 1 \\pmod{m} $$\n$\\gcd \\left( e , (p-1)(q-1) \\right) = 1$であるため、$e$の逆元$d$は次のように求められる。 $$ d = e^{-1} = e^{\\phi \\left( (p-1)(q-1) \\right) -1} $$ これは単に$e$を累乗するだけなので、連続累乗法で容易かつ迅速に計算することができる。$\\gcd ( c , pq ) = 1$が成立する場合と成立しない場合に分けて考えよう。\nケース1. $\\gcd ( c , pq ) = 1$\n合同方程式の累乗根を求める問題に帰着される。\nトーシェント関数の乗法的性質: $$ \\gcd (p , q) =1 \\implies \\phi ( p q ) = \\phi (p) \\phi (q) $$\n$\\phi (p) = p-1$且つ$\\phi (q) = q-1$であるため $$ \\phi (pq) = \\phi (p) \\phi (q) = (p-1)(q-1) $$\nオイラーのφ関数定理: $$ \\gcd(c,N) = 1 \\implies c^{ \\phi (N) } \\equiv 1 \\pmod{N} $$\n$c^{(p-1)(q-1)} = c^{ \\phi (pq ) }$であるため $$ \\begin{align*} \\left( c^{d} \\right)^{e} \u0026amp; \\equiv c^{de} \\\\ \u0026amp; \\equiv c^{1 + k (p-1)(q-1)} \\\\ \u0026amp; \\equiv c \\cdot \\left( c^{(p-1)(q-1)} \\right)^{k} \\\\ \u0026amp; \\equiv c \\cdot 1^{k} \\\\ \u0026amp; \\equiv c \\pmod{pq} \\end{align*} $$\nケース2. $\\gcd ( c , pq ) \\ne 1$\n$$ C \\equiv c^{(p-1)(q-1)} \\pmod{pq} $$ $c$が$pq$の倍数であれば$x^{e} \\equiv 0$で、$x = 0$である必要があるが、どうでも良い。したがって、$c$は$p$または$q$のどちらかの倍数であるとするが、どちらの倍数であっても構わないので$q$の倍数としよう。すると、ある$n_{0} \\in \\mathbb{Z}$と$p$の倍数ではないある$K \\in \\mathbb{Z}$に対し $$ \\begin{align*} C =\u0026amp; c^{(p-1)(q-1)} + (pq) \\cdot n_{0} \\\\ =\u0026amp; \\left[ qK \\right]^{(p-1)(q-1)} + p \\cdot q n_{0} \\end{align*} $$ 上述の式を満たす$q n_{0}$が存在するため $$ c^{(p-1)(q-1)} \\equiv \\left[ (qK)^{q-1} \\right]^{p-1} \\pmod{p} $$\nパート1. $\\left( c^{d} \\right)^{e} \\equiv c \\pmod{p}$\nフェルマーの小定理: 素数$p$と$p$と互いに素な整数$a$に対し、$a^{p-1} \\equiv 1 \\pmod{p}$\n$K$は$p$の倍数ではないので、$\\left[ (qK)^{q-1} \\right]$は$p$と互いに素であり、 $$ \\begin{align*} \\left( c^{d} \\right)^{e} \u0026amp; \\equiv c^{de} \\\\ \u0026amp; \\equiv c^{1 + k (p-1)(q-1)} \\\\ \u0026amp; \\equiv c \\cdot \\left[ (qK)^{k(q-1)} \\right]^{p-1} \\\\ \u0026amp; \\equiv c \\cdot 1 \\pmod{p} \\end{align*} $$\nパート2. $\\left( c^{d} \\right)^{e} \\equiv c \\pmod{q}$\n$$ \\begin{align*} \\left( c^{d} \\right)^{e} \u0026amp; \\equiv c^{de} \\\\ \u0026amp; \\equiv \\left[ qK \\right]^{de} \\\\ \u0026amp; \\equiv 0 \\\\ \u0026amp; \\equiv qK \\\\ \u0026amp; \\equiv c \\pmod{q} \\end{align*} $$\nパート3. $\\left( c^{d} \\right)^{e} \\equiv c \\pmod{pq}$\nモジュロの乗法: $\\gcd ( m_{1} , m_{2} ) = 1$であれば$\\begin{cases} a \\equiv b \\pmod{ m_{1} } \\\\ a = b \\pmod{ m_{2} } \\end{cases} \\implies a \\equiv b \\pmod{ m_{1} m_{2} }$\nパート1、2で$\\begin{cases} \\left( c^{d} \\right)^{e} \\equiv c \\pmod{p} \\\\ \\left( c^{d} \\right)^{e} \\equiv c \\pmod{q} \\end{cases}$だったから、次を得る。 $$ \\left( c^{d} \\right)^{e} \\equiv c \\pmod{pq} $$\nしたがって、$x=c^{d}$はどのような場合でも$x^{e} \\equiv c \\pmod{pq}$の解として存在する。また、別の解$u$が存在するとしても、常に$u \\equiv c^{d} \\pmod{pq}$として現れることは容易に示される。\n■\n暗号化 イブは$e$を知っているが、逆元$d$を見つけるためには素因数分解問題$N=pq$を解かなければならない。これは非常に困難であるため、ボブはメッセージ$m$をアリスに安全に伝達することができる。\n■\n参照 素因数分解 素因数分解問題の難しさを利用したセキュリティアルゴリズム RSA公開鍵暗号システム ゴールドワッサー-ミカリ確率キー暗号システム 素因数分解問題への攻撃アルゴリズム ポラードのp-1素因数分解アルゴリズム 半素数の素因数分解問題が容易に解ける条件 Hoffstein. (2008). An Introduction to Mathematical Cryptography: p115~122.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1173,"permalink":"https://freshrimpsushi.github.io/jp/posts/1173/","tags":null,"title":"RSA公開鍵暗号方式の証明"},{"categories":"매트랩","contents":"方法 MATLABでグラフの各軸が何を意味するか示すためにラベルを付ける場合、xlabelとylabelを使用する。特殊記号やボールド体、イタリック体も使用できる。\nx=-3*pi:0.2:3* pi;\ry=sin(x-pi/6);\rplot(x,y);\rxlabel(\u0026#39;\\beta\u0026#39;), ylabel(\u0026#39;\\nabla f(x)\u0026#39;),; x=-3*pi:0.2:3* pi;\ry=sin(x-pi/6);\rplot(x,y);\rxlabel(\u0026#39;진폭{\\bf Volt}\u0026#39;), ylabel(\u0026#39;시간{\\it sec}{\\sl sec}{\\rm sec}\u0026#39;); 記号 コード 名前 記号 コード 名前 記号 コード 名前 $\\alpha$ \\alpha アルファ $\\beta$ \\beta ベータ $\\gamma$ \\gamma ガンマ $\\delta$ \\delta デルタ $\\epsilon$ \\epsilon イプシロン $\\zeta$ \\zeta ゼータ $\\eta$ \\eta エータ $\\theta$ \\theta シータ $\\vartheta$ \\vartheta バーシータ $\\iota$ \\iota イオタ $\\kappa$ \\kappa カッパ $\\lambda$ \\lambda ラムダ $\\mu$ \\mu ミュー $\\nu$ \\nu ニュー $\\xi$ \\xi クシー $\\pi$ \\pi パイ $\\rho$ \\rho ロー $\\sigma$ \\sigma シグマ $\\varsigma$ \\varsigma バーシグマ $\\tau$ \\tau タウ $\\upsilon$ \\upsilon ウプシロン $\\phi$ \\phi ファイ $\\chi$ \\chi カイ $\\psi$ \\psi プサイ $\\omega$ \\omega オメガ $\\Gamma$ \\Gamma 大文字ガンマ $\\Delta$ \\Delta 大文字デルタ $\\Theta$ \\Theta 大文字シータ $\\Lambda$ \\Lambda $ 대문자 람다 $ \\Xi $ \\Xi 대문자 크시 $ \\Pi $ \\Pi 대문자 파이 $ \\Sigma $ \\Sigma 대문자 시그마 $ \\Upsilon $ \\Upsilon 대문자 웁실론 $ \\Phi $ \\Phi 대문자 피 $ \\Psi $ \\Psi 대문자 프사이 $ \\Omega $ \\Omega 대문자 오메가 $ \\forall $ \\forall $ \\exists $ \\exsits $ \\in $ in $ \\surd $ \\surd $ \\cong $ \\cong $ \\approx $ \\approx $ \\equiv $ \\equiv $ \\Re $ \\Re $ \\Im $ \\Im $ \\otimes $ \\otimes $ \\oplus $ \\oplus $ \\cup $ \\cup $ \\cap $ \\cap $ \\subset $ \\subset $ \\supset $ \\supset $ \\subseteq $ \\subseteq $ \\supseteq $ \\supseteq $ \\lceil $ \\lceil $ \\rceil $ \\rceil $ \\lfloor $ \\lfloor $ \\rfloor $ \\rfloor $ \\displaystyle \\int $ \\int $ \\perp $ \\perp $ \\wedge $ \\wedge $ \\vee $ \\vee $ \\langle $ \\langle $ \\rangle $ \\rangle $ \\neg $ \\neg $ \\sim $ \\sim $ \\cdot $ \\cdot $ \\times $ \\times $ \\leq $ \\leq $ \\geq $ \\geq $ \\propto $ \\propto $ \\neq $ \\neq $ \\varnothing $ \\0 $ \\infty $ \\infty $ \\clubsuit $ \\clubsuit $ \\diamondsuit $ \\diamondsuit $ \\heartsuit $ \\heartsuit $ \\spadesuit $ \\spadesuit $ \\pm $ \\pm $ \\leftrightarrow $ \\leftrightarrow $ \\uparrow $ \\uparrow $ \\downarrow $ \\downarrow $ \\rightarrow $ \\rightarrow $ \\leftarrow $ \\leftarrow $ \\circ $ \\circ $ \\partial $ \\partial $ \\div $ \\div $ \\aleph $ \\aleph $ \\wp $ \\wp $ \\oslash $ \\oslash $ \\nabla $ \\nabla $ \\ldots $ \\ldots $ \\prime $ \\prime $ \\mid ","id":1191,"permalink":"https://freshrimpsushi.github.io/jp/posts/1191/","tags":null,"title":"MATLABでグラフに使用できる特殊記号一覧"},{"categories":"정수론","contents":"コード R 次に、Rコードで実装されたエラトステネスの篩である。自然数$n$が与えられると、エラトステネスの篩と同じ方法で素数かどうかを判断してくれる。$n$そのものが返されれば素数で、$n$より小さい数が返されれば、それは$n$の約数の中で最小の数を意味する。\neratosthenes\u0026lt;-function(n){\rresidue\u0026lt;-2:n\rwhile(n %in% residue){\rp\u0026lt;-residue[1]\rresidue\u0026lt;-residue[as.logical(residue%%p)]\r}\rreturn(p)\r}\reratosthenes(101)\reratosthenes(1517) 例えば、$101$は素数なので、$101$がそのまま返され、$1517=37 \\times 41$なので、$37$が返される。\nジュリア 以下はもっと効率的に実装されたジュリアコードである。\nfunction factorize(n)\rfactors = []\rwhile n \u0026gt; 1\rfor k in 2:n\rif n % k == 0\rn ÷= k\rpush!(factors, k)\rend\rend\rend\rreturn factors\rend\rfunction eratosthenes(n::Integer)\rif n == 1 return [[1]] end\rif n == 2 return [[1], [2]] end\rprimes = [2]\rfactorized = [[1], [2]]\rfor k ∈ 3:n\rm = k\rfor p ∈ primes\rif m % p == 0\rm ÷= p\rtemp = [p; factorized[m]]\rpush!(factorized, temp)\rbreak\rend\rend\rif length(factorized) != k\rpush!(primes, k)\rpush!(factorized, [k])\rend\rend\rreturn factorized, primes\rend\rF, P = eratosthenes(20)\rF\rP 一緒に見る 素因数分解 素因数分解問題の難しさを利用したセキュリティアルゴリズム RSA公開鍵暗号体系 ゴールドバサー-ミカリ確率鍵暗号体系 素因数分解問題に対する攻撃アルゴリズム ポラードのp-1素因数分解アルゴリズム 準素数の素因数分解問題が容易に解ける条件 ","id":775,"permalink":"https://freshrimpsushi.github.io/jp/posts/775/","tags":null,"title":"素因数分解"},{"categories":"수치해석","contents":"定義 1次微分方程式の存在性・一意性定理のステートメントで、リプシッツ条件Lipschitz Conditionが見られる。\n$D \\subset \\mathbb{R}^2$で定義された連続関数について、初期値問題$\\begin{cases} y ' = f(x,y) \\\\ y( x_{0} ) = Y_{0} \\end{cases}$が与えられている。$f$がすべての$(x,y_{1}) , (x , y_{2} ) \\in D$及び$K \u0026gt; 0$に対してリプシッツ条件 $$ |f(x,y_{1} ) - f(x,y_{2}) | \\le K | y_{1} - y_{2} | $$ を満たすならば、$(x_{0} , Y_{0}) \\in D^{\\circ}$に対して適切な区間$I := [ x_{0} - \\alpha , x_{0} + \\alpha ]$で一意解$Y(x)$が存在する。\n説明 リプシッツ条件を私たちに馴染みのある表現で書くならば、 $$ \\left| { f(x,y_{1} ) - f(x,y_{2}) } \\over { y_{1} - y_{2} } \\right| \\le K $$ と表すことができる。最悪の場合でも、 $$ K = \\max_{(x,y) \\in D} \\left| {{ \\partial f(x,y) } \\over { \\partial y }} \\right| $$ だから、$f$の導関数が有界であることは似たような条件になる。これは、少なくとも初期値$( x_{0} , Y_{0} )$に関しては、関数の値が急激に変化することがないということであり、中には解きやすい問題もあるという意味になる。\nこのような条件は、解の安定性Stabilityという概念を説明するために必要である。仮定で与えられた初期値問題に、少々の変動$\\delta (x)$、$\\epsilon$が追加された $$ \\begin{cases} y ' (x ; \\epsilon) = f(x, Y(x ;\\epsilon ) ) + \\delta (x) \\\\ Y( x_{0} ; \\epsilon ) = Y_{0} + \\epsilon \\end{cases} $$ を考えてみよう。この二つの問題は数学的には完全に異なるものであるが、$| \\delta |$及び$ | \\epsilon |$が十分に小さく、リプシッツ条件も満たされる場合、次のようになる。\n$D \\subset \\mathbb{R}^2$で定義された連続関数について、初期値問題$\\begin{cases} y ' (x ; \\epsilon) = f(x, Y(x ;\\epsilon ) ) + \\delta (x) \\\\ Y( x_{0} ; \\epsilon ) = Y_{0} + \\epsilon \\end{cases}$が与えられている。$f$がリプシッツ条件を満たす場合、$(x_{0} , Y_{0}) \\in D^{\\circ}$に対して適切な区間$I := [ x_{0} - \\alpha , x_{0} + \\alpha ]$及び十分に小さい$\\epsilon_{0} \u0026gt;0$において、$| \\epsilon | \\le \\epsilon_{0}$と$ | \\delta |_{\\infty}$を満たす一意解$Y(x ; \\delta, \\epsilon )$が存在する。\n微分方程式の解法において安定性が重要になるのは、数値的な近似解を気にする時だ。新しいデータが絶えず追加されており、少しの数値の変化でモデル全体を変えなければならなくなるのは厄介だ。リプシッツ条件を満たさない場合はどういう場合か例を見てみよう。初期値問題 $$ \\begin{cases} y ' = 100 y - 101 e^{-x} \\\\ y( 0 ) = 1 \\end{cases} $$ の解は単純に$y = e^{-x}$で求められる。初期値を$y(0) = 1 + \\epsilon$に変えると、その解は$y = e^{-x} + \\epsilon e^{100x}$となるが、$| \\epsilon |$がかなり小さくない限り、誤差が大きすぎる。従って、初期値を変えなかった時に得た元の解は使用が難しく、このような場合には条件が悪いill-conditionedという。逆に、増加する$x$に対して、$\\displaystyle \\int_{x_{0}}^{x} {{ \\partial f (t, Y(t) ) } \\over {\\partial y }} dt$が小さな正数で有界であれば、条件が良いwell-conditionedという。区間$I$でリプシッツ連続な関数の集合を$C^{0,1} ( I )$と表すこともある。\n参照 強いリプシッツ条件 $\\implies$ リプシッツ条件 $\\implies$ 局所リプシッツ条件\n","id":684,"permalink":"https://freshrimpsushi.github.io/jp/posts/684/","tags":null,"title":"リプシッツ条件"},{"categories":"고전역학","contents":"概要 ハミルトンの原理、汎関数、作用、変分などについて、可能な限り簡単に説明しています。他の場所で満足のいく説明を見つけられなかった場合は、最後まで読むことをお勧めします。特に、大学1〜2年生でも十分に読めるように作成しました。\nラグランジュ力学1 物体が時間 $t_{1}$ から $t_{2}$ まで運動するとき、運動経路に対するラグランジアンの積分を作用actionといい、以下のように $J$ で表します。\n$$ \\begin{equation} J=\\int_{t_{1}}^{t_{2}} L dt \\end{equation} $$\nこのとき、可能なすべての運動経路の作用の中で、実際の運動経路の作用が最小になります。ラグランジアンLagrangianは、運動エネルギーとポテンシャルエネルギーの差で定義され、一般に$L$で表されます。\n$$ L = T-V $$\nこの内容は、ハミルトンの原理Hamilton\u0026rsquo;s variational principleまたは最小作用の原理principle of least actionと呼ばれます。最小作用の原理という名前は$(1)$の積分を作用と呼ぶからです。元々最小値と極小値は異なる概念ですが、ここでは同じ意味を持つとします。正確には極値（極大または極小）が適切です。マリオンの教科書を基準にすれば、恐らく1学期、ファウルズの教科書を基準にすれば2学期に学ぶラグランジュ力学の最初の内容です。しかし、教科書に忠実であるだけでは、この内容を理解するのが非常に難しかったです。新しい概念が登場しますが、それが何であるかを親切に説明してくれません。例えば、ファウルズの教科書では以下のような式が登場します。\n$$ \\begin{equation} \\delta J =\\delta \\int_{t_{1}}^{t_{2}} L dt = 0 \\end{equation} $$\nそして、新しく登場した記号$\\delta$についての説明は以下のようです。\n\"$\\delta$は、全体の積分の變分(variation)に対する極値である。\"\nこれを読んで$\\delta$が何を意味するのかどうやって分かるでしょうか。変分が何かもちゃんと教えてくれず、その後の計算はどんどん進みます。等式がなぜ成立するのかも分からないので、一行一行読むスピードも非常に遅く、内容を理解すること自体が非常に困難でした。そこで、ラグランジュ力学を初めて学ぶ学生のために、できるだけ親切に説明しようと思います。まず、ハミルトンの原理を記述する際に使用される用語を整理する必要があります。\n汎関数 多くの資料で$(2)$の積分を汎関数と言いますが、普通に勉強してきた物理学部の学生であれば、汎関数が何であるか知らないのが普通です。皆さんは、実数を入力すると実数（または複素数）が出力されるものを関数として知っているでしょう。\n$$ f(x)=x^2,\\quad g(x)=e^{2x} $$\nしかし、関数の数学的定義を考えると、数字を入力して数字が出力される必要はありません。何かを入力してそれに対応する結果が出力されるものが関数なので、入力するものに制限はありません。このとき、ある関数に関数を入力してそれに応じてある数が出力される場合、その関数を汎関数functionalと言います。例えば、以下のように定義された関数$F$は汎関数です。\n$$ {\\color{blue}F\\big( {\\color{orange}f(x)} \\big)} := {\\color{red}\\int_{1}^{2} f(x) dx} $$\nつまり、関数$F$はある関数を$1$から$2$まで定積分した値を関数値として持ちます。実際に計算してみると、\n$$ {\\color{blue}F( {\\color{orange} e^{x} })} = \\int_{1}^2 e^x dx = {\\color{red}e^2-e},\\quad {\\color{blue}F({\\color{orange}x^2})}=\\int_{1}^2 x^{2} dx = {\\color{red}\\frac{7}{3} } $$\n上記のように、関数を入力したときに実数（または複素数）が出力される関数を汎関数と言います。続く内容ですが、最小作用の原理で作用はまさに汎関数です。\u0026lsquo;各運動経路に対するラグランジアン\u0026rsquo;という関数を入力したときにある値が出るので、汎関数です。汎関数に関する数学的な内容を含む記事がブログにありますが、リンクは紹介しません。おそらく読めばさらに混乱するでしょうから、できれば読まないことをお勧めします。本当に興味があれば、右上の検索バーで汎関数を検索して読んでみてください。よく分からなければ、忘れてしまいましょう。\n作用とラグランジアン 運動エネルギーからポテンシャルエネルギーを引いたものをラグランジアンと呼び、$L$で表します。\n$$ L=T-V $$\nラグランジアンは速度、位置、時間に影響を受けるため、位置を$y$とすると、以下のように表すこともできます。\n$$ L=L(y^{\\prime},\\ y,\\ t) $$\nラグランジアンという名前は、フランスの数学者ジョゼフ・ルイ・ラグランジュの名前から付けられました。ラグランジアンを時間に対して定積分したものを作用、またはアクションと呼び、一般に$J$で表します。\n$$ J = \\int_{t_{1}}^{t_{2}} L dt = \\int_{t_{1}}^{t_{2}} L(y^{\\prime},\\ y,\\ t) dt $$\nハミルトンの原理 1834年、イギリスの数学者ウィリアム・ローアン・ハミルトンが考案したもので、物体が実際に動く経路は作用が最小になるような原理です。これは証明可能な事実ではなく、$F=ma$のように自然界に存在する基本原理の一つと受け入れれば良いです。例えば、私たちが物体を高い場所から投げて落とすとき、物体がどのような経路で地面まで動くか知りたいとします。私たちが予想できる経路は数えきれないほど多いでしょうが、その中で実際に物体が動く経路には何か特別な点があるということです。それは、各経路に対するラグランジアンを時間に対して積分したとき、実際に動く経路に対するラグランジアンの積分値が最も小さいということです。つまり、作用が最小になる経路を見つければ、それが実際に物体が動く経路です。そのため、ハミルトンの原理は最小作用の原理とも呼ばれます。この原理を基に物体の運動を扱うことがラグランジュ力学Lagrangian mechanicsです。驚くべきことに、ラグランジュ力学はニュートン力学とは全く異なって見えますが、同じ結果を与えるということです。つまり、表現方法は異なるものの、本質は同じです。ニュートン力学はベクトル計算に基づいて物体の動きを扱い、ラグランジュ力学はスカラー（エネルギー）の計算によって力学を記述します。\n変分 簡単に言うと、上で詳しく説明した内容を数学的に整理したものです。まず、簡単な例として2次関数の最小値を見つける問題を考えてみましょう。\n上の図のような2次関数が与えられたとします。関数値の最小値は$1$で、関数値が最小になる場所は$x=3$です。最小値（極小値）を持つ点では傾きが$0$なので、微分したとき$0$であることが分かります。したがって、\n$$ \\dfrac{dy}{dx} \\bigg|_{x=3}=0 $$\nです。この内容を最小作用の原理にそのまま適用することになります。\n上の図に示されているように、物体が実際に運動する経路を$y(0, t)$としましょう。物体が運動できる任意の経路を上の図のように$y(\\alpha, t)=y(0,t)+\\alpha \\eta (t)$としましょう。参考までに$\\eta$はギリシャ文字のエタです。$\\alpha \\eta (t)$は実際の経路と比較したときの誤差と考えれば良いです。図と数式を見れば分かるように、誤差がないとき、つまり$\\alpha=0$のとき、可能な任意の経路$y(\\alpha, t)$は実際の経路になります。また、最小作用の原理は、可能なすべての経路に対する作用の中で、実際の経路に対する作用が最小の値であるという内容です。両方の内容を組み合わせて、上で挙げた例を適用すれば、作用を微分して$\\alpha=0$を代入したとき、その値が$0$であるという結果を得ます。\n$$ \\dfrac{\\partial J}{\\partial \\alpha}=\\dfrac{\\partial }{\\partial \\alpha} \\int_{t_{1}}^{t_{2}} L\\big( y^{\\prime}(\\alpha,t),\\ y(\\alpha,t),\\ t \\big) dt =0 $$\nこれを簡単に表記すると、以下のようになり、$\\delta J$を$J$の変分と呼びます。\n$$ \\delta J = 0 $$\nつまり、$\\delta=\\dfrac{\\partial }{\\partial \\alpha}$と理解すれば良いです。したがって、以下のような等式が成立します。\n$$ \\delta \\dot{y}=\\dfrac{\\partial }{\\partial \\alpha}\\frac{dy}{dt}=\\dfrac{d}{dt}\\frac{\\partial y}{\\partial \\alpha}=\\dfrac{d}{dt}\\delta y $$\n関連項目 オイラー-ラグランジュ方程式 Grant R. Fowles and George L. Cassiday, Analytical Mechanics (7th Edition, 2005), p417-420\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1182,"permalink":"https://freshrimpsushi.github.io/jp/posts/1182/","tags":null,"title":"ラグランジュ力学とハミルトンの変分原理"},{"categories":"해석개론","contents":"定義 実数上のある点$x \\in \\mathbb{R}$と部分集合$A \\subset \\mathbb{R}$に対して、$x$を含む任意の開集合$O$に対して$ O \\cap ( A \\setminus \\left\\{ x \\right\\} ) \\ne \\emptyset $が成立するならば、$x$を集積点Limit Pointと定義する。$A$の集積点の集合を$A$の導集合Derived setと呼び、$a '$で表記する。\n説明 上の定義で、条件が$( O \\setminus \\left\\{ x \\right\\} ) \\cap A \\ne \\emptyset$でも構わない。直感的に例として、$[a,b]$の導集合は依然として$[a,b]$であるということが挙げられるだろう。\n一方で、集積点が与えられた集合の内部に属している必要はないため、$(a,b)$の導集合もまた$[a,b]$になる。英語で説明するならば、$\\lim$として表される極限Limitと非常に似ていることがわかる。よく考えてみれば、「任意の開集合」に対して条件を満たすということは、実際には極限の概念と大きく異ならない。条件が数学者にとって受け入れがたい程ではないが、その表現上では混乱するかもしれない。\n両者の違いを明確に言うならば、極限は関数が「収束する際の値」であり、集積点は「極限になり得る候補」をすべて指すものであると考えられる。数列$\\displaystyle {{1} \\over {n}}$の極限は$0$であり、集積点も$0$だけであるが、区間$(a,b)$では導集合は$[a,b]$となり、極限が何であるかを具体的に言うわけではない。この使用例からわかることは、集積点を論じる際には、それが唯一という前提がないという点である。以下の簡単な定理を証明しながら概念を掴むとよい。\n定理 (a) 有限集合には集積点が存在しない。\n(b) $\\mathbb{Q} ' = \\mathbb{R}$\n証明 (a) 定義によれば、単元素集合$A:=\\left\\{ x \\right\\}$はどのように$O$を選んでも$A \\setminus \\left\\{ x \\right\\} = \\emptyset$を満たすことがないので、$x$は集積点の条件を満たすことができない。\n■\n(b) 有理数もまた実数であるため、実数の密度により簡単に確認できる。\n■\n参照 距離空間での集積点 ","id":379,"permalink":"https://freshrimpsushi.github.io/jp/posts/379/","tags":null,"title":"実数集合における集積点"},{"categories":"거리공간","contents":"定義 距離空間 $\\left( X, d \\right)$ について、$A \\subset X$ とする。\n$x \\in O \\subset A$ を満たす開集合 $O$ が存在する時、$x$ を $A$ の内点という。\n$A$ の内点の集合 $A^{\\circ}$ を $A$ の内部という。\n$A$ とその値域の和集合 $\\overline{A} : = A \\cup a '$ を $A$ の閉包という。\n$x \\in \\overline{A}$ であり、かつ $x \\in \\overline{X \\setminus A}$ の時、$x$ を $A$ の境界点という。\n$\\partial A : = \\overline{A} \\cap \\overline{X \\setminus A}$ を $A$ の境界という。\n説明 定義する必要はないかもしれないが、インテリアと対照的な $\\overline{A}$ の外側の集合をエクステリアと呼ぶ。\n開集合とこれらの概念は異なり定義されることもあるが、本質的には同じである。\n定義は慎重に読めば誰でも理解できるものであり、図を通じて早く理解しよう。\n$$ A $$\n与えられた集合が上のような時、これらの概念を考えてみよう。\n$$ A^{\\circ} $$\nインテリアは $A$ が含む $X$ の部分集合の中で最も大きな開集合である。\n$$ \\overline{A} $$\nクロージャーは $A$ を含む $X$ の部分集合の中で最も小さな閉集合である。\n$$ \\partial A $$\nバウンダリーはクロージャーからインテリアを引いた $X$ の部分集合と見ることができる。\nインテリアとクロージャーの区別はさほど難しくないが、バウンダリーは一見すると点線か実線かによって混乱するかもしれない。境界であれば、迷わずバウンダリーと考えればいい。\nこのような定義を通じて、以下の性質は事実上、開集合と閉集合の定義と見ることができる。\n性質: 開集合と閉集合 $A$ が距離空間 $X$ の部分集合とする。\n$A$ が開集合であることと $A = A^{\\circ}$ は等価である。\n$A$ が閉集合であることと $A = \\overline{A}$ は等価である。\nもちろん、これらの性質は証明可能だが、ただの事実として受け入れても問題ない。\n","id":383,"permalink":"https://freshrimpsushi.github.io/jp/posts/383/","tags":null,"title":"距離空間における内部閉包境界"},{"categories":"편미분방정식","contents":"ビルドアップ1 ハミルトニアン$H$が$Du$のみに依存するハミルトン-ヤコビ方程式の初期値問題を見てみよう。\n$$ \\begin{equation} \\left\\{ \\begin{aligned} u_{t} + H(Du)\u0026amp;=0 \u0026amp;\u0026amp; \\text{in } \\mathbb{R}^n \\times (0,\\infty) \\\\ u\u0026amp;=g \u0026amp;\u0026amp; \\text{on } \\mathbb{R}^n \\times \\left\\{ t=0 \\right\\} \\end{aligned} \\right. \\label{eq1} \\end{equation} $$\n一般に、ハミルトニアンは空間変数に依存して$H(Du, x)$のような形を取るが、ここでは$x$に対して影響を受けないとする。そして、ハミルトニアン$H\\in C^\\infty$に対して次のような仮定をする。\n$$ \\begin{cases} H \\mathrm{\\ is\\ convex} \\\\ \\lim \\limits_{|p|\\to \\infty} \\dfrac{H(p)}{|p|}=\\infty \\end{cases} $$\nそして$L=H^{\\ast}$とすると、ラグランジアン$L$も同様の特性を満たす。最後に、初期値$g : \\mathbb{R}^n \\to \\mathbb{R}$がリプシッツ連続であるとする。すなわち、\n$$ \\mathrm{Lip}(g):=\\sup \\limits_{x,y\\in \\mathbb{R}^n \\\\ x \\ne y} \\dfrac{ |g(x)-g(y)| }{|x-y|} \u0026lt; \\infty $$\nまた、与えられたハミルトン-ヤコビ方程式$\\eqref{eq1}$の特性方程式は次のようになる。\n$$ \\begin{align*} \\dot{\\mathbf{p}}(s) \u0026amp;= -D_{x}H \\big( \\mathbf{p}(s), \\mathbf{x}(s) \\big) \\\\ \\dot{z}(s) \u0026amp;= D_{p} H\\big( \\mathbf{p}(s),\\ \\mathbf{x}(s)\\big)\\cdot \\mathbf{p}(s) -H\\big( \\mathbf{p}(s), \\mathbf{x}(s)\\big) \\\\ \\dot{\\mathbf{x}}(s) \u0026amp;= D_{p}H\\big( \\mathbf{p}(s), \\mathbf{x}(s) \\big) \\end{align*} $$\nここで$H$は$x$に無関係であると仮定したので、再び書くと次のようになる。\n$$ \\begin{align*} \\dot{\\mathbf{p}} \u0026amp;= 0 \\\\ \\dot{z} \u0026amp;= D H( \\mathbf{p} )\\cdot \\mathbf{p} -H ( \\mathbf{p} ) \\\\ \\dot{\\mathbf{x}} \u0026amp;= DH ( \\mathbf{p}) \\end{align*} $$\nこのとき$t(s)=s, p(s)=Du(x(s), s), z(s)=u(x(s), s)$である。$p$に対する微分と$x$に対する微分を区別する必要がないため、$D$の下付き文字を省略した。オイラー-ラグランジュ方程式は固定された開始点と終了点に対して成立するため、与えられたハミルトン-ヤコビ方程式$\\eqref{eq1}$の解が存在する場合、以下のようなlocal in time solutionである。\n$$ u= u(x,t) \\in C^2\\big( \\mathbb{R}^n \\times [0,T]\\big) $$\n上記の特性方程式では、第一式と第三式はラグランジアン$L=H*$によって定義される作用の最小化問題から導かれるオイラー-ラグランジュ方程式を満たすハミルトン方程式である。\n$H$と$L$が$p$, $v\\in \\mathbb{R}^n$で微分可能であれば、以下の内容はすべて同等である。\n$$ \\begin{cases} p\\cdot v=L(v) + H(p) \\\\ p=DL(v) \\\\ v=DH(p) \\end{cases} $$\nこのとき$p=D_{v}L(v)$で定義されるため、上記の補題を使用すると次を得る。\n$$ \\begin{align*} \\dot{z}(s) \u0026amp;= DH(\\mathbf{p})\\cdot \\mathbf{p}-H(\\mathbf{p}) \\\\ \u0026amp;= \\mathbf{v} \\cdot \\mathbf{p}-H(\\mathbf{p}) \\\\ \u0026amp;= L(\\mathbf{v})+H(\\mathbf{p})-H(\\mathbf{p}) \\\\ \u0026amp;= L(\\mathbf{v}) = L\\big(\\dot{\\mathbf{x}}(s)\\big) \\end{align*} $$\nしたがって$z(t)$を求めると、次のようになる。\n$$ \\begin{align*} z(t) \u0026amp;= \\int_{0}^t \\dot{z}(s)dx +z(0) \\\\ \u0026amp;= \\int_{0}^tL \\big( \\dot{\\mathbf{x}}(s) \\big) + u\\big( \\mathbf{x}(0),\\ 0\\big) \\\\ \u0026amp;= \\int_{0}^t L\\big( \\dot{\\mathbf{x}}(s)\\big) ds +g\\big(\\mathbf{x}(0) \\big) \\end{align*} $$\nしかし、このとき上記の条件では$z(t)=u(x(t), t)$であったので、次を得る。\n$$ u(x,t)=\\int_{0}^t L\\big( \\dot{\\mathbf{x}}(s)\\big) ds +g\\big(\\mathbf{x}(0) \\big) \\quad (0 \\le t \u0026lt;T) $$\nこれはlocal in time smooth solutionであるため、global in time weak solutionを求めることができるかという問題が残る。再び作用の最小化問題に戻るが、オイラー-ラグランジュ方程式を導いた際と異なる点は終点のみを固定することである。\n固定された$x \\in \\mathbb{R}^n, t\u0026gt;0$が与えられたとする。そして、許容クラス$\\mathcal{A}$を次のようにする。\n$$ \\mathcal{A}=\\left\\{ \\mathbf{w}\\in C^1\\big( [0,t];\\mathbb{R}^n \\big)\\ :\\ \\mathbf{w}(t)=x \\right\\} $$\nそして、以下のような作用に対する最小化問題を考えてみよう。\n$$ \\mathbf{w}(\\cdot) \\in \\mathcal{A} \\mapsto \\int_{0}^t L\\big( \\dot{\\mathbf{w}}(s)\\big) ds + g(\\mathbf{w}(0)) $$\nもし上記の作用のミニマイザー$\\mathbf{x}(\\cdot)$が存在するならば、$\\mathbf{p}(s):=DL(\\dot{\\mathbf{x}}(s))$であり、オイラー-ラグランジュ方程式を満たし、したがってハミルトン方程式も満たす。したがって、上記のlocal in time solutionの場合と同様に、解は以下のように与えられるだろう。\n$$ u(x,t)=\\int_{0}^tL\\big( \\dot{\\mathbf{x}}(s)\\big)ds +g \\big( \\mathbf{x}(0) \\big) $$\n上記の内容をモチーフに、global in time weak solutionが存在する場合、次のように定義できる。\n$$ \\begin{equation} u(x,t):=\\inf \\limits_{\\mathbf{w} \\in \\mathcal{A}} \\left\\{ \\int_{0}^t L\\big( \\dot{\\mathbf{w}}(s) \\big)ds + g\\big( \\mathbf{w}(0) \\big) \\right\\} \\label{eq2} \\end{equation} $$\n定理 $x \\in \\mathbb{R}^n$であり、$t\u0026gt;0$とする。それならば、$\\eqref{eq2}$の最小化問題の解は次のように与えられる。\n$$ u(x,t) = \\min \\limits_{y \\in \\mathbb{R}^n} \\left\\{ tL\\left( \\dfrac{x-y}{t} \\right) +g(y) \\right\\} $$\nこれをホップ-ラックス公式Hopf-Lax formulaと呼ぶ。\n証明 まず$\\inf$に対して成立することを示し、その次に実際に$\\min$になることを示す順序で証明する。\nステップ 1.\n固定された任意の$y \\in \\mathbb{R}^n, t\\in \\mathbb{R}$がある。そして、$\\mathbf{w}$を次のように定義しよう。\n$$ \\mathbf{w}(s) :=y+\\frac{s}{t}(x-y) \\quad (0 \\le s \\le t) $$\nすると$\\mathbf{w}(0)=y$であり、$\\mathbf{w}(t)=x$である。それならば$\\mathbf{w}$は許容クラス$\\mathcal{A}$の要素である。\n$$ \\mathcal{A}= \\left\\{ \\mathbf{w}(\\cdot) \\ \\big| \\ \\mathbf{w}(0)=y,\\ \\mathbf{w}(t)=x\\right\\} $$\nそれならば～の定義により、次の不等式が成立する。\n$$ \\begin{align*} u(x,t) \u0026amp; \\le\u0026amp; \\int_{0}^t L \\left( \\frac{x-y}{t}\\right)ds + g(y) \\\\ \u0026amp;= tL\\left( \\frac{x-y}{t}\\right)+g(y) \\end{align*} $$\nこの不等式はすべての$y \\in \\mathbb{R}^n$に対して成立するので、次を得る。\n$$ u(x,t) \\le \\inf \\limits_{y \\in \\mathbb{R}^n} \\left(t L\\left(\\frac{x-y}{t} \\right) +g(y)\\right) $$\nステップ 2.\n▷eq41\n◁としよう。それならば$\\mathbf{w}(\\cdot) \\in C^1([0;t];\\mathbb{R}^n)$であり、$\\mathbf{w}(t)=x$である。\nイェンセンの不等式\n関数$f$が凸関数であると仮定しよう。それならば、以下の式が成立する。 $$ f \\left( -\\!\\!\\!\\!\\!\\! \\int_{U} u dx \\right) \\le -\\!\\!\\!\\!\\!\\! \\int_{U} f(u) dx $$\nそれならば、上記の補題により、次が成立する。\n$$ L \\left( \\frac{1}{t}\\int_{0}^t \\dot{\\mathbf{w}}(s) dx\\right) \\le \\dfrac{1}{t}\\int_{0}^t L \\big( \\dot{\\mathbf{w}(s)} \\big)ds $$\nそして、開始点を$y$としよう$\\mathbf{w}(0)=y$。それならば、上記の不等式は以下のようになる。\n$$ \\begin{align*} \u0026amp;\u0026amp; L\\left( \\dfrac{1}{t} \\big( \\mathbf{w}(t)-\\mathbf{w}(0) \\big) \\right) \u0026amp;\\le \\dfrac{1}{t}\\int_{0}^tL \\big( \\dot{\\mathbf{w}}(s) \\big)ds\n\\\\ \\implies\u0026amp;\u0026amp; L\\left( \\dfrac{x-y}{t} \\right) \u0026amp;\\le \\dfrac{1}{t}\\int_{0}^tL \\big( \\dot{\\mathbf{w}}(s) \\big)ds \\end{align*} $$\n両辺に$t$を掛けて$g(y)$を足すと、次のようになる。\n$$ tL\\left( \\dfrac{x-y}{t} \\right) + g(y) \\le \\int_{0}^tL \\big( \\dot{\\mathbf{w}}(s) \\big)ds + g(y) $$\n右辺の$\\inf$が$u(x,t)$であるので、次のようになる。\n$$ tL\\left( \\dfrac{x-y}{t} \\right) + g(y) \\le u(x,t) $$\n最後に、両辺に$\\inf \\limits_{y\\in \\mathbb{R}^n}$を取ると、次を得る。\n$$ \\inf \\limits_{y \\in \\mathbb{R}^n} \\left( tL\\left( \\dfrac{x-y}{t} \\right) + g(y) \\right) \\le u(x,t) $$\nしたがって、ステップ 1. と ステップ 2. により、次が成立する。\n$$ \\begin{equation} u(x,t) = \\inf \\limits_{y \\in \\mathbb{R}^n} \\left( tL\\left( \\dfrac{x-y}{t} \\right) + g(y) \\right) \\label{eq3} \\end{equation} $$\nステップ 3.\n$\\left\\{y_{k} \\right\\}_{k=1}^\\infty$を$\\eqref{eq3}$の最小化シーケンスminimizing sequenceとしよう。それならば、次が成立する。\n$$ \\begin{equation} tL\\left( \\dfrac{x-y_{k}}{t} \\right) + g(y_{k}) \\to u(x,t)\\in [-\\infty, \\infty) \\quad \\mathrm{as}\\ k\\to \\infty \\label{eq4} \\end{equation} $$\nまず、$\\left\\{y_{k} \\right\\}$が有界でないと仮定しよう。これが矛盾であることを確認して、$\\left\\{ y_{k} \\right\\}$が有界であることを示す。仮定により、$|y_{k}| \\to \\infty$であり、$y_{k}=0$の$k$は多くても有限個である。したがって、$y_{k}\\ne 0$を満たすものだけを集めた部分列を再び$\\left\\{ y_{k} \\right\\}$としよう。次が成立する。\n$$ \\left| \\dfrac{x-y_{k}}{t} \\right| \\to \\infty $$\nそれならば、ラグランジアン$L$の性質により、次が成立する。\n$$ a_{k}:= \\dfrac{L\\left( \\dfrac{x-y_{k}}{t}\\right)}{\\left| \\dfrac{x-y_{k}}{t}\\right|} \\to \\infty $$\nしたがって、$L\\left( \\dfrac{x-y_{l}}{t}\\right) \\to \\infty$であり、ここに定数を掛けても同じ結果を得る。\n$$ \\begin{equation} tL\\left( \\dfrac{x-y_{k}}{t}\\right) \\to \\infty \\label{eq5} \\end{equation} $$\n$g$のリプシッツ条件を再び書くと、次のようになる。\n$$ \\dfrac{|g(x)-g(y_{k})|}{|x-y_{k}|} \\le \\mathrm{Lip}(g)=C \\quad \\forall \\ k \\in \\mathbb{N} $$\nしたがって、次を得る。\n$$ g(x) -g(y_{k}) \\le C|x-y_{k}| $$\n両辺に$\\eqref{eq5}$を足すと、次のようになる。\n$$ tL\\left( \\dfrac{x-y_{k}}{t}\\right)+ g(x) -g(y_{k}) \\le C|x-y_{k}|+ tL\\left( \\dfrac{x-y_{k}}{t}\\right) \\quad \\mathrm{for\\ large}\\ k $$\n上記の式を適切に移項すると、以下のようになる。\n$$ tL\\left( \\dfrac{x-y_{k}}{t}\\right)-C|x-y_{k}| + g(x) \\le tL\\left( \\dfrac{x-y_{k}}{t}\\right) + g(y_{k}) $$\n再び書くと、次のようになる。\n$$ a_{k}|x-y_{k}| -C|x-y_{k}| + g(x) =|x-y_{k}|(a_{k}-C)+g(x) \\le tL\\left( \\dfrac{x-y_{k}}{t}\\right) + g(y_{k}) $$\n$a_{k}\\to \\infty$であり、$|x-y_{k}| \\to \\infty$であるため、左辺が$\\infty$に発散し、右辺も発散する。したがって、$u(x,t)$の定義により、$u(x,t)\\to \\infty$である。これは$\\eqref{eq4}$に矛盾するため、$\\left\\{ y_{k} \\right\\}$は有界である。\n$\\left\\{ y_{k} \\right\\}$が有界であるため、$y_{k} \\to y_{0}$と仮定しよう。すると、次が成立する。\n$$ tL \\left( \\dfrac{x-y_{k}}{t} \\right)+g(y_{k}) \\to tL \\left( \\dfrac{x-y_{0}}{t}\\right)+g(y_{0}) =\\min\\limits_{y \\in \\mathbb{R}^n}\\left( tL \\left( \\dfrac{x-y}{t}\\right)+g(y) \\right) $$\nそれならば、$\\eqref{eq4}$により、次が成立する。\n$$ tL\\left( \\dfrac{x-y_{k}}{t} \\right) + g(y_{k}) \\to u(x,t)\\in [-\\infty, \\infty) \\quad \\mathrm{as}\\ k\\to \\infty $$\nしたがって、次を得る。\n$$ u(x,t) = \\min \\limits_{y \\in \\mathbb{R}^n} \\left( tL\\left( \\dfrac{x-y}{t} \\right) +g(y) \\right) $$\n■\nローレンス・C・エヴァンス, 偏微分方程式 (第2版, 2010年), p122-124\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1174,"permalink":"https://freshrimpsushi.github.io/jp/posts/1174/","tags":null,"title":"ホップ・ラックス・フォーミュラ"},{"categories":"해석개론","contents":"定義 $\\mathbb{R}$ の部分集合 $E \\ne \\emptyset$、関数 $f : E \\to \\mathbb{R}$ および関数列 $\\left\\{ f_{n} : E \\to \\mathbb{R} \\right\\}_{n=1}^{\\infty}$ を定義しよう。全ての $\\varepsilon \u0026gt; 0$ に対し、$n \\ge N \\implies | f_{n} (x) - f(x) | \u0026lt; \\varepsilon$ を満たす $N \\in \\mathbb{N}$ が存在するならば、$E$ において $f_{n}$ は $f$ に一様収束uniformly convergenceすると言い、以下のように示される。\n$$ f_n \\rightrightarrows f $$\nまたは\n$$ f_{n} \\overset{\\text{unif}}{\\to} f $$\nまたは\n$$ f_{n} \\to f \\quad \\text{uniformly} $$\n説明 関数列 $f_{n}$ が実際に 関数 $f$ に収束するかまで気にする一様収束は、関数値が収束することだけを気にする点収束と違う概念である。一様収束する関数列はより強い条件がついて、それだけ多くの性質を持つ。\n逆に言えば、数学者たちが研究するために最低限「これくらいはあるべきだ」と考える常識的な性質を持たせるために強い条件を与えたのが一様収束である。点収束する関数列と違い、一様収束する関数列では次のように $f_{n}$ の性質が $f$ まで保持される。\n定理 $E$ において $f_{n}$ が $f$ に一様収束するとしよう。\n(a) 連続性: $f_{n}$ が $x_{0} \\in E$ で連続なら、$f$ も $x_{0} \\in E$ で連続である。\n(b) 微分可能性: $f_{n}$ が $E = (a,b)$ で微分可能であり、$f_{n} ' $ が $E$ で一様収束するなら、$f$ も $E$ で微分可能で、\n$$ \\lim_{n \\to \\infty} {{ d } \\over { dx }} f_{n} (x) = {{ d } \\over { dx }} \\left( \\lim_{n \\to \\infty} f_{n} (x) \\right) $$\n(c) 積分可能性: $f_{n}$ が $E = [a,b]$ で積分可能なら、$f$ も $E$ で積分可能で、\n$$ \\lim_{n \\to \\infty} \\int_{a}^{b} f_{n} (x) dx = \\int_{a}^{b} \\left( \\lim_{n \\to \\infty} f_{n} (x) \\right) dx $$\n$\\int_{a}^{b}$ と $\\displaystyle {{ d } \\over { dx }}$ が $\\displaystyle \\lim_{n \\to \\infty}$ を自由に移動できることは非常に望ましい性質だ。なぜそれが良いのかと問われれば、その問い自体が答えであるようなものだ。数学以外の分野では、関数列が現れても一様収束のような概念を考慮せず当たり前のように一様数列の性質を使うケースが結構あるが、もし一様収束性がなくなってそのような操作を使えなくなると、地獄のような状況が展開されるだろう。\n参照 関数列の点収束と一様収束の違い ","id":1154,"permalink":"https://freshrimpsushi.github.io/jp/posts/1154/","tags":null,"title":"関数列の一様収束"},{"categories":"해석개론","contents":"定義 $\\mathbb{R}$ の部分集合 $E \\ne \\emptyset$ に対して関数 $f : E \\to \\mathbb{R}$ を定義しよう。関数列 $\\left\\{ f_{n} : E \\to \\mathbb{R} \\right\\}_{n=1}^{\\infty}$ が各 $x \\in E$ について $f(x) = \\lim \\limits_{n \\to \\infty} f_{n} (X)$ を満たす場合、$E$ で $f_{n}$ に 逐点収束pointwise convergenceすると言い、以下のように表記される。\n$$ f_{n} \\to f $$\n解説 上の定義を ε-δ 論法で書き直すと、次の必要十分条件が得られる。\nすべての $\\varepsilon \u003e 0$ と $x \\in E$ に対して $n \\ge N \\implies | f_{n} (x) - f(x) | \u003c \\varepsilon$ を満たす $N \\in \\mathbb{N}$ が存在する。\r数列は単に「定義域が $\\mathbb{N}$ である関数」に過ぎないので、その値域が関数の集合であっても全く問題なく、関数列 $\\left\\{ f_{n} \\right\\}_{n \\in \\mathbb{N}}$ のような恐ろしい存在を思い浮かべることができる。まだ数列の概念を「$n$ が増加するにつれて数直線上で動く点」と大雑把に考えているなら、受け入れがたいだろう。\n新しい数列が現れたことで、新しい収束についても話さないわけにはいかない。もちろん、逐点で収束するという概念は実際そんなに難しくない。なぜなら、$E$ で一点以上の例外を許して収束するなら、それは$E$ での収束とは言えないからだ。しかし、このように常識的な「収束」をわざわざ「逐点収束」と呼ぶ理由は何だろう？\nその理由は明らかに、逐点収束が関数それ自体の収束を議論するにはまだ不十分であるためだ。実際、逐点収束はこれよりも良い収束と対比して「十分に良くない収束」を言うためにある言葉とも言える。率直に言って、$f_{n} (x)$ というのも具体的な $x_{0}$ を一つ固定すれば $a_{n} := f_{n} (x_{0} )$ のように現れるので、わざわざ関数列という概念を考える必要もない。\n次は、$E$ で $f_{n}$ が $f$ に逐点収束したとき、元の $f_{n}$ の性質を保持しない例である。\n定理 $E$ で $f_{n}$ が $f$ に逐点ごとに収束するとする。\n(a) $f_{n}$ が微分可能であっても、$f$ が微分可能であるわけではない。\n(b) $f_{n}$ が積分可能であっても、$f$ が積分可能であるわけではない。\n(c) $f_{n}, f$ が微分可能であっても、$\\lim \\limits_{n \\to \\infty} \\dfrac{d}{dx} f_{n} (x) = \\dfrac{d}{dx} \\left( \\lim \\limits_{n \\to \\infty} f_{n} (x) \\right)$ が成立するわけではない。\n(d) $f_{n}, f$ が積分可能であっても、$\\displaystyle \\lim \\limits_{n \\to \\infty} \\int_{a}^{b} f_{n} (x) dx = \\int_{a}^{b} \\left( \\lim \\limits_{n \\to \\infty} f_{n} (x) \\right) dx$ が成立するわけではない。\n特に (a) は連続性も保持されない例でもある。\n証明 反例(a) $E = [0,1]$ で以下のように $f_{n} , f$ を定義しよう。\n$$ \\begin{align*} f_{n} (x) \u0026amp;:= x^{n} \\\\ f(x) \u0026amp;:= \\begin{cases} 0 \u0026amp;, 0 \\le x \u0026lt; 1 \\\\ 1 \u0026amp;, x=1 \\end{cases} \\end{align*} $$\n明らかに$E$ で逐点ごとに $f_{n} \\to f$ に収束する。しかし、$f_{n}$ は $[0,1]$ で微分可能だが、$f$ は $x=1$ で連続ではないので微分可能ではない。\n■\n反例(b) $E = [0,1]$ で以下のように $f_{n} , f$ を定義しよう。\n$$ \\begin{align*} f_{n} (x) \u0026amp;:= \\begin{cases} 1 \u0026amp;, x = {{ p } \\over { m }} , p \\in \\mathbb{Z} , m \\in \\left\\{ 1 , \\cdots , n \\right\\} \\\\ 0 \u0026amp;, \\text{otherwise} \\end{cases} \\\\ f(x) \u0026amp;:= \\begin{cases} 1 \u0026amp;, x \\in \\mathbb{Q} \\\\ 0 \u0026amp;, \\text{otherwise} \\end{cases} \\end{align*} $$\n$f_{n}$ の設定は少し複雑だが、$f_{1} (x)$ は $ x \\in \\left\\{ 0 , 1 \\right\\}$ でのみ$1$ であり、$f_{2} (x)$ は $\\displaystyle x \\in \\left\\{ 0 , {{ 1 } \\over { 2 }} , 1 \\right\\}$ でのみ$1$ であり、$f_{3} (x)$ は$x \\in \\left\\{ 0 , {{ 1 } \\over { 3 }} , {{ 1 } \\over { 2 }} , {{ 2 } \\over { 3 }} , 1 \\right\\}$ でのみ$1$ である。この方法で$n$ を増やしていくと、最終的にはすべての$x \\in \\mathbb{Q}$ でのみ$1$ になるはずで、それ故に$E$ で逐点ごとに$f_{n} \\to f$ に収束することがわかる。しかし、$f_{n}$ は $[0,1]$ で積分可能だが、ディリクレ関数の$f$ は積分可能ではない。\n■\n反例(c) $E = [0,1]$ で以下のように $f_{n} , f$ を定義しよう。\n$$ \\begin{align*} f_{n} (x) \u0026amp;:= {{ x^{n} } \\over { n }} \\\\ f(x) \u0026amp;:= 0 \\end{align*} $$\n明らかに$E$ で逐点ごとに$f_{n} \\to f$ に収束し、それぞれの導関数は\n$$ \\begin{align*} f\u0026rsquo;_{n} (x) =\u0026amp; x^{n-1} \\\\ f '(x) =\u0026amp; 0 \\end{align*} $$\nのように求められる。しかし$x=1$ で\n$$ 1 = \\lim \\limits_{n \\to \\infty} \\dfrac{d}{dx} f_{n} (1) \\ne \\dfrac{d}{dx} \\left( \\lim \\limits_{n \\to \\infty} f_{n} (1) \\right) = 0 $$\n■\n反例(d) $E = [0,1]$ で以下のように $f_{n} , f$ を定義しよう。\n$$ \\begin{align*} f_{1} (x) \u0026amp;:= 1 \\\\ f_{n} (x) \u0026amp;:= \\begin{cases} n^2 x \u0026amp;, 0 \\le x \u0026lt; {{ 1 } \\over { n }} \\\\ 2n - n^2 x \u0026amp;, {{ 1 } \\over { n }} \\le x \u0026lt; {{ 2 } \\over { n }} \\\\ 0 \u0026amp;, {{ 2 } \\over { n }} \\le x \\le 1 \\end{cases} \\\\ f(x) \u0026amp;:= 0 \\end{align*} $$\n$f_{n}$ は複雑に見えるが、上の図を見ると非常にシンプルであり、$E$ で逐点ごとに $f_{n} \\to f$ に収束していることが分かる。ここで、$\\displaystyle \\int_{0}^{1} f_{n} (x) dx$ は三角形の内部の面積と同じで、高さが$n$、底辺の長さが${{ 2 } \\over { n }}$ であるため、$n$ が何であれ常に$1$ に等しい。しかし、\n$$ \\int_{0}^{1} f(x) dx = \\int_{0}^{1} 0 dx = 0 $$\nなので、\n$$ 1 = \\lim \\limits_{n \\to \\infty} \\int_{0}^{1} f_{n} (x) dx \\ne \\int_{0}^{1} \\left( \\lim \\limits_{n \\to \\infty} f_{n} (x) \\right) dx = 0 $$\n■\n参照 関数列の逐点収束と一様収束の違い ","id":1148,"permalink":"https://freshrimpsushi.github.io/jp/posts/1148/","tags":null,"title":"関数列の各点収束"},{"categories":"매트랩","contents":"方法 MatlabにはExcelのデータを読み込む機能がある。まずはホームメニューからデータの取り込みをクリックする。\n読み込みたいデータが保存されているExcelファイルを選択する。\nそうすると、読み込むデータを選択できる。最初は自動的に選択されている。確認して、「選択した項目を取り込む」を押せばいい。\n「選択した項目を取り込む」で「データを取り込む」をクリックする。\nすると、ExcelファイルのデータがExcelファイルと同じ名前を持った変数に入力される。しかし、これは配列ではなくテーブルなので、table2array()を使用して配列に変換してから使う必要がある。読み込んだExcelファイルのタイトルがXの時の例コードと実行結果は下記のとおりだ。\nX1=table2array(X) 関連項目 Matlabで計算したデータをExcelファイルに保存する方法 ","id":1163,"permalink":"https://freshrimpsushi.github.io/jp/posts/1163/","tags":null,"title":"MATLABでExcelのデータをインポートする方法"},{"categories":"편미분방정식","contents":"ハミルトン方程式を得る方法は二つある。一つはオイラー-ラグランジュ方程式から得るもので、もう一つはこの記事で紹介するハミルトン・ヤコビ方程式の特性方程式から得る方法だ。\n定義1 以下の偏微分方程式を一般ハミルトン・ヤコビ方程式と呼ぶ。\n$$ G(Du, u_{t}, u, x, t)=u_{t}+H(Du, x)=0 $$\n$t \u0026gt;0 \\in \\mathbb{R}$ $x \\in \\mathbb{R}^{n}$ $u : \\mathbb{R}^{n} \\to \\mathbb{R}$ ここで、微分演算子$D$はマルチインデックス表記に従い、常に空間変数$x$に対する微分とする。すなわち$D=D_{x}$であり、$Du=D_{x}u=(u_{x_{1}}, \\cdots, u_{x_{n}})$である。そして$H : \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}$をハミルトニアンと呼ぶ。\n特性方程式 便宜上$H \\in C^{\\infty} \\big(\\mathbb{R}^{n} \\times (0,\\infty) \\big)$とする。そして上記のようなハミルトン・ヤコビ方程式が与えられている。このとき、式を簡単にするために時空間変数を一つにまとめて$y$と表す。\n$$ y=(x,t)=(x_{1}, \\cdots, x_{n}, t) $$\nまた$u$の時間微分、空間微分も$q$で一度に表す。\n$$ \\begin{align*} q \u0026amp;=q(Du, u_{t}) =q(u_{x_{1}}, u_{x_{2}},\\dots, u_{x_{n}}, u_{t}) \\\\ \u0026amp;= (p, p_{n+1}) =(p_{1}, p_{2}, \\dots, p_{n}, p_{n+1}) \\end{align*} $$\n最後に$z=u$とすると、ハミルトン・ヤコビ方程式は以下のように表される。\n$$ \\begin{equation} G(q, z, y)=p_{n+1}+H(p, x)=0 \\quad \\forall (q, z, y)\\in\\mathbb{R}^{n+1}\\times \\mathbb{R} \\times \\big( \\mathbb{R}^n\\times (0, \\infty) \\big) \\label{eq1} \\end{equation} $$\n$G$の微分を求めると、それぞれ次のようになる。\n$$ \\begin{align} D_{q} G(q, z, y) \u0026amp;= (G_{p_{1}}, \\cdots, G_{p_{n+1}})=\\big(H_{p_{1}}(p,x), \\dots , H_{p_{n}}(p,x), 1\\big)=\\big( D_{p} H(p,x), 1\\big) \\label{eq2} \\\\ D_{z} G(q, z, y) \u0026amp;= G_{z}=0 \\label{eq3} \\\\ D_{y} G(q, z, y) \u0026amp;= \\big( G_{y_{1}}, \\cdots, G_{y_{n+1}} \\big)=\\big( H_{x_{1}}(p,x), \\cdots, H_{x_{n}}(p,x), H_{t}(p,x) \\big) =\\big( D_{x}H (p,x), 0\\big) \\label{eq4} \\end{align} $$\nまた$G(q,z,y)$の特性方程式は以下のようである。\n$$ \\left\\{ \\begin{align*} \\dot{q}(s) \u0026amp;= -D_{y} G\\big(q(s), z(s), y(s) \\big)-D_{z} G\\big(q(s), z(s), y(s) \\big)q(s) \\\\ \\dot{z}(s) \u0026amp;= D_{q} G\\big(q(s), z(s), y(s) \\big) \\cdot q(s) \\\\ \\dot{y}(s) \u0026amp;= D_{q} G\\big(q(s), z(s), y(s) \\big) \\end{align*} \\right. $$\nすると$\\dot{q}(s)$は以下のようになる。\n$$ \\begin{align*} \\dot{ q}(s) \u0026amp;= -D_{y} G\\big(q(s), z(s), y(s) \\big)-D_{z} G\\big(q(s), z(s), y(s) \\big)q(s) \\\\ \u0026amp;= -D_{y} G\\big(q(s), z(s), y(s) \\big) \\\\ \u0026amp;=- (D_{x} H(p,x), 0) \\end{align*} $$\n二番目の等号は$\\eqref{eq2}$によるものであり、三番目の等号は$\\eqref{eq4}$によるものである。$q=(p, p_{n+1})$であるため、$\\dot{q}$の各成分は以下のようになる。\n$$ \\begin{align*} \\dot{p}^{i}(s) \u0026amp;= -H_{x_{i}} \\big( p(s), x(s) \\big) \u0026amp;( i=1,\\dots,n) \\\\ \\dot{p}^{n+1}(s) \u0026amp;= 0 \\end{align*} $$\n$\\dot{z}(s)$は以下のようになる。\n$$ \\begin{align*} \\dot{z}(s) \u0026amp;= D_{q} G\\big(q(s), z(s), y(s) \\big) \\cdot q(s) \\\\ \u0026amp;= \\Big( D_{p}H\\big(p(s), x(s) \\big), 1 \\Big)\\cdot\\big( p(s), p_{n+1}(s) \\big) \\\\ \u0026amp;= D_{p} H\\big( p(s), x(s)\\big)\\cdot p(s) +p_{n+1}(s) \\\\ \u0026amp;= D_{p} H\\big( p(s), x(s)\\big)\\cdot p(s) -H\\big( p(s), x(s)\\big) \\end{align*} $$\n二番目の等号は$\\eqref{eq2}$によるもので、四番目の等号は$\\eqref{eq1}$によるものである。$\\dot{y}(s)$は以下のようになる。\n$$ \\begin{align*} \\dot{y}(s) \u0026amp;= D_{q}G\\big(q(s), z(s), y(s) \\big) \\\\ \u0026amp;= \\big(D_{p}H(p,x), 1 \\big) \\end{align*} $$\n$y=(x,t)$であるため、$\\dot{y}=(\\dot{x}, \\dot{t})$の各成分は以下のようになる。\n$$ \\begin{cases} \\dot{x}(s) = D_{p}H\\big( p(s), x(s) \\big) \\\\ \\dot{t}(s)=1 \\end{cases} $$\n上記の結果から、$s$を$t$と同じものと考えることができる。ここまで計算したものを総合して、ハミルトン・ヤコビ方程式の特性方程式を次のように得る。\n$$ \\begin{align*} \\dot{p}(s) \u0026amp;= -D_{x}H \\big( p(s), x(s) \\big) \\\\ \\dot{z}(s) \u0026amp;= D_{p} H\\big( p(s), x(s)\\big)\\cdot p(s) -H\\big( p(s), x(s)\\big) \\\\ \\dot{x}(s) \u0026amp;= D_{p}H\\big( p(s), x(s) \\big) \\end{align*} $$\nここで特に、最初と三番目の式をまとめてハミルトン方程式と呼ぶ。\n■\nLawrence C. Evans, Partial Differential Equations (2nd Edition, 2010), p113-114\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1162,"permalink":"https://freshrimpsushi.github.io/jp/posts/1162/","tags":null,"title":"ハミルトン-ヤコビ方程式とハミルトニアン方程式"},{"categories":"편미분방정식","contents":"定義1 ラグランジアンLagrangian\nスムース関数 $L : \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}$が与えられたとしよう。これをラグランジアンと呼び、以下のように表記する。\n$$ L = L(v,x)=L(v_{1}, \\dots, v_{n}, x_{1}, \\dots, x_{n}) \\quad v,x\\in \\mathbb{R}^{n} \\\\ D_{v}L = (L_{v_{1}}, \\dots, L_{v_{n}}), \\quad D_{x}L = (L_{x_{1}}, \\dots, L_{x_{n}}) $$\n変数を$v, x$と書く理由は、実際に物理学で各変数が速度と位置を意味するからだ。\nアクションaction、許容クラスadmissible class\n固定された二点$x,y \\in \\mathbb{R}^{n}$と時間$t\u0026gt;0$に対して、以下のように定義される汎関数$I$をアクションと言う。\n$$ I[ \\mathbf{w}(\\cdot)] := \\int_{0}^tL(\\dot{\\mathbf{w}}(s), \\mathbf{w}(s) ) ds \\quad \\left( \\dot{　}=\\dfrac{d}{ds}\\right) $$\nこの際、関数$\\mathbf{w}(\\cdot)=\\big( w^1(\\cdot), \\cdots, w^n(\\cdot) \\big)$は、以下のように定義される許容クラス$\\mathcal{A}$の要素である。\n$$ \\mathcal{A} := \\left\\{ \\mathbf{w}(\\cdot) = \\in C^2 \\big([0,t];\\mathbb{R}^n \\big) \\ \\big| \\ \\mathbf{w}(0)=y, \\mathbf{w}(t)=x\\right\\} $$\nつまり、$\\mathcal{A}$は時間が$0$から$t$まで流れる間、位置が$y$から始まり$x$で終わる、2回連続的に微分可能な全ての経路を集めた集合を意味する。\n説明 変分法calculus of variationsの目的は、アクション$I$の積分値が最小になるような$\\mathbf{x} \\in \\mathcal{A}$を見つけることである。この時の$\\mathbf{x}$を$I$の最小化器minimiserと呼ぶ。\n$$ I[ \\mathbf{x} (\\cdot) ] = \\inf_{\\mathbf{w}(\\cdot)\\in \\mathcal{A}} I[\\mathbf{w}(\\cdot)] $$\nこのような$\\mathbf{x}$を求める理由は、ラグランジアンのアクションを最小化する経路が実際に物体が運動する経路であるからである。つまり、物体の運動について知りたいからであり、本質的には$F=ma$を解くことと同じだ。古典力学では、ラグランジアンは具体的に運動エネルギーとポテンシャルエネルギーの差として与えられる。\n最小化器の判定に関して、以下の定理がある。\n定理 $\\mathbf{x}(\\cdot) \\in \\mathcal{A}$をアクション$I$の最小化器と仮定しよう。すると、$\\mathbf{x}(\\cdot)$は以下の式を満たす。\n$$ -\\dfrac{d}{ds} \\Big[ D_{v}L\\big( \\dot{\\mathbf{x}}(s), \\mathbf{x}(s) \\big) \\Big] + D_{x}L\\big( \\dot{\\mathbf{x}}(s), \\mathbf{x}(s)\\big)=0 \\quad (0 \\le s \\le t) $$\nこの式をオイラー-ラグランジュ方程式Euler-Lagrange equationsと呼ぶ。\n注意すべき点は、最小化器はオイラー-ラグランジュ方程式を満たすが、オイラー-ラグランジュ方程式を満たすからといって最小化器であるわけではないことだ。最小値を持つ点で微分すると$0$になるが、微分して$0$になる点が最小値を持つわけではないのと同じだ。このような意味で、オイラー-ラグランジュ方程式を満たす$\\mathbf{x}(\\cdot) \\in \\mathcal{A}$を$I$の臨界点critical pointと呼ぶ。よって、最小化器は臨界点だが、臨界点であるからといって最小化器であるわけではない。\n証明 $\\mathbf{x} \\in \\mathcal{A}$をアクション$I$の最小化器と仮定しよう。\nステップ 1.\n関数$\\mathbf{y} : [0,t] \\to \\mathbb{R}^{n}, \\mathbf{y}(\\cdot) = (y^1(\\cdot), \\cdots, y^n(\\cdot) )$が以下の式を満たすスムース関数であるとしよう。\n$$ \\begin{equation} \\mathbf{y}(0)=\\mathbf{y}(t)=\\mathbf{0} \\label{eq1} \\end{equation} $$\nそして任意の$\\tau \\in \\mathbb{R}$に対して$\\mathbf{w}(\\cdot)$を以下のように定義しよう。\n$$ \\mathbf{w}(\\cdot) : = \\mathbf{x}(\\cdot) + \\tau \\mathbf{y}(\\cdot) \\in \\mathcal{A} $$\nすると、$\\mathbf{w}$は$\\mathbf{x}$と同じ始点と終点の値を持ち、その間で$\\tau \\mathbf{y}(\\cdot)$だけ異なる経路を持つ。また、$\\mathbf{x}(\\cdot)$は$I$の最小化器であるため、以下の式が成立する。\n$$ I[\\mathbf{x}(\\cdot)] \\le I[\\mathbf{w}(\\cdot)]=I[\\mathbf{x}(\\cdot) + \\tau \\mathbf{y}(\\cdot)] =: i(\\tau) $$\nまた、関数$i$は最小化器の定義により$\\tau=0$で最小値を持つ。従って、$i$の微分が存在すれば、それは$i^{\\prime}(0)=0$である。\nステップ 2.\n上で定義した通り、$i$は以下の通りである。\n$$ i(\\tau) = \\int_{0} ^t L\\big( \\dot{\\mathbf{x}}(s) + \\tau \\dot{\\mathbf{y}}(s), \\mathbf{x}(s)+ \\tau \\mathbf{y}(s) \\big)ds $$\n$L$、$\\mathbf{y}$はスムース関数であるため、$i$の微分を行うと以下のようになる。\n$$ i^{\\prime}(\\tau) = \\int_{0}^t \\sum_{i=1}^{n} \\left[ L_{v_{i}} ( \\dot{\\mathbf{x}} + \\tau \\dot{\\mathbf{y}}, \\mathbf{x}+ \\tau \\mathbf{y} )\\dot{y}^i + L_{x_{i}} ( \\dot{\\mathbf{x}} + \\tau \\dot{\\mathbf{y}}, \\mathbf{x}+ \\tau \\mathbf{y} ) y^i \\right] ds $$\n$\\tau=0$を代入すると、**ステップ 1.**の結果により以下を得る。\n$$ 0=i^{\\prime}(0) = \\int_{0}^t \\sum_{i=1}^{n} \\left[ L_{v_{i}} ( \\dot{\\mathbf{x}} , \\mathbf{x})\\dot{y}^i + L_{x_{i}} ( \\dot{\\mathbf{x}} , \\mathbf{x} )y^i \\right] ds $$\n$L_{v_{i}}\\dot{y}^i$の各項に部分積分を適用すると、仮定$\\mathbf{y}(0)=\\mathbf{y}(t)=\\mathbf{0}$によって以下を得る。\n$$ \\int_{0}^t L_{v_{i}}\\dot{y}^i ds = \\left. L_{v_{i}}y^i \\right]_{0}^t- \\int_{0}^t \\dfrac{d}{ds}L_{v_{i}}y^i=\\int_{0}^t-\\dfrac{d}{ds} L_{v_{i}}y^i $$\n従って、以下の式が成立する。\n$$ 0=i^{\\prime}(0) = \\sum_{i=1}^{n} \\int_{0}^t \\left[ -\\dfrac{d}{ds} L_{v_{i}} ( \\dot{\\mathbf{x}} , \\mathbf{x})\\ + L_{x_{i}} ( \\dot{\\mathbf{x}} , \\mathbf{x} ) \\right]y^i ds $$\n**ステップ 2.**の結果は、$\\eqref{eq1}$を満たす全てのスムース関数$\\mathbf{y} : [0,t] \\to \\mathbb{R}^n$に対して成り立つ。よって、括弧内の値は$0$でなければならない。従って、以下が成立する。\n$$ -\\dfrac{d}{ds} L_{v_{i}}( \\dot{\\mathbf{x}}, \\mathbf{x} ) +L_{x_{i}}( \\dot{\\mathbf{x}}, \\mathbf{x}) =0 $$\n■\nこの結果から、ハミルトン方程式を導き出せる。\n参照 物理学における最小作用の原理 Lawrence C. Evans, Partial Differential Equations (第2版, 2010), p115-117\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1157,"permalink":"https://freshrimpsushi.github.io/jp/posts/1157/","tags":null,"title":"偏微分方程式におけるラグランジアンとオイラー・ラグランジュ方程式"},{"categories":"수치해석","contents":"定義 $f : [a,b] \\to \\mathbb{R}$が$[a,b]$で積分可能であり、$[a,b]$を$\\displaystyle h:= {{b-a} \\over {n}}$の間隔で$a = x_{0} \u0026lt; \\cdots \u0026lt; x_{n} = b$のようなノードポイントに分割したとする。次のように定義される数値積分オペレーター$I_{n}^{1}$を台形則と呼ぶ。 $$ I_{n}^{1} (f) := \\displaystyle \\sum_{k=1}^{n} {{h} \\over {2}} \\left( f(x_{k-1}) + f(x_{k} ) \\right) $$\n定理 $f \\in C^2 [a,b]$とする。台形則のエラー$E_{1}^{1}$とアシンプトティックエラー$\\tilde{E}_{n}^{1}$は以下の通りである。\n[1]: $$E_{1}^{1} (f) = - {{1} \\over {12}} h^{3} f '' ( \\xi )$$ [2]: $$\\tilde{E}_{n}^{1} (f) = - {{ h^2 } \\over {12}} [ f '(b) - f '(a) ]$$ 説明 $I_{n}^{1} (f)$を解いてみると次のようになる。 $$ I_{n}^{1} (f) = h \\left[ {{1} \\over {2}} f(x_{0}) + f ( x_{1} ) + \\cdots + f ( x_{n-1} ) + {{1} \\over {2}} f(x_{n} ) \\right] $$ 台形則は、定積分$\\displaystyle I (f) = \\int_{a}^{b} f(x) dx$の数値積分を得るための最もシンプルな方法の一つで、区分求積法を知っていればすぐに思い浮かぶ方法でもある。\n証明 1 [1] 戦略：台形は与えられた関数の線形補間であるため、多項式補間の性質を利用することができる。\n$$ I_{1}^{1} (f) := \\left( {{ b - a } \\over { 2 }} \\right) [ f(a) + f(b) ] $$ これは、区間$[a,b]$で$f$を線形補間して、その関数の積分値を$I(f)$に近似したものと見なせる。それならば、実際の$I(f)$と$I_{1}^{1} (f)$の誤差$E_{n}^{1} (f)$は、ある$\\xi \\in [a,b]$に対して次のように計算される。\n多項式補間:\n[4] 実際の関数との誤差: $(n+1)$回微分可能な$f : \\mathbb{R} \\to \\mathbb{R}$とある$\\xi \\in \\mathscr{H} \\left\\{ x_{0} , \\cdots , x_{n} \\right\\}$に対して、$f$の多項式補間$p_{n}$はある$t \\in \\mathbb{R}$に対して次を満たす。 $$ f(t) - p_{n} (t) = {{ (t - x_{0}) \\cdots (t - x_{n}) } \\over { (n+1)! }} f^{(n+1)} ( \\xi ) $$ $$ \\begin{align*} E_{1}^{1} (f) :=\u0026amp; I(f) - I_{1}^{1} (f) \\\\ =\u0026amp; \\int_{a}^{b} \\left[ f(x) - {{ f(b) ( x - a ) - f(a) (x - b) } \\over { b - a }} \\right] dx \\\\ =\u0026amp; \\int_{a}^{b} \\left[ f(x) - p_{1} (x) \\right] dx \\\\ =\u0026amp; {{1} \\over {2}} f '' ( \\xi ) \\int_{a}^{b} (x-a) (x-b) dx \\\\ =\u0026amp; \\left[ {{1} \\over {2}} f '' ( \\xi ) \\right] \\left[ - {{1} \\over {6}} (b-a)^{3} \\right] \\\\ =\u0026amp; - {{1} \\over {12}} (b-a)^{3} f '' ( \\xi ) \\\\ =\u0026amp; - {{1} \\over {12}} h^{3} f '' ( \\xi ) \\end{align*} $$\n■\n[2] 戦略：リーマン和を導出すれば、その次は微積分学の基本定理によって自然に演繹される。\n定理[1]により、実際の$I(f)$と$I_{n}^{1} (f)$の誤差は、ある$\\xi_{k} \\in [x_{k-1}, x_{k} ]$に対して次のように計算される。 $$ \\begin{align*} \\displaystyle E_{n}^{1} (f) =\u0026amp; I (f) - I_{n}^{1} (f) \\\\ =\u0026amp; \\sum_{k=1}^{n} \\left( - {{ h^3 } \\over { 12 }} f '' ( \\xi_{k} ) \\right) \\end{align*} $$ これに対して $$ \\begin{align*} \\lim_{n \\to \\infty} {{ E_{n}^{1} (f) } \\over { h^2 }} =\u0026amp; \\lim_{n \\to \\infty} {{1} \\over {h^2}} \\sum_{k=1}^{n} \\left( - {{ h^3 } \\over { 12 }} f '' ( \\xi_{k} ) \\right) \\\\ =\u0026amp; - {{ 1 } \\over { 12 }} \\lim_{ n \\to \\infty} \\sum_{k=1}^{n} h f '' ( \\xi_{k} ) \\\\ =\u0026amp; - {{ 1 } \\over { 12 }} \\int_{a}^{b} f ''(x) dx \\\\ =\u0026amp; - {{ 1 } \\over { 12 }} [ f '(b) - f '(a) ] \\end{align*} $$ したがって $$ \\lim_{n \\to \\infty} {{\\tilde{E}_{n} (f) } \\over { E_{n} (f) }} = 1 $$\n$$ E_{n}^{1} (f) \\approx \\tilde{E}_{n}^{1} (f) = - {{ h^2 } \\over { 12 }} [ f '(b) - f '(a) ] $$\n■\nAtkinson. (1989). An Introduction to Numerical Analysis(2nd Edition): p253.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1130,"permalink":"https://freshrimpsushi.github.io/jp/posts/1130/","tags":null,"title":"台形則"},{"categories":"매트랩","contents":"方法 MATLABで計算したデータをExcelに整理したい時、データ量が多くない場合は、直接コピー＆ペーストすることが可能だ。しかし、上の写真のように128*128行列のデータでは、その方法では無理だ。この時は、xlswriteを使ってデータをExcelファイルに保存すればいい。\n上の写真と比較して、最後の行にxlswrite('test', Y)が追加された。Yのデータがtestという名前のExcelファイルに作成される。\n該当フォルダにExcelファイルが作成された。開いてみると、128*128行列の値が自動的に整理されている。\n参考 MATLABでExcelのデータを読み込む方法 ","id":1150,"permalink":"https://freshrimpsushi.github.io/jp/posts/1150/","tags":null,"title":"MATLABで計算したデータをExcelファイルに保存する方法"},{"categories":"매트랩","contents":"方法 コメントアウトしたい部分をドラッグして選択した後、Ctrl+Rを入力すれば、ドラッグした部分全体をコメントアウトできる。元に戻したいときは同じようにドラッグして選択し、Ctrl+Tを入力すると、各行の%が消える。\n","id":1149,"permalink":"https://freshrimpsushi.github.io/jp/posts/1149/","tags":null,"title":"MATLABで一度に複数行のコメントとコメント解除をする方法"},{"categories":"수치해석","contents":"定義 1 $f : [a,b] \\to \\mathbb{R}$が$[a,b]$で積分可能であり、$[a,b]$を$a = x_{0} \u0026lt; \\cdots \u0026lt; x_{n} = b$のようなノードポイントで分割したとする。\n積分オペレータ$I$を$\\displaystyle I(f) := \\int_{a}^{b} f(x) dx$のように定義する。 積分オペレータ$I_{n}$を$\\displaystyle I_{n} (f) := \\sum_{k=1}^{n} \\int_{x_{k-1}}^{x_{k}} f(x) dx$のように定義する。 エラー$E_{n}$を$E_{n} (f) := I (f) - I_{n} ( f )$のように定義する。 $\\displaystyle \\lim_{n \\to \\infty} {{\\tilde{E}_{n} (f) } \\over { E_{n} (f) }} = 1$を満たす$\\tilde{E}_{n}$を$E_{n}$のアシンプトティックエラーという。 説明 定積分を計算することは数値解析の目標の一つであることに誰もが異論はないだろう。実際、区分求積法のアイデアを思い起こせば、空間を可能な限り細かく分割すればするほど、積分値をよく近似できるだろうと予測できる。だから、数値解析で興味を持つ数値的積分とは、具体的にその誤差がどれくらいか、閉区間でない場合はどのように積分すべきかなどであろう。\n例示 台形則 数値積分の例として、$f \\in C^2 [a,b]$に対する台形則を見てみよう。台形則は$I(f)$に対して$f$を区間$[a,b]$でリニアインターポレーションしてその関数の積分値として$I(f)$を近似する方法である。\n$$ I_{1}^{1} (f) := \\left( {{ b - a } \\over { 2 }} \\right) [ f(a) + f(b) ] $$ この場合、実際の$I(f)$と$I_{1}^{1} (f)$の誤差$E_{1}^{1} (f)$はある$\\xi \\in [a,b]$に対して以下のように計算される。\nポリノミアルインターポレーション:\n[4] 実際の関数との誤差：$(n+1)$回微分可能な$f : \\mathbb{R} \\to \\mathbb{R}$とある$\\xi \\in \\mathscr{H} \\left\\{ x_{0} , \\cdots , x_{n} \\right\\}$に対して、$f$のポリノミアルインターポレーション$p_{n}$はある$t \\in \\mathbb{R}$に対して$\\displaystyle f(t) - p_{n} (t) = {{ (t - x_{0}) \\cdots (t - x_{n}) } \\over { (n+1)! }} f^{(n+1)} ( \\xi )$を満たす。 $$ \\begin{align*} \\displaystyle E_{1}^{1} (f) :=\u0026amp; I(f) - I_{1}^{1} (f) \\\\ =\u0026amp; \\int_{a}^{b} \\left[ f(x) - {{ f(b) ( x - a ) - f(a) (x - b) } \\over { b - a }} \\right] dx \\\\ =\u0026amp; \\int_{a}^{b} \\left[ f(x) - p_{1} (x) \\right] dx \\\\ =\u0026amp; {{1} \\over {2}} f '' ( \\xi ) \\int_{a}^{b} (x-a) (x-b) dx \\\\ =\u0026amp; \\left[ {{1} \\over {2}} f '' ( \\xi ) \\right] \\left[ - {{1} \\over {6}} (b-a)^{3} \\right] \\\\ =\u0026amp; - {{1} \\over {12}} (b-a)^{3} f '' ( \\xi ) \\end{align*} $$\n合成台形則 今、$[a,b]$を$n$個の区間に分割し、各$[x_{k-1} , x_{k} ]$で台形則で積分を近似すると考えよう。この方法を合成台形則という。 $$ I_{n}^{1} (f) := \\sum_{k=1}^{n} {{ h } \\over { 2 }} \\left( f (x_{k-1}) + f( x_{k} ) \\right) $$ では、実際の$I(f)$と$I_{n}^{1} (f)$の誤差はある$\\xi_{k} \\in [x_{k-1}, x_{k} ]$に対して以下のように計算される。 $$ \\begin{align*} \\displaystyle E_{n}^{1} (f) =\u0026amp; I (f) - I_{n}^{1} (f) \\\\ =\u0026amp; \\sum_{k=1}^{n} \\left( - {{ h^3 } \\over { 12 }} f '' ( \\xi_{k} ) \\right) \\\\ =\u0026amp; - {{ h^3 n } \\over { 12 }} \\left( {{ 1 } \\over { n }} \\sum_{k=1}^{n} f '' ( \\xi_{k} ) \\right) \\end{align*} $$ ここで、$\\displaystyle \\left( {{ 1 } \\over { n }} \\sum_{k=1}^{n} f '' ( \\xi_{k} ) \\right)$は$f\u0026rsquo;\u0026rsquo; ( \\xi_{k} )$の平均なので、中間値の定理により、ある$\\xi \\in [a,b]$が存在して$\\displaystyle \\min_{x \\in [a,b]} f ''(x) \\le f ''( \\xi ) \\le \\max_{x \\in [a,b]} f ''(x)$を満たすべきである。したがって、 $$ E_{n}^{1} (f) = - {{ (b-a) h^2 } \\over {12}} f '' (\\xi ) $$ このように誤差を直接式で見ることで、積分区間$[a,b]$が広がるほど誤差が大きくなり、$h$が小さいほど正確になることがわかる。しかし、$f\u0026rsquo;\u0026rsquo; ( \\xi ) $のために具体的にどれほど間違っているかはわからない。これを克服するために以下のようにアシンプトティックエラーを求める。 $$ \\begin{align*} \\lim_{n \\to \\infty} {{ E_{n}^{1} (f) } \\over { h^2 }} =\u0026amp; \\lim_{n \\to \\infty} {{1} \\over {h^2}} \\sum_{k=1}^{n} \\left( - {{ h^3 } \\over { 12 }} f '' ( \\xi_{k} ) \\right) \\\\ =\u0026amp; - {{ 1 } \\over { 12 }} \\lim_{ n \\to \\infty} \\sum_{k=1}^{n} h f '' ( \\xi_{k} ) \\\\ =\u0026amp; - {{ 1 } \\over { 12 }} \\int_{a}^{b} f ''(x) dx \\\\ =\u0026amp; - {{ 1 } \\over { 12 }} [ f '(b) - f '(a) ] \\end{align*} $$ 誤差$E_{n}^{1} (f)$だけを知っていても定理によれば$f\u0026rsquo;\u0026rsquo; ( \\xi )$が存在することはわかるが、具体的な数値をつけることはできなかった。しかし、今、 $$ \\tilde{E}_{n}^{1} (f) := \\lim_{n \\to \\infty} E_{n}^{1} (f) $$ とした場合、十分に大きい$n$に対して $$ E_{n}^{1} (f) \\approx - {{ h^2 } \\over { 12 }} [ f '(b) - f '(a) ] = \\tilde{E}_{n}^{1} (f) $$ であることがわかり、$f ' (a) , f '(b)$の計算は比較的簡単なので、比較的簡単かつ正確に誤差を把握することができる。しかも、ここまで来れば誤差がその程度であると終わる理由もない。計算をする前からこのように誤差が出ると予測できるならば、始めから誤差を補正することもできる。 $$ I(f) - I_{n}^{1} (f) \\approx \\tilde{E}_{n}^{1} (f) \\implies I(f) \\approx I_{n}^{1} (f) + \\tilde{E}_{n}^{1} (f) $$\n修正台形則 $I(f)$を計算するオペレータとして以下のように$CT_{n}$を定義しよう。 $$ CT_{n} (f) := h \\left[ {{1} \\over {2}} f (x_{0} ) + f (x_{1} ) + \\cdots + f (x_{n-1} ) + {{1} \\over {2}} f (x_{n} ) \\right] - {{ h^2 } \\over { 12 }} [ f '(b) - f '(a) ] $$\nこの方法を修正台形則と言う。[ 注記: 上で台形則が改良される過程を見ていれば、わざわざこれらを明確に区別する必要はない。単に台形則と言っても意味は通じる。 ]\n実装 実際に$\\displaystyle \\int_{0}^{\\pi} e^{x} \\cos (x) dx = -{{e^{\\pi} + 1} \\over {2}} \\approx - 12.0703$を$I_{n}^{1}$と$CT_{n}$で計算してみた結果は以下の通り。\n最初の列は$n$を示し、2番目の列は$\\left| I(f) - I_{n}^{1} (f) \\right|$、3番目の列は$\\left| I(f) - CT (f) \\right|$を示している。合成台形則はノードを2倍に増やすたびに誤差が$\\displaystyle {{1} \\over {4}}$倍に減少し、修正台形則は$\\displaystyle {{1} \\over {16}}$倍に減少していることが確認できる。数値だけを見ても修正台形則がはるかに優れていることは明らかだが、特に速度が非常に速い点が良い。\n以下はPythonで書かれた例示コードである。\nimport numpy as np\rdef f(x) :\rreturn np.exp(x)*np.cos(x)\rdef d(f,x,tol=10**(-8)) :\rh=1\rd1=1\rd2=0\rwhile(abs(d1-d2)\u0026gt;tol) :\rd1=d2\rd2=(f(x+h)-f(x))/h\rh=h/2\rreturn(d2)\rdef I(f,a,b,n) :\rh=(b-a)/n\rx=np.linspace(a,b,n+1)\rx=f(x)\rx[0]=x[0]/2\rx[-1]=x[-1]/2\rreturn(h*sum(x))\rdef CT(f,a,b,n) :\rh=(b-a)/n\rx=np.linspace(a,b,n+1)\rx=f(x)\rx[0]=x[0]/2\rx[-1]=x[-1]/2\rreturn(h*sum(x)-(h**2)*(d(f,b)-d(f,a))/12)\rTV=-(np.exp(np.pi)+1)/2\rprint(\u0026#34;True Value : %2.4f\u0026#34; % TV)\rprint(\u0026#34;| n | I(f) Err | CT(f) Err |\u0026#34;)\rprint(\u0026#34;=\u0026#34;*30)\rfor n in range(1,10) :\rprint(\u0026#34;| %3d | %8.2e | %8.2e |\u0026#34; % (2**n,abs(TV-I(f,0,np.pi,2**n)),abs(TV-CT(f,0,np.pi,2**n)))) アトキンソン. (1989). 数値解析入門(第2版): p249.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1128,"permalink":"https://freshrimpsushi.github.io/jp/posts/1128/","tags":null,"title":"数値積分"},{"categories":"측도론","contents":"定義1 関数$f : X \\to \\mathbb{R}$に対して$f^{+}$と$f^{-}$を以下のように定義しよう。\n$$ \\begin{align*} f^{+} (x) \u0026amp;:= \\max \\left\\{ f(x),\\ 0 \\right\\} \\\\ f^{-} (x) \u0026amp;:= \\max \\left\\{ -f(x),\\ 0 \\right\\} \\end{align*} $$\n$f^{+}$を$f$の正の部分positive partといい、$f^{-}$を$f$の負の部分negative partという。\n説明 名前のせいで混乱するかもしれないが、$f^{+}$と$f^{-}$は両方とも非負non negativeの関数だ。これらがなぜ正の部分や負の部分と呼ばれるのか、定義だけではピンとこないかもしれない。以下の図を見よう。\n図を見ればわかるが、正の部分$f^{+}$は正確に$f$の値が正の部分を表し、$f^{-}$は$f$の値が負の部分を（正として）表す。上の定義により、以下の式が成り立つことは簡単にわかる。\n$$ f=f^{+} -f^{-},\\quad |f|=f^{+}+f^{-} $$\n$$ \\begin{array}{c} f^{+}=\\frac{1}{2}(|f| + f),\\quad f^{-}=\\frac{1}{2}(|f|-f) \\end{array} $$\n定理 (1) 三つの関数$f,g,h : X \\to \\mathbb{R}$が以下の条件を満たすとする。\n$$ f(x)=g(x)-h(x), \\quad \\min \\left\\{ g(x),\\ h(x) \\right\\} \\ge 0\\ \\quad \\forall\\ x \\in X $$\nすると、次の式が成り立つ。\n$$ f^{+} (x) \\le g(x), \\quad f^{-} (x) \\le h(x) \\quad \\forall\\ x \\in X $$\n任意の関数を非負の二つの関数の差として表すとき、$f$の正の部分$f^{+}$と$f$の負の部分$f^{-}$がそれを満たす最小の関数となるという意味だ。\n(2) $f$が可測関数ならば、$f^{\\pm}$も可測である。\n証明 (1) 任意の$x$に対して$ f(x)=g(x)-h(x)$であり、$h(x) \\ge 0$なので$f(x) \\le g(x)$である。また、仮定により$0 \\le g(x)$である。$g(x)$が$f(x)$と$0$の両方よりも大きいか等しいため、二つのうち大きい方よりも大きいか等しい。したがって、次が成り立つ。\n$$ f^{+}(x) = \\max \\left\\{ f(x), 0 \\right\\} \\le g(x) $$\n任意の$x$に対して$-f(x)=h(x)-g(x)$であり、$g(x) \\ge 0$なので$-f(x) \\le h(x)$である。また、仮定により$0 \\le h(x)$である。$h(x)$が$-f(x)$と$0$の両方よりも大きいか等しいため、二つのうち大きい方よりも大きいか等しい。したがって、次が成り立つ。\n$$ f^{-}(x) = \\max \\left\\{ -f(x), 0 \\right\\} \\le h(x) $$\n■\n参照 任意の関数の絶対値を二つの非負の関数で表す方法 Robert G. Bartle, The Elements of Integration and Lebesgue Measure (1995), p10\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1145,"permalink":"https://freshrimpsushi.github.io/jp/posts/1145/","tags":null,"title":"任意の関数を二つの非負の関数として表す方法"},{"categories":"측도론","contents":"定義1 $(X, \\mathcal{E})$を可測空間としよう。集合$S_{f}(\\alpha)$を次のように定義する。\n$$ S_{f}(\\alpha):=\\left\\{ x\\in X\\ |\\ f(x) \u0026gt;\\alpha \\right\\} = f^{-1}\\left( (\\alpha, \\infty) \\right),\\quad \\forall \\alpha \\in \\mathbb{R} $$\n全ての実数$\\alpha \\in \\mathbb{R}$に対して、$S_{f}(\\alpha) \\in \\mathcal{E}$が成立するならば、拡張実数値を取る関数$f : X \\to \\overline{\\mathbb{R}}$を**$\\mathcal{E}$-可測**$\\mathcal{E}$-measurableまたは単に可測measurableという。\n説明 特に$X=\\mathbb{R}$の時は、ルベーグ可測という。関数が可測かどうかを判断する時、上の定義に合っているかを確認することになるが、その時に役立つ定理がある。\n定理 関数$f : X \\to \\overline{\\mathbb{R}}$について、以下の四つの条件は互いに同値である。\n(a) 全ての$\\alpha \\in \\mathbb{R}$に対して、$A_{\\alpha} = S_{f}(\\alpha) =\\left\\{ x\\in X : f(x) \u0026gt; \\alpha \\right\\}$ $\\in$ $\\mathcal{E}$である。 (b) 全ての$\\alpha \\in \\mathbb{R}$に対して、$B_{\\alpha}=\\left\\{ x\\in X : f(x) \\le \\alpha \\right\\}$ $\\in$ $\\mathcal{E}$である。 (c) 全ての$\\alpha \\in \\mathbb{R}$に対して、$C_{\\alpha}=\\left\\{ x\\in X : f(x) \\ge \\alpha \\right\\}$ $\\in$ $\\mathcal{E}$である。 (d) 全ての$\\alpha \\in \\mathbb{R}$に対して、$D_{\\alpha}=\\left\\{ x\\in X : f(x) \u0026lt; \\alpha \\right\\}$ $\\in$ $\\mathcal{E}$である。 証明 最初に、$A_{\\alpha}$と$B_{\\alpha}$は互いに補集合なので、σ-代数の性質**(D2)によって、(a)と(b)は同値である。同様に、(c)と(d)も同値である。したがって、(a)と(c)**が同値であることを示せば証明完了である。\n$\\sigma$-代数\n集合$X$が与えられたとする。以下の条件を満たす$X$の部分集合たちのコレクション $\\mathcal{E} \\subset \\mathcal{P}(X)$を**$\\sigma$-代数**という。\n(D1) $\\varnothing, X \\in \\mathcal{E}$ (D2) $E \\in \\mathcal{E} \\implies E^c \\in \\mathcal{E}$ (D3) $E_{k} \\in \\mathcal{E}\\ (\\forall k \\in \\mathbb{N}) \\implies \\bigcup_{k=1}^\\infty E_{k} \\in \\mathcal{E}$ (D4) $E_{k} \\in \\mathcal{E}\\ (\\forall\\ k \\in \\mathbb{N}) \\implies \\bigcap_{k=1}^\\infty E_{k} \\in \\mathcal{E}$ (a) $\\implies$　(c) 条件**（a)が成立すると仮定しよう。すると全ての$n\\in \\mathbb{N}$に対して$A_{\\alpha-\\frac{1}{n}}\\in\\mathcal{E}$が成立する。そして$C_{\\alpha}=\\bigcap_{n=1}^\\infty A_{\\alpha-\\frac{1}{n}}$である。従って、$\\sigma$-代数の定義（D3)**により$C_{\\alpha} \\in \\mathcal{E}$である。\n(c) $\\implies$　(a) 条件**（c)が成立すると仮定しよう。すると全ての$n\\in \\mathbb{N}$に対して$C_{\\alpha+\\frac{1}{n}}\\in\\mathcal{E}$が成立する。そして$A_{\\alpha}=\\bigcup_{n=1}^\\infty C_{\\alpha+\\frac{1}{n}}$である。従って、$\\sigma$-代数の定義（D3)**により$A_{\\alpha} \\in \\mathcal{E}$である。\n■\nRobert G. Bartle, The Elements of Integration and Lebesgue Measure (1995), p8\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1135,"permalink":"https://freshrimpsushi.github.io/jp/posts/1135/","tags":null,"title":"予測可能関数"},{"categories":"해석개론","contents":"整理[^1] 補助定義 $X$に対して、$A \\subset C(X)$としよう。\n異なる$x_{1}, x_{2} \\in X$について、常に$f \\in A$が存在して$f(x_{1}) \\ne f(x_{2})$を満たすなら、$A$が$X$の点を分けるSeparateと言う。 $X$がメトリック空間で、全ての$\\varepsilon \u0026gt; 0$と$f \\in C(X)$に対して$| g - f | \u0026lt; \\varepsilon$を満たす$g \\in A$が存在するなら、$A$は$C(X)$で一様に密Uniformly Denseであると言う。 $C \\left( X \\right)$は、定義域が$X$で値域が$\\mathbb{R}$の連続関数のクラスだ。 ストーン-ワイエルシュトラスの定理 $X$がコンパクトメトリック空間だとしよう。$A$が、定数関数を含む$C(X)$の代数で、$X$の点を分けるなら、$A$は$C(X)$で一様に密である。\n説明 ストーン-ワイエルシュトラスの定理は、他の関数で連続関数を近似できることを保証する。しかし、この表現は少し抽象的すぎる感じが否めない。よく知られている$1$次元の多項式に関するストーン-ワイエルシュトラスの定理は、次のステートメントとして書かれる。\nワイエルシュトラス近似定理：$f$が$[a,b]$上で連続なら、与えられた$\\epsilon \u0026gt; 0$に対して、$\\displaystyle \\max_{x \\in [a,b]} | f(x) - p (x) | \u0026lt; \\epsilon$を満たす多項式$p(x)$が存在する。\n$p(x)$が存在するという質素な表現も良いが、いま改めてその重要性を考えると、過度に控えめな感じもする。イプシロン$\\epsilon$は一種の許容誤差であるが、厳密に$\\epsilon$の数学的誇張をするならば、「どんな連続関数でも、多項式で表すことができる」と言っても良い。\n証明 戦略：決して簡単ではない。$F \\in C(X)$に対して、$A$ではなく、そのクロージャ$\\overline{A}$で具体的に$| F - G | \u0026lt; \\varepsilon$となるような$G$を見つけ出す。$G$を作るには、$\\overline{A}$は閉代数だから、良い性質を使用しなければならないし、具体的にそのような$G$を見つけた後には、$A$のシークエンスを一つだけ提示すれば終わりである。\n部分 1. $a, b \\in \\mathbb{R} , x_{1} \\ne x_{2} \\implies \\exists f \\in A : \\begin{cases} f(x_{1}) = a \\\\ f(x_{2}) = b \\end{cases}$\n$A$は$X$の点を分けるから、異なる$x_{1} , x_{2}$に対して、$g(x_{1} ) \\ne g (x_{2} )$を満たす$g \\in A$が存在する。\n代数：次の三つの条件を満たす集合$A$を$C(X)$の代数という：\n(i): $\\emptyset \\ne A \\subset C(X)$ (ii): $f,g \\in A \\implies (f+g) , fg \\in A$ (iii): $f \\in A , c \\in \\mathbb{R} \\implies cf \\in A$ $$ f(t) := a {{ g(t) - g(x_{2} ) } \\over { g(x_{1} ) - g( x_{2} ) }} + b {{ g(t) - g(x_{1} ) } \\over { g(x_{2} ) - g( x_{1} ) }} $$\n$A$は定数関数を含む代数なので、値がそれぞれ$g(x_{1}) , g(x_{2})$の定数関数も含んでおり、$a, b \\in \\mathbb{R}$に対して$f$を上述のように定義すると$f \\in A$であり、$t=x_{1} , x_{2}$を代入してみると$f(x_{1}) = a$であり$f(x_{2} ) = b$である。\n部分 2. $f_{1} ,f_{2} \\in \\overline{A} \\implies ( f_{1} \\land f_{2} ), ( f_{1} \\lor g_{2} ) \\in \\overline{A}$\n$\\land$と$\\lor$は、$f,g \\in C(X)$と$x \\in X$に対して次のことを意味する： $$ \\begin{align*} (f \\land g) (x) :=\u0026amp; \\min \\left\\{ f(x) , g(x) \\right\\} \\\\ (f \\lor g) (x) :=\u0026amp; \\max \\left\\{ f(x) , g(x) \\right\\} \\end{align*} $$\n一様閉包の性質：メトリック空間 $X$に対して、$A \\subset C(X)$としよう。$A$の全てのシーケンス$\\left\\{ f_{n} \\in A : n \\in \\mathbb{N} \\right\\}$がある$f \\in A$に対して$n \\to \\infty$のとき、$\\displaystyle | f - f_{n} | \\to 0$ならば、$A$が一様にクローズドであるとする。もし$X$がコンパクトメトリック空間で、$A$が定数関数を含みつつ、$C(X)$の一様にクローズドな代数であれば、次が成立する： $$ f,g \\in A \\implies (f \\land g), ( f \\lor g ) \\in A $$\n$A$の一様閉包$\\displaystyle \\overline{A} := \\left\\{ f \\in C(X) : \\lim_{n \\to \\infty} | f_{n} - f | = 0, f_{n} \\in A \\right\\}$を考えると、$A$が代数なので、$\\overline{A}$も代数であり、一様閉包の性質によって\n$$ f_{1} ,f_{2} \\in \\overline{A} \\implies (f_{1} \\land f_{2}), ( f_{1} \\lor f_{2} ) \\in \\overline{A} $$\n部分 3. $\\displaystyle | F - G | \u0026lt; {{\\varepsilon} \\over {2}}$\n$F \\in C(X)$と$\\displaystyle {{\\varepsilon} \\over {2}} \u0026gt; 0$が与えられたとき、$\\displaystyle | F - G | \u0026lt; {{\\varepsilon} \\over {2}}$を満たす$G \\in \\overline{A}$の存在を証明しようとする。\n部分 3-1. $\\displaystyle g_{x_{0}} ( x ) \u0026lt; F(x) + {{\\varepsilon} \\over {2}}$\n$x_{0} \\in X$を固定して、$y \\ne x_{0}$とすると、部分1によって\n$$ \\begin{align*} f_{y} (x_{0}) =\u0026amp; F ( x_{0} ) \\\\ f_{y} ( y ) =\u0026amp; F ( y ) \\end{align*} $$ 連続関数$f_{y} \\in A \\subset \\overline{A} \\subset C(X)$が存在する。$f_{y}$と$F$が連続関数であるため、 $$ V_{y} := \\left\\{ x \\in X : f_{y} (x) \u0026lt; F(x) + {{ \\varepsilon } \\over { 2 }} \\right\\} $$ はオープンセットであり、 $$ X = \\bigcup_{y \\ne x_{0}} V_{y} $$ そして、$X$がコンパクトセットであるので、 $$ X = \\bigcup_{i=1}^{N_{1}} V_{i} $$ を満たす有限の要素$y_{1} , \\cdots , y_{N_{1}} \\in X$が存在する。今、$i = 1 , \\cdots , N_{1}$について $$ \\begin{align*} f_{i} :=\u0026amp; f_{y_{i}} \\\\ g_{y_{0}} :=\u0026amp; f_{1} \\land \\cdots \\land f_{N_{1}} \\end{align*} $$ とすると、部分2により、$g_{x_{0}} \\in \\overline{A}$である。ここに$x = x_{0}$を代入してみると、 $$ \\begin{align*} g_{x_{0}} ( x_{0} ) =\u0026amp; f_{1} ( x_{0} ) \\land \\cdots \\land f_{N_{1}} ( x_{0} ) \\\\ =\u0026amp; F ( x_{0} ) \\land \\cdots \\land F ( x_{0} ) \\\\ =\u0026amp; F ( x_{0} ) \\end{align*} $$ $x \\in X$なら、$x$が$V_{y_{1}} , \\cdots , V_{y_{N_{1}}}$のいずれかに属するという意味であり、少なくとも一つの$1 \\le k \\le N_{1}$に対して $$ f_{k} (x) \u0026lt; F(x) + {{\\varepsilon} \\over {2}} $$ が成立し、$g_{x_{0}}$の定義により、全ての$i = 1, \\cdots , N_{1}$に対して$g_{x_{0}} ( x ) \\le f_{i} (x)$であるので、次を得る。 $$ g_{x_{0}} ( x ) \u0026lt; F(x) + {{\\varepsilon} \\over {2}} $$\n部分 3-2. $\\displaystyle F(x) - {{\\varepsilon} \\over {2}} \u0026lt; G(x) \u0026lt; F(x) + {{\\varepsilon} \\over {2}}$\n$\\left\\{ V_{y_{i}} \\right\\}_{i=1}^{N_{1}}$と同様に、$X$をカバーするオープンセットの有限コレクション$\\left\\{ W_{x_{i}} \\right\\}_{i=1}^{N_{2}}$を次のように定義しよう。 $$ W_{x_{i}} := \\left\\{ x \\in X : g_{x_{i}} (x) \u0026gt; F(x) - {{ \\varepsilon } \\over { 2 }} \\right\\} $$ 部分3-1と同様に、このとき\n","id":1117,"permalink":"https://freshrimpsushi.github.io/jp/posts/1117/","tags":null,"title":"ストーン-ワイエルシュトラスの定理の証明"},{"categories":"수치해석","contents":"ビルドアップ 数値計算を行う際、コンピュータが人間よりも圧倒的に速いのは事実だけれど、それが超越関数や無理数を理解しているからではない。例えば、$\\displaystyle \\sin {{ \\pi } \\over {6}} = {{1} \\over { 2 }}$を計算させる場合、三角関数の幾何学的な定義を利用して直角三角形を描き、斜辺と高さの比を求めるのではなく、多項式関数を使って級数展開し、四則演算で計算する方式だ。\nコンピュータの立場から言えば、$\\displaystyle \\sin x \\approx x - {{x^3} \\over {3!}} + {{x^5} \\over {5!}}$が実際に何を意味しているかはあまり気にしない。人間よりも四則演算が速いから、ある多項式関数の公式をそのまま使っているだけである。そして、このような多項式関数の存在は次の定理によって常に保証されている。\nストーン・ワイエルシュトラスの定理: $f$が$[a,b]$上で連続である場合、与えられた$\\epsilon \u0026gt; 0$に対して$\\displaystyle \\max_{x \\in [a,b]} | f(x) - p (x) | \u0026lt; \\epsilon$を満たす多項式関数$p(x)$が存在する。\nしかし、「コンピュータの立場」の話が出たところで、$\\displaystyle {{\\pi^{31} } \\over {31!}}$のような数の計算はコンピュータにとってもかなり難しいことが分かる。まず、分子も分母もかなり長く、計算を始める前に大きな数字を保存すること自体が一仕事で、人とは異なり計算を省略するため計算順序を変えるという発想もない。[ 注: 実際には、暗号学などの応用数学分野で連続べき乗法といったアルゴリズムを介してこのような技巧が実装されている。] さらに、「人間の立場」から見ると、テイラー級数は十分に小さい$(-h, h)$でのみ収束性が保証されることが知られている。これは、実際にテイラー公式を使用した計算がポイントワイズであるということだ。\nもちろん、テイラー展開は理論的に数学史上最も強力なツールの一つとして高く評価されるが、上述の考察から数値計算のための公式としては必ずしも良いとは言えないことが分かる1。別の言い方をすれば、$f(x)$を $$ f(x) \\approx \\sum_{k=0}^{n} a_{k} \\varphi_{n} (x) $$ のような式で計算したい場合、$\\displaystyle \\left\\{ \\varphi_{n} (x) = {{x^{n}} \\over {n!}} : n \\ge 0 \\right\\}$はあまり良いベースではないということだ。\nグラム・シュミットの過程: すべての有限次元の内積空間は正規直交基底を持つ。\n幸いなことに、多項式関数の集合$\\mathbb{Q} [ x ]$は有限次元ノルムベクトル空間を形成するため、正規直交基底を持つことは保証されている。数値解析では、特に次の性質を持つ基底に注目している。\n良い基底とは？ (i): $\\displaystyle p_{n} (x)$が$\\displaystyle \\left\\{ \\varphi_{n} (x) \\in \\mathbb{Q} [ x ] : n \\ge 0 \\right\\}$の線形結合として表され、$\\displaystyle \\max_{x \\in [a,b]} | f(x) - p_{n} (x) |$が望むほど小さくなるような$n \\in \\mathbb{N}$が存在する必要がある。 (ii): $p_{n} (x)$は$\\displaystyle p_{n} (x) = \\sum_{k=0}^{n} a_{k} \\varphi_{n} (X)$のように計算されるのが良い。つまり、誤差を減らすたびに完全に新しく計算するのではなく、新しい項を加えることによって計算するのが良い。 (iii): $| a_{n} |$が速やかに$0$に収束するのが良い。つまり、少ない項を加えても速やかに収束するのが良い。 $\\mathbb{R} [ x ]$ではなく、係数が有理数の多項式関数の集合$\\mathbb{Q} [ x ]$を考える理由は簡単だ。それは、数値計算を行う際、コンピュータは無理数を理解できないからである。\nAtkinson. (1989). An Introduction to Numerical Analysis(2nd Edition): p199.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1107,"permalink":"https://freshrimpsushi.github.io/jp/posts/1107/","tags":null,"title":"数値解析における関数の近似"},{"categories":"해석개론","contents":"定義 $S(x) : = \\sum \\limits_{k=0}^{\\infty} a_{k} ( x - x_{0} )^{k}$ を冪級数と言い、$S(x)$ の中心Centerを $x_{0}$ とする。 $S(x)$ が $|x - x_{0}| \u0026lt; R$ に対して絶対収束し、$|x - x_{0}| \u0026gt; R$ に対して発散するとき、$R$ を $S(x)$ の収束半径Radius of Convergenceと言う。 $S(x)$ が収束する最大の区間を 収束区間Interval of Convergenceと言う。 収束区間 $[c,d] \\subset (a,b)$ で $x_{0} \\in (c,d)$ を中心とする冪級数 $\\sum \\limits_{k=0}^{\\infty} a_{k} ( x - x_{0} )^{k} = f(x)$ が存在するなら、$f$ は $(a,b)$ で解析的Analyticであると言う。 すべての $n$ に対して $a_{n}=b_{n}$ が成り立てば、二つの冪級数 $\\sum \\limits_{n=0}^\\infty a_n(x-x_0) ^n$、$\\sum \\limits_{n=0}^\\infty b_n(x-x_0) ^n$ が等しいと言う。 $a_0=a_1=a_2=\\cdots=0$ ならば $\\sum \\limits_{n=0}^\\infty a_n(x-x_0)^{n}=0,\\quad \\forall x$ 説明 解析学を大学の時最初に学んでみて、なぜ「微積分学」と「解析学」が分けているのか、また解析学ではなぜそのように数列や級数にこだわるのか理解できないかもしれない。でも、興味を失わずに冪級数まで学べれば、少なくともヒントを得ることができるだろう。\nすぐに解析学を学ぶ理由が何かと聞かれたら、「難しい関数を簡単な関数に引き下ろすため」と答えてもいい。例えば、超越関数は難しいが、多項式関数は簡単だ。もし超越関数が解析的であれば、それは幸運なことだ。解析的な関数とは級数展開できる関数であり、級数展開が可能な関数とは、結局多項式の和として解くことができる関数であるという意味だ。\n冪級数は基本解析学で重要に扱われる概念であり、特に収束性に多くの条件が付いている。制約が多いだけに、良い性質も多く持っており、無限級数でありながら、いわゆる「常識的」に扱いやすい。\n整理 (a) $R := \\lim_{k \\to \\infty} {{ | a_{k} | } \\over { | a_{k+1} | }}$ が存在するなら、$R$ は $S(x)$ の収束半径である。 (b) 収束半径 $R \u0026gt; 0$ が存在するなら、$S(x)$ はすべての $x \\notin [ x_{0} - R , x_{0} + R ]$ において発散する。 (c) 収束半径 $R \u0026gt; 0$ が存在するなら、$S(x)$ はすべての $x \\in ( x_{0} - R , x_{0} + R )$ に対して絶対収束する。 (d) 収束半径 $R \u0026gt; 0$ が存在するなら、$S(x)$ はすべての $[a,b] \\subset ( x_{0} - R , x_{0} + R )$ において一様収束する。 (e) 収束半径 $R \u0026gt; 0$ が存在するなら、$S(x)$ は $( x_{0} - R , x_{0} + R )$ において連続である。 (f) 収束半径 $R \u0026gt; 0$ が存在するなら、$S(x)$ は $( x_{0} - R , x_{0} + R )$ において無限回微分可能で、 $$ S^{(k)} (x) = \\sum \\limits_{n=k}^{\\infty} {{n!} \\over {(n-k)!}} a_{n} (x - x_{0} )^{n-k} $$ (g) $S(x)$ が $[a,b]$ において収束するなら、$[a,b]$ で積分可能で、 $$ \\int_{a}^{b} S(x) dx = \\sum \\limits_{k=0}^{\\infty} a_{k} \\int_{a}^{b} (x - x_{0} )^{k} dx $$ 参照 生成関数 ","id":1090,"permalink":"https://freshrimpsushi.github.io/jp/posts/1090/","tags":null,"title":"累乗級数"},{"categories":"푸리에해석","contents":"ビルドアップ フーリエ変換を導出する過程で、逆変換の定義も同時に導出された。しかし、これは理解を助けるために簡単に説明したもので、変換式を正確に導出したわけではない。まずフーリエ逆変換は以下のようである。\n$$ \\begin{equation} f(x) =\\dfrac{1}{2\\pi} \\int \\hat{f}(\\xi) e^{i\\xi x}d\\xi \\end{equation} $$\nこの式には、$f$から$\\hat{f}$を得て、$\\hat{f}$から再び$f$を得ることができるという意味がある。これが何を意味するのか、当たり前のことではないかと思われるかもしれないが、全く当たり前ではない。例えば、微分と積分を考えてみよう。ある多項式を微分したとき、定数項という情報を失い、これは積分によって再び得ることができない。しかし、フーリエ変換は情報をそのまま保持する。$f$からフーリエ変換$\\hat{f}$を得て、この$\\hat{f}$に逆変換を施せば、$f$をそのまま得ることができる。\n$$ \\begin{equation} \\hat{f} (\\xi) := \\int f(x) e^{-i\\xi x}dx \\end{equation} $$\n上記のようにフーリエ変換を定義した場合、$(1)$がその逆変換になることは次のように示せる。\nフーリエ逆変換の定理 $f$のフーリエ変換を$\\hat{f}$として$(2)$のように定義する。$f$が積分可能で部分的に連続であると仮定する。$f$は不連続な点では以下のように定義される。\n$$ f(x)=\\dfrac{1}{2} \\big[ f(x-)+f(x+)\\big] $$\nすると、以下の式が成立する。\n$$ f(x)=\\lim \\limits_{\\epsilon \\rightarrow 0}\\dfrac{1}{2\\pi}\\int\\hat{f}(\\xi)e^{i\\xi x}e^{-\\epsilon^2\\xi^2/2}d\\xi $$\nさらに、$\\hat{f} \\in$を$L^1$と仮定すると、$f$は連続であり、\n$$ f(x) =\\dfrac{1}{2\\pi}\\int\\hat{f}(\\xi) e^{i\\xi x}d\\xi = \\mathcal{F}^{-1}[\\hat{f}] (x) $$\n結論 $\\hat{f}=\\hat{g}$と仮定すると、$f=g$である。\n証明 $$ f=\\mathcal{F}^{-1}[\\hat{f}]=\\mathcal{F}^{-1}[\\hat{g}] $$\nしたがって、\n$$ f=\\mathcal{F}^{-1}[\\hat{g}]=\\mathcal{F}^{-1}\\mathcal{F}[g]=g $$\n■\n説明 この結論を見ると、「何？ 当たり前じゃないか？」と思うかもしれないが、全く当たり前ではない。微分という操作を考えてみよう。$f(x)=x^2+1$、$g(x)=x^2+4$とする。この場合、$f^{\\prime}(x)=2x=g^{\\prime}(x)$という事実が$f(x)=g(x)$を保証しない。\n証明 戦略:\n問題を簡単に解決するために、カットオフ関数を使用する。カットオフ関数とは、乗算して極限を取ったときに、文字通りある範囲外をカットオフするような効果が発生するものを指す。原点の近くでは元の値を保持し、原点から離れるにつれて$0$に収束させるために使用される。モリファイアと似た概念である。この部分の説明が理解しにくい場合は、スキップしても構わない。いずれにせよ、このようなカットオフ関数を逆変換する式に乗算する。\n$$ \\eta (\\xi)=\\dfrac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}, \\quad \\eta_\\epsilon (\\xi)=\\dfrac{1}{\\epsilon}\\eta \\left( \\frac{\\xi}{\\epsilon} \\right)=\\dfrac{1}{\\epsilon\\sqrt{2\\pi}}e^{-\\frac{x^2}{2\\epsilon^2}} $$\nこのカットオフ関数を乗算すると、\n$$ \\dfrac{1}{2\\pi}\\int\\hat{f} (\\xi) e^{i\\xi x}e^{-\\epsilon^2\\xi^2/2}d\\xi $$\nこの式はフーリエ変換の定義により以下のようになる。\n$$ \\dfrac{1}{2\\pi}\\int \\int f(y) e^{-i\\xi y}dy e^{i\\xi x}e^{-\\epsilon^2\\xi^2/2}d\\xi $$\nフーリエ変換の定義や畳み込みなどを使用して式を変換すると、\n$$ \\begin{align*} \\dfrac{1}{2\\pi}\\int \\int f(y) e^{-i\\xi y}dy e^{i\\xi x}e^{-\\epsilon^2\\xi^2/2}d\\xi \u0026amp;= \\dfrac{1}{2\\pi}\\int {\\color{blue}\\int} f(y) {\\color{blue}e^{-i\\xi (y-x)}e^{-\\epsilon^2\\xi^2/2}}dy {\\color{blue}d\\xi} \\\\ \u0026amp;= \\dfrac{1}{2\\pi}\\int {\\color{blue}\\mathcal{F} \\left[ e^{-\\epsilon^2 \\xi^2 /2}\\right] (y-x)} f(y) dy \\\\ \u0026amp;= \\dfrac{1}{2\\pi}\\int {\\color{blue}\\sqrt{\\dfrac{2\\pi}{\\epsilon^2}}e^{-\\frac{(y-x)^2}{2\\epsilon^2}}} f(y) dy \\\\ \u0026amp;= \\int \\dfrac{1}{\\epsilon \\sqrt{2\\pi}} e^{-\\frac{(x-y)^2}{2\\epsilon^2}} f(y) dy \\\\ \u0026amp;= \\int \\eta_\\epsilon (x-y)f(y) dy \\\\ \u0026amp;= f \\ast \\eta_\\epsilon (x) \\end{align*} $$\n3つ目の等号では以下の公式を使用した。\nガウス関数のフーリエ変換\nガウス関数$f(x)=e^{-Ax^2}$のフーリエ変換は以下のようである。\n$$ \\mathcal{F}[f] (\\xi) = \\mathcal{F} \\left[ e^{-Ax^2} \\right] (\\xi)=\\sqrt{\\frac{\\pi}{A}}e^{-\\frac{\\xi ^2}{4A}} $$\nすると、$f$が部分的に連続であるという仮定により、$f$のモリフィケーションは以下のように収束する。\n$$ \\begin{align*} \\lim \\limits_{\\epsilon \\rightarrow 0} \\dfrac{1}{2\\pi}\\int\\hat{f} (\\xi) e^{i\\xi x}e^{-\\epsilon^2\\xi^2/2}d\\xi \u0026amp;= \\lim \\limits_{\\epsilon \\rightarrow 0} f \\ast \\eta_\\epsilon (x) \\\\ \u0026amp;= \\dfrac{1}{2} \\big[ f(x-)+f(x+)\\big] \\\\ \u0026amp;= f(x) \\end{align*} $$\nまた、$\\hat{f} \\in L^1$と仮定すると、\n$$ \\left| e^{i\\xi x}e^{-\\epsilon^2|\\xi|^2 /2} \\hat{f}(\\xi) \\right| \\le \\left| \\hat{f}(\\xi) \\right| $$\nしたがって、支配収束定理により、\n$$ \\begin{align*} f(x) \u0026amp;= \\lim \\limits_{\\epsilon \\rightarrow 0} \\dfrac{1}{2\\pi}\\int\\hat{f} (\\xi) e^{i\\xi x}e^{-\\epsilon^2\\xi^2/2}d\\xi \\\\ \u0026amp;= \\dfrac{1}{2\\pi}\\int \\lim \\limits_{\\epsilon \\rightarrow 0} \\hat{f} (\\xi) e^{i\\xi x}e^{-\\epsilon^2\\xi^2/2}d\\xi \\\\ \u0026amp;= \\dfrac{1}{2\\pi}\\int \\hat{f} (\\xi) e^{i\\xi x}d\\xi \\end{align*} $$\nしたがって、\n$$ f(x)=\\dfrac{1}{2\\pi}\\int \\hat{f} (\\xi) e^{i\\xi x}d\\xi $$\n■\n","id":1112,"permalink":"https://freshrimpsushi.github.io/jp/posts/1112/","tags":null,"title":"フーリエ逆変換定理"},{"categories":"통계적분석","contents":"モデル 1 $\\nabla_{s} Y_{t} := Y_{t} - Y_{t-s}$ として定義されたオペレーター$\\nabla_{s}$ をシーズナル差分Seasonal Differenceと言う。 $W_{t} := \\nabla^{d} \\nabla_{s}^{D} Y_{t}$ として定義された$\\left\\{ W_{t} \\right\\}_{t \\in \\mathbb{N}}$が$ARMA(P,Q)$で、$\\left\\{ Y_{t} \\right\\}_{t \\in \\mathbb{N}}$が$ARMA(p,q)$なら、$\\left\\{ Y_{t} \\right\\}_{t \\in \\mathbb{N}}$はシーズナルARIMA過程 $ARIMA(p,d,q)\\times(P,D,Q)_{s}$ と言う。このような形式をシーズナルARIMAモデルと言う。 説明 今日の気温はもちろん昨日の気温に最も大きく影響されるだろうが、基本的に夏は暑く、冬は寒い。例年になく寒い日や、例外的に暖かい冬があるかもしれないが、グローバルに見れば、季節に従わざるを得ない。このように、マクロ的な周期性を持って回ってくる性質を季節性Seasonalityと言う。\n例えば、AirPassengerは1949年から1960年までの月別の航空機の乗客数に関するデータで、夏の休暇シーズンにピークを迎え、徐々に減少するパターンを示す。年によって多かったり少なかったり、年が経つにつれて増加する傾向があるが、ともかく一年周期でみると、明確な季節性が現れる。\nもちろん、季節性が実際の「季節」に関連する必要はなく、週による週間のトレンドであれ日単位で回る日間のトレンドであれ、ボードゲームのターンであれ、季節性と呼ぶことができる。順序があるデータなら、最終的には時系列データとして見ることができるだろうが、抽象的にアプローチすれば、実際に時間に関係する性質である必要もない。\nシーズナルARIMA過程の定義は、一見非常に複雑に見えるが、その概念はARIMA過程のARIMA過程を超えない。数式的にも、ただ長い差分が生じて複雑に見えるだけで、なんかエレガントな洞察を得るのは難しい。ただ「もっと大きな流れがある」と受け止めてもいい。\nCryer. (2008). Time Series Analysis: With Applications in R(2nd Edition): p233~234.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1067,"permalink":"https://freshrimpsushi.github.io/jp/posts/1067/","tags":null,"title":"季節性ARIMAモデル"},{"categories":"푸리에해석","contents":"要約1 $\\cal{F}f, \\hat{f}$を$f$のフーリエ変換としよう。$f \\in L^{1}$とする。すると、フーリエ変換について次の性質が成り立つ。\n(a) 任意の実数$a$に対して $$ \\mathcal{F} \\left[ f(x-a) \\right] ( \\xi ) = e^{-ia\\xi}\\hat{f}(\\xi) \\quad \\mathrm{and} \\quad \\mathcal{F} \\left[ e^{iax}f(x)\\right] (\\xi) = \\hat{f}(\\xi-a) $$\n(b) $\\delta \u0026gt;0$に対して$f_\\delta (x) := \\frac{1}{\\delta}f ( \\frac{x}{\\delta} )$と定義すると $$ \\mathcal{F}\\left[ f_\\delta \\right] (\\xi ) = (\\mathcal{F}f)(\\delta \\xi) \\quad \\mathrm{and} \\quad \\mathcal{F} \\left[ f(\\delta x) \\right] (\\xi) = ( \\mathcal{F} f ) _{\\delta} (\\xi) $$\n(c) $f$が連続で、部分的に滑らかだとすると $$ \\mathcal{F} \\left[ f^{\\prime} \\right] (\\xi) = i \\xi \\mathcal{F} f (\\xi) $$\n一方、$xf(x)$が積分可能なら\n$$ \\mathcal{F} \\left[ xf(x) \\right] (\\xi) = i ( \\mathcal{F} f ) ^{\\prime} (\\xi) $$\n(d) もし$g\\in L^{1}$なら $$ \\mathcal{F} \\left[ f \\ast g \\right] (\\xi)= \\hat{f} (\\xi) \\hat{g}(\\xi) $$\nこの時$f \\ast g$は$f$と$g$の畳み込みだ。\n(d\u0026rsquo;) $\\left\\{ f_{n} \\right\\} \\subset L^{1}$に対して、 $$ \\mathcal{F}\\left[ f_{1} \\ast f_{2} \\ast \\cdots \\ast f_{n} \\right]=\\hat{f_{1}} \\hat{f_{2}} \\cdots \\hat{f_{n}} $$\n説明 (a) 平行移動と指数関数の乗法が変換を通じて互いに交換されるという意味だ。平行移動した後に変換すると指数関数が乗算され、指数関数を乗算してから変換すると平行移動が現れる。 (b) 同様に、変数に$\\delta$を乗算することと関数に$_\\delta$を取る操作が変換を通して互いに交換される。 (c) 微分のフーリエ変換はフーリエ変換に定数$i\\xi$を乗算したものと同じだ。\n証明 (a) $$ \\begin{align*} \\mathcal{F} \\left[ f(x-a) \\right] (\\xi) \u0026amp;= \\int f(x-a)e^{-i\\xi x} dx \\\\ \u0026amp;= \\int f(y)e^{-i\\xi (y+a)} dy \\\\ \u0026amp;= e^{-ia\\xi} \\int f(y)e^{-i\\xi y}dy \\\\ \u0026amp;= e^{-ia\\xi} \\hat{f}(\\xi) \\end{align*} $$\n二番目の等式は$x-a=y$で置き換えると成り立つ。\n$$ \\begin{align*} \\mathcal{F}\\left[ e^{iax}f(x) \\right] (\\xi) \u0026amp;= \\int f(x)e^{-i\\xi x}e^{iax} dx \\\\ \u0026amp;= \\int f(x) e^{-i(\\xi-a)x}dx \\\\ \u0026amp;= \\hat{ f }(\\xi-a) \\end{align*} $$\n三番目の等式はフーリエ変換の定義により成り立つ。\n■\n(b) (a)と同様に、簡単に証明できる。\n$$ \\begin{align*} \\mathcal{F} \\left[ f_\\delta \\right] (\\xi) \u0026amp;= \\displaystyle {\\int} f_\\delta (x) e^{-i\\xi x} dx \\\\ \u0026amp;= {\\displaystyle \\int} \\dfrac{1}{\\delta}f \\left( \\frac{x}{\\delta} \\right)e^{-i\\xi x} dx \\\\ \u0026amp;= \\displaystyle{ \\int} f(y) e^{-i(\\delta \\xi )y} dy \\\\ \u0026amp;= \\hat{f}(\\delta\\xi) \\end{align*} $$\n二番目の等式は$\\frac{x}{\\delta}=y$で置き換えると成り立つ。\n$$ \\begin{align*} \\mathcal{F} \\left[ f(\\delta x) \\right] (\\xi) \u0026amp;= \\displaystyle{ \\int} f(\\delta x)e^{-i\\xi x}dx \\\\ \u0026amp;= \\dfrac{1}{\\delta} \\displaystyle{ \\int} f(y)e^{-i(\\xi / \\delta)y} dy \\\\ \u0026amp;= \\dfrac{1}{\\delta} \\hat{f} ( \\xi / \\delta) \\\\ \u0026amp;= \\hat{f}_{\\delta}(\\xi) = ( \\mathcal{F }f )_{\\delta} (\\xi) \\end{align*} $$\n三番目の等式は$\\delta x=y$で置き換えると成り立つ。\n■\n(c) まず、\n$$ \\int_{0}^\\infty f^{\\prime}(x)dx=\\lim \\limits_{t \\rightarrow \\infty} \\int_{0}^tf^{\\prime}(x)dt=\\lim \\limits_{t \\rightarrow \\infty} f(t)-f(0) $$\nそして$f^{\\prime} \\in L^{1}$なので、$\\displaystyle \\int f^{\\prime}(x)dx$が存在し、したがって$\\lim \\limits_{t \\rightarrow \\infty} f(t)$が存在する。仮定により、$f \\in L^{1}$なのでその値は$0$だ。これは$\\lim \\limits_{t \\rightarrow -\\infty}f(t)$の時も同様なので、\n$$ \\begin{equation} \\lim \\limits_{x \\rightarrow \\pm \\infty} f(x)=0 \\label{eq1} \\end{equation} $$\nだから、\n$$ \\begin{align*} \\mathcal{F} \\left[ f^{\\prime} \\right] (\\xi) \u0026amp;= \\int f^{\\prime}(x)e^{-i\\xi x} dx \\\\ \u0026amp;= \\left[ e^{-i\\xi x} f(x)\\right]_{-\\infty}^\\infty + i\\xi \\int f(x) e^{-i \\xi x} dx \\\\ \u0026amp;= i \\xi \\int f(x) e^{-i\\xi x}dx \\\\ \u0026amp;= i \\xi \\hat{f}(\\xi) \\end{align*} $$\n二番目の等式は部分積分を用いると成り立つ。三番目の等式は$\\eqref{eq1}$により成り立つ。\n$$ \\begin{align*} \\mathcal{F} \\left[ xf(x) \\right] (\\xi) \u0026amp;= \\int x f(x)e^{-i \\xi x}dx \\\\ \u0026amp;= i\\dfrac{d}{d\\xi} \\int f(x) e^{-i \\xi x}dx \\\\ \u0026amp;= i\\dfrac{d}{d \\xi} \\mathcal{F} f (\\xi) \\\\ \u0026amp;= i (\\mathcal{F} f )^{\\prime}(\\xi) \\end{align*} $$\n■\n(d) 畳み込みの一般的な定義を考えると、実際には(d)は性質ではなく定義だ。\n$$ \\begin{align*} \\mathcal{F} \\left[ f \\ast g \\right] (\\xi) \u0026amp;= \\int (f \\ast g)(x)e^{-i \\xi x}dx \\\\ \u0026amp;= \\int \\left[ \\int f(x-y)g(y)dy\\right]e^{-i \\xi x}dx \\\\ \u0026amp;= \\int \\left[ \\int f(x-y)g(y)dy\\right]e^{-i \\xi (x-y)}e^{-i\\xi y}dx \\\\ \u0026amp;= \\int \\int f(x-y)g(y)e^{-i \\xi (x-y)}e^{-i\\xi y}dydx \\\\ \u0026amp;= \\int \\int f(x-y)g(y)e^{-i \\xi (x-y)}e^{-i\\xi y}dxdy \\\\ \u0026amp;= \\int \\left[ \\int f(x-y)e^{-i \\xi (x-y)}dx \\right] g(y)e^{-i \\xi y} dy \\\\ \u0026amp;= \\int \\left[ \\int f(z)e^{-i \\xi z}dz \\right] g(y)e^{-i \\xi y} dy \\\\ \u0026amp;= \\int \\hat{f}(\\xi) g(y)e^{-i \\xi y} dy \\\\ \u0026amp;= \\hat{f}(\\xi)\\int g(y)e^{-i \\xi y} dy \\\\ \u0026amp;= \\hat{f}(\\xi) \\hat{g}(\\xi) \\end{align*} $$\n七番目の等式は$x-y=z$で置き換えると成り立つ。\n■\n(d') 畳み込みは結合法則が成り立つので、**(d)**によって直ちに成り立つ。\n■\nGerald B. Folland, Fourier解析及びその応用 (1992), p214-215\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1101,"permalink":"https://freshrimpsushi.github.io/jp/posts/1101/","tags":null,"title":"フーリエ変換の性質"},{"categories":"함수","contents":"関数 $f : X \\to Y$が与えられたとしよう。$a, b \\in X$、$a_{i} \\in X\\ (i=1,\\cdots)$とする。\n部分加法関数 関数$f$が下の式を満たす時、部分加法関数subadditive functionという。\n$$ f(a+b) \\le f(a)+f(b) $$\n絶対値が例として挙げられる。\n$$ |3+(-4)| \\le |3|+|-4| $$\n別の例で、$f(x)=2x+3$だとすると\n$$ 13=f(2+3) \\le f(2)+f(3)=7+9=16 $$\n加法関数 関数$f$が下の式を満たす時、加法関数additive functionという。\n$$ f(a+b) = f(a)+f(b) $$\n部分加法性から等式が成り立つ場合だ。\n例えば、$f(x)=4x$とすると\n$$ 20=f(2+3)=f(2)+f(3)=20 $$\n集合$E_{1},\\ E_2$が$E_{1} \\cap E_2 = \\emptyset$を満たし、$n(E_{i})=E_{i}$の要素の数だとする時\n$$ n(E_{1} \\cup E_2) = n(E_{1}) + n(E_2) $$\n可算部分加法関数 関数$f$が下の式を満たす時、可算部分加法関数countable subadditive/$\\sigma$-subadditive functionという。\n$$ f \\left( \\sum_{i=1}^\\infty a_{i} \\right) \\le \\sum \\limits_{i=1}^\\infty f(a_{i}) $$\n部分加法性、加法性を見ると、任意の$N$個の要素に対しても成り立つことが分かる。可算個の要素に対して成り立つなら、可算部分加法性を持つと言われる。可算部分加法性を持つ例に外測度がある。\n可算加法関数 関数$f$が下の式を満たす時、可算加法関数countable additive/$\\sigma$-additive functionという。\n$$ f \\left( \\sum_{i=1}^\\infty a_{i} \\right) = \\sum \\limits_{i=1}^\\infty f(a_{i}) $$\n可算部分加法性から等式が成り立つ場合だ。\n別々に識別される要素に対しては、外測度が可算加法性を持つ。$E_{i} \\cap E_{j} =\\emptyset \\quad \\forall\\ i,j$ならば\n$$ \\mu^{\\ast} \\left( \\bigsqcup _{i=1}^\\infty E_{i} \\right) = \\sum _{i=1}^\\infty \\mu^{\\ast}(E_{i}) $$\n部分乗法関数 関数$f$が下の式を満たす時、部分乗法関数submultiplicative functionという。\n$$ f(ab) \\le f(a)f(b) $$\n上で話した加算に関する性質を乗算に適用したものだ。\n乗法関数 関数$f$が下の式を満たす時、乗法関数multiplicative functionという。\n$$ f(ab) = f(a)f(b) $$\n部分乗法性から等式が成り立つ場合だ。\n","id":1096,"permalink":"https://freshrimpsushi.github.io/jp/posts/1096/","tags":null,"title":"加法関数と乗法関数"},{"categories":"르벡공간","contents":"説明 $\\Omega \\subset \\mathbb{R}^{n}$を開集合と呼ぼう。次の式を満たす二つの定数$1 \\lt p \\lt \\infty, 1 \\lt p^{\\prime} \\lt \\infty$が与えられたとしよう。\n$$ \\dfrac{1}{p} + \\dfrac{1}{p^{\\prime}} = 1 \\left(\\text{or } p^{\\prime} = \\frac{p}{p-1} \\right) $$\nもし$u \\in L^p(\\Omega)$、$v\\in L^{p^{\\prime}}(\\Omega)$ならば$uv \\in L^1(\\Omega)$であり、下の不等式が成立する。\n$$ \\| uv \\|_{1} = \\int_{\\Omega} |u(x)v(x)| dx \\le \\| u \\|_{p} \\| v \\|_{p^{\\prime}} $$\n上記の定理の不等式は、ヘルダーの不等式と呼ばれる。ヘルダーの不等式から、以下の二つの系が容易に成立することが示される。\n定理1 定理1 三つの定数$p\u0026gt;0, q\u0026gt;0, r\u0026gt;0$が$\\dfrac{1}{p} + \\dfrac{1}{q} = \\dfrac{1}{r}$を満たし、$u \\in {L}^{p}(\\Omega), v \\in {L}^{q}(\\Omega)$ならば、$uv \\in L^{r}(\\Omega)$であり、下の不等式が成立するとする。\n$$ \\| uv \\|_{r} = \\left( \\int_{\\Omega} |u(x)v(x)|^{r} dx \\right)^{1/r} \\le \\| u \\|_{p} \\| v \\|_{q} $$\n$r=1$の場合は、ヘルダーの不等式と同じである。\n証明 仮定により、\n$$ \\dfrac{1}{p}+\\dfrac{1}{q}=\\dfrac{1}{r} \\implies \\dfrac{1}{p/r}+\\dfrac{1}{q/r}=1 $$\nそして$u \\in L^p(\\Omega)$と仮定したので、$\\left( \\int_{\\Omega}|u|^p dx \\right)^{1/p} \u0026lt; \\infty$であり、従って、\n$$ \\left( \\int_{\\Omega}|u^r|^{\\frac{p}{r}} dx \\right)^{1/p} \u0026lt; \\infty \\implies \\left( \\int_{\\Omega}|u^r|^{\\frac{p}{r}} dx\\right)^{r/p} \u0026lt; \\infty $$\n従って、$u^r \\in {L}^{p/r}(\\Omega)$であり$v^r \\in{L}^{q/r}(\\Omega)$も同じ方法で確認できる。そうすると、ヘルダーの不等式により、\n$$ \\int_{\\Omega} |u(x)v(x)|^{r} dx = \\int_{\\Omega} |u^{r}(x)v^{r}(x) | dx \\le \\| u^r \\|_{p/r} \\|v^r\\|_{q/r} $$\n右側を積分形式で書き直せば、\n$$ \\begin{align*} \\int_{\\Omega} |u(x)v(x)|^{r} dx \\le\u0026amp; \\left(\\int_{\\Omega} |u(x)^{r}|^{p/r} dx \\right)^{q/p} \\left(\\int_{\\Omega} |v(x)^r|^{q/r} dx \\right)^{r/q} \\\\ =\u0026amp;\\ \\left(\\int_{\\Omega}|u(x)|^{p} dx \\right)^{r/p} \\left(\\int_{\\Omega} |v(x)|^{q} dx \\right)^{r/q} \\end{align*} $$\n両方に$\\dfrac{1}{r}$乗を取れば、\n$$ \\left( \\int_{\\Omega} |u(x)v(x)|^rdx \\right)^{1/r} \\le \\left(\\int_{\\Omega}|u(x)|^{p} dx \\right)^{1/p} \\left(\\int_{\\Omega} |v(x)|^{q} dx \\right)^{1/q} $$\n従って、\n$$ \\| uv \\|_{r} = \\left( \\int_{\\Omega} |u(x)v(x)|^rdx \\right)^{1/r} \\le \\| u \\|_{p} \\| v\\|_{q} $$\n■\n定理2 $1\\le j \\le N$に対して、$p_{j}\u0026gt;0$であり、$\\sum\\limits_{j=1}^N\\dfrac{1}{{p}_{j}}=\\dfrac{1}{{p}_{1}}+\\dfrac{1}{{p}_2}+\\cdots+\\dfrac{1}{{p}_{N}}=\\dfrac{1}{r}$とする。そして、$u=\\prod _{j=1}^N u_{j}=u_{1}u_2\\dots u_{N}$であり$u_{j}\\in L^{{p}_{j}}(\\Omega)$と仮定する。すると、$u\\in {L}^r (\\Omega)$であり、下の不等式が成立する。\n$$ \\| u \\|_{r} = \\left( \\int_{\\Omega} |u(x)|^{r} dx \\right)^{1/r} \\le \\prod_{j=1}^{N} \\| u_{j} \\|_{{p}_{j}} = \\| u_{1} \\|_{{p}_{1}} \\cdots \\| u_{N} \\|_{p_{N}} $$\n上の定理1は、二つの関数に対するものだけでなく、任意の$N$個の関数に対しても成立することがわかる。\n証明 数学的帰納法を使う。まず、$N=2$の時は、定理1によって成立する。それから、$N=k$の時に成立すると仮定した時、$N=k+1$の時にも成立することを示せば、証明が完了する。\n$\\sum\\limits_{j=1}^k \\dfrac{1}{{p}_{j}}=\\dfrac{1}{r}$であり、$N=k$の時に成立するとする。すると、次のことが成立する。\n$$ \\left\\| \\prod_{j=1}^N u_{j} \\right\\|_{r} \\le \\| u_{1} \\|_{p_{1}} \\| u_{2} \\|_{p_{2}} \\cdots \\| u_{k} \\|_{p_{k}} $$\n今、$\\sum_{j=1}^{k+1}\\dfrac{1}{{p}_{j}}=\\dfrac{1}{r}+\\dfrac{1}{{p}_{k+1}}=\\dfrac{1}{r^{\\prime}}$としよう。すると、\n$$ \\begin{align*} \\| u \\|_{r^{\\prime}} =\u0026amp;\\ \\left\\| \\left( \\prod_{j=1}^k u_{j} \\right) u_{k+1} \\right\\|_{r^{\\prime}} \\\\ \\le\u0026amp; \\left\\| \\prod \\limits_{j=1}^{k+1}u_{j} \\right\\|_{r} \\| u_{k+1} \\|_{p_{k+1}} \\\\ \\le\u0026amp; \\| u_{1} \\|_{p_{1}} \\| u_{2} \\|_{p_{2}} \\cdots \\| u_{k} \\|_{p_{k}} \\| u_{k+1} \\| _{p_{k+1}} \\\\ =\u0026amp;\\ \\prod \\limits_{j=1}^{k+1} \\| u_{j}\\|_{p_{j}} \\end{align*} $$\n二行目は定理1によって成立する。三行目は仮定によって成立する。従って、$N=k$の時に成立すると仮定すれば、$N=k+1$の時にも成立する。従って、数学的帰納法により証明完了。■\n参照 ユークリッド空間におけるヘルダーの不等式 ヘルダーの不等式 Robert A. AdamsとJohn J. F. Foutnier, Sobolev Space（第2版、2003年）、p24-25\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1091,"permalink":"https://freshrimpsushi.github.io/jp/posts/1091/","tags":null,"title":"一般化されたヘルダーの不等式、ヘルダーの不等式の系"},{"categories":"푸리에해석","contents":"定義 関数としてのフーリエ変換 関数$f \\in$$L^{1}$のフーリエ変換Fourier transform of $f$を次のように定義する。\n$$ \\hat{f}(\\xi) := \\int_{-\\infty}^{\\infty} f(t) e^{-i \\xi t}dt $$\n演算子としてのフーリエ変換 以下のように定義される作用素$\\mathcal{F} : L^{1} \\to$$C_{0}$をフーリエ変換という。\n$$ \\mathcal{F}[f] (\\xi) = \\int_{-\\infty}^{\\infty} f(t) e^{-i \\xi t}dt $$\n説明 定義から分かるように、フーリエ変換という言葉は、演算子$\\mathcal{F}$自体を意味する言葉であり、かつ、$\\mathcal{F}$の関数値$\\hat{f} = \\mathcal{F}f = \\mathcal{F}[f]$を意味する言葉でもある。$\\mathcal{F}$の値域が$C_{0}$であることは、リーマン・ルベーグ補題により保証される。さらに、以下が成り立つことを簡単に示せる。$f \\in L^{1}$に対して、\n$$ \\left\\| \\mathcal{F}f \\right\\|_{\\infty} \\le \\left\\| f \\right\\|_{1} $$\n証明\n$$ \\begin{align*} \\left\\| \\mathcal{F}f \\right\\|_{\\infty} = \\max\\limits_{\\xi \\in \\mathbb{R}} \\left| \\mathcal{F}f(\\xi) \\right| \u0026amp;= \\max\\limits_{\\xi \\in \\mathbb{R}} \\left| \\int_{-\\infty}^{\\infty} f(t) e^{-i \\xi t}dt \\right| \\\\ \u0026amp;\\le \\max\\limits_{\\xi \\in \\mathbb{R}} \\int_{-\\infty}^{\\infty} \\left| f(t) e^{-i \\xi t} \\right| dt \\\\ \u0026amp;= \\int_{-\\infty}^{\\infty} \\left| f(t) \\right| dt = \\left\\| f \\right\\|_{1} \\end{align*} $$\nフーリエ変換は積分変換の一種であり、その逆変換は以下のようである。\n$$ f(t) = \\mathcal{F}^{-1}\\hat{f}(t) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\hat{f}(\\xi) e^{i t \\xi} d \\xi $$\n前の定数$\\dfrac{1}{2\\pi}$は、逆変換の前にも、変換の前にもつけてもどこにつけても関係ないため、または両方に$\\frac{1}{\\sqrt{2\\pi}}$をつけることもある。これは作者の便宜により異なり、本質的な違いはない。また、定義を見ると、$f$が積分可能で、つまり$f\\in L^{1}$の条件を満たさなければならないことから、フーリエ変換がうまく定義されることが分かる。$\\hat{f}$も積分可能であれば、フーリエ逆変換もまた、うまく定義される。\n多変数関数のフーリエ変換 多変数関数のフーリエ変換は、次のように定義する。多変数関数$f \\in L^{1}(\\mathbb{R}^{n})$のフーリエ変換は、\n$$ \\mathcal{F}f(\\boldsymbol{\\xi}):=\\int f(x)e^{-i \\boldsymbol{\\xi} \\cdot \\mathbf{x} }d\\mathbf{x} $$\n$$ \\mathcal{F} f(\\xi_{1},\\ \\cdots ,\\ \\xi_{n}) := \\int_{-\\infty}^{\\infty} \\dots \\int_{-\\infty}^{\\infty} f(x_{1},\\ \\cdots,\\ x_{n})e^{-i(\\xi_{1} x_{1}+\\cdots+\\xi_{n} x_{n})}dx_{1}\\cdots dx_{n} $$\n表記法 $f$のフーリエ変換に一般的に使われる二つの表記法がある。\n$$ \\mathcal{F}(f),\\quad \\hat{f} $$\n教科書では、著者がどの記号を好むかによって異なるが、どちらもよく使われている。右側のハット記号を使う方が便利に見えるが、混乱の余地があるため、正確さが求められる場合は左側の表現を使う方が良い。例えば、入力関数自体の記号が長くなった場合、ハット記号を使うと混乱したり、見た目が良くない場合がある。このような場合は、$\\mathcal{F}$を使うと、式の意味を明確かつきれいに表すことができる。例えば、$W_{c}f$のフーリエ変換は、以下に示すように、$\\mathcal{F}$を使って表記する方が良い。\n$$ \\mathcal{F}(\\mathcal{W}_{c}f),\\quad \\hat{\\mathcal{W}_{c}f},\\quad \\widehat{\\mathcal{W}_{c}f} $$\nしかし、混乱の余地がない場合は、ハット記号の方が便利である。このように、同じ概念に対していくつかの表記法が存在するのは、微分にも同じことが言える。\n$$ f^{\\prime}, \\quad \\dfrac{df}{dx} $$\n$\\hat{f}$と$\\mathcal{F}$の二つの表記法の長所と短所は、微分において左側のニュートン記法が経済性と利便性に優れ、一方で右側のライプニッツ記法が連鎖律などを計算する際に、厳密さと正確さで優れているのと似ている。\n導出1 有限区間で定義された関数は、フーリエ級数を使って近似することができる。これは有用だが、周期関数にしか使えないため、非周期関数に対しても同様の役割を果たすツールが必要である。このアイデアからフーリエ変換Fourier transformが生まれた。フーリエ変換を導出する過程での核心的なアイデアは、非周期関数をまるで実数全体の区間を周期に持ち、周期が数直線全体に1回繰り返される関数として考えることである。\n$f$を区間$[-L,L)$で定義された関数とする。すると、$f$のフーリエ級数と複素フーリエ係数は次のようになる。\n$$ \\begin{equation} f(t)=\\sum \\limits_{n=-\\infty}^{\\infty} c_{n} e^{i\\frac{n\\pi t}{L}} \\end{equation} $$\n$$ c_{n} = \\dfrac{1}{2L}\\int_{-L}^{L}f(t)e^{-i\\frac{n \\pi t}{L} }dt $$\n以下の変数変換を行う。\n$$ \\Delta \\xi = \\dfrac{\\pi}{L},\\quad \\xi_{n}=n\\Delta\\xi=\\dfrac{n\\pi}{L} $$\nすると、$(1)$は次のようになる。\n$$ f(t) = \\sum \\limits_{n=-\\infty}^{\\infty} c_{n} e^{i\\xi_{n} t}, \\quad c_{n} = \\dfrac{1}{2L}\\int_{-L}^{L}f(t)e^{-i \\xi_{n} t }dt $$\n$f(t)$に適切な定数を掛け、$c_{n}$の積分項を$\\hat{f}(\\xi_{n})$とする\n$$ f(t)=\\dfrac{L}{\\pi}\\sum \\limits_{n=-\\infty}^{\\infty} c_{n} e^{i\\xi_{n} t}\\Delta \\xi , \\quad c_{n} = \\dfrac{1}{2L}\\hat{f}(\\xi_{n}) $$\n$f(t)$が$t \\rightarrow \\pm \\infty$の時に速やかに$0$に収束すると仮定する。すると、$c_{n}$に対して積分区間を$[-L,L)$から$(-\\infty,\\infty)$に拡張しても、元の$c_{n}$と大きく変わらないだろう。\n$$ c_{n} \\approx \\dfrac{1}{2L} \\int_{-\\infty}^{\\infty} f(t) e^{-i\\xi_{n} t}dt $$\nこれは$\\xi_{n}$のみの関数なので、$c_{n} = \\frac{1}{2L}\\hat{f}(\\xi_{n})$としよう。$f(t)$に代入すると\n$$ f(t) \\approx \\dfrac{1}{2 \\pi}\\sum \\limits_{n=-\\infty}^{\\infty} \\hat{f}(\\xi_{n}) e^{i\\xi_{n} t}\\Delta \\xi $$\nこれはリーマン和と非常に似ている。今、$L\\rightarrow \\infty$の極限を取ると$\\Delta\\xi \\rightarrow 0$になり、上記の式の$\\approx$は等式となり、和は積分になる。\n$$ f(t) = \\dfrac{1}{2 \\pi}\\int_{-\\infty}^{\\infty} \\hat{f}(\\xi) e^{i\\xi t} d\\xi \\quad \\text{and} \\quad \\hat{f}(\\xi)=\\int_{-\\infty}^{\\infty} f(t) e^{-i\\xi t}dt $$\nこの時点で、$\\hat{f}$を$f$のフーリエ変換と呼び、$f$を$\\hat{f}$のフーリエ逆変換Fourier inverse transformと呼ぶ。\nGerald B. Folland, Fourier Analysis and Its Applications (1992), p204-205\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1086,"permalink":"https://freshrimpsushi.github.io/jp/posts/1086/","tags":null,"title":"フーリエ変換"},{"categories":"르벡공간","contents":"要約：正規直交集合が持つ同値条件 $\\left\\{ \\phi_{n} \\right\\}_{1}^\\infty$が$L^2(a,b)$の正規直交集合であり、$f \\in L^2(a,b)$とする。すると、以下の条件はすべて同値である。\n$(a)$すべての$n$に対して$\\left\\langle f, \\phi_{n} \\right\\rangle=0$ならば、$f=0$である。\n$(b)$すべての$f\\in L^2(a,b)$に対して、級数$\\sum_{1}^\\infty \\left\\langle f,\\phi_{n}\\right\\rangle\\phi_{n}$が$f$にノルムセンスで収束する。つまり、以下の式が成り立つ。\n$$ f=\\sum_{1}^\\infty \\left\\langle f,\\phi_{n}\\right\\rangle\\phi_{n} $$\n$(c)$すべての$f \\in L^2(a,b)$に対して、パーセバルの方程式と呼ばれる以下のような式を満たす。\n$$ \\| f \\|^2 = \\sum \\limits_{n=1}^{\\infty} \\left| \\left\\langle f,\\phi_{n} \\right\\rangle \\right|^{2} $$\n説明 $(a) - (c)$を満たす正規直交集合を正規直交基底または完全正規直交集合と呼ぶ。\nこれらの三条件をよく見ると、正規直交基底は有限次元のベクター空間で基底と同じ役割を果たすことがわかる。\n$\\left\\{ \\phi_{n} \\right\\}$が正規直交基底のとき、定数$\\left\\langle f, \\phi_{n}\\right\\rangle$を（一般化された）フーリエ係数と呼ぶ。\n級数$\\sum \\left\\langle f, \\phi_{n}\\right\\rangle\\phi_{n}$を（一般化された）フーリエ級数と呼ぶ。\n補助定理\n$f \\in L^2(a,b)$であり、$\\left\\{ \\phi_{n} \\right\\}$が$L^2(a,b)$で正規直交集合であるとする。すると級数$\\sum \\left\\langle f,\\phi_{n} \\right\\rangle\\phi_{n}$はノルムセンスで収束する。そして、次のような不等式を満たす。\n$$ \\left\\| \\sum \\left\\langle f,\\phi_{n}\\right\\rangle \\phi_{n} \\right\\| \\le | f| $$\n証明 $(a) \\implies (b)$\n$(a)$とする。すると補助定理によって$\\sum \\left\\langle f, \\phi_{n} \\right\\rangle\\phi_{n}$はノルムセンスで収束する。級数の差を$g$と定義する。\n$$ g=f-\\sum \\limits_{n=1}^{\\infty} \\left\\langle f, \\phi_{n} \\right\\rangle\\phi_{n} $$\nすると、$g=0$を示すことができる。\n$$ \\begin{align*} \\left\\langle g,\\phi_{m} \\right\\rangle \u0026amp;=\\ \\left\\langle f,\\phi_{m}\\right\\rangle - \\sum \\limits_{n=1}^{\\infty}\\left\\langle f,\\phi_{n} \\right\\rangle \\left\\langle \\phi_{n}, \\phi_{m} \\right\\rangle \\\\ \u0026amp;=\\ \\left\\langle f,\\phi_{m}\\right\\rangle - \\left\\langle f,\\phi_{m}\\right\\rangle \\\\ \u0026amp;=\\ 0 \\end{align*} $$\nしたがって、仮定により$g=0$である。ゆえに、$f= \\sum_{n=1}^\\infty \\left\\langle f, \\phi_{n} \\right\\rangle\\phi_{n}$\n■\n$(b) \\implies (c)$\n$(b)$とする。すると、$f=\\sum_{1}^\\infty \\left\\langle f, \\phi_{n}\\right\\rangle\\phi_{n}$なので\n$$ \\begin{align*} \\| f \\|^2 \u0026amp;=\\ \\left\\| \\sum \\limits_{n=1}^{\\infty} \\left\\langle f, \\phi_{n} \\right\\rangle \\phi_{n} \\right\\| ^2 \\\\ \u0026amp;= \\left\\| \\lim \\limits_{N \\rightarrow \\infty} \\sum \\limits_{n=1} ^{N} \\left\\langle f, \\phi_{n} \\right\\rangle\\phi_{n} \\right\\| ^2 \\\\ \u0026amp;= \\lim \\limits_{N \\rightarrow \\infty} \\left\\| \\sum \\limits_{n=1} ^{N} \\left\\langle f, \\phi_{n} \\right\\rangle\\phi_{n} \\right\\| ^ 2 \\\\ \u0026amp;= \\lim \\limits_{N \\rightarrow \\infty} \\sum _{n=1}^{N} | \\left\\langle f,\\phi_{n} \\right\\rangle |^2 \\\\ \u0026amp;= \\sum \\limits _{n=1} ^{\\infty} | \\left\\langle f, \\phi_{n} \\right\\rangle |^2 \\end{align*} $$\n三番目の等式は、仮定により級数がノルムセンスで収束するから成り立つ。四番目の等式はピタゴラスの定理により成り立つ。\n■\n$(c) \\implies (a)$\n$(c)$とする。すると、\n$$ \\| f \\|^2 =\\sum \\limits _{n=1} ^{\\infty}\\left| \\left\\langle f,\\phi_{n} \\right\\rangle \\right|^{2} $$\nしたがって、すべての$n$に対して$\\left\\langle f, \\phi_{n} \\right\\rangle=0$ならば、$f=0$であることが示される。\n■\n","id":1082,"permalink":"https://freshrimpsushi.github.io/jp/posts/1082/","tags":null,"title":"完全正規直交基底と完全正規直交集合"},{"categories":"수치해석","contents":"式 異なる$x_{0} , \\cdots , x_{n}$のデータ$(x_{0}, f(x_{0} )) , \\cdots , (x_{n} , f( x_{n} ) )$について $$ p_{n} (x) =\\sum_{i=0}^{n} f [ x_{0} , \\cdots , x_{i} ] \\prod_{j=0}^{i-1} (x - x_{j} ) $$\n説明 複雑に見えるが、$n=0,1,2$について実際に展開してみると、次のように単純に表される。 $$ \\begin{align*} p_{0} (x) =\u0026amp; f(x_{0}) \\\\ p_{1} (x) =\u0026amp; f( x_{0} ) + (x - x_{0} ) f [ x_{0} , x_{1} ] \\\\ p_{2} (x) =\u0026amp; f( x_{0} ) + (x - x_{0} ) f [ x_{0} , x_{1} ] + ( x - x_{0} ) ( x - x_{1} ) f [ x_{0} , x_{1} , x_{2} ] \\end{align*} $$ ニュートンの差分公式は、差分法を使って多項式補間を表す方法で、ラグランジュの公式に比べて複雑に見えるが、データが追加されるにつれて更新をする必要がある場合は、はるかに便利だ。ニュートンの差分公式を再帰的に表現すると $$ p_{n} (x) = p_{n-1}(x) + f[x_{0} , \\cdots , x_{n} ] \\prod_{i=0}^{n-1} (x - x_{i}) $$ のような形になるが、ラグランジュの公式しかなかったら、$p_{n-1}$をすでに知っているにも関わらず、また$(n+1) \\times (n+1)$サイズの逆行列を計算しなければならない手間がある。どのような計算方法であれ、結果自体はラグランジュの公式と全く同じであるため、追加されるデータが少ない場合はニュートンの差分公式が便利に使える。\nまた、ラグランジュの公式と正確に同じでありながらも異なる形を持っていることを利用して、差分に関する様々な便利な定理を証明できる。\n導出 戦略: $p_{n}$と$p_{n-1}$の差を$C$とすると、$p_{n}$と$C$の最高次項の係数は同じでなければならないので、$C$の係数だけを計算して、間接的に再帰式を導出する。\n$$ p_{n}(x) = p_{n-1}(x) + C(x) $$とした場合、$\\deg C = n$であり、$i= 0, \\cdots, (n-1)$に対して $$ p_{n}(x_{i}) = f(x_{i}) = p_{n-1}(x_{i}) $$ であるため、ある定数$a_{n} \\ne 0$について $$ C (x) = a_{n} (x - x_{0}) \\cdots (x - x_{n-1}) $$ のように表される。$p_{n}$と$C$の最高次項は同じであるため、$a_{n} = f [ x_{0} , \\cdots , x_{n}]$が成立することを示せばよい。データ$\\left\\{ x_{0}, \\cdots , x_{k} \\right\\}$から$x_{k}$を抜いたデータ$\\left\\{ x_{0}, \\cdots , x_{k-1} \\right\\}$で得られた多項式補間を$L_{k-1}(x)$、$x_{0}$を抜いたデータ$\\left\\{ x_{1}, \\cdots , x_{k} \\right\\}$で得られた多項式補間を$R_{k-1}(x)$としよう。各アルファベットは左Leftと右Rightを意味する。これに対して$p_{k}$を次のように置く。 $$ p_{k}(x) := {{ (x-x_{0}) R_{k-1} (x) - (x-x_{k}) L_{k-1} (x) } \\over { x_{k} - x_{0} }} $$ すると、$i=1,\\cdots , k-1$について $$ p_{k}(x_{0}) = L_{k-1} (x_{0}) = f(x_{0}) $$\n$$ p_{k}(x_{i}) = L_{k-1} (x_{i}) = R_{k-1} (x_{i}) = f(x_{i}) $$\n$$ p_{k}(x_{k}) = R_{k-1} (x_{k}) = f(x_{k}) $$ したがって、$p_{k}$は与えられたデータをうまく補間することが保証される。$p_{k}$の最高次項の係数を$a_{k}$とすると、$a_k = f[ x_{0} , \\cdots , x_{k} ]$が数学的帰納法で示される。\n$k=1$のとき $$ p_{1}(x) = {{ (x-x_{0}) f(x_{1}) - (x-x_{k}) f(x_{0}) (x) } \\over { x_{1} - x_{0} }} $$ であるため、最高次項$x$の係数は $$ a_{1} = {{ f(x_{1} - x_{0}) } \\over { x_{1} - x_{0} }} = f[x_{0},x_{1}] $$ $k \u0026gt; 1$のとき$a_{k-1} = f[ x_{0} , \\cdots , x_{k-1} ]$が成立すると仮定すると、$L_{k-1}$の最高次項の係数は$f[ x_{0} , \\cdots , x_{k-1} ]$であり、$R_{k-1}$の最高次項の係数は$f[ x_{1} , \\cdots , x_{k} ]$である。$p_{k}$の分数形を解くと $$ p_{k}(x) = {{ R_{k-1} (x) - L_{k-1} (x) } \\over { x_{k} - x_{0} }} x + {{ x_{k} L_{k-1} (x) - x_{0} R_{k-1} (x) } \\over { x_{k} - x_{0} }} $$ したがって、$p_{k}(x)$の最高次項の係数は $$ a_{k} = {{ f[ x_{1} , \\cdots , x_{k} ]- f[ x_{0} , \\cdots , x_{k-1} ] } \\over { x_{k} - x_{0} }} = f[ x_{0} , \\cdots , x_{k} ] $$ 数学的帰納法により、すべての自然数 $n$に対して $$ a_{n} = f [ x_{0} , \\cdots , x_{n}] $$\n■\n","id":1025,"permalink":"https://freshrimpsushi.github.io/jp/posts/1025/","tags":null,"title":"ニュートンの前進差分公式の導出"},{"categories":"전자기학","contents":"概要1 動く点電荷に関するスカラーポテンシャルとベクトルポテンシャルは遅延ポテンシャルretarded potentialと呼ばれ、これらは以下の通りである。\n$$ \\begin{align*} V(\\mathbf{r},\\ t) \u0026amp;= \\dfrac{1}{4\\pi\\epsilon_{0}} \\int \\dfrac{ \\rho (\\mathbf{r}^{\\prime},\\ t_{r}) }{ \\cR } d\\tau^{\\prime} \\\\[1em] \\mathbf{A}( \\mathbf{r},\\ t) \u0026amp;= \\dfrac{\\mu_{0}}{4\\pi} \\int \\dfrac{\\mathbf{J}(\\mathbf{r}^{\\prime},\\ t_{r})}{\\cR}d\\tau^{\\prime} \\end{align*} $$\nここで$t_{r}$は遅延時間である。\n遅延時間 電荷と電流の分布が時間によって変わらない場合、スカラーポテンシャルとベクトルポテンシャルは次のポアッソン方程式を満たす。\n$$ \\nabla^2 V=-\\dfrac{1}{\\epsilon_{0}} \\rho,\\quad \\nabla^2 \\mathbf{A}=-\\mu_{0}\\mathbf{J} $$\nこれを解くと以下のようになる。\n$$ \\begin{equation} V(\\mathbf{r})=\\dfrac{1}{4\\pi\\epsilon_{0}} \\int \\dfrac{ \\rho (\\mathbf{r}^{\\prime}) }{ \\cR } d\\tau^{\\prime},\\quad \\mathbf{A}( \\mathbf{r} ) = \\dfrac{\\mu_{0}}{4\\pi} \\int \\dfrac{\\mathbf{J}(\\mathbf{r}^{\\prime})}{\\cR}d\\tau^{\\prime} \\end{equation} $$\n$\\bcR$は分離ベクトルである。\nしかし、電磁波は光速で進むため、電流の分布が動けば、「現在の時刻」のポテンシャルは現在の電流の分布によるものではなく、「過去のある時刻」の電流の分布によるものである。大切なのは、今の電流の分布ではなく、電磁波が出発した「過去のある時刻」の電流の分布である。過ぎ去った距離が$\\cR$であり、光速が$c$であるため、現時点$\\mathbf{r}$に到着した電磁波が出発した時刻は以下の通りである。\n$$ t_{r} \\equiv t-\\dfrac{\\cR}{c} $$\nこれを遅延時間retarded timeと呼ぶ。もっと簡単に言えば、「今」の時刻$t$に到着した情報2が出発した当時の時刻を遅延時間という。時刻として表現するのはこのためである。動く点電荷、電流の分布を扱う場合、分離ベクトルは$\\mathbf{r}-\\mathbf{r}^{\\prime}$ではなく、$\\bcR=\\mathbf{r}-\\mathbf{w}$である。\n遅延ポテンシャル よって、電荷の分布が時間によって変わる時、つまり電荷が動く時にについて、$(1)$を一般化すると以下の通りである。\n$$ V(\\mathbf{r},\\ t)=\\dfrac{1}{4\\pi\\epsilon_{0}} \\int \\dfrac{ \\rho (\\mathbf{r}^{\\prime},\\ t_{r}) }{ \\cR } d\\tau^{\\prime},\\quad \\mathbf{A}( \\mathbf{r},\\ t) = \\dfrac{\\mu_{0}}{4\\pi} \\int \\dfrac{\\mathbf{J}(\\mathbf{r}^{\\prime},\\ t_{r})}{\\cR}d\\tau^{\\prime} $$\n$\\rho (\\mathbf{r}^{\\prime}, t_{r})$は時刻が$t_{r}$での点$\\mathbf{r}^{\\prime}$での電荷密度である。上述の二つの遅延時間に関するポテンシャルを遅延ポテンシャルと呼ぶ。これらの方程式は数学的に導出されたわけではない。しかし、物理的に正しい論理で理にかなった説明をした。論理的な飛躍はあるものの、幸いにも結果は現実によく合っている。これを証明するためには、新しく得られたポテンシャルが以下の波動方程式$(2)$とローレンツゲージ$(3)$を満たすか確認しなければならない。\n$$ \\begin{equation} \\begin{aligned} \\Box ^2 V \u0026amp;= -\\dfrac{1}{\\epsilon_{0}}\\rho \\\\ \\Box ^2 \\mathbf{A} \u0026amp;= -\\mu_{0}\\mathbf{J} \\end{aligned} \\end{equation} $$\n$$ \\begin{equation} \\nabla \\cdot \\mathbf{A} = -\\mu_{0} \\epsilon_{0} \\frac{\\partial V}{\\partial t} \\end{equation} $$\nこのように念入りに確認する必要があるのは、同じ論理をポテンシャルではなく、電場、磁場に適用した場合、現実と一致しない結果になるからである。\nDavid J. Griffiths, 基礎電磁気学(Introduction to Electrodynamics, 金甚星訳) (4版, 2014), p.480-483\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n電磁気学では、電磁波、光を意味する。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1075,"permalink":"https://freshrimpsushi.github.io/jp/posts/1075/","tags":null,"title":"遅延時刻連続分布に関する遅延ポテンシャル"},{"categories":"수치해석","contents":"公式 1 異なる$x_{0} , \\cdots , x_{n}$のデータ$(x_{0}, y_{0}) , \\cdots , (x_{n} , y_{n})$について$\\displaystyle l_{i} (x) := \\prod_{i \\ne j} \\left( {{ x - x_{j} } \\over { x_{i} - x_{j} }} \\right)$とすると、 $$ p_{n} (x) = \\sum_{i=0}^{n} y_{i} l_{i} (X) $$\n説明 ラグランジュの公式は、多項式補間を見つける方法の中で最もシンプルな公式だ。\n導出 戦略: $l_{i}$がインデックスに対するクロネッカーのデルタ関数であることを示す。\n$$ l_{i} (x_{i}) = \\prod_{i \\ne j} \\left( {{ x_{i} - x_{j} } \\over { x_{i} - x_{j} }} \\right) = 1 $$\n$$ l_{i} (x_{j}) = \\prod_{i \\ne j} \\left( {{ x_{j} - x_{j} } \\over { x_{i} - x_{j} }} \\right) = 0 $$ 整理すると$l_{i}(x_{j}) = \\delta_{ij}$である。 $$ p_{n}(x) = y_{0} l_{0}(x) + y_{1} l_{1}(x) + \\cdots y_{n} l_{n}(X) $$ これを設定すると、全ての$i=0,1, \\cdots , n$について、 $$ p_{n}(x_{i}) =0 + \\cdots + y_{i} \\cdot 1 + \\cdots + 0 = y_{i} $$ が成立する。\n■\nAtkinson. (1989). An Introduction to Numerical Analysis(2nd Edition): p134.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1023,"permalink":"https://freshrimpsushi.github.io/jp/posts/1023/","tags":null,"title":"ラグランジュの公式の導出"},{"categories":"편미분방정식","contents":"説明1 $x$と$p$について、偏微分方程式の変数であることを強調する場合、通常のフォントで $x,p \\in \\mathbb{R}^{n}$ と表示し、$s$に関する関数であることを強調する場合、太字のフォントで $\\mathbf{x}, \\mathbf{p} \\in \\mathbb{R}^{n}$ と表示します。 特性方程式\n$$ \\begin{cases} \\dot{\\mathbf{p}} (s) = -D_{x}F\\big(\\mathbf{p}(s),\\ z(s),\\ \\mathbf{x}(s) \\big)-D_{z}F\\big(\\mathbf{p}(s),\\ z(s),\\ \\mathbf{x}(s) \\big)\\mathbf{p}(s) \\\\ \\dot{z}(s) = D_{p}F\\big(\\mathbf{p}(s),\\ z(s),\\ \\mathbf{x}(s) \\big) \\cdot \\mathbf{p}(s) \\\\ \\dot{\\mathbf{x}}(s) = D_{p}F\\big(\\mathbf{p}(s),\\ z(s),\\ \\mathbf{x}(s) \\big) \\end{cases} $$\n特性方程式を用いた非線形1階偏微分方程式の解法は、微分方程式がどのように与えられるかによって少しずつ異なります。これは与えられた微分方程式の線形性によって区別され、線形、準線形、完全非線形の場合に応じて解法が異なります。非線形性が高いほど難易度が高くなります。\n解法 同次線形 与えられた偏微分方程式が完全に線形であれば、最も簡単に解くことができます。特性方程式の $\\mathbf{p}(s)$ に関する条件は必要ないほど単純です。次の線形および同次の微分方程式を考えてみましょう。\n$$ \\begin{equation} F(Du, u, x) = \\mathbf{b}(x)\\cdot Du(x)+c(x)u(x)=0 \\quad (x\\in \\Omega \\subset \\mathbb{R}^{n}) \\label{eq1} \\end{equation} $$\nここで、各変数 $p, z, x$ を $p, z, x$とします。\n$$ \\begin{equation} F(p,\\ z,\\ x)=\\mathbf{b}(x)\\cdot p +c(x)z=b_{1}p_{1}+\\cdots +b_{n}p_{n}+cz = 0 \\label{eq2} \\end{equation} $$\n$D_{p}F$ を計算すると、次のようになります。\n$$ D_{p}F=(F_{p_{1}}, \\dots, F_{p_{n}})=(b_{1}, \\dots, b_{n})=\\mathbf{b}(x) $$\nしたがって、特性方程式は次のようになります。\n$$ \\begin{align*} \\dot{\\mathbf{x}}(s) \u0026amp;= \\mathbf{b}(x) \\\\ \\dot{z}(s) \u0026amp;= \\mathbf{b}(\\mathbf{x}(s))\\cdot \\mathbf{p}(s) \\end{align*} $$\nこのとき、$(2)$により、$\\dot{z}(s)$ は次のようになります。\n$$ \\dot{z}(s) = -c(\\mathbf{x}(s))z $$\nしたがって、同次線形1階偏微分方程式の特性方程式は次のようになります。\n$$ \\left\\{ \\begin{align*} \\dot{\\mathbf{x}}(s)\u0026amp;=\\mathbf{b}(x) \\\\ \\dot{z}(s) \u0026amp;= -c(\\mathbf{x}(s))z \\end{align*} \\right. $$\nこのとき、$\\mathbf{p}(s)$ に関する特性方程式は問題を解くのに必要ありませんことを例を通じて確認できます。\n例 次のような微分方程式が与えられたとします。\n$$ \\left\\{ \\begin{align*} x_{1} u_{x_{2}} - x_{2} u_{x_{1}} \u0026amp;= u \u0026amp;\u0026amp; \\text{in } \\Omega \\\\ u\u0026amp;=g \u0026amp;\u0026amp; \\text{on } \\Gamma \\end{align*} \\right. $$\n$\\Omega=\\left\\{ x_{1}\u0026gt;0, x_{2}\u0026gt;0 \\right\\}$ $\\Gamma=\\left\\{ x_{1}\u0026gt;0, x_{2}=0 \\right\\}$ その場合、$(1)$ から $\\mathbf{b}=(-x_{2}, x_{1}), c=-1$ です。したがって、特性方程式は次のようになります。\n$$ \\left\\{ \\begin{align*} \\dot{x}^{1} \u0026amp;= -x^{2} \\\\ \\dot{x}^{2} \u0026amp;=x^{1} \\\\ \\dot{z}\u0026amp;=z \\end{align*} \\right. $$\nこれは簡単な常微分方程式なので、次のように簡単に解けます。\n$$ \\left\\{ \\begin{align*} x^{1}(s) \u0026amp;=x^{0}\\cos s \\\\ x^{2}(s)\u0026amp;=x^{0} \\sin s \\\\ z(s)\u0026amp;=z^{0}e^s=g(x^{0})e^s \\end{align*} \\right. $$\nここで、 $x^{0}$ は $s=0$ のときに $x_{1}-$軸($\\Gamma$) を通過するように選ばれた定数です。その後、点 $(x_{1}, x_{2}) \\in \\Omega$ を固定しましょう。\n$$ (x_{1},\\ x_{2})=(x^{1}(s),\\ x^{2}(s)) = (x^{0} \\cos (s),\\ x^{0} \\sin (s)) $$\nすると、 $s\u0026gt;0, x^{0}\u0026gt;0$ の場合、次の結果が得られます。\n$$ x_{1}^{2} + x_{2}^{2} = (x^{0})^{2}\\cos^{2}(s) + (x^{0})^{2}\\sin^{2}(s) = (x^{0})^{2} \\implies x^{0}=({x_{1}}^{2}+{x_{2}}^{2})^{1/2} \\\\ \\dfrac{x_{2}}{x_{1}} = \\dfrac{x^{0}\\sin (s)}{x^{0} \\cos (s)} = \\tan (s) \\implies s=\\arctan \\left( \\frac{x_{2}}{x_{1}} \\right) $$\nしたがって、方程式の解は次のようになります。\n$$ \\begin{align*} u(x)\u0026amp;=u(x^{1}(s),\\ x^{2}(s)) \\\\ \u0026amp;= z(s) \\\\ \u0026amp;=g(x^{0})e^s \\\\ \u0026amp;= g(({x_{1}}^{2}+{x_{2}}^{2})^{1/2})e^{\\arctan \\left(\\frac{x_{2}}{x_{1}}\\right)} \\end{align*} $$\n■\n準線形 次に、与えられた微分方程式が最高微分項に関して線形である場合を考えます。今扱っているのは1階微分方程式なので、1階微分項に関して線形な場合です。\n$$ F(Du,\\ u,\\ x)=\\mathbf{b}(x,\\ u(x))\\cdot Du(x)+c(x,\\ u(x))=0 $$\nここで、各変数 $p, z, x$ を $p, z, x$ とします。\n$$ \\begin{equation} F(p, z, x)=\\mathbf{b}(x, z)\\cdot p + c(x, z)=b_{1}p_{1} + \\cdots + b_{n} p_{n} +c=0 \\label{eq3} \\end{equation} $$\n$D_{p}F$ を計算すると、次のようになります。\n$$ D_{p}F=(F_{p_{1}},\\ \\cdots,\\ F_{p_{n}})=(b_{1},\\ \\cdots,\\ b_{n})=\\mathbf{b}(x,\\ z) $$\nしたがって、特性方程式は次のようになります。\n$$ \\begin{align*} \\dot{\\mathbf{x}}(s) \u0026amp;= \\mathbf{b}(\\mathbf{x}(s),\\ z(s)) \\\\ \\dot{z}(s) \u0026amp;= \\mathbf{b}(\\mathbf{x}(s),\\ z(s))\\mathbf{p}(s)=-c(\\mathbf{x}(s),\\ z(s)) \\end{align*} $$\n$\\dot{z}$ の2つ目の等号は $(3)$ によって成立します。この場合も $\\mathbf{p}(s)$ に関する条件は問題を解くのに必要ありません。\n例 次のような微分方程式が与えられたとします。\n$$ \\left\\{ \\begin{align*} u_{x_{1}} + u_{x_{2}} \u0026amp;= u^{2} \u0026amp;\u0026amp; \\text{in } \\Omega \\\\ u\u0026amp;=g \u0026amp;\u0026amp; \\text{on } \\Gamma \\end{align*} \\right. $$\n$\\Omega=\\left\\{ x_{2} \\gt 0 \\right\\}$ $\\Gamma=\\left\\{ x_{2} = 0 \\right\\}$ その場合、$(3)$ から $\\mathbf{b}=(1, 1)$, $c=-z^{2}$ です。したがって、特性方程式は次のようになります。\n$$ \\left\\{ \\begin{align*} \\dot{x}^{1} \u0026amp;=1, \\dot{x}^{2}=1 \\\\ \\dot{z} \u0026amp;= z^{2} \\end{align*} \\right. $$\nこれはそれぞれ単純な常微分方程式なので、次のように解けます。\n$$ \\left\\{ \\begin{align*} x^{1}(s) \u0026amp;= x^{0}+s, x^{2}(s)=s \\\\ z(s)\u0026amp;=\\frac{z^{0}}{1-sz^{0}}=\\frac{g(x^{0})}{1-sg(x^{0})} \\end{align*} \\right. $$\nここで、$x^{0}$ は $s=0$ のときに $x_{2}-$軸($\\Gamma$) を通過するように選ばれた定数です。また、与えられた微分方程式は $u_{x_{1}}u_{x_{2}}=u$ ですから、$p_{1}^{0}p_{2}^{0}=z^{0}=(x^{0})^{2}$ となり、$p_{1}^{0}=\\frac{x^{0}}{2}$ です。これらを全て $(4)$ に代入すると、次の結果が得られます。\n$$ \\left\\{ \\begin{align*} p^{1}(s) \u0026amp;= \\frac{x^{0}}{2}e^s \\\\ p^{2}(s) \u0026amp;= 2x^{0}e^s \\\\ z(s) \u0026amp;= (x^{0})^{2}e^{2s} \\\\ x^{1}(s) \u0026amp;= 2x^{0}(e^s-1) \\\\ x^{2}(s) \u0026amp;= x^{0}+\\frac{x^{0}}{2}(e^s-1) \\end{align*} \\right. $$\nその後、点\n$(x_{1}, x_{2}) \\in \\Omega$ を固定しましょう。\n$$ (x_{1}, x_{2})=(x^{1}(s), x^{2}(s))=\\left( 2x^{0}(e^s -1), \\frac{x^{0}}{2}(e^s+1) \\right) $$\nすると、$s, x^{0}$ に関して次の結果が得られます。\n$$ x^{0}=\\frac{4x_{2}-x_{1}}{4},\\ \\ e^s=\\frac{x_{1}+4x_{2}}{4x_{2}-x_{1}} $$\nしたがって、方程式の解は次のようになります。\n$$ u(x)=u(x^{1}(s),\\ x^{2}(s))=z(s)=(x^{0})^{2}e^{2s}=\\dfrac{(x_{1}+4x_{2})^{2}}{16} $$\n■\n完全非線形 最後に、次のような微分方程式が与えられたとします。\n$$ \\begin{align*} u_{x_{1}}u_{x_{2}} \u0026amp;= u \u0026amp;\u0026amp; \\text{in } \\Omega \\\\ u \u0026amp;= x_{2}^{2} \u0026amp;\u0026amp; \\text{on } \\Gamma \\end{align*} $$\n$\\Omega=\\left\\{ x_{1}\u0026gt;0 \\right\\}$ $\\Gamma=\\left\\{ x_{1}=0 \\right\\}$ $F$ の変数を $P, z, x$ とすると、次のようになります。\n$$ F(p, z, x)=p_{1}p_{2}-z $$\nしたがって、特性方程式は次のようになります。\n$$ \\begin{align*} \\dot{p}^{1} \u0026amp;= p^{1},\\quad \\dot{p}^{2}=p^{2} \\\\ \\dot{z} \u0026amp;= 2p^{1}p^{2} \\\\ \\dot{x}^{1} \u0026amp;= p^{2},\\quad \\dot{x}^{2}=p^{1} \\end{align*} $$\nまず、 $p$ に関する微分方程式を解くと、次のようになります。\n$$ p^{1}(s)=p_{1}^{0}e^s,\\ \\ p^{2}(s)=p_{2}^{0}e^s $$\nこのとき、 $p_{1}^{0}=p(0)$ および $p_{2}^{0}=p(0)$ です。したがって、$\\dot{z}(s)=2p_{1}^{0}p_{2}^{0}e^{2s}$ であるため、$z$ は次のようになります。\n$$ z(s)=p_{1}^{0}p_{2}^{0}e^{2s}+C $$\n$z(0)=z^{0}=p_{1}^{0}p_{2}^{0}+C$ なので、$C=z^{0}-p_{1}^{0}p_{2}^{0}$ です。したがって、次のようになります。\n$$ z(s)=z^{0}+p_{1}^{0}p_{2}^{0}(e^{2s}-1) $$\n同様の方法で $x^{1}$ および $x^{2}$ も計算すると、次のようになります。\n$$ \\begin{equation} \\left\\{ \\begin{aligned} p^{1}(s) \u0026amp;= p_{1}^{0}e^s \\\\ p^{2}(s) \u0026amp;= p_{2}^{0}e^s \\\\ z(s) \u0026amp;= z^{0}+p_{1}^{0}p_{2}^{0}(e^{2s}-1) \\\\ x^{1}(s) \u0026amp;= p_{2}^{0}(e^s-1) \\\\ x^{2}(s) \u0026amp;= x^{0}+p_{1}^{0}(e^s-1) \\end{aligned} \\right. \\label{eq4} \\end{equation} $$\nこのとき、 $x^{0}$ は $s=0$ のときに $x_{1}-$軸($\\Gamma$) を通過するように選ばれた定数です。$u_{x_{2}}=p^{2}$ および境界条件により、$x_{2}^{0}=u(0, x^{0})=2x^{0}$ です。また、与えられた微分方程式は $u_{x_{1}}u_{x_{2}}=u$ であるため、$p_{1}^{0}p_{2}^{0}=z^{0}=(x^{0})^{2}$ であり、$p_{1}^{0}=\\frac{x^{0}}{2}$ です。これらを全て $(4)$ に代入すると、次の結果が得られます。\n$$ \\left\\{ \\begin{align*} p^{1}(s) \u0026amp;= \\frac{x^{0}}{2}e^s \\\\ p^{2}(s) \u0026amp;= 2x^{0}e^s \\\\ z(s) \u0026amp;= (x^{0})^{2}e^{2s} \\\\ x^{1}(s) \u0026amp;= 2x^{0}(e^s-1) \\\\ x^{2}(s) \u0026amp;= x^{0}+\\frac{x^{0}}{2}(e^s-1) \\end{align*} \\right. $$\nその後、点 $(x_{1}, x_{2})\\in \\Omega$ を固定しましょう。\n$$ (x_{1}, x_{2})=(x^{1}(s), x^{2}(s))=\\left( 2x^{0}(e^s -1), \\frac{x^{0}}{2}(e^s+1) \\right) $$\nすると、 $s, x^{0}$ に関して次の結果が得られます。\n$$ x^{0}=\\frac{4x_{2}-x_{1}}{4},\\ \\ e^s=\\frac{x_{1}+4x_{2}}{4x_{2}-x_{1}} $$\nしたがって、方程式の解は次のようになります。\n$$ u(x)=u(x^{1}(s),\\ x^{2}(s))=z(s)=(x^{0})^{2}e^{2s}=\\dfrac{(x_{1}+4x_{2})^{2}}{16} $$\n■\nLawrence C. Evans, Partial Differential Equations (第2版, 2010年), p99-102\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1074,"permalink":"https://freshrimpsushi.github.io/jp/posts/1074/","tags":null,"title":"特性方程式を利用した非線形1系偏微分方程式の解法。"},{"categories":"수치해석","contents":"定義 1 異なる$x_{0} , \\cdots , x_{n}$のデータ$(x_{0}, y_{0}) , \\cdots , (x_{n} , y_{n})$について、$p (x_{i} ) = y_{i}$と$\\deg p \\le n$を満たす多項式関数$p$を多項式補間Polynomial Interpolationという。\n定理 存在性と一意性 [1]: 与えられたデータに対し、 $p$は一意に存在する。 ラグランジュの公式 [2]: $$p_{n} (x) = \\sum_{i=0}^{n} y_{i} l_{i} (X)$$ ニュートンの差分公式 [3]: $$p_{n} (x) = f(x_{0}) + \\sum_{i=1}^{n} f [ x_{0} , \\cdots , x_{i} ] \\prod_{j=0}^{i-1} (x - x_{j} )$$ 誤差解析 [4]: $(n+1)$回微分可能な$f : \\mathbb{R} \\to \\mathbb{R}$とある$\\xi \\in \\mathscr{H} \\left\\{ x_{0} , \\cdots , x_{n} \\right\\}$に対し、$f$の多項式補間$p_{n}$はある$t \\in \\mathbb{R}$に対して次を満たす。$$\\displaystyle f(t) - p_{n} (t) = {{ (t - x_{0}) \\cdots (t - x_{n}) } \\over { (n+1)! }} f^{(n+1)} ( \\xi )$$ $\\mathscr{H} \\left\\{ a,b,c, \\cdots \\right\\}$は$a,b,c, \\cdots$を含む最小の区間を表す。 説明 多項式補間は韓国語で多項式補間法と簡化される。\n条件 $\\deg p \\le n$ $\\deg p \\le n$という条件は、$p$が$\\deg p = n$を満たす保証が常にあるわけではないことを意味する。例えば$n=2$の時、上のように3点が一直線上にある場合、$(n+1)=3$個の点を通る$p_{2} (x) = a_{2} x^{2} + a_{1} x + a_{0}$は存在しないが、それよりも低い次数の$p_{1} (x) = a_{1} x + a_{0}$は存在する。これは実際に$p_{2} (x) = a_{2} x^{2} + a_{1} x + a_{0}$を見つけたが$a_{2} = 0$の場合を意味する。\nラグランジュの公式とニュートンの差分公式は同じである 公式[2]と[3]は違うように見えても、実際には[1]によって同じであることがわかる。本質的に2つの公式の違いは$p_{n}$をどう表すかの差に過ぎず、機能的な違いは新しいデータが追加された時にニュートンの差分公式が更新しやすいという点だけである。\n実際の関数との誤差 定理[4]は、ある関数$f$を補間する$p_{n}$が$f$とどの程度違うかを示す。通常の場合には$(n+1)!$の発散速度は非常に速いため、データが多ければ多いほど補間$p_{n}$の正確性は上がる。しかし、この公式は特に収束性を論じるわけではないことに注意が必要だ。簡単な例で$t$が$\\mathscr{H} \\left\\{ x_{0} , \\cdots , x_{n} \\right\\}$から非常に遠い場所にある場合を考えることができる。また、$f$がそれほど良くなくて微分するたびに値が大きくなり、それが更に$(n+1)!$の発散速度よりも速い場合、どんなに変になるかだってある。\n証明 [1] 戦略: $(n+1)$個の連立方程式を行列で表し、逆行列の存在性と一意性を同時に持ってくる。\n全ての$i = 0, \\cdots , n$に対して$y_{i} = p_{n} (x_{i}) = a_{0} + a_{1} x_{i} + \\cdots + a_{n} x_{i}^{n}$を満たす$p_{n}$の係数$a_{0} , a_{1} , \\cdots , a_{n}$が一意であることを示せば良い。 $$ \\mathbb{y} := \\begin{bmatrix} y_{0} \\\\ y_{1} \\\\ \\vdots \\\\ y_{n} \\end{bmatrix}, X := \\begin{bmatrix} 1 \u0026amp; x_{0} \u0026amp; \\cdots \u0026amp; x_{0}^{n} \\\\ 1 \u0026amp; x_{1} \u0026amp; \\cdots \u0026amp; x_{1}^{n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 1 \u0026amp; x_{n} \u0026amp; \\cdots \u0026amp; x_{n}^{n} \\end{bmatrix} , \\mathbb{a} := \\begin{bmatrix} a_{0} \\\\ a_{1} \\\\ \\vdots \\\\ a_{n} \\end{bmatrix} $$と定義し、連立方程式を行列で表すと $$ \\begin{bmatrix} y_{0} \\\\ y_{1} \\\\ \\vdots \\\\ y_{n} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; x_{0} \u0026amp; \\cdots \u0026amp; x_{0}^{n} \\\\ 1 \u0026amp; x_{1} \u0026amp; \\cdots \u0026amp; x_{1}^{n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 1 \u0026amp; x_{n} \u0026amp; \\cdots \u0026amp; x_{n}^{n} \\end{bmatrix} \\begin{bmatrix} a_{0} \\\\ a_{1} \\\\ \\vdots \\\\ a_{n} \\end{bmatrix} $$ 今、$\\mathbb{y} = X \\mathbb{a}$を満たす解$\\mathbb{a}$を見つける問題に変わった。\nヴァンデルモンド行列の行列式: $X$の行列式は$\\displaystyle \\det X = \\prod_{1 \\le i \u0026lt; j \\le n } (x_{j} - x_{i})$\n仮定から$x_{0} , \\cdots , x_{n}$は異なるので$\\det X \\ne 0$であり、$X^{-1}$が存在する。したがって、$\\mathbb{a} = X^{-1} \\mathbb{y}$も存在する。一方で、与えられた行列に対する逆行列は一意なので、$\\mathbb{a}$も一意である。\n■\n[2] クロネッカーデルタ関数で見る。\n■\n[3] 差分そのままを使って正直に計算する。\n■\n[4] 戦略: 新しいダミー関数を定義し、それらの微分可能性を利用して直接的な計算を回避する。設定が複雑なので、実際には後ろから理解するほうが楽である。\nClaim: $E (x) := f(x) - p_{n} (X)$に対して次が成立する。 $$ E(t) = {{ (t - x_{0}) \\cdots (t - x_{n}) } \\over { (n+1)! }} f^{(n+1)} ( \\xi ) $$\nPart 1.\nまず、$t$がノードポイント$x_{0} , \\cdots , x_{n}$と同じなら自明に成立するので、$t$がこれらのノードポイントと異なると仮定する。 $$ \\begin{align*} \\Psi (x) :=\u0026amp; (x - x_{0} ) (x - x_{1}) \\cdots (x - x_{n}) \\\\ G (x) :=\u0026amp; E(x) - {{ \\Psi (x) } \\over { \\Psi (t) }} E(t) \\end{align*} $$ 上記のように新しい関数を定義すると、$f$、$p_{n}$、$\\Psi$が$(n+1)$回微分可能であるため、$G$も$(n+1)$回微分可能である。\nPart 2.\n$x = x_{i}$を$G$に代入すると$E(x_{i}) = f(x_{i}) - p_{n} (x_{i}) = 0$であり、$\\Psi (x_{i} ) = 0$なので $$ G(x_{i}) = E(x_{i} ) - {{ \\Psi (x_{i}) } \\over { \\Psi (t) }} E(t) = 0 $$ $x = t$を$G$に代入すると$\\displaystyle {{ \\Psi (t) } \\over { \\Psi (t) }} = 1$なので $$ G(t) = E(t) - E(t) =0 $$ したがって、$G$は$(n+2)$個の異なる零点$x_{0} , \\cdots , x_{n} , t$を持つ。\nPart 3.\n便宜上$x_{n+1} :=t$としよう。\nロルの定理: 関数$f(x)$が$[a,b]$で連続であり、$(a,b)$で微分可能であり、$f(a)=f(b)$ならば、$f ' (c)=0$を満たす$c$が$(a,b)$に少なくとも一つ存在する。\n全ての$i=0, \\cdots , n$に対して $$ G(x_{i}) = 0 = G(x_{i+1}) $$ なのでロルの定理により$g ' ( x\u0026rsquo;_{i}) = 0$を満たす$x'_{i} \\in \\mathscr{H} \\left\\{ x_{i} , x_{i+1} \\right\\}$が存在し、同様に全ての$i=0, \\cdots , (n-1)$に対して $$ g ' (x\u0026rsquo;_{i}) = 0 = g ' (x\u0026rsquo;_{i+1}) $$ なのでロルの定理により$G''( x\u0026rsquo;\u0026rsquo;_{i}) = 0$を満たす$x''_{i} \\in \\mathscr{H} \\left\\{ x\u0026rsquo;_{i} , x\u0026rsquo;_{i+1} \\right\\}$が存在する。このように帰納的にロルの定理を$(n+1)$回使用して $$ G^{(n+1)} ( \\xi ) = 0 $$ を満たす$\\xi \\in \\mathscr{H} \\left\\{ x_{0} , \\cdots , x_{n+1} \\right\\}$の存在を保証することができる。\n一方で、$p_{n}$は$n$次の多項式なので $$ E^{(n+1)} (x) = f^{(n+1)} ( x) $$ $\\Psi$の最高次項は$x^{n+1}$であるため $$ \\Psi^{(n+1)} (x) = (n+1)! $$ $\\displaystyle G (x) = E(x) - {{ \\Psi (x) } \\over { \\Psi (t) }} E(t)$の両辺を$x$に対して$(n+1)$回微分すると $$ G^{(n+1)} (x) = f^{(n+1)} ( x) - {{ (n+1)! } \\over { \\Psi (t) }} E(t) $$ $x=\\xi$を$G^{(n+1)}$と$f^{(n+1)}$に代入すると $$ 0 = f^{(n+1)} ( \\xi ) - {{ (n+1)! } \\over { \\Psi (t) }} E(t) $$ $E (t) = f(t) - p_{n} (t)$なので、次を得る。 $$ f(t) - \\sum_{j=0}^{n} f( x_{j} ) l_{j} (t) = {{ (t - x_{0}) \\cdots (t - x_{n}) } \\over { (n+1)! }} f^{(n+1)} ( \\xi ) $$\n■\nAtkinson. (1989). 数値解析入門(第2版): p131.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1021,"permalink":"https://freshrimpsushi.github.io/jp/posts/1021/","tags":null,"title":"多項式補間"},{"categories":"편미분방정식","contents":"表記法1 非線形1次偏微分方程式は次のように表記される。\n$$ \\begin{equation} F(Du, u, x) = F(p, z, x) = 0 \\label{eq1} \\end{equation} $$\n$\\Omega \\subset \\mathbb{R}^{n}$は開集合 $x\\in \\Omega$ $F : \\mathbb{R}^n \\times \\mathbb{R}^n \\times \\bar{ \\Omega } \\to \\mathbb{R}$は与えられた関数 $u : \\bar{ \\Omega } \\to \\mathbb{R}$は $F$の変数だ 説明 非線形1次偏微分方程式 $F$を解くとは、与えられた$F$に対して$F=0$を満たす変数$u$を見つけることだ。この時$x$は時間と空間を両方含む変数だとしよう。\n$$ x=(x_{1}, \\dots, x_{n}=t) $$\nこの関数$F$は次のように表記される。\n$$ F=F(p, z, x)=F(p_{1}, \\dots, p_{n}, z, x_{1}, \\cdots, x_{n}) $$\n$p=Du(x) \\in\\mathbb{R}^n$ $z=u(x)\\in \\mathbb{R}$ $x\\in \\bar{ \\Omega }$ そして関数$F$は十分に滑らかであり偏微分可能と仮定する。一般的にそうだから特に強い条件ではない。それで、各変数に対する$F$の勾配は次のようになる。\n$$ \\begin{cases} D_{p} F=(F_{p_{1}},\\ \\cdots,\\ F_{p_{n}}) \\\\ D_{z}F=Fz \\\\ D_{x}F=(F_{x_{1}},\\ \\cdots,\\ F_{x_{n}} )\\end{cases} $$\nクレロの方程式をこの表記法で示すと次のようになる。\n$$ F(Du,\\ u,\\ x)=xDu+f(Du) $$\n境界値問題 よく微分方程式$\\eqref{eq1}$は境界条件と一緒に与えられる。その場合には次のように表記される。\n$$ \\begin{align*} F(Du,\\ u,\\ x)\u0026amp;=0 \u0026amp;\u0026amp; \\text{in } \\mathbb{\\Omega} \\\\ u\u0026amp;=g \u0026amp;\u0026amp; \\text{on } \\Gamma \\end{align*} $$ この時、$\\Gamma \\subset \\partial \\Omega, g : \\Gamma \\to \\mathbb{R}$だ。\nLawrence C. Evans, Partial Differential Equations (2nd Edition, 2010), p91-92\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1071,"permalink":"https://freshrimpsushi.github.io/jp/posts/1071/","tags":null,"title":"非線形一階偏微分方程式の表記法"},{"categories":"수치해석","contents":"定義 1 与えられた$(n+1)$ペアのデータ$(x_{0}, y_{0}) , \\cdots , (x_{n} , y_{n})$に対して、$f (x_{i} ) = y_{i}$を満たしつつある特定の性質を持つ$f$を見つける方法、またはその関数自体を内挿法（インターポレーション）という。\n説明 例えば、上に示されたようなデータがあるけど、真ん中のデータが空いている状況を想像してみよう。もちろん、実際のデータがあるのが最高だけど、ない場合は予測をしてでも使わなければならない状況があるかもしれない。このように、空いている部分を埋めるという点で、インターポレーションという表現は適切である。数値解析だけでなく、このようなアプリケーションはいつも必要になるかもしれない。\nインターポレーションの最も簡単な例として、点と点を直線で結ぶ線形内挿法（リニア・インターポレーション）を考えることができる。このようなインターポレーションは直感的という利点があるが、各データが存在する地点で微分をすることはできない。従って、下に示されているように、点々を滑らかにつなげる方法が必要な場合、使用できなくなる。このように、インターポレーションは一つの方法に限らず、必要な方法、望む方法を見つけなければならない。\nAtkinson. (1989). An Introduction to Numerical Analysis(2nd Edition): p131.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1016,"permalink":"https://freshrimpsushi.github.io/jp/posts/1016/","tags":null,"title":"数値解析における補間"},{"categories":"편미분방정식","contents":"定義[^1] 要素が非負の整数の組$\\alpha=(\\alpha_{1}, \\alpha_{2}, \\cdots, \\alpha_{n})$をオーダーが$|\\alpha|$のマルチインデックスmulti-indexと言う。ここで、$| \\alpha|$は以下のように定義される。\n$$ |\\alpha| = \\sum _{i}^{n} \\alpha_{i} = \\alpha_{1} + \\cdots + \\alpha_{n} $$\n表記法 $x = (x_{1}, x_{2}, \\dots, x_{n}) \\in \\mathbb{R}^{n}$に対して、$x^{\\alpha}$は以下のように定義される。\n$$ x^{\\alpha} := x_{1}^{\\alpha_{1}} x_{2}^{\\alpha_{2}} \\cdots x_{n}^{\\alpha_{n}} $$\nマルチインデックスは、以下のように偏微分を表す際によく使用される。\n$$ \\begin{align*} D^\\alpha :=\u0026amp;\\ \\dfrac{\\partial ^{|\\alpha|} } {{\\partial x_{1}}^{\\alpha_{1}}\\cdots {\\partial x_{n}}^{\\alpha_{n}}} \\\\ =\u0026amp;\\ \\left( \\frac{ \\partial }{ \\partial x_{1}} \\right)^{\\alpha_{1}}\\left( \\frac{ \\partial }{ \\partial x_{2}} \\right)^{\\alpha_{2}}\\cdots \\left( \\frac{ \\partial }{ \\partial x_{n}} \\right)^{\\alpha_{n}} \\\\ =\u0026amp;\\ \\partial^{\\alpha_{1}}_{x_{1}}\\cdots\\partial^{\\alpha_{n}}_{x_{n}} \\end{align*} $$\n例えば、$\\alpha=(2,1,0)$とするならば、$D^{\\alpha} u(x)$は以下を意味する。\n$$ D^{\\alpha} u(x)=\\dfrac{ \\partial^3 u(x)} {\\partial x_{1} \\partial x_{1} \\partial x_{2}}=\\dfrac{ \\partial^3 u(x)} {\\partial x_{1} ^{2} \\partial x_{2}} $$\nまた、整数$k \\ge 0$に対して、$D^k$を以下のように定義する。\n$$ D^ku:=\\left\\{ D^{\\alpha} u : |\\alpha|=k \\right\\} $$\n$D^{k}u$はオーダーが$k$の全てのマルチインデックス$\\alpha$に対する$D^{\\alpha} u$を集めた集合である。$k$はマルチインデックスではなく、非負の整数であることに注意。$D^{k}u$の要素にそれぞれ順序を付けること、つまり、それぞれが何番目の成分かを定めると、$D^k u$を$\\mathbb{R}^{k}$の点として考えることができる。[^2]次の例を見よ。\nケース 1. $k=1$\n勾配を意味する。\n$$ D^1 u=Du:=(u_{x_{1}},\\ u_{x_{2}},\\ \\cdots,\\ u_{x_{n}})=\\nabla u \\ \\in \\ \\mathbb{R^n} $$\nケース 2. $k=2$\nヘッセ行列を意味する。\n$$ D^2u := \\begin{pmatrix} u_{x_{1}x_{1}} \u0026amp; \\cdots \u0026amp; u{x_{1}x_{n}} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\cdots \\\\ u_{x_{n}x_{1}} \u0026amp; \\cdots \u0026amp; u_{x_{n}x_{n}} \\end{pmatrix} \\in \\ \\mathbb{R^2} $$\n特に、$u$のラプラシアンの場合、$u$のヘッセ行列の対角成分をすべて足したものと同じである。\n$$ \\Delta u=\\nabla^2=\\nabla \\cdot \\nabla u=\\mathrm{div} Du = \\sum_{i=1}^nu_{x_{i}x_{i}} = \\mathrm{tr} (D^2u) $$\n","id":1062,"permalink":"https://freshrimpsushi.github.io/jp/posts/1062/","tags":null,"title":"マルチインデックス表記法"},{"categories":"물리학","contents":"定義 波を三角関数で表したものをサイン波sinusoidal waveという。\n説明 サイン波の一般的な形は以下の通りだ。式が$\\cos$である理由は下で説明するが、それは複素波動関数の実部が$\\cos$だからだ。$\\sin$は虚部だ。\n$$ f(x,t) = A \\cos \\big( k(x-vt)+\\delta \\big) $$\nここで、$A$を波の振幅amplitudeとし、コサイン関数の変数$k(z-vt)+\\delta$を位相phase、$\\delta$を位相定数phase constantという。位相定数に$2\\pi$を加えても$f(x,t)$は変わらない。だから普通は位相定数として$0\\le \\delta \\lt 2\\pi$の範囲の値を使う。$k$は波数wave numberであり、波長wavelength$\\lambda$とは次のような関係がある。\n$$ k=\\dfrac{2\\pi}{\\lambda} $$\n波が完全に1回転する時間を周期periodという。時間=距離/速度なので、波の周期$T$は\n$$ T=\\dfrac{\\lambda}{v} = \\dfrac{2 \\pi}{kv} $$\n周期は1回振動するのにかかる時間なので、単位時間あたりの振動数である振動数frequency$\\nu$は当然周期の逆数と同じだ。\n$$ \\nu=\\dfrac{1}{T}=\\dfrac{v}{\\lambda} $$\n角振動数angular frequencyは一般に$\\omega$で表され、振動を等速円運動に対応させて表現するものだ。振動数を単位時間あたりの回転角度に変えた値であり、単位はラディアンだ。\n$$ \\omega=2\\pi \\nu=2\\pi\\dfrac{1}{T}=kv $$\n$(1)$を角振動数で表したら\n$$ f(x,t)=A \\cos \\big( kx-\\omega t +\\delta \\big) $$\nこれは波数が$k$で角振動数が$\\omega$の右へ進行する波動関数だ。\n上の図のように、$\\dfrac{\\delta}{k}$を波動関数が原点から遅れた距離として定義する。だから、波の進行方向が変われば、位相定数の符号も変わる。波が左へ進むなら、右へ移動したのが遅れたということだ。つまり、波数が$k$で角振動数が$\\omega$の左へ進行する波動関数は以下の通りだ。\n$$ f(x,t)=A \\cos \\big( kx+\\omega t -\\delta \\big) $$\nでも、コサイン関数は偶関数なので、上の式は下の式と同じだ。 $$ f(x,t)=A \\cos \\big( -kx-\\omega t +\\delta \\big) $$\nこれは、波数が$k$で角振動数が$\\omega$の右へ進行する波動関数$(2)$と比較して、波数$k$の符号だけが異なる。つまり、波数$k$の符号を変えると、振幅、位相定数、振動数、波長などはすべて同じだが、進行方向だけが反対の波になることが分かる。\n複素波動関数 波動関数がコサインで表されるので、オイラーの公式を使って複素指数関数の形でも表せる。虚部を広げて複素波動関数を扱う理由は、複素関数がコサインやサインよりも多くの面で計算に便利だからだ。$e^{ix}=\\cos x +i\\sin x$を使って$(2)$を表すと\n$$ f(x,t)=\\text{Re}(Ae^{i(kx-\\omega t +\\delta)}) $$\nここで、$\\text{Re}(a+ib)=a$。すなわち実部を表す。$f$は$Ae^{i(kx-\\omega t +\\delta)}$の実部だけを表した関数なので、$\\tilde{f}=Ae^{i(kx-\\omega t +\\delta)}$としよう。つまり、$\\text{Re}(\\tilde{f})=f$だ。すると、以下のように簡単に整理できる。\n$$ \\tilde{f}(x,t)=Ae^{i(kx-\\omega t+\\delta)}=Ae^{i\\delta}e^{i(kx-\\omega t)}=\\tilde{A}e^{i(kx-\\omega t)} $$\nこの記事で扱っている波動関数$f(x,t)$は複素波動関数の実部だ。\n$$ f(x,t)=\\text{Re}\\big( \\tilde{f}(x,t) \\big) $$\n","id":1066,"permalink":"https://freshrimpsushi.github.io/jp/posts/1066/","tags":null,"title":"サイン波と複素波動関数"},{"categories":"최적화이론","contents":"定義 1 スカラー関数 $\\varphi : \\mathbb{R}^{n} \\to \\mathbb{R}$ をコスト関数と言う。コスト関数 $ \\varphi ( \\mathbb{x} )$ の極小値を求めるために、$\\mathbb{x} = \\mathbb{x}_{n}$で$\\varphi ( \\mathbb{x}_{n+1} ) \u0026lt; \\varphi ( \\mathbb{x}_{n} )$を満たす$\\mathbb{x}_{n+1}$を見つけるアルゴリズムを降下法と言う。\n説明 例えば、家を一軒建てることをコスト関数 $\\varphi$ の例として考える。家を建てるために必要な資源は木、石、鉄、ガラス、労働費、不動産などがあり、これらは結局「いくらお金がかかるか」という問題に帰結する。この場合、$\\varphi$は各資源のベクトルをコストというスカラーにマッピングする関数になる。誰もが気になる質問は「その最小コストはいくらか」ということだろう。一方、機械学習などではコスト関数を損失関数とも呼び、この場合は「実際の値と予測値の差が最小になるのはいつか」となる。\nどのような問題であれ、これを数学的に抽象化すれば「スカラー関数の最小値を求める問題」と要約できる。降下法はこの問題を解くために$n$次元マニフォールド上でスカラーが極小値を見つけるメソッドだ。注意すべきは、最小値ではなく極小値であることだ。$\\varphi$が凸関数なら話は別だが、一般的な場合では降下法はローカルミニマムを見つけ出せるだけで、それがグローバルミニマムであるという保証はできない。\n勾配降下法 降下法の中でも勾配降下法は、コスト関数の勾配を利用して極小値を見つける、最も人気のある方法の一つだ。この方法の実地例を知りたければ、機械学習での勾配降下法を見るといい。\n適切な$\\alpha\u0026gt;0$に対して、$\\mathbb{x}_{n+1} := \\mathbb{x}_{n} - \\alpha \\nabla \\varphi ( \\mathbb{x}_{n} )$を勾配降下法と言う。\n$- \\nabla \\varphi ( \\mathbb{x}_{n} )$はマニフォールドが最も急激に減少する方向を示すので、$\\alpha$が適切ならば、$\\left\\{ \\mathbb{x}_{n} \\right\\}$は極小点へと収束するだろう。\nしかし、この方法は簡単で扱いやすい一方で、収束速度に関して他の方法より速いと言い難く、貪欲アルゴリズムである。つまり、各ステップで最善を尽くして局所的には最も大きく減少するが、グローバルに見た場合にはそれほど良い方法ではないかもしれないということだ。\nコード 以下はRコードで勾配降下法を実装したもので、勾配を計算するためにnumDerivパッケージを使った。\nlibrary(numDeriv)\roptimizer\u0026lt;-function(f,x0,alpha=0.01){\rwhile(abs(f(x0))\u0026gt;10^(-8)){\rx0\u0026lt;-x0-alpha*grad(f,x0)\r}\rreturn(x0)\r}\rz\u0026lt;-function(v){\rreturn((v[1]-2)^2+(v[2]-3)^2)\r}\roptimizer(z,c(0,0))\rz(c(1.999945,2.999918)) 上のコードを実行した結果は次の通りである。\nこの結果は非常に簡単な例で、$z(x,y) = (x-2)^2 + (y-3)^2$の最小値となる点が見つかったことを示している。次の曲面で、$z$が最小になる点は$(2,3)$である。\n一緒に見る 機械学習での勾配降下法 Atkinson. (1989). An Introduction to Numerical Analysis(2nd Edition): p113.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1012,"permalink":"https://freshrimpsushi.github.io/jp/posts/1012/","tags":null,"title":"数学における勾配降下法"},{"categories":"다변수벡터해석","contents":"定義 スカラー場 $f : \\mathbb{R}^{n} \\to \\mathbb{R}$の全微分を特にグラジエントgradient, 傾きと呼び、$\\nabla f$と表記する。\n$$ \\begin{align*} \\nabla f := f^{\\prime} =\u0026amp; \\begin{bmatrix} D_{1}f \u0026amp; D_{2}f \u0026amp; \\cdots \u0026amp; D_{n}f\\end{bmatrix} \\\\ =\u0026amp; \\begin{bmatrix} \\dfrac{\\partial f}{\\partial x_{1}} \u0026amp; \\dfrac{\\partial f}{\\partial x_{2}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial f}{\\partial x_{n}} \\end{bmatrix} \\\\ =\u0026amp; \\dfrac{\\partial f}{\\partial x_{1}}\\hat{x}_{1} + \\dfrac{\\partial f}{\\partial x_{2}}\\hat{x}_{2} + \\dots + \\dfrac{\\partial f}{\\partial x_{n}}\\hat{x}_{n} \\end{align*} $$\n説明 グラジエントは、簡単に言えば多変数関数の導関数だ。物理学などでよく使用される3次元スカラー関数のグラジエントは以下の通り。\n$$ \\nabla f = \\dfrac{\\partial f}{\\partial x}\\hat{\\mathbf{x}} + \\dfrac{\\partial f}{\\partial y}\\hat{\\mathbf{y}} + \\dfrac{\\partial f}{\\partial z}\\hat{\\mathbf{z}} = \\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right) $$\n注目すべき点は、関数値がスカラーであるスカラー関数の導関数が、関数値がベクトルであるベクトル関数になることである。これは全微分の定義から当然とも言えるが、直感的にも理解することができる。\n例として、上の図を考えてみよう。この図は、$z(x,y) = x^2 - y^2$ として定義される関数 $z : \\mathbb{R}^{2} \\to \\mathbb{R}$ を視覚的に示したものである。$y = f(x)$のような一変数関数とは異なり、変数が2つ以上ある関数の変化率を考える場合、その大きさだけでなく方向も考慮する必要があることがわかる。\nこの概念を反映した方向微分は、任意の方向への微分を意味する。したがって、多変数関数は無数の方向の微分を持っているが、下の定理からグラジエントは変化率が最も大きい方向を指すことがわかる。\n証明 $\\left\\| \\mathbb{d} \\right\\| = 1$ となる方向ベクトル $\\mathbb{d} : = ( d_1 , \\cdots , d_n )$ を定義しよう。多変数関数のテイラーの定理により、\n$$ f \\left( x_{0} + h \\mathbb{d} \\right) = f ( \\mathbb{x}_{0} ) + h \\left[ {{ \\partial f ( \\mathbb{x}_{0} ) } \\over { \\partial x_{1} }} d_{1} + \\cdots + {{ \\partial f ( \\mathbb{x}_{0} ) } \\over { \\partial x_{n} }} d_{n} \\right] + O (h^2) $$\n行列の形に変換すると、\n$$ f \\left( x_{0} + h \\mathbb{d} \\right) - f ( \\mathbb{x}_{0} ) = h \\begin{bmatrix} {{ \\partial f ( \\mathbb{x}_{0} ) } \\over { \\partial x_{1} }} \\\\ \\vdots \\\\ {{ \\partial f ( \\mathbb{x}_{0} ) } \\over { \\partial x_{n} }} \\end{bmatrix} \\cdot \\begin{bmatrix} d_{1} \\\\ \\vdots \\\\ d_{n} \\end{bmatrix} + O (h^2) $$\nベクトルの形にすると、\n$$ {{ f \\left( x_{0} + h \\mathbb{d} \\right) - f ( \\mathbb{x}_{0} )} \\over {h}} = \\nabla f \\left( \\mathbb{x}_{0} \\right) \\cdot \\mathbb{d} + O (h) $$\n$h \\to 0$ の時、\n$$ \\nabla f \\left( \\mathbb{x}_{0} \\right) \\cdot \\mathbb{d} = \\lim_{h \\to 0} {{ f \\left( x_{0} + h \\mathbb{d} \\right) - f ( \\mathbb{x}_{0} )} \\over {h}} $$\n$\\mathbb{b}$ が $\\mathbb{x}_{0}$ から $f$ への傾きと同じ方向であるということは、$\\mathbb{d}$ が\n$$ \\lim_{h \\to 0} {{ f \\left( x_{0} + h \\mathbb{d} \\right) - f ( \\mathbb{x}_{0} )} \\over {h}} $$\nこれを最大化するという意味であり、これを満たす場合は $\\displaystyle \\mathbb{d} = {{\\nabla f \\left( \\mathbb{x}_{0} \\right) } \\over { \\left\\| \\nabla f \\left( \\mathbb{x}_{0} \\right) \\right\\| }}$ のみであり、\n$$ \\nabla f \\left( \\mathbb{x}_{0} \\right) = \\left\\| \\nabla f \\left( \\mathbb{x}_{0} \\right) \\right\\| \\mathbb{d} $$\nこれが $\\mathbb{x}_{0}$ から $f$ のグラジエントになる。\n■\n参照 3次元直交座標系でのグラジエント ベクトル場の微分係数：ヤコビ行列 ","id":1010,"permalink":"https://freshrimpsushi.github.io/jp/posts/1010/","tags":null,"title":"スカラーフィールドの勾配"},{"categories":"다변수벡터해석","contents":"定義 $D \\subset \\mathbb{R}^{n}$ で定義された多変数スカラ関数 $f : D \\to \\mathbb{R}$ に対して、次のような行列 $H \\in \\mathbb{R}^{n \\times n}$ を $f$ のヘッセ行列と呼ぶ。\n$$ H := \\begin{bmatrix} {{\\partial^2 f } \\over {\\partial x_{1}^2 }} \u0026amp; \\cdots \u0026amp; {{\\partial^2 f } \\over { \\partial x_{1} \\partial x_{n} }} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ {{\\partial^2 f } \\over {\\partial x_{n} \\partial x_{1} }} \u0026amp; \\cdots \u0026amp; {{\\partial^2 f_{m} } \\over {\\partial x_{n}^2 }} \\end{bmatrix} $$\n説明 $f$のヘッシアンについて、次のような表記が使われる。\n$$ H,\\quad H(f),\\quad H_{f},\\quad \\mathbf{H},\\quad \\nabla^{2}f $$\nこのとき、$\\nabla^{2}$はラプラシアンとしてもよく使われる表記なので注意しよう。\nヤコビ行列が関数の高次元的な微分に該当するなら、ヘッセ行列は高次元的な二階微分とみなすことができる。もちろんヤコビ行列ほど頻繁に現れないが、数理統計学のように思わぬところにたまに登場する。また、ヘッセ行列はスカラ関数に対してのみ定義されることに注意が必要だ。\n","id":992,"permalink":"https://freshrimpsushi.github.io/jp/posts/992/","tags":null,"title":"ヘシアン行列とは何か？"},{"categories":"머신러닝","contents":"定義 実際の生物の閾値を模倣した非線形関数を活性化関数activation functionと言う。\n数学的定義 ディープラーニングでは非線形スカラー関数 $\\sigma : \\mathbb{R}^{n} \\to \\mathbb{R}$を活性化関数と呼ぶ。\nもちろん、この定義から外れるソフトマックスみたいなのもあるが、例外としよう。 説明 一方でベクトル関数はレイヤーlayer、層と呼ばれる。\n$\\sigma : \\mathbb{R} \\to \\mathbb{R}$で定義された活性化関数が入力としてベクトルを受け取るという表現やコードがあれば、成分ごとに適用するという意味だ。\n$$ \\sigma (\\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n} \\end{bmatrix}) = \\begin{bmatrix} \\sigma (x_{1}) \\\\ \\sigma (x_{2}) \\\\ \\vdots \\\\ \\sigma (x_{n}) \\end{bmatrix} $$\nモチーフ 閾値とは、生物が刺激に対してどのような反応を引き起こすために必要な最小限の刺激の強さのことで、ディープラーニングはこれを模倣するために、各ノードの計算結果に活性化関数を適用して次のレイヤーへと渡す。このような非線形的補正がなければ、ディープラーニングでヒドゥンレイヤーを置いて計算を何度もする意味がない。活性化関数には様々な種類があるが、どれが良いかはまさにケースバイケースだ。どの活性化関数を使えばパフォーマンスがどう変わるのかという理論はほとんどなく、ただ試してみて結果が良ければ採用するという感じだ。\n例 ステップ関数 $$u (x) := \\begin{cases} 0 \u0026amp; , x\u0026lt;0 \\\\ 1 \u0026amp; x \\ge 0 \\end{cases} $$\nステップ関数は閾値という名前に最も適した関数だが、計算結果をあまりに単純化しすぎるため、実際に使うのは難しい。他の活性化関数もステップ関数ではないが、ステップ関数のように作用するように設計されたと考えると良い。\nシグモイド関数 シグモイド関数の中で最も有名なのはおそらくロジスティック関数 $\\displaystyle \\sigma (x) := {{1} \\over { 1 + e^{-x} }}$ で、まるでステップ関数を連続関数につなげたような形状をしている。値域は異なるが、$\\tanh x$ も同じ理由で使用された。最近では、グラディエントバニッシングという問題が発見され、ほとんど使われなくなった。\nReLu(整流線形ユニット) 関数 $$\\operatorname{ReLU} (x) := \\max \\left\\{ 0 , x \\right\\}$$\nシグモイド関数の問題点を克服するために考案された関数だ。$x \u0026lt;0$ ならば関数値を完全に殺し、$0$ を超えないとそのまま伝達される、という点で活性化関数らしい。\n","id":991,"permalink":"https://freshrimpsushi.github.io/jp/posts/991/","tags":null,"title":"ディープラーニングにおける活性化関数"},{"categories":"머신러닝","contents":"定義 ディープラーニングは、人工神経網を使用した機械学習の一種で、特に人工神経網を構成する際に複数のレイヤーを使用する技術を言います。\nモチベーション 人間の脳がニューロンの複雑な接続関係で構成されているように、ディープラーニングも人工神経網の接続をより複雑にしてパフォーマンスを上げます。感覚細胞から受けた刺激が脊髄を通じて脳に伝えられるように、人工神経網は複数のレイヤーを経て計算を伝えます。これらのレイヤーをヒドゥンレイヤーHidden Layer, 隠れ層と呼びます。\nまた、全ての人の脳が異なるように、人工神経網の構造によって、得意な問題、苦手な問題があります。そのため、問題に適した人工神経網を構築し、それを最適化することもこの分野での熱い議論です。\n","id":996,"permalink":"https://freshrimpsushi.github.io/jp/posts/996/","tags":null,"title":"ディープラーニングとは？"},{"categories":"다변수벡터해석","contents":"定義 $D \\subset \\mathbb{R}^{n}$で定義された多変数ベクトル関数 $\\mathbb{f} : D \\to \\mathbb{R}^{m}$が各スカラー関数 $f_{1} , \\cdots , f_{m} : D \\to \\mathbb{R}$に対して\n$$ \\mathbb{f} ( x_{1} , \\cdots , x_{n} ) : = \\begin{bmatrix} f_{1} ( x_{1} , \\cdots , x_{n} ) \\\\ \\vdots \\\\ f_{m} ( x_{1} , \\cdots , x_{n} ) \\end{bmatrix} $$\nと定義されているとしよう。\n$$ J := \\begin{bmatrix} {{\\partial f_{1} } \\over {\\partial x_{1} }} \u0026amp; \\cdots \u0026amp; {{\\partial f_{1} } \\over {\\partial x_{n} }} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ {{\\partial f_{m} } \\over {\\partial x_{1} }} \u0026amp; \\cdots \u0026amp; {{\\partial f_{m} } \\over {\\partial x_{n} }} \\end{bmatrix} $$\nを$\\mathbb{f}$のヤコビ行列という。\n説明 次のような記法もよく使われる。\n$$ J = \\dfrac{\\partial (f_{1}, \\dots f_{m})}{\\partial (x_{1}, \\dots, x_{n})} $$\n$\\mathbb{f}$のヤコビ行列は$D \\mathbb{f} := J$になるような演算子$D$を定義して表現されることもある。ヤコビ行列という名称は19世紀ドイツの数学者カール・グスタフ・ヤコブ・ヤコビから来ているので、ヤコビ行列と書いて読むのが正しいが、実際には$J$が\u0026rsquo;ヤコビアン\u0026rsquo;と読まれることが非常に多い。\n全微分とも呼ばれ、多変数ベクトル関数の微分を意味する。したがって、多変数関数にヤコビ行列が存在する場合は、微分可能であるとされ、逆に微分可能な関数 $f : \\mathbb{R} \\to \\mathbb{R}$が$1 \\times 1$サイズのヤコビ行列を持つと考えることもできる。簡単に言えば、ヤコビ行列はベクトル関数の微分係数行列である。\n通常は極座標とともに解析学で最初に接するもので、\n$$ \\int_{B} \\int_{A} f(x,y) dx dy $$\nで使用される直交座標を$x= r \\cos \\theta$, $y= r \\sin \\theta$のように変更すると、ご存じの通り\n$$ \\int_{B} \\int_{A} f( r \\cos \\theta , r \\sin \\theta ) r dr d \\theta $$\nとして$r$が一つ追加される。これは\n$$ \\begin{bmatrix} {{\\partial x } \\over {\\partial r }} \u0026amp; {{\\partial x } \\over {\\partial \\theta }} \\\\ {{\\partial y } \\over {\\partial r }} \u0026amp; {{\\partial y } \\over {\\partial \\theta }} \\end{bmatrix} = \\begin{bmatrix} \\cos \\theta \u0026amp; \\sin \\theta \\\\ -r \\sin \\theta \u0026amp; r \\cos \\theta \\end{bmatrix} $$\nの行列式が$r \\cos^2 \\theta + r \\sin^2 \\theta = r$のように求められるからである。同じセンスで、ヤコビ行列は高校で積分の変数置換を行うときすでに接した概念そのものである。例えば、\n$$ \\int_{0}^{1} ( 27x^3 + 9 x^2 + 3 x ) dx $$\nを計算する場合、$3x = y$のような置換を行うと考えてみる。これを$y$が$x$に対する関数$y(x) = 3x$であると見ると、そのヤコビ行列は\n$$ \\begin{bmatrix} {{\\partial 3x } \\over {\\partial x }} \\end{bmatrix} = \\begin{bmatrix} 3 \\end{bmatrix} $$\nとなる。これは$3x = y$の両辺をそれぞれの変数で微分して$3dx = dy$を得るのと同じである。\n参照 スカラー場の微分係数：デル演算子 座標変換とヤコビアン ","id":989,"permalink":"https://freshrimpsushi.github.io/jp/posts/989/","tags":null,"title":"ヤコビ行列あるいはジャコビ行列とは"},{"categories":"머신러닝","contents":"概要 損失関数の勾配を利用して損失関数の極小値を見つけるアルゴリズムの中でもっとも単純な方法として 勾配降下法Gradient Descent Algorithmがある。\n説明 ただし、このときの損失関数$L$はデータセット$X$が固定された状態での重みとバイアスに対する関数と見なされる。入力データが$\\mathbb{x} \\in \\mathbb{R}^{m}$のように見える場合、$L$は$(w_{1} , w_{2} , \\cdots , w_{m} , b) \\in \\mathbb{R}^{m+1}$に対する関数となる。同じデータであっても重みとバイアスによって損失関数の値は異なり、損失関数が小さくなるということはそれだけ良いモデルを作り出したことを意味する。\n勾配降下法は、このような関数$L$が作り出す多様体に沿って極小値となる最適な重みを見つける。この原理をもう少し厳密に理解したい場合は、数値解析学の勾配降下法について学ぶといい。\n最初に選んだ重みとバイアスのベクトル$\\mathbb{w}_{1} \\in \\mathbb{R}^{m+1}$について損失関数の値をより小さくする$\\mathbb{w}_{2}$は、ある適切な正数$\\alpha$によって $$ \\mathbb{w}_{2} := \\mathbb{w}_{1} - \\alpha \\nabla L (\\mathbb{w}_{1} ) $$ のように計算される。これを繰り返す $$ \\mathbb{w}_{n+1} := \\mathbb{w}_{n} - \\alpha \\nabla L (\\mathbb{w}_{n} ) $$ も損失関数の値を次第に小さくすることができる。これにより$\\mathbb{w}_{n}$を更新することを バックプロパゲーションと呼ぶ。機械学習では$\\alpha$を 学習率Learning Rate, ラーニングレートと呼び、この値によって勾配降下法が成功することも失敗することもある。\n成功する場合とは、上の図のように計算を繰り返しながら$L$が極小値になる重みとバイアスを正確に見つけた場合だ。特に図では極小値でありながら最小値になっているが、一般的に極小値は極小値に過ぎず、最小値であるか確信できるわけではない。\n$\\alpha$が大きすぎると、上のように値が急激に変わり学習のスピードは速くなるが、過度に大きい場合は収束しないことがある。これを オーバーシューティングと呼ぶ。\n反対に$\\alpha$が小さすぎると、数学的には収束性が保証されるが、変化が小さすぎて時間がかかりすぎ、局所最小値に引っかかるとその近くから抜け出すことができない。\nこれが勾配降下法の基本的な概念で、実際には上記のような問題を補うためにさまざまな技術を用いる。\n確率的勾配降下法 ミニバッチごとに勾配降下法を適用することを 確率的勾配降下法stochastic gradient descent, SGDという。ある文献では以下のように説明されている。\nバッチ学習で学習する: バッチ勾配降下法 ミニバッチ学習で学習する: ミニバッチ勾配降下法 オンライン学習で学習する: 確率的勾配降下法 しかし、この区分は実際には無意味だ。一般的にディープラーニングではミニバッチ学習のみが使用され、ミニバッチ学習でバッチサイズを$1$にするとそれがオンライン学習になるためだ。したがって、実際のディープラーニングでは「勾配降下法 = 確率的勾配降下法 = ミニバッチ勾配降下法」と受け入れてもよい。\n「確率的」という言葉に大きな意味を置く必要もない。全データセットを母集団と見なすと、ミニバッチで学習することは標本集団に対して繰り返し学習することと同じなので、確率的と呼んで理解しても問題ない。\n併せて見る 最適化理論の勾配降下法 最適化理論の確率的勾配降下法 ","id":987,"permalink":"https://freshrimpsushi.github.io/jp/posts/987/","tags":null,"title":"機械学習における勾配降下法と確率的勾配降下法"},{"categories":"수리물리","contents":"概要 断然この記事は、テンソルが何か分からなくてこのページにたどり着いた物理学部の学生に最も分かりやすいテンソルについての説明だから、是非読むことをお勧めする。\n数学的に間違っている部分に対する指摘は受け付けない。負の数を習っていない人に対して「小さい数から大きい数を引くことはできない」、虚数を習っていない人に対して「ルートの中に負の数は入れられない」と教えることは、間違った内容を教えるわけではないからだ。この文章の目的は、テンソルの正確な定義を教えることではなく、テンソルを理解するために不必要に時間を浪費しないようにすることだ。\n物理学を学んでいくと、学年が上がるにつれてテンソルtensorというものに出会うようになる。私が最初に見たテンソルは、力学で出てきた慣性モーメントテンソルmoment of inertia tensorだ。\n$$ \\mathbf{I} =\\overleftrightarrow{\\mathbf{I}}= \\begin{pmatrix} I_{xx} \u0026amp; I_{xy} \u0026amp; I_{xz} \\\\ I_{yx} \u0026amp; I_{yy} \u0026amp; I_{yz} \\\\ I_{zx} \u0026amp; I_{zy} \u0026amp;I_{zz} \\end{pmatrix} $$\n当時、大学2年生だった私には、これが何なのか理解することが不可能だった。様々な本を探したりインターネットで検索しても、テンソルが何なのか分かりにくかった1。テンソルの概念を正確に理解するために必要な知識が不足しているにも関わらず、無理に理解しようとしたからだ。私は物理に興味があるだけでなく、数学にも興味があったので、テンソルの正確な定義や意味も知らずに使ったり受け入れたりすることはできなかった。しかし、間違いなくこれは物理を学ぶ上で良い態度ではない。\nテンソルの数学的な定義を受け入れて理解することは可能だが、それには線形代数学に関する深い知識が必要だ2。これは、学部の物理学を学ぶのに必要な数学をはるかに超える。数学を学ぶことは物理学を学ぶ上で大いに役立つが、この場合はやりすぎだということだ。数学的にテンソルを理解しようとすると、力学の中間試験を台無しにすることになるだろう。 だから、物理学部の学生は、テンソルがどのようなものか**「感覚的に」**知ることが重要だ。様々な例を見て、なぜ必要で、どのように、どのような時に使うかを学ぶので十分だ。物理の多くの部分で数学を大まかに使っているが、その理由は数学的な厳密さが保証されているからだ。テンソルもそのような厳密さが保証されているので、学部の物理を勉強する人はそのような厳密さを追う必要がない。\nテンソルという概念は、スカラーやベクトルだけでは表現できない物理量が存在するために生まれたと考えればいい。実際、テンソルさえあればすべての物理量を表現することができるので、テンソルはスカラーとベクターを含む一般的な概念だ。だから、スカラーやベクトルと言わずにテンソルだけを使ってもいい。しかし、そうすることは非効率的で、物理を初めて学ぶときには全く役に立たないので、スカラーやベクターであるテンソルは特にテンソルとは呼ばない。学部の物理学では、よく$3\\times 3$行列で表されるものをテンソルと言う。長い記事を読むのが嫌いで、分からない場合でも、「3x3の行列をテンソルと呼ぶのかな」と思っても問題ない。学部の範囲では、間違った説明ではないからだ。\nテンソルの分類 一般的に$(m,n)$-テンソルまたは$\\binom{m}{n}$テンソルと表示される。ここで、$m$は空間の次元を意味し、$n$はテンソルの成分に付ける下付き添字の数だと考えるとよい。この時、テンソルの成分の数は$m^{n}$個だ。相対論を除くほとんどの物理では、常に3次元空間を扱うので、常に$m=3$だ。したがって、$m$がいくつかを特に述べることなく、$n$の値によって、テンソルを$0$階テンソル、$1$階テンソルなどと区別することもある。\n$0$階テンソル=スカラー $0$階テンソルは$\\binom{3}{0}$テンソルを意味し、これはスカラーと同じだ。$0$階テンソルの代表的な例として質量がある。3次元空間では、質量は単に$m$で表記するので、下付き添字が0個であり、したがって$0$階テンソルだ。しかし、わざわざテンソルと言わずにスカラーと言う。成分の数は$3^0=1$個。\n$1$階テンソル=ベクター $1$階テンソルは$\\binom{3}{1}$テンソルを意味し、これはベクターと同じだ。$1$階テンソルの代表的な例として速度がある。3次元空間で速度は$\\mathbf{v}=(v_{x},v_{y},v_{z})$で表記され、その成分に付いた下付き添字の数が1個であるため、$1$階テンソルだ。しかし、わざわざテンソルと言わずにベクターと言う。成分の数は$3^1=3$個。\n$2$階テンソル $2$階テンソルは$\\binom{3}{2}$テンソルを意味する。以下のような**$3 \\times 3$行列**を考えてみよう。\n$$ A=\\begin{pmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \\end{pmatrix} $$\n行列の成分は$a_{ij}$で表され、下付き添字の数が2個であるため、$2$階テンソルだ。通常、学部の物理学の教科書では、$3 \\times 3$行列が登場し、この時$2$階テンソルという表現よりも、テンソルという表現を多く使う。クロネッカーデルタ $\\delta_{ij}$も下付き添字が2個あるため、$2$階テンソルだ。しかし、通常の教科書ではクロネッカーデルタをわざわざテンソルとは呼ばない。\n$$ \\begin{align*} \\delta_{ij}\u0026amp;=\\begin{cases} 1 \u0026amp; \\mathrm{if} \\quad i=j \\\\ 0 \u0026amp; \\mathrm{if} \\quad i\\ne j \\end{cases} \\\\ \u0026amp;= \\begin{pmatrix} \\delta_{11} \u0026amp; \\delta_{12} \u0026amp; \\delta_{13} \\\\ \\delta_{21} \u0026amp; \\delta_{22} \u0026amp;\\delta_{23} \\\\ \\delta_{31} \u0026amp;\\delta_{32} \u0026amp;\\delta_{33} \\end{pmatrix} = \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} \\end{align*} $$\n成分の数は$3^2=9$個だ。\n$3$階テンソル 代表的な例としては、レヴィチヴィタ記号Levi-Civita symbolがある。\n$$ \\epsilon_{ijk}=\\begin{cases} 1 \u0026amp; \\mathrm{if} \\quad ijk=123=231=312 \\\\ -1 \u0026amp; \\mathrm{if} \\quad ijk=132=213=321 \\\\ 0 \u0026amp; \\mathrm{if} \\quad i=j \\ \\mathrm{or}\\ j=k\\ \\mathrm{or} \\ k=i \\end{cases} $$\n下付き添字の数が3個であるため、$3$階テンソルだ。しかし、通常の教科書ではレヴィチヴィタ記号をわざわざテンソルとは呼ばない。成分の数は$3^3=27$個だ。\n$4$階テンソル この時点で、$4$階テンソルが下付き添字が4個の物理量を指す名前であることは理解できるが、学部の物理学では$4$階以上のテンソルは登場しない。\nWikipediaでは、テンソルをできる限り一般的な概念として説明しているが、言い換えれば、できる限り難しく説明しているということだ。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n一般的に数学科の2年生が学ぶ線形代数学のレベルでは、到底足りない。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1040,"permalink":"https://freshrimpsushi.github.io/jp/posts/1040/","tags":null,"title":"物理学におけるテンソルとは"},{"categories":"전자기학","contents":"公式 マックスウェルの方程式Maxwell\u0026rsquo;s equations\n$(\\text{i}) \\quad \\nabla \\cdot \\mathbf{E}=\\dfrac{1}{\\epsilon_{0}}\\rho$ (ガウスの法則)\n$(\\text{ii}) \\quad \\nabla \\cdot \\mathbf{B}=0$ (磁場に対するガウスの法則)\n$(\\text{iii}) \\quad \\nabla \\times \\mathbf{E} = -\\dfrac{\\partial \\mathbf{B}}{\\partial t}$ (ファラデーの法則)\n$(\\text{iv}) \\quad \\nabla \\times \\mathbf{B} = \\mu_{0} \\mathbf{J}+\\mu_{0}\\epsilon_{0}\\dfrac{\\partial \\mathbf{E}}{\\partial t}$ (アンペールの法則)\n説明1 マックスウェルがマックスウェルの方程式を完成させる前、電場と磁場に関する4つの方程式は以下のようであった。\n$(\\text{i}) \\quad \\nabla \\cdot \\mathbf{E}=\\dfrac{1}{\\epsilon_{0}}\\rho$\n$(\\text{ii}) \\quad \\nabla \\cdot \\mathbf{B}=0$\n$(\\text{iii}) \\quad \\nabla \\times \\mathbf{E} = -\\dfrac{\\partial \\mathbf{B}}{\\partial t}$\n$(\\text{iv}) \\quad \\nabla \\times \\mathbf{B} = \\mu_{0} \\mathbf{J}$\n理論的には、電場と磁場の発散と回転に関するこれら4つの式だけで、ほぼ全ての電磁気学を説明できる。無駄にまとめられたわけではない。しかし、上の式の$\\text{(iv)}$には大きな誤りがあった。回転の発散は常に$0$であるため、$(\\text{iv})$の発散をとると次のようになる。\n$$ \\begin{equation} 0 = \\nabla \\cdot (\\nabla \\times \\mathbf{B})=\\mu_{0} (\\nabla \\cdot \\mathbf{J}) \\ne 0 \\end{equation} $$\nここで問題が生じる。定常電流の場合はアンペールの法則がうまく成り立ち右辺が$0$になるが、一般的にはそうではない。\n右辺を$0$にするためのアイデアを得るために連続の方程式とガウスの法則を使って右辺を変更すると\n$$ \\nabla \\cdot \\mathbf{J}=-\\dfrac{\\partial \\rho}{\\partial t}=-\\dfrac{ \\partial( \\epsilon_{0} \\nabla \\cdot \\mathbf{E})}{\\partial t}=-\\nabla \\cdot \\left(\\epsilon_{0} \\dfrac{\\partial \\mathbf{E}}{\\partial t } \\right) $$\nしたがって、$\\mathbf{J}$の代わりに$\\mathbf{J}+\\epsilon_{0}\\dfrac{\\partial \\mathbf{E} }{\\partial t}$を使えば、$(1)$の右辺を$0$にすることができる。修正された$\\text{(iv)}$は以下の通り。\n$$ \\text{(iv)} \\quad \\nabla \\times \\mathbf{B} = \\mu_{0}\\mathbf{J} + \\mu_{0}\\epsilon_{0}\\dfrac{\\partial \\mathbf{E}}{\\partial t} $$\n修正された式も静磁気学をそのまま満たすため、既存の法則を破ることなく誤りのある部分をうまく修正した。実際、マックスウェルがこれを後から修正したには理由がある。多くの電磁気学の法則は実験を通じて発見、証明された。しかし、通常、上記の式の二項の大きさの差があまりにも大きいため、実験的に発見することは非常に難しかった。\n$$ \\left| \\epsilon_{0}\\dfrac{\\partial \\mathbf{E}}{\\partial t} \\right| \\ll \\left| \\mathbf{J} \\right| $$\nマックスウェルによって新しく修正された式は'変化する電場は磁場を生み出す'という意味を含んでいる。これは1888年のヘルツの電磁波実験で確認された。\nDavid J. Griffiths, 電磁気学入門(Introduction to Electrodynamics, 김진승 訳) (第4版, 2014), p356-359\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1038,"permalink":"https://freshrimpsushi.github.io/jp/posts/1038/","tags":null,"title":"マックスウェルの方程式"},{"categories":"다변수벡터해석","contents":"定義 集合 $D$ を $n$次元のユークリッド空間の部分集合 $D\\subset \\mathbb{R}^{n}$ とする。\n$D$ を定義域とする関数を多変数関数function of several variablesと呼ぶ。 $f : D \\to \\mathbb{R}$ をスカラー関数scalar functionと呼ぶ。 スカラー関数 $f_{1} , \\cdots , f_{m} : D \\to \\mathbb{R}$ に対して次のように定義された $\\mathbb{f} : D \\to \\mathbb{R}^{m}$ をベクトル値関数vector-valued functionと呼ぶ。 $$ \\mathbb{f} ( x_{1} , \\cdots , x_{n} ) : = \\begin{bmatrix} f_{1} ( x_{1} , \\cdots , x_{n} ) \\\\ \\vdots \\\\ f_{m} ( x_{1} , \\cdots , x_{n} ) \\end{bmatrix} $$ 説明 多変数関数 多変数関数を意味する英語には、function of several variables, multivariable function, multivariate function 等がある。\n多変数関数という表現は特に微分積分学を含む解析学で使われる。もともとスカラー関数であれベクトル値関数であれ、ただの関数に過ぎないが、その値域を簡単に区別するために使われる言葉だ。線型代数学の観点から見れば、ベクトル値関数が $m=1$ であればスカラー関数になると言えるので、概念的な差は全くないと言える。\nスカラー関数 スカラー関数の例として、$ F ( m , a ) := ma$ を考えることができる。$m$ が質量であろうと$a$ が加速度であろうと、数学者の目には $(m , a) \\in \\left( [0,\\infty) \\times \\mathbb{R} \\right) \\subset \\mathbb{R}^2$ のような $2$次元ベクトルに見えるべきだ。$ma$ は単に2つの実数 $m$ と $a$ の積であり、$ma \\in \\mathbb{R}$ であるため、スカラー関数の条件をよく満たしている。一方、ベクトル解析学では、与えられた空間上の全ての点に対してスカラー値が1つずつ対応している点で、スカラー場Scalar Fieldとも呼ばれる。\nベクトル値関数 ベクトル値関数の例として、 $$ \\mathbb{q} ( m , v , a ) : = \\begin{bmatrix} ma \\\\ mv \\\\ {{1} \\over {2}} m v^2 \\end{bmatrix} $$ を考えることができる。物理学者の目には、最初の成分から順に力、運動量、運動エネルギーだと思うかもしれないが、ベクトル値関数として考えれば、単に $\\mathbb{q} : D \\to \\mathbb{R}^3$ に過ぎない。一方、ベクトル解析学では、与えられた空間上の全ての点に対してベクトルが1つずつ対応している点で、ベクトル場Vector Fieldとも呼ばれる。\n","id":970,"permalink":"https://freshrimpsushi.github.io/jp/posts/970/","tags":null,"title":"スカラー関数とベクトル値関数"},{"categories":"머신러닝","contents":"定義 データ$Y = \\begin{bmatrix} y_{1} \\\\ \\vdots \\\\ y_{n} \\end{bmatrix}$の推定値が$\\widehat{Y} = \\begin{bmatrix} \\widehat{ y_{1} } \\\\ \\vdots \\\\ \\widehat{y_{n}} \\end{bmatrix}$として与えられた時、データとその推定値の乖離を表すスカラー関数$L : \\mathbb{R}^{n} \\to [ 0 , \\infty )$を損失関数と呼ぶ。\n別名 損失関数は、学習を通じて得たデータの推定値が実際のデータとどれだけ違うかを評価する指標として使用される。この値が大きいほど、より間違えている意味であり、この値が$0$であることは、「無損失」つまり完璧に推定できることを意味する。これは数学で言うメトリックと大きく異なるわけではない。\nもともと経済学で先に使われた言葉なので、$L$は時々**コスト関数Cost Function**とも呼ばれる。\n種類 次の二つは代表的な損失関数であり、使用する際に適切に知っておくだけで十分である。\nMSE（平均二乗誤差） $$ L \\left( \\widehat{Y} \\right) := {{1} \\over {n}} \\sum_{i=1}^{n} ( y_{i} - \\widehat{ y_{i} } )^2 $$ MSEは、歴史がある損失関数であり、$y_{i} \\in \\mathbb{R}$時に有意義に使用できる。\nクロスエントロピー $$ L \\left( \\widehat{Y} \\right) := - {{1} \\over {n}} \\sum_{i=1}^{n} \\left\u0026lt; y_{i} , \\log \\left( \\sigma ( \\hat{y_{i}} ) \\right) \\right\u0026gt; $$\nクロスエントロピーは、いわゆるワンホットエンコーディングを行う時に効果的な手段となり、$Y$がカテゴリカルデータである場合、$\\widehat{Y}$が推計した各カテゴリの確率を用いて計算する。主に分類問題に使用される。\nワンホットエンコーディングとは、単に標準基底へのマッピングを意味する。$m$クラスがある場合、$\\mathbb{R}^{n}$の標準基底は$\\beta = \\left\\{ e_{i} \\right\\}_{i=1}^{m}$で表記されると、各々の$y_{i}$と$\\hat{y_{i}}$は\n$$ y_{i} \\in \\mathbb{R}^{m},\\qquad \\hat{y}_{i} \\in \\beta $$\nのようなベクトルで表される。例えば、$Y$が３つのクラスを持つ場合、$y_{i}$が1番の分類に属しているなら$y_{i} = [1,0,0]^{t}$、$3$番の分類に属していれば$y_{i} = [0,0,1]^{t}$のように表される。\n$\\sigma$はソフトマックス関数であり、与えられたベクトルの値を$[0,1]$にバウンドして、確率分布の条件を満たすようにする関数だ。$\\left\u0026lt; \\cdot , \\cdot \\right\u0026gt;$は内積だ。$\\sigma ( \\hat{y_{i}} ) \\in [0,1]$なので、$\\log \\left( \\sigma ( \\hat{y_{i}} ) \\right) \u0026lt; 0$であり、$y_{i}$は$0$か$1$なので $$ L \\left( \\widehat{Y} \\right) = - {{1} \\over {n}} \\sum_{i=1}^{n} \\left\u0026lt; y_{i} , \\log \\left( \\sigma ( \\hat{y_{i}} ) \\right) \\right\u0026gt; \\ge 0 $$ 容易に確認できる。\n$y_{i} = ( y_{i1} , \\cdots , y_{ij} , \\cdots , y_{im} )$の推定値$\\hat{ y_{i} } = ( \\hat{ y_{i1} } , \\cdots , \\hat{y_{ij}} , \\cdots , \\hat{y_{im}} )$の各成分は、確率が高いほど大きな値、低ければ低い値を取る。これをソフトマックス関数に入れると、確率が高いほど$1$に近く、低いほど$0$に近い値に調整される。実際の成分が$1$であるが、確率を低く計算した場合$- 1 \\cdot \\log (c)$は$c\\ll 1$となり、かなり大きな値になる。逆に、実際の成分が$0$であり、確率を高く計算しても$- 0 \\cdot \\log(c)$は大きな意味を持たない。従って、間違えれば間違えるほど、クロスエントロピーが急騰することが容易に予想できる。\n参照 数値解析における損失関数 ","id":967,"permalink":"https://freshrimpsushi.github.io/jp/posts/967/","tags":null,"title":"機械学習における損失関数"},{"categories":"그래프이론","contents":"定義1 頂点とそれらを結ぶ線から成る集合をグラフまたはネットワークと呼ぶ。頂点の集合を$V$、線の集合を$E$としよう。 $V(G) := V$の要素を$G$の ヴァーテックスVertexまたは ノードNodeと呼ぶ。 $E(G) := E$の要素を$G$の エッジEdgeまたは リンクLinkと呼ぶ。 自分自身に繋がるエッジをループLoopと呼ぶ。 二つのヴァーテックスがエッジで繋がっている場合、隣接しているAdjacentと言う。 エッジに向きがあるグラフを有向グラフDigraphと言う。 有限なヴァーテックスを持ち、二つのヴァーテックスを結ぶエッジが一つだけで、ループが存在せず、有向グラフでないグラフを単純グラフSimple Graphと言う。 説明 必ずしもそうではないが、同じ概念であっても純粋数学ではグラフという言葉を好んで用い、応用数学ではネットワークという言葉を好んで用いる傾向がある。ただし、どちらかと言えば、同義語があり、それぞれの分野ではかなりの影響力を持っているので、混在して使うことはあまりない。\n一般的に言うグラフとは、上の図のように自由な形をしている。 黒い丸はそれぞれのヴァーテックスを意味し、位置の概念は持たない。純粋なグラフ理論では、グラフのオーダーOrderは通常、このヴァーテックス集合の基数 $n = |V(G)|$を指す。上のグラフのオーダーは$5$である。 ヴァーテックスを結ぶエッジも同様に、その関係のみを表すもので、形や長さの概念はない。ちなみに、エッジは通常、韓国人が直感的に思うように[エッヂ]ではなく[エッジ]と発音されるのが正しい。純粋なグラフ理論では、グラフのサイズSizeは通常、このエッジ集合の基数$m = |E(G)|$を指す。上のグラフのサイズは$8$である。しかし、応用ネットワーク理論では、サイズは単にグラフのオーダー$|V(G)|$を呼ぶことが多い。これは分野による文脈で区別する必要がある。 左上のヴァーテックスは自分自身に繋がるエッジを持ち、このためにループと呼ばれる。 二つのヴァーテックスが隣接しているとは、エッジで繋がっているということを意味し、再び、その関係のみが重要であり、目に見える距離は重要ではない。二つのヴァーテックス$u, v$が隣接している場合、$u \\sim v$のように表され、グラフ$G$でヴァーテックス$v \\in V(G)$に隣接するヴァーテックスを集めた集合を$v$のネイバーフッド$N_{G} (v)$のように表現することもある。 有向グラフとは、上の図のようにエッジに方向性があるグラフを言う。有向グラフでは、エッジはアークArcとも呼ばれる。ヴァーテックス$u$から$v$へ入るエッジは$u \\to v$と表記され、$u$をテイルtail、$v$をヘッドheadと呼ぶ。 単純グラフという言葉は難しそうだが、簡単に言えば、ループやマルチエッジ、ディレクションなどがなく、上の図のようなきれいなグラフを指す。一般に、グラフ理論と言えば、このような単純な形に興味を持つのが普通である。 難しい定義 これらの定義は、グラフの概念を簡単に説明するが、厳密さにはいくらか問題がある。したがって、以下のより複雑な定義を紹介する。概念は文字通り上で簡単に定義されたものと同じなので、数学的な表現に慣れていれば理解するのに大きな困難はないだろう。\n集合 $V \\ne \\emptyset$ と 二項関係 $\\sim \\subset V^2$ について$G := \\left( V, \\sim \\right)$をグラフまたはネットワークと呼ぶ。 $V(G) := V$の要素を$G$のヴァーテックスまたは ノードと呼ぶ。 $E(G) := \\sim$の要素を$G$のエッジまたは リンクと呼ぶ。 $v \\in V(G)$に対して$(v,v) \\in E(G)$をループと呼ぶ。 $v_{1} , v_{2} \\in V(G)$について$(v_{1} , v_{2} ) \\in E(G)$であれば$v_{1}$と$v_{2}$が隣接していると言う。 $\\sim$が対称関係でなければ有向グラフDigraphと呼ぶ。 有限集合$V$に対し、対称関係$\\sim \\subset \\left\\{ (v_{1} , v_{2} ) \\in V^2 \\mid v_{1} \\ne v_{2} \\right\\}$をエッジとして、二つのヴァーテックスを結ぶエッジが一つだけのグラフを単純グラフと呼ぶ。 Wilson. (1970). Introduction to Graph Theory: p8~9.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":966,"permalink":"https://freshrimpsushi.github.io/jp/posts/966/","tags":null,"title":"数学におけるグラフとネットワーク"},{"categories":"머신러닝","contents":"定義 実際の生物の神経系を模倣したネットワークを人工神経網artificial neural network (ANN)と言う。\n数学的定義 ディープラーニングでは、ベクトル関数 $W : \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$をレイヤーと呼ぶ。 ディープラーニングでは、非線形なスカラー関数 $\\sigma : \\mathbb{R} \\to \\mathbb{R}$を活性化関数と呼ぶ。 レイヤーと活性化関数の合成 $\\sigma \\circ W$を人工神経網と言う。 モチーフ 神経系は、ニューロンの結合で構成される。神経細胞体は樹状突起によって刺激を受け取り、軸索突起を通じて電気刺激を伝達する。人間を含む多くの生物は、このように単純なニューロンの結合を環境に適したものに進化させてきた。その結果、神経系は光を検出したり、足を動かしたり、記憶したり、想像するなどの複雑な仕事をすることができるようになった。\n人工神経網はニューロンから、神経細胞体をノードとして、軸索をリンクとして模倣したネットワークだと言える。各ノードは、神経細胞体と同じように、情報を受け取り、それを伝達する過程で有意義な結果を得る計算を実行する。\n例 簡単な例として、データ $Y := \\begin{bmatrix} 5 \\\\ 7 \\\\ 9 \\end{bmatrix}$ と $X := \\begin{bmatrix} 2.2 \\\\ 3.1 \\\\ 3.9 \\end{bmatrix}$ に対して、$Y$ と $X$ の相関関係について把握する問題を考えよう。\nこの問題は非常に簡単なので、$Y \\approx {\\color{red}2} X + \\color{blue}{1}$ のような線形相関関係があることを容易に推測することができる。\nこの問題を単回帰分析 $Y \\gets X$で解いた場合、設計行列で表した時 $$ \\begin{bmatrix} 5 \\\\ 7 \\\\ 9 \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; 2.2 \\\\ 1 \u0026amp; 3.1 \\\\ 1 \u0026amp; 3.9 \\end{bmatrix} \\begin{bmatrix} \\color{blue} {\\beta_{0} } \\\\ {\\color{red}\\beta_{1}} \\end{bmatrix} $$ の最小二乗解 $( \\color{blue} {\\beta_{0} } , {\\color{red}\\beta_{1}} )$を求める問題になる。\n一方、この問題に対する人工神経網は\nのように構成できる。まずは$Y = {\\color{red}w} X + \\color{blue}{b}$のような関係があるだろうと想定してみる。この時、$\\color{red}{w}$を重みWeight、$\\color{blue}{b}$をバイアスBiasと呼ぶ。与えられたデータ$\\begin{bmatrix} 2.2 \\\\ 3.1 \\\\ 3.9 \\end{bmatrix}$を受け取ったノード$X$は、まずは$( {\\color{red}w_{1}} , \\color{blue}{b_{1}} )$を使って$\\begin{bmatrix} {\\color{red}w_{1}} 2.2 +\\color{blue}{b_{1}} \\\\ {\\color{red}w_{1}} 3.1 +\\color{blue}{b_{1}} \\\\ {\\color{red}w_{1}} 3.9 +\\color{blue}{b_{1}} \\end{bmatrix}$を計算し、ノード$Y$へと渡す。\nもし、このように大雑把にピックした値が気に入らなければ、以下のようにもっと良い重みで更新し続けて、満足のいく結果が出るまで繰り返す。\nこの意味で人工神経網は、機械が自ら学習するという概念のマシン ラーニングMachine Learningを実現していると見なすことができ、このプロセスがより複雑で効率的に進化したものがディープ ラーニングDeep Learningであると見なしてもよい。\n理論的側面 統計学や数学に精通している学習者は、これらの技術に対して理論的な基盤が整っていないという点で強い嫌悪感を表すことが多い。誤差が最小になるような、学習が最適化される条件について知られていることもないし、どんな関数を使用しながらもなぜそれを使うのかその理由を知らない場合が多いからである。しかし、新しい論文の新技術がベンチマーク上でパフォーマンスが向上していれば何も言えない。\nもちろん、これらのことに対して何らかの数学的アプローチをする学者がいたかもしれない。しかし、研究がある程度進展する頃に産業界では既に古くなってしまうのが現実だ。理論を学ぶ人にとってはあまり価値がないことだ。\nそれにもかかわらず、これらの技術を軽視することはできない明白な理由がある。結果を信頼できないものとしてもパフォーマンスがあまりにも優れているからだ。ディープラーニングはデータ科学において拒否できない誘惑である。すぐに流行が過ぎ去るとしても、パフォーマンスが圧倒的に良いので学ぶ価値は十分にあり、数学ほど厳密ではなくてもその分野ではその分野なりの理論的基礎が築かれているので、心を開いて受け入れてみるのも悪くない。\n一緒に見る ディープラーニングの数学的基礎、シェブニコ定理の証明 ","id":962,"permalink":"https://freshrimpsushi.github.io/jp/posts/962/","tags":null,"title":"人工ニューラルネットワークとは?"},{"categories":"동역학","contents":"定義 カオティック・オービット1 マップ $f : \\mathbb{R} \\to \\mathbb{R}$ のバウンデッド・オービットが以下の条件を満たす場合、このオービットはカオティックと言われる。\n(i) アシンプトティカリー・ピリオディックではない。 (ii): $h (x_{1} ) \u0026gt; 0$ バウンデッド・オービットとは、全ての $n \\in \\mathbb{N}$ に対して $|x_{n} | \u0026lt; M$ を満たす $M \\in \\mathbb{R}$ が存在することを意味する。 $h(x_{1} )$ はリアプノフ指数を指す。 カオティック・マップ 全ての $n \\in \\mathbb{N}$ に対してピリオディック-$n$ オービットが存在する場合、マップ $f$ はカオティックと言われる。\n説明 英語の発音を基準にすると、Chaos は [カオス]よりも [ケイオス] に近く、Chaotic の発音は [カオティック]よりも [ケイオティック] に近いため、カオティックという表記にする。 通常、数学では条件と式が与えられれば、求める答えを得ることができる。しかし、カオティックなオービットでは、リアプノフ指数が正であるため、どれだけマップを適用しても同期やアトラクションを起こさず、さらには似たようなピリオディックなオービットさえ見つけることができない。現在の条件をどれだけよく知っていても、遠い未来を予測することはできないという事実を数学的によく表した定義である。\n一つ注意すべき点は、マップ $f$ で作られるシステムにカオティックなオービットが存在するとしても、そのシステム自体がカオティックであるわけではないということである。\n一般化 多次元マップのカオス Yorke. (1996). CHAOS: An Introduction to Dynamical Systems: p110.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":864,"permalink":"https://freshrimpsushi.github.io/jp/posts/864/","tags":null,"title":"1次元マップのカオス"},{"categories":"확률론","contents":"定義 $s\u0026lt; t \u0026lt; t+u$ とした時、以下の条件を満たす確率過程 $\\left\\{ W_{t} \\right\\}$ をウィーナー過程と呼ぶ。\n(i): $W_{0} = 0$ (ii): $\\left( W_{t+u} - W_{t} \\right) \\perp W_{s}$ (iii): $\\left( W_{t+u} - W_{t} \\right) \\sim N ( 0, u )$ (iv): $W_{t}$ のサンプルパスはほとんど至る所で連続である。 基本性質 [1]: $\\displaystyle W_{t} \\sim N ( 0 , t )$ [2]: $\\displaystyle E ( W_{t} ) = 0$ [3]: $\\displaystyle \\text{Var} ( W_{t} ) = t$ 4: $\\displaystyle \\text{cov} ( W_{t} , W_{s} ) = E (W_{t}W_{s}) = {{1} \\over {2}} (|t| + |s| - |t-s|) = \\min \\left\\{ t , s \\right\\}$ 説明 ウィーナー過程はブラウン運動Brownian Motionとも呼ばれる。\n(ii): $\\left( W_{t+u} - W_{t} \\right) \\perp W_{s}$ と言うことは\n(iii): 増分が正規分布 $N(0,t)$ に従うということは、ウィーナー過程は特定の時点には関心がなく、二つの時点を比ぼうした時、その時差が大きくなるほど不確実性が大きくなることを意味する。\n(iv): サンプルパスがほとんど至る所で連続であるということは、ウィーナー過程に従うある点があった時、その点が「瞬間移動」する確率が$0$ と見ても良いということだ。難しいなら、突然の跳躍をしないとだけ知っておけば十分だ。\n[1]: 面白い事実は$W_{t}$ の確率密度関数 $$ f_{W_{t}} (x,t) = {{1} \\over { \\sqrt{ 2 \\pi t } }} e^{ - {{x^2} \\over {2t} } } $$ が熱方程式 $$ {{\\partial u } \\over { \\partial t }} = {{1} \\over {2}} {{\\partial^2 u } \\over { \\partial x^2 }} $$ の解になるということだ。\n証明 [1] (i)と(iii)によって、$W_{t} = W_{t} - 0 = W_{t} - W_{0} \\sim N ( 0 , t )$\n■\n[2] [1]により$W_{t}$ が正規分布に従うため、$\\displaystyle E ( W_{t} ) = 0$\n■\n[3] [1]により$W_{t}$ が正規分布に従うため、$\\displaystyle \\text{Var} ( W_{t} ) = t$\n■\n4 $t \u0026gt; s$ とすると、共分散の定義と[2]により $$ \\text{cov} ( W_{t} , W_{s} ) = E \\left( \\left[ W_{t} - E ( W_{t} ) \\right] \\left[ W_{s} - E ( W_{s} ) \\right] \\right) = E \\left( W_{t} W_{s} \\right) $$\n$W_{t} = ( W_{t} - W_{s} ) + W_{s}$ だから\n$$ \\begin{align*} E \\left( W_{t} W_{s} \\right) =\u0026amp; E \\left[ \\left( ( W_{t} - W_{s} ) + W_{s} \\right) \\cdot W_{s} \\right] \\\\ =\u0026amp; E \\left[ ( W_{t} - W_{s} ) \\cdot W_{s} \\right] + E \\left( W_{s}^{2} \\right) \\end{align*} $$\n(ii)と[2]による最初の項は\n$$ E \\left[ ( W_{t} - W_{s} ) \\cdot W_{s} \\right] = E ( W_{t} ) \\cdot E ( W_{t} - W_{s} ) = 0 $$\n[3]による2番目の項は\n$$ E \\left( W_{s}^{2} \\right) - 0^2 = E \\left( W_{s}^{2} \\right) - \\left[ E ( W_{s} ) \\right]^2 = \\text{Var} ( W_{s} ) = s $$\nまとめると、$\\displaystyle \\text{cov} ( W_{t} , W_{s} ) = s$ となる。一方、$s \u0026gt; t$ の時も同じ結果が得られるため\n$$ \\text{cov} ( W_{t} , W_{s} ) = \\min \\left\\{ t , s \\right\\} $$\n■\n","id":957,"permalink":"https://freshrimpsushi.github.io/jp/posts/957/","tags":null,"title":"ウィーナープロセス"},{"categories":"동역학","contents":"定義1 マップ $f : X \\to X$ と $p \\in X$ に対して $f^{k} (p) = p$ を満たす最小の自然数を $k \\in \\mathbb{N}$ としよう。\nマップ $f : X \\to X$ と点 $x \\in X$ に対して、$f$ の下での集合 $\\left\\{ x , f(x) , f^{2} , \\cdots \\right\\}$ を $x$ の軌道Orbitという。この時、$x$ を軌道の初期値Initial Valueという。 初期値 $p$ を持つ軌道 $\\left\\{ p , f (p) , f^{2} (p) , \\cdots \\right\\}$ を周期-$k$ 軌道といい、$p$ を周期-$k$ ポイントという。 $p$ が $f^{k}$ のシンクならば、$p$ の周期-$k$ 軌道を**(周期的)シンクといい、$f^{k}$ のソースならば、$p$ の周期-$k$ 軌道を(周期的)ソース**という。 ある$N \\in \\mathbb{N}$ と全ての $n \\ge N$ に対して $f^{n+k} (p) = f^{n} (p)$ を満たすなら、$p$ はイベンチュアリー周期Eventually Periodicという。 軌道 $\\left\\{ p , f (p) , f^{2} (p) , \\cdots , f^{n} (p) , \\cdots \\right\\}$ に対して $\\displaystyle \\lim_{n \\to \\infty} | f^{n} (p) - x_{n} | = 0$ を満たす周期軌道 $\\left\\{ x_{1} , \\cdots , x_{n} \\right\\}$ が存在するなら、$\\left\\{ p , f (p) , f^{2} (p) , \\cdots , f^{n} (p) , \\cdots \\right\\}$ はアシンプトティカリー周期Asymtotically Periodicという。 説明 周期-$k$ 軌道が存在するということは本質的に $f^{k}$ が固定点を持つことと同じである。したがって、周期を持つことや固定点を持つことは、マップを何回適用するかの違いに過ぎない。したがって、概念的な勉強が終わった後には、全ての定理や上位概念が固定点を基準にその表現を合わせることになる。「周期」とは自然数に対して一般化された「固定点」と考えよう。\nアシンプトティカリー周期であり、正確にその$\\left\\{ x_{1} , \\cdots , x_{n} \\right\\}$ と同じになるならば、それをイベンチュアリー周期とも言える。また、周期シンク軌道に収束する軌道はアシンプトティカリー周期である。\n一方 $X = \\mathbb{R}$ の場合、以下のような簡単な定理を考えることができる。\n定理2 $f$ の周期-$k$ 軌道を $\\left\\{ p_{1} , p_{2} , \\cdots , p_{k} \\right\\}$ としよう。\n$\\left| f '(p_{1}) \\cdots f '(p_{k}) \\right| \u0026lt; 1$ ならば、$\\left\\{ p_{1} , p_{2} , \\cdots , p_{k} \\right\\}$ はシンクであり、$\\left| f '(p_{1}) \\cdots f '(p_{k}) \\right| \u0026gt; 1$ であれば、$\\left\\{ p_{1} , p_{2} , \\cdots , p_{k} \\right\\}$ はソースである。\n証明 チェーンルールにより、\n$$ \\begin{align*} ( f^{k} )' ( p_{1} ) =\u0026amp; \\left( f \\left( f^{k-1} \\right) \\right)' ( p_{1} ) \\\\ =\u0026amp; f ' \\left( \\left( f^{k-1} \\right) \\right) \\left( f^{k-1} \\right)' ( p_{1} ) \\\\ =\u0026amp; f ' \\left( \\left( f^{k-1} \\right) \\right) f ' \\left( \\left( f^{k-2} \\right) \\right) \\cdots f ' ( p_{1} ) \\\\ =\u0026amp; f ' ( p_{k} ) f ' ( p_{k-1} ) \\cdots f ' ( p_{1} ) \\end{align*} $$\nスムースな $f : \\mathbb{R} \\to \\mathbb{R}$ に対して、ある $p \\in \\mathbb{R}$ が固定点だとしよう。\n[1] $| f ' (p) | \u0026lt; 1$ ならば、$p$ はシンクである。\n[2] $| f ' (p) | \u0026gt; 1$ ならば、$p$ はソースである。\n$| ( f^{k} )' ( p_{1} ) | = | f ' ( p_{k} ) f ' ( p_{k-1} ) \\cdots f ' ( p_{1} ) |$ に1次元マップのシンクとソースの判定法を適用すれば、求める結果を得る。\n■\nYorke. (1996). CHAOS: An Introduction to Dynamical Systems: p13, 108.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYorke. (1996). CHAOS: An Introduction to Dynamical Systems: p10.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":858,"permalink":"https://freshrimpsushi.github.io/jp/posts/858/","tags":null,"title":"マップシステムのオービット"},{"categories":"통계적검정","contents":"仮説検定 量的データ $\\left\\{ x_{i} \\right\\}_{i = 1}^{n}$ が与えられたとする。\n$H_{0}$: データ$\\left\\{ x_{i} \\right\\}_{i = 1}^{n}$は正規分布に従う。 $H_{1}$: データ$\\left\\{ x_{i} \\right\\}_{i = 1}^{n}$は正規分布に従わない。 説明 ジャーク-ベラ検定は、正規性を検定するために使用される仮説検定で、通常、正規性の存在を示すために使用される。帰無仮説が採用されることが「分析者の意図」と一致する珍しいケースなので、仮説を正確に理解している必要がある。\nシャピロ-ウィルク検定との違いは、歪度と尖度を使用して検定する点のみである。正規分布は、母歪度と母尖度が共に$0$であり、標本歪度$g_{1}$と標本尖度$g_{2}$に基づく検定統計量$JB$は、自由度$2$のカイ二乗分布に従う。 $$ JB := {{n g_{1}^2} \\over {6}} + {{n g_{2}^2} \\over {24}} \\sim \\chi^{2} (2) $$ とはいえ、正規性検定だから、何を使っても特に差はないが、ジャーク-ベラ検定は、外れ値に敏感な歪度を使用するため、シャピロ-ウィルク検定に比べて外れ値を除去した際に正規分布であることがより多く示される。その理由からだと断言はできないが、通常は、回帰分析より時系列分析で正規性を証明するために使用される。実際に、Rではtseriesパッケージのjarque.bera.test()関数でジャーク-ベラ検定を行う。\nコード 実習 以下の二つのランダムサンプルを作成して、実際にジャーク-ベラ検定をやってみよう。\nNは正規分布から出たデータで、geoは幾何分布から出たデータだ。\n検定結果は正確に予想通りに現れる。\n全体コード 以下はRの例コードだ。\nlibrary(tseries)\rset.seed(150421)\rN\u0026lt;-rnorm(100)\rwin.graph(4,4); hist(N)\rjarque.bera.test(N)\rgeo\u0026lt;-rgeom(100,0.5)\rwin.graph(4,4); hist(geo)\rjarque.bera.test(geo) 一緒に見るべき シャピロ-ウィルク検定 ","id":949,"permalink":"https://freshrimpsushi.github.io/jp/posts/949/","tags":null,"title":"ハルケ・ベラ検定"},{"categories":"편미분방정식","contents":"定義1 2 以下の偏微分方程式は、熱方程式heat equationまたは拡散方程式diffusion equationと呼ばれる。\n$$ \\dfrac{\\partial u}{\\partial t} = \\dfrac{\\partial^{2} u}{\\partial x^{2}} $$\n空間座標が$n$次元の場合、 $$ \\dfrac{\\partial u}{\\partial t} = \\Delta u = \\nabla^{2}u $$ ここで$\\Delta = \\nabla^{2} = \\sum\\limits_{i=1}^{n} \\dfrac{\\partial^{2} }{\\partial x_{i}^{2}}$はラプラシアンを指す。\n外力forcing term$f = f(x,t)$がある場合、 $$ \\dfrac{\\partial u}{\\partial t} = \\dfrac{\\partial^{2} u}{\\partial x^{2}} + f $$\n拡散係数diffusion coefficient$a = a(x) \u0026gt; 0$がある場合、 $$ \\dfrac{\\partial u}{\\partial t} = \\dfrac{\\partial }{\\partial x} \\left[ a(x) \\dfrac{\\partial u}{\\partial x} \\right] $$\n初期条件と境界条件 熱方程式では、通常、初期条件と境界条件が与えられる。初期条件だけでは解が一意に決定されない。$u$が$\\Omega \\times [0, T]$で定義された関数だとすると、\n$$ \\text{initial condition : } u(x, 0) = g(x) \\quad \\text{ on } \\Omega \\times \\left\\{ 0 \\right\\} $$\n$$ \\text{boundary condition : } u(x, t) = h(x, t) \\quad \\text{ on } \\partial \\Omega \\times [0, T] $$\n$\\partial \\Omega$は$\\Omega$の境界である。\n説明 ラプラス方程式に時間に関する項が追加された形である。ラプラス方程式は時間の流れに無関係なので、平衡状態に関する方程式であり、熱方程式は時間の流れの影響を受けるので、何らかの物理量が流れる（拡散する）状態に関する方程式である。熱方程式という名前がついたのも、熱力学で初めて現れたからである。\n導出 $U \\subset \\mathbb{R}^n$が開集合で、物理的な空間を意味するとする。$u:U\\times (0,\\ \\infty) \\to \\mathbb{R}$をある物理量の密度関数とする。すると$u(x,\\ t)$は、時刻$t\u0026gt;0$での点$x\\in U$での密度を意味する。ある開集合$V$が$V \\Subset U$であり$V\\in C^{\\infty}$を満たすとする。また、$\\mathbf{F} : U \\times (0, \\infty) \\to \\mathbb{R}^n$を$u$のフラックスfluxとする。すると$u$と$\\mathbf{F}$の間に次の式が成り立つ必要がある。\n$$ \\dfrac{d}{dt}\\int_{V}u(x,t)dx = -\\int_{\\partial V}\\mathbf{F}(x, t) \\cdot \\nu (x) dS(x) $$\n左辺は、ある空間の内部で物理量$u$の変化量を述べており、右辺はその空間の境界で出入りした量を述べている。自ら生成されたり消えたりしない限り、その量は一定である。内部の物理量に変化があった場合、必ず入るものまたは出るものがあり、その両方の値は等しいということである。\n簡単な例として、人々が自由に出入りできる部屋があるとしよう。部屋の中で人数の変化を数える観察者$A$がいる。ドアの前に立ち、出て行く人を見るたびに$+1$を、入ってくる人を見るたびに$-1$を数える観察者$B$がいる。もし3人が部屋から出た場合、部屋の中で$A$が測定した変化量は-3であり、ドアの前で$B$が数えた数は3である。右辺にマイナス符号が付いた理由はこれである。\n$u \\in C^{2}$であるため、左辺の微分を積分の内側に入れ、右辺にグリーンの定理を適用すると、次のようになる。\n$$ \\int_{V} u_{t}(x,t)dx=-\\int_{V} \\nabla \\cdot \\mathbf{F}(x,t)dx\\quad \\forall t\u0026gt;0 $$\nしたがって、次を得る。\n$$ u_{t}=-\\nabla \\cdot \\mathbf{F}\\quad \\mathrm{in}\\ U\\times(0,\\infty) $$\nラプラス方程式を導いたときと同じように、$F$が$u$の勾配に比例する量であるとすると$\\mathbf{F}=-aDu$であり、次を得る。\n$$ u_{t}=-\\nabla \\cdot(-aDu)=a\\nabla \\cdot Du=a\\Delta u $$\n$a=1$とすると、熱方程式を得る。\nLawrence C. Evans, Partial Differential Equations (2nd Edition, 2010), p44\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Iserles, A First Course in the Numerical Analysis of Differential Equations (2nd, 2009), p349-351\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1001,"permalink":"https://freshrimpsushi.github.io/jp/posts/1001/","tags":null,"title":"熱方程式, 拡散方程式"},{"categories":"푸리에해석","contents":"定義 $\\mathbb{R}$で定義された二つの関数$f$、$g$が与えられたとする。以下の積分が存在する場合、これを二つの関数$f$、$g$の畳み込みと呼び、$f \\ast g$で示す。\n$$ f \\ast g(x):=\\int _{-\\infty} ^{\\infty} f(y)g(x-y)dy $$\n$f$、$g$が離散関数の場合、以下のように定義する。\n$$ (f \\ast g)(m)=\\sum \\limits_{n}f(n)g(m-n) $$\n説明 畳み込みという翻訳があるが、コンボリューションという言葉の方がよく使われる。一般に上記の定義をコンボリューションとして学ぶが、もう少し一般的に言うと、これは積分変換の一種であるフーリエ変換に対するコンボリューションである。交換法則、分配法則など多くの良い特性を持っているため、様々な分野で使用される。\n離散畳み込みの場合、解析的数論では少し異なる定義をすることもある。\n畳み込みが定義される条件は以下の通り：\n(a)\n$f\\in L^{1}$、$|g|\u0026lt;M$である場合、\n$$ \\left| \\int f(y)g(x-y)dy \\right| \\le \\int \\left| f(y)g(x-y) \\right|dy \\le M\\int \\left| f(y) \\right|dy \\lt \\infty $$\n(b)\n$\\left| f \\right| \\le M$、$g\\in L^{1}$である場合、\n$$ \\left| \\int f(y)g(x-y)dy \\right| \\le \\int \\left| f(y) g(x-y) \\right|dy \\le M\\int \\left| g(x-y) \\right|dy \\lt \\infty $$\n(c)\n$f,g\\in L^{2}$であり、$\\tilde{g}_{x}(y)=g(x-y)$とする。すると$\\tilde{g}_{x}\\in L^{2}$、$\\left\\| g \\right\\|_{2}=\\left\\| \\tilde{g}_{x} \\right\\|_{2}$であり、コーシー・シュワルツの不等式により\n$$ \\begin{align*} \\left| \\int f(y)g(x-y)dy \\right| \u0026amp;= \\left| \\int f(y)\\tilde{g}_{x}(y)dy \\right| \\\\ \u0026amp; = \\left| \\left\\langle f,\\tilde{g}_{x} \\right\\rangle \\right| \\\\ \u0026amp;\\le \\left\\| f \\right\\|_{2} \\left\\| \\tilde{g}_{x} \\right\\|_{2} \\\\ \u0026amp;\u0026lt;\\infty \\end{align*} $$\n(d)\n$f$が閉区間$[a,b]$を除き$0$で有界であり、$g$が区分的に連続である場合、\n$$ \\int _{-\\infty} ^{\\infty} f(y)g(x-y)dy=\\int _{a}^{b}f(y)g(x-y)dy\u0026lt;\\infty $$\n","id":1000,"permalink":"https://freshrimpsushi.github.io/jp/posts/1000/","tags":null,"title":"畳み込みの定義"},{"categories":"편미분방정식","contents":"定義1 $\\ U \\in \\mathbb{R}^n$は開集合 $\\ x\\in U$ $u=u(x) : \\overline{U} \\rightarrow \\mathbb{R}^n$ ラプラス方程式 下の偏微分方程式をラプラス方程式という。\n$$ \\Delta u=0 $$\nここで、$\\Delta$はラプラシアンである。ラプラス方程式を満たす$u$を特に調和関数という。\nポアソン方程式 非同次ラプラス方程式をポアソン方程式という。\n$$ -\\Delta u = f $$\n説明 ラプラス方程式は物理学の様々な場所に現れる。通常、$u$は平衡状態でのある物理量の密度を意味する。平衡状態で、$V \\subset U$とするとき、以下の式が成り立つ。\n$$ \\int_{\\partial V}\\mathbf{F} \\cdot \\boldsymbol{\\nu}dS=0 $$\n$\\mathbf{F}$は$u$のフラックス密度、$\\boldsymbol{\\nu}$は外向き単位法線ベクトルである。\nこの式の意味は、$u$の正味のフラックスは$0$であるということである。例えば、熱平衡状態にある何か空間があるとする。その空間の外から内へ入る熱もなく、内から外へ出る熱もない。つまり、その空間の境界面で熱の流れがないということである。この話は正味のフラックスが$0$であるという話と同じである。ここでグリーン・ガウスの定理を適用すると、次の式を得る。\n$$ 0 = \\int_{\\partial V} \\mathbf{F} \\cdot \\nu dS=\\int_{V} \\nabla \\cdot \\mathbf{F} dx \\\\ \\implies \\nabla \\cdot \\mathbf{F}=0 $$\nここで、$\\mathbf{F}$が$u$の勾配 $Du$に比例する値だとしよう。多くの場合、物理的な理由から逆方向を仮定するのが都合がいい。熱力学の第二法則(熱は常に高い所から低い所へ流れる)を例に挙げることができる。\n$$ \\begin{equation} \\mathbf{F}=-aDu \\label{eq1} \\end{equation} $$\nこのとき、$a\u0026gt;0$である。\nもし、$u$が化学物質の濃度、温度、静電気ポテンシャルを意味するなら、$\\eqref{eq1}$はそれぞれフィックの拡散法則、フーリエの熱伝導法則、オームの法則を意味する。\n以上の内容からラプラス方程式が導かれる。\n$$ \\nabla \\cdot \\mathbf{F} = \\nabla \\cdot (-aDu)=-a\\Delta u=0 \\\\ \\implies \\Delta u = 0 $$\nLawrence C. Evans, Partial Differential Equations (2nd Edition, 2010), p20-21\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":997,"permalink":"https://freshrimpsushi.github.io/jp/posts/997/","tags":null,"title":"ラプラス方程式とポアソン方程式"},{"categories":"정수론","contents":"要旨 1 グループ $G = F_{p}$ の要素 $g$ がオーダー $N$ であるとしよう。すると、離散対数問題 $g^{x} = h$ は、次の条件下では比較的簡単に解かれることになる。\r- (i): $p$ がスムーズ素数である。\n(ii): $p \\equiv 3 \\pmod{4}$ と $a$ がモジュロ $p$ に関する二次剰余である。 証明 (i) $p$ がスムーズな素数であれば、ポーリグ・ヘルマンアルゴリズムを利用できるため、離散対数問題は比較的簡単に解かれる。\n■\n(ii) $$ x^{2} \\equiv a \\pmod{p} $$ $a$ がモジュロ $p$ に関する二次剰余であるということは、上記合同方程式を満たす解が存在するということである。もし素数 $p$ を $4$ で割った余りが $3$ であれば、 $$ b \\equiv a^{(p+1)/4} \\pmod{p} $$ ある $a \\equiv g^{2k} \\pmod{p}$ に対して、 $$ \\begin{align*} b^{2} \\equiv \u0026amp; a^{{ p+1 } \\over { 2 }} \u0026amp; \\pmod{p} \\\\ \\equiv \u0026amp; \\left( g^{2k} \\right)^{{ p+1 } \\over { 2 }} \u0026amp; \\pmod{p} \\\\ \\equiv \u0026amp; g^{(p+1)k} \u0026amp; \\pmod{p} \\\\ \\equiv \u0026amp; g^{2k + (p-1)k} \u0026amp; \\pmod{p} \\\\ \\equiv \u0026amp; a \\cdot \\left( g^{p-1} \\right)^{k} \u0026amp; \\pmod{p} \\\\ \\equiv \u0026amp; a \u0026amp; \\pmod{p} \\end{align*} $$ よって、与えられた合同方程式の解となる。この公式により、$a$ の平方根を非常に速く見つけ出すことができ、そこで離散対数問題は比較的簡単に解かれるようになる。\n■\n関連項目 離散対数問題 離散対数問題の難しさを利用したセキュリティアルゴリズム ディフィー・ヘルマン鍵交換アルゴリズム エルガマル公開鍵暗号 離散対数問題への攻撃アルゴリズム シャンクスアルゴリズム ポーリグ・ヘルマンアルゴリズム 離散対数問題が簡単に解かれる条件 Hoffstein. (2008). An Introduction to Mathematical Cryptography: p84~92.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":942,"permalink":"https://freshrimpsushi.github.io/jp/posts/942/","tags":null,"title":"離散対数問題が容易に解決される条件"},{"categories":"통계적분석","contents":"モデル 1 白色雑音 $\\left\\{ e_{t} \\right\\}_{t \\in \\mathbb{N}}$ について、 $$ \\nabla^{d} Y_{t} := \\sum_{i = 1}^{p} \\phi_{i} \\nabla^{d} Y_{t-i} + e_{t} - \\sum_{i = 1}^{q} \\theta_{i} e_{t-i} $$ のように定義された $\\left\\{ Y_{t} \\right\\}_{ t \\in \\mathbb{N} }$ を $(p,d,q)$次のアリマ過程 $ARIMA (p,d,q)$ と言います。このような形の時系列分析モデルを アリマモデル と呼びます。\n説明 $ARI(p,d) \\iff ARIMA(p,d,0)$ を アリモデル、$IMA(d,q) \\iff ARIMA(0,d,q)$ を イマモデル ということもあるが、あまり使用されない。むしろ、$ARIMA(p,d,0)$ や $ ARIMA(0,d,q)$ のような表現を好んで使用する。\n式が難しそうに見えるが、思ったより難しくないんだ。ただ アルマモデル $$ Y_{t} = \\sum_{i = 1}^{p} \\phi_{i} Y_{t-i} + e^{t} - \\sum_{i = 1}^{q} \\theta_{i} e_{t-i} $$ で$Y_{t}$ が $\\nabla^{d} Y_{t}$ に変わっただけだから。ただ$d$ 回の差分を通じて定常性を得たデータをアルマモデルで分析すると見ればいい。\nCryer. (2008). Time Series Analysis: With Applications in R(2nd Edition): p992.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":941,"permalink":"https://freshrimpsushi.github.io/jp/posts/941/","tags":null,"title":"アリマモデル"},{"categories":"편미분방정식","contents":"まとめ $u, v \\in C^2( \\bar{U})$としよう。それでは、以下の式が成り立つ。\n(i) $\\displaystyle \\int_{U} \\Delta u dx=\\int_{\\partial U} \\dfrac{\\partial u}{\\partial \\nu}dS$\n(ii) $\\displaystyle \\int_{U} Dv \\cdot Du dx = -\\int_{U} u \\Delta v dx+\\int_{\\partial U}\\dfrac{\\partial v}{\\partial \\nu}udS$\n(iii) $\\displaystyle \\int_{U} (u\\Delta v - v\\Delta u )dx = \\int_{\\partial U} \\left( \\dfrac{\\partial v}{\\partial \\nu}u - \\dfrac{\\partial u}{\\partial \\nu} v\\right)dS$\nこれらをまとめて グリーンの公式Green\u0026rsquo;s formulaという。\n$\\Delta$はラプラシアン $D$はグラディエント $\\nu$は外向き単位法線ベクトル 証明 部分積分公式\n$u, v \\in C^1(\\bar{U})$とする。それでは、以下の式が成り立つ。\n$$ \\int_{U} u_{x_{i}}vdx = -\\int_{U} uv_{x_{i}}dx + \\int_{\\partial U} uv\\nu^{i} dS\\quad (i=1,\\dots , n) $$\n(i) 部分積分公式において、$u$の代わりに$u_{x_{i}}$を、$v$の代わりに$1$を代入すると、以下の式を得る。\n$$ \\int_{U} u_{x_{i} x_{i}}dx = \\int_{\\partial U} u_{x_{i}}\\nu^{i} dS \\quad (i=1,\\cdots , n) $$\n全ての$i=1,\\cdots, n$に対して加算すると、以下のようになる。\n$$ \\int_{U} (u_{x_{1} x_{1}}+\\cdots +u_{x_{n} x_{n}} )dx = \\int_{\\partial U}( u_{x_{1}}\\nu^{1} +\\cdots u_{x_{n}}\\nu^n)dS $$\nラプラシアンの定義と$\\dfrac{\\partial u}{\\partial \\nu}:=\\boldsymbol{\\nu}\\cdot Du$により、次が成り立つ。\n$$ \\int_{U} \\Delta u dx=\\int_{\\partial U} \\dfrac{\\partial u}{\\partial \\nu}dS $$\n■\n(ii) 部分積分公式において、$v$の代わりに$v_{x_{i}}$を代入すると、以下の式を得る。\n$$ \\int_{U} u_{x_{i}}v_{x_{i}}dx = -\\int_{U} uv_{x_{i}x_{i}}dx + \\int_{\\partial U} uv_{x_{i}}\\nu^{i} dS \\quad (i=1,\\cdots , n) $$\n全ての$i=1,\\cdots ,n$に対して加算すると、以下のようになる。\n$$ \\int_{U} (u_{x_{1}}v_{x_{1}}+\\cdots +u_{x_{n}}v_{x_{n}} )dx = -\\int_{U} u(v_{x_{1}x_{1}}+\\cdots v_{x_{n} x_{n}})dx + \\int_{\\partial U} ( v_{x_{1}}\\nu^1 +\\cdots v_{x_{n}}\\nu^n )udS $$\n整理すると、以下のようになる。\n$$ \\int_{U} Du\\cdot Dvdx = -\\int_{U} u\\Delta vdx + \\int_{\\partial U} \\dfrac{\\partial v}{\\partial \\nu}u dS $$\n■\n(iii) (ii)で$u$と$v$の位置を入れ替えると、以下の式を得る。\n$$ \\int_{U} Du \\cdot Dv dx = -\\int_{U} v \\Delta u dx+\\int_{\\partial U}\\dfrac{\\partial u}{\\partial \\nu}vdS $$\nこの式から(ii)を引くと、以下のようになる。\n$$ 0= -\\int_{U} ( v \\Delta u -u\\Delta v) dx+\\int_{\\partial U} \\left( \\dfrac{\\partial u}{\\partial \\nu}v -\\dfrac{\\partial v}{\\partial \\nu}u \\right)dS $$\n整理すると、次を得る。\n$$ -\\int_{U} ( v \\Delta u -u\\Delta v) dx=\\int_{\\partial U} \\left( \\dfrac{\\partial v}{\\partial \\nu}u -\\dfrac{\\partial u}{\\partial \\nu}v \\right)dS $$\n■\n参照 微分積分学におけるグリーンの定理 ","id":974,"permalink":"https://freshrimpsushi.github.io/jp/posts/974/","tags":null,"title":"グリーンの定理"},{"categories":"편미분방정식","contents":"定義1 $U\\subset \\mathbb{R}^{n}$を開集合とする。$U$の境界 $\\partial U$が$\\partial U \\in C^1$だとする。そうすると、以下のような外向きの単位法線ベクトルを定義できる。\n$$ \\boldsymbol{\\nu}=(\\nu^{1}, \\nu^{2}, \\dots, \\nu^{n}) \\quad \\text{and} \\quad |\\boldsymbol{\\nu}|=1 $$\n$\\boldsymbol{\\nu}$は境界のある点に接しており、大きさが1で外側を向いているベクトルである。これを$u \\in C^{1}(\\bar{U})$としよう。すると、方向微分 $\\dfrac{\\partial u}{\\partial \\nu}$を以下のように定義する。\n$$ \\dfrac{\\partial u}{\\partial \\nu} := \\boldsymbol{\\nu} \\cdot Du=(\\nu^1,\\cdots,\\nu^n)\\cdot(u_{x_{1}}, \\cdots, u_{x_{n}}) $$\n$D=D^{1}$は多重指数表記であり、$Du$は$u$の勾配である。\nLawrence C. Evans, Partial Differential Equations (2nd Edition, 2010), p710-711\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":988,"permalink":"https://freshrimpsushi.github.io/jp/posts/988/","tags":null,"title":"外向き単位法線ベクトル"},{"categories":"정수론","contents":"アルゴリズム 群 $G$ の元 $g$ がオーダー $N = q_{1}^{r_{1}} q_{2}^{r_{2}} \\cdots q_{t}^{r_{t}}$ だとしよう。そうすると、離散対数問題 $g^{x} = h$ は、以下のアルゴリズムに従って、多くても $\\displaystyle O \\left( \\sum_{i=1}^{t} S_{q_{i}^{r_{i}}} + \\log N \\right)$ ステップで解かれる。\nステップ 1.\n$\\displaystyle g_{i} : = g^{N / q_{i}^{r_{i}}}$ と $\\displaystyle h_{i} := h^{N / q_{i}^{r_{i}}}$ を計算する。\nステップ 2.\nシャンクスのアルゴリズムを使って、離散対数問題 $g_{i}^{y} = h_{i}$ の解 $y_{i}$ を求める。\nステップ 3.\n中国剰余定理を使って、$\\begin{cases} x \\equiv y_{1} \\pmod{ q_{1}^{r_{1}} } \\\\ \\qquad \\vdots \\\\ x \\equiv y_{t} \\pmod{ q_{t}^{r_{t}} } \\end{cases}$ を満たす $1 \\le x \\le N$ を求める。\n$S_{q_{i}^{r_{i}}}$ は、シャンクスのアルゴリズムがかかる時間で、$\\displaystyle O \\left( S_{q_{i}^{r_{i}}} \\right) \\approx O \\left( q_{i}^{r_{i} / 2} \\right) $ 程度だと考えればいい。 シャンクスのアルゴリズムは、オーダーがわかっているときに使用できる攻撃方法だ。ポラード・ロー アルゴリズムは、$p$ がスムーズであれば、$(p-1)$ の素因数分解が容易になり、その結果 $\\displaystyle g_{i} = g^{N / q_{i}^{r_{i}}}$ を見つけやすくなるという事実を利用する。こうして作られた $g^{i}$ のオーダーは自明に $q_{i}^{r_{i}}$ なので、シャンクスのアルゴリズムを使う制約がなくなる。これはもとの問題を小さな問題に分けて個別に解決した後、中国剰余定理で答えを得るというものだ。\n証明 パート 1. 存在性\n$x$ が $g^{x} = h$ の解であるということは、全ての $i = 1, \\cdots , t$ に対して、$x = y_{i} + q_{i}^{r_{i}} z_{i}$ を満たす $z_{i}$ が存在するという意味だ。\nパート 1-1. $\\displaystyle {{N} \\over { q_{i}^{r_{i}} } } x \\equiv {{N} \\over { q_{i}^{r_{i}} } } \\log_{g} ( h ) \\pmod{N} $\n$N$ と $g_{i} , y_{i} , h_{i}$ の定義により、 $$ \\begin{align*} \\left( g^{x} \\right)^{N / q_{i}^{r_{i}} } =\u0026amp; \\left( g^{y_{i} + q_{i}^{r_{i}} z_{i}} \\right)^{N / q_{i}^{r_{i}} } \\\\ =\u0026amp; \\left( g^{ N / q_{i}^{r_{i}} } \\right)^{ y_{i} } \\cdot g^{N z_{i} } \\\\ =\u0026amp; \\left( g^{ N / q_{i}^{r_{i}} } \\right)^{ y_{i} } \\\\ =\u0026amp; g_{i}^{ y_{i} } \\\\ =\u0026amp; h_{i} \\\\ =\u0026amp; h^{N / q_{i}^{r_{i}} } \\end{align*} $$ である。整理すると、 $$ \\left( g^{x} \\right)^{N / q_{i}^{r_{i}} } = h^{N / q_{i}^{r_{i}} } $$ となり、対数を取ると、以下を得る。 $$ {{N} \\over { q_{i}^{r_{i}} } } x \\equiv {{N} \\over { q_{i}^{r_{i}} } } \\log_{g} ( h ) \\pmod{N} $$ パート 1-2. $\\displaystyle \\sum_{i=1}^{t} {{N} \\over { q_{i}^{r_{i}} } } c_{i} = 1$\n$N$ の定義に従い、$\\displaystyle \\gcd \\left( {{N} \\over { q_{1}^{r_{1}} } } , \\cdots , {{N} \\over { q_{t}^{r_{t}} } } \\right) = 1$ が成立するのは自明であり、拡張ユークリッド定理 により、 $$ {{N} \\over { q_{1}^{r_{1}} } } c_{1} + \\cdots + {{N} \\over { q_{t}^{r_{t}} } } c_{t} = 1 $$ $c_{1} , \\cdots , c_{t} \\in \\mathbb{Z}$ が存在する。 パート 1-3. $x = \\log_{g} (h) \\pmod{N}$\nパート 1-1で得た、 $$ {{N} \\over { q_{i}^{r_{i}} } } x \\equiv {{N} \\over { q_{i}^{r_{i}} } } \\log_{g} ( h ) \\pmod{N} $$ の両辺に $c_{i}$ を掛けると、 $$ {{N} \\over { q_{i}^{r_{i}} } } c_{i} x \\equiv {{N} \\over { q_{i}^{r_{i}} } } c_{i} \\log_{g} ( h ) \\pmod{N} $$ $i=1$ から $t$ まで全てを加えると、 $$ \\sum_{i=1}^{t} {{N} \\over { q_{i}^{r_{i}} } } c_{i} x \\equiv \\sum_{i=1}^{t} {{N} \\over { q_{i}^{r_{i}} } } c_{i} \\log_{g} ( h ) \\pmod{N} $$ $x$ と $log_{g} (h)$ はインデックス $i$ に対して独立なので、 $$ x \\sum_{i=1}^{t} {{N} \\over { q_{i}^{r_{i}} } } c_{i} \\equiv \\log_{g} ( h ) \\sum_{i=1}^{t} {{N} \\over { q_{i}^{r_{i}} } } c_{i} \\pmod{N} $$ パート 1-2で $\\displaystyle \\sum_{i=1}^{t} {{N} \\over { q_{i}^{r_{i}} } } c_{i} = 1$ だから、次を得る。 $$ x \\equiv \\log_{g} (h) \\pmod{N} $$ パート 2. 時間計算量\n$i = 1, \\cdots , t$ ごとにステップ 2が繰り返されるので、$\\displaystyle O \\left( S_{q_{i}^{r_{i}}} \\right) $ かかる。さらに、ステップ 3で中国剰余定理を使うと、$O ( \\log N )$ かかるので、$\\displaystyle O \\left( \\sum_{i=1}^{t} S_{q_{i}^{r_{i}}} + \\log N \\right)$\n■\nコード 以下は、R言語で実装されたポラード・ロー アルゴリズムのコードだ。因数分解コード、オーダーを求めるコード、シャンクスのアルゴリズム、連続冪乗法、中国剰余定理を使うコードが使われた。\nprime = read.table(\u0026#34;../attachment\r/cfile8.uf@25411C3C5968BBE322F0D4.txt\u0026#34;); prime = prime[,1]\rfactorize\u0026lt;-function(p)\r{\rq=p\rfactors\u0026lt;-numeric(0)\ri=1; j=1\rwhile(q!=1)\r{\rif(q%%prime[i]) {i=i+1}\relse\r{\rq\u0026lt;-q/prime[i]\rfactors[j]\u0026lt;-prime[i]\ri=1\rj=j+1\r}\r}\rreturn(factors)\r}\rorder\u0026lt;-function(g,p,h=1) #Calculate a order of g in modulo p\r{\rqe\u0026lt;-table(factorize(p-1))\rqe\u0026lt;-rbind(as.numeric(names(qe)),qe)\rdivisor\u0026lt;-qe[1,1]^(0:qe[2,1])\rif((length(qe)/2)==1) {return(qe[1,1]^qe[2,1])}\rfor(i in 2:(length(qe)/2)) {divisor=c(divisor%*%t(qe[1,i]^(0:qe[2,i])))}\rfor(i in divisor) {if((FPM(g,i,p))%%p==1) break;}\rreturn(i)\r}\rFPM\u0026lt;-function(base,power,mod) #It is equal to (base^power)%%mod\r{\ri\u0026lt;-0\rif (power\u0026lt;0) {\rwhile((base*i)%%mod != 1) {i=i+1}\rbase\u0026lt;-i\rpower\u0026lt;-(-power)}\rif (power==0) {return(1)}\rif (power==1) {return(base%%mod)}\rn\u0026lt;-0\rwhile(power\u0026gt;=2^n) {n=n+1}\rA\u0026lt;-rep(1,n)\rA[1]=base\rfor(i in 1:(n-1)) {A[i+1]=(A[i]^2)%%mod}\rfor(i in n:1) {\rif(power\u0026gt;=2^(i-1)) {power=power-2^(i-1)}\relse {A[i]=1} }\rfor(i in 2:n) {A[1]=(A[1]*A[i])%%mod}\rreturn(A[1])\r}\rshanks\u0026lt;-function(g,h,p)\r{\rN\u0026lt;-order(g,p)\rn\u0026lt;-1+floor(sqrt(N))\rgn\u0026lt;-FPM(g,-n,p) #gn := g^{-n}\rx\u0026lt;-p\rList_1\u0026lt;-numeric(n+1)\rList_1[1]=1\rfor(i in 1:n) {List_1[i+1]=(List_1[i]*g)%%p}\rList_2\u0026lt;-numeric(n+1)\rList_2[1]=h\rfor(i in 1:n) {List_2[i+1]=(List_2[i]*gn)%%p}\rfor(i in 0:n+1) {\rfor(j in 0:n+1) {\rif (List_1[i]==List_2[j]) {x[length(x)+1]\u0026lt;-((i-1)+(j-1)*n)}\r}\r}\rreturn(min(x))\r}\rCRA\u0026lt;-function(S) #Algorithm of chinese remainder theorem\r{\rr\u0026lt;-S[,1] # matrix S express below sysyem.\rmod\u0026lt;-S[,2] # x = r[1] (mod mod[1])\rn\u0026lt;-length(r) # x = r[2] (mod mod[2])\r# x = r[3] (mod mod[3])\rA\u0026lt;-seq(r[1],to=mod[1]*mod[2],by=mod[1])\rfor(i in 2:n)\r{\rB=seq(r[i],to=mod[1]*mod[i],by=mod[i])\rr[1]=min(A[A %in% B])\rmod[1]=mod[1]*mod[i]\rif (i\u0026lt;n) {A=seq(r[1],to=mod[1]*mod[i+1],by=mod[1])}\r}\rreturn(r[1])\r}\rPHA\u0026lt;-function(g,h,p){\rN\u0026lt;-order(g,p)\rm_\u0026lt;-table(factorize(N))\rm_\u0026lt;-rbind(as.numeric(names(m_)),m_)\rm_\u0026lt;-c(data.frame(m_)[1,]^data.frame(m_)[2,])\ry_\u0026lt;-numeric(0)\rfor(i in 1:length(m_)){\rg_i\u0026lt;-FPM(g,N/m_[i],p)\rh_i\u0026lt;-FPM(h,N/m_[i],p)\ry_[i]\u0026lt;-shanks(g_i,h_i,p)\r}\rreturn(CRA(cbind(y_,m_)))\r}\rPHA(7,166,433)\rFPM(7,47,433)\rPHA(10,243278,746497)\rFPM(10,223755,746497)\rPHA(2,39183497,41022299)\rFPM(2,33703314,41022299) 上のコードを実行した結果は次の通りであり、連続冪乗法で検算をし、正しく動作していることを確認した。\n併せて読みたい 離散対数問題 離散対数問題の難しさを利用したセキュリティアルゴリズム ディフィー・ヘルマン鍵交換アルゴリズム エルガマル公開鍵暗号体系 離散対数問題に対する攻撃アルゴリズム シャンクスのアルゴリズム ポラード・ロー アルゴリズム 離散対数問題が容易に解ける条件 ","id":940,"permalink":"https://freshrimpsushi.github.io/jp/posts/940/","tags":null,"title":"ポラード・ロー アルゴリズムの証明"},{"categories":"수치해석","contents":"定義 関数 $f : \\mathbb{R} \\to \\mathbb{R}$ に対して異なる $x_{1} , \\cdots , x_{n}$ における 区分差分Divided Differenceは以下のように定義される。\n$$ \\begin{align*} f[x_{0}] :=\u0026amp; f( x_{0} ) \\\\ f [ x_{0} , x_{1} ] :=\u0026amp; {{ f ( x_{1} ) - f ( x_{0} ) } \\over { x_{1} - x_{0} }} \\\\ f [ x_{0} , x_{1} , x_{2} ] :=\u0026amp; {{ f [ x_{1} , x_{2} ] - f [ x_{0} , x_{1} ] } \\over { x_{2} - x_{0} }} \\\\ f [ x_{0} , \\cdots , x_{n} ] :=\u0026amp; {{ f [ x_{1} , \\cdots , x_{n} ] - f [ x_{0} , \\cdots , x_{n-1} ] } \\over { x_{n} - x_{0} }} \\end{align*} $$\n定理 [1]: $f [ x_{0} , \\cdots , x_{n} ]$ は $x_{0} , \\cdots , x_{n}$ の順序に関わらず常に同じである。 [2]: $$f [ x_{0} , \\cdots , x_{n} ] = \\sum_{i=0}^{n} {{ f( x_{i} ) } \\over { \\prod_{j \\in \\left\\{ 0 , \\cdots , n \\right\\} \\setminus \\left\\{ i \\right\\} } ( x_{i} - x_{j} ) }}$$ [3]: 次を満たす $\\xi$ が $\\mathscr{H} \\left\\{ x_{0} , x_{1} \\right\\}$ に存在する。$$f\u0026rsquo; ( \\xi ) = f [ x_0 , x_{1} ]$$ [4]: 次を満たす $\\xi$ が $\\mathscr{H} \\left\\{ x_{0} , \\cdots , x_{n} \\right\\}$ に存在する。$${{1} \\over {n!}} f^{(n)} ( \\xi ) =f [ x_{0} , \\cdots , x_{n} ]$$ [3]\u0026rsquo;: $$f [ x_{i} , x_{i} ] = f ' ( x_{i} )$$ [4]\u0026rsquo;: $$f [ \\underbrace{ x_{i} , \\cdots , x_{i} }_{ n+1 } ] = {{1} \\over {n!}} f^{(n)} ( x_{i} )$$ $\\mathscr{H} \\left\\{ a,b,c, \\cdots \\right\\}$ は $a,b,c, \\cdots$ を含む最小の区間を表す。 説明 区分差分は、数値解析の様々な理論を表現する際に空間を節約するのに大きな助けとなる。\n定理 [2]は実際の計算において特に便利で、 $$ f [ x_{0} , x_{1} , x_{2} ] = {{ f( x_{0} ) } \\over { ( x_{0} - x_{1} ) ( x_{0} - x_{2} ) }} + {{ f( x_{1} ) } \\over { ( x_{1} - x_{2} ) ( x_{1} - x_{0} ) }} + {{ f( x_{2} ) } \\over { ( x_{2} - x_{0} ) ( x_{2} - x_{1} ) }} $$ のように表現されるため、実際の定義と同じように反復計算を省くことができる。\n[3]は実質平均値の定理であり、微分した際に「瞬間変化率」の概念を考えたことがあれば、なぜ区分差分が微分を代替し、Divided Differenceという名前を持つのか理解できるだろう。定理 [4]はこのように一般化可能である。証明は区分差分の変数が無限に近づく、つまり極限を取ることを考えると、[3]\u0026rsquo;, [4]\u0026lsquo;のように$n$階の微分係数と見ることができる。このような概念的アプローチは、区分差分の定義から来る限界を克服し、数値解析の理論をさらに豊かにする。\n例 簡単に$f(x) := x^2 + 1$ に対していくつかの区分差分を計算してみよう。\n一点に対しては、 $$ f[3] = f(3) = 10 $$\n二点に対しては、 $$ f[0,5] = {{ f(0) - f(5) } \\over { 0 - 5}} = {{1 - 26 } \\over { -5 }} = 5 $$\n三点に対しては、 $$ \\begin{align*} f[2,3,-1] =\u0026amp; {{ \\displaystyle {{ f(2) - f( 3) } \\over { 2 - 3 }} - {{ f(3) - f(-1) } \\over { 3 - (-1) }} } \\over { 2 - (-1) }} \\\\ =\u0026amp; {{ \\displaystyle {{ 5 - 10 } \\over { -1 }} - {{ 10 - 2 } \\over { 4 }} } \\over { 3}} \\\\ =\u0026amp; {{ 5 -2 } \\over {3}} \\\\ =\u0026amp; 1 \\end{align*} $$ 区分差分はもちろん有限ベクトルに対して常に計算できるが、通常は上述のように$n=2$ まで使用され、それ以外ではほとんど使用されない。\n実装 以下は区分差分を R コードで実装したもので、関数 $f$ とベクトル $\\mathbb{x} = ( x_{0} , \\cdots , x_{n} )$ を受け取り、$f [ x_{0} , \\cdots , x_{n} ]$ を返す。\nf\u0026lt;-function(x) {x^2 + 1}\rdd\u0026lt;-function(f,X){\rif(length(X)==1) {return(f(X))}\rtemp\u0026lt;-numeric(0)\rfor(i in 1:length(X)) {temp[i]\u0026lt;-prod(X[i]-X[-i])}\rreturn(sum(f(X)/temp))\r}\rdd(f,c(3))\rdd(f,c(0,5))\rdd(f,c(2,3,-1)) 上記コードを実行した結果は以下の通りであり、$f(x) = x^2 +1$ に対する手計算と正確に一致することが確認できる。\n証明 [2] 戦略: ラグランジュの公式とニュートンの区分差分公式の次数を比較する。\n$$ \\Psi_{n} (x) := (x-x_{0}) \\cdots (x - x_{n}) $$ $\\Psi_{n}$ を$x$ に対して微分し、$x = x_{j}$ を代入すると $$ \\Psi\u0026rsquo;_{n} (x_{j}) := (x-x_{0}) \\cdots (x - x_{j-1})(x - x_{j+1}) \\cdots (x - x_{n}) $$\n$$ \\implies {{ \\Psi (x)_{n} } \\over { (x - x_{j}) \\Psi\u0026rsquo;_{n}(x_{j}) }} \\equiv {{ (x - x_{0} ) \\cdots (x - x_{j-1} ) (x - x_{j+1} ) \\cdots (x - x_{n} ) } \\over { (x_{j} - x_{0} ) \\cdots (x_{j} - x_{j-1} ) (x_{j} - x_{j+1} ) \\cdots (x_{j} - x_{n} ) }} = l_{j} (x) $$ ラグランジュの公式によれば、$f$ のポリノミアルの補間は $$ p_{n} (x) = \\sum_{j=0}^{n} {{ \\Psi_{n} (x) } \\over { (x - x_{j}) \\Psi\u0026rsquo;_{n} (x_{j}) }} \\cdot f( x_{j}) $$ したがって、$p_{n}$ の最高次項の係数は $\\displaystyle \\sum_{j=0}^{n} {{ f( x_{j}) } \\over { \\Psi\u0026rsquo;_{n} (x_{j}) }}$ であり、ニュートンの区分差分公式での$p_{n}$ の最高次項の係数は $f [x_{0} , \\cdots , x_{n} ]$ であるため、 $$ f [ x_{0} , \\cdots , x_{n} ] = \\sum_{i=0}^{n} {{ f( x_{i} ) } \\over { \\displaystyle \\prod_{j \\in \\left\\{ 0 , \\cdots , n \\right\\} \\setminus \\left\\{ i \\right\\} } ( x_{i} - x_{j} ) }} $$\n■\n[1] 定理 [2]により、$[x_{0} , \\cdots , x_{n}]$ 内の順序を変えることは$\\displaystyle \\sum_{i=0}^{n} {{ f( x_{i} ) } \\over { \\displaystyle \\prod_{j \\in \\left\\{ 0 , \\cdots , n \\right\\} \\setminus \\left\\{ i \\right\\} } ( x_{i} - x_{j} ) }}$ を計算する際に加算の順序を変えることに過ぎず、したがって常に同じである。\n■\n[4] 戦略: ラグランジュの公式とニュートンの区分差分公式の次数を比較する。\n$p_{n}(x) = p_{n-1}(x) + C(x)$ とすると $\\deg C = n$ であり、$i= 0, \\cdots, (n-1)$ に対して $p_{n}(x_{i}) = f(x_{i}) = p_{n-1}(x_{i})$ であるため、ある定数 $a_{n} \\ne 0$ に対して$C (x) = a_{n} (x - x_{0}) \\cdots (x - x_{n-1})$ のように表される。$C(x)$ に$x=x_{n}$ を代入してみると $$ C(x_{n} ) = p_{n} (x_{n}) - p_{n-1}(x_{n}) $$\n$$ C (x_{n}) = a_{n} (x_{n} - x_{0}) \\cdots (x_{n} - x_{n}) $$ $p_{n}$ は$f$ のポリノミアルの補間であるため、$p_{n} (x_{n}) = f(x_{n})$ である $$ a_{n} (x_{n} - x_{0}) \\cdots (x_{n} - x_{n-1}) = f (x_{n}) - p_{n-1}(x_{n}) $$\n$$ a_{n} = {{ f (x_{n}) - p_{n-1}(x_{n-1}) } \\over { (x_{n} - x_{0}) \\cdots (x_{n} - x_{n-1}) }} $$\nポリノミアル補間と実際の関数との誤差: $n$回微分可能な$f : \\mathbb{R} \\to \\mathbb{R}$ とある$\\xi \\in \\mathscr{H} \\left\\{ x_{0} , \\cdots , x_{n-1} \\right\\}$ に対して、$f$ のポリノミアル補間 $p_{n-1}$ はある$x_{n} \\in \\mathbb{R}$ に対して $$ f(x_{n}) - p_{n-1} (x_{n}) = {{ (x_{n} - x_{0}) \\cdots (x_{n} - x_{n-1}) } \\over { n! }} f^{(n)} ( \\xi ) $$\nポリノミアル補間と実際の関数との誤差の式に従って、 $$ a_{n} = {{ f^{(n)} ( \\xi ) } \\over { n! }} $$ 一方でニュートンの区分差分公式での$p_{n}$ の最高次項の係数は$a_{n} = f [x_{0} , \\cdots , x_{n} ]$ であるため、 $$ {{1} \\over {n!}} f^{(n)} ( \\xi ) =f [ x_{0} , \\cdots , x_{n} ] $$\n■\n[3] 定理 [4]により自明である。\n■\n","id":969,"permalink":"https://freshrimpsushi.github.io/jp/posts/969/","tags":null,"title":"数値解析学における階差段"},{"categories":"해석개론","contents":"定義 $[a,\\ b]$から$f(x)$の平均値は区間に対して積分した後、区間の長さで割ることと同じだ。\n$$ \\dfrac{1}{b-a}\\int_{a}^bf(x)dx $$\n導出 区間$[a,\\ b]$の分割を$P$としよう。\n$$ P=\\left\\{ x_{1},\\ x_{2},\\ \\cdots ,\\ x_{n} \\right\\} $$\nこの時、$a=x_{1} \u0026lt; x_{2} \u0026lt; \\cdots \u0026lt; x_{n}=b$であり、各点の間の距離は等しい。そして、$\\Delta x=x_{i+1}-x_{i}$。$f(x_{i})$の合計を$n$で割って$f(x)$の平均値を推定しようとする。\n$$ \\dfrac{ f(x_{1}) + f(x_{2}) + \\cdots +f(x_{n}) } {n} $$\nこれは$n$が大きくなるほど、関数の値の平均に近づくということだ。分子と分母に$\\Delta x$を掛けると以下のようになる。\n$$ \\dfrac{\\Big( f(x_{1}) + f(x_{2}) + \\cdots +f(x_{n}) \\Big)\\Delta x} {n \\Delta x} $$\n$n\\Delta x=b-a$なので次のようになる。\n$$ \\dfrac{\\Big( f(x_{1}) + f(x_{2}) + \\cdots +f(x_{n}) \\Big)\\Delta x} {b-a} $$\n$n \\rightarrow \\infty$であり、$\\Delta x \\rightarrow 0$の極限を取れば分子は$\\int_{a}^bf(x)dx$となる。\n$$ \\dfrac{1}{b-a}\\int_{a}^bf(x)dx $$\n■\n例 三角関数の一周期の平均は$0$だ。\nコサイン関数\n$\\cos (kx)$の一周期の平均を求めてみると次の通り。 $$ \\int_{0}^\\frac{2\\pi}{k} \\cos(kx)dx = \\dfrac{1}{k}\\left[ \\sin(kx)\\right]_{0}^{\\frac{2\\pi}{k}} =\\dfrac{1}{k}(\\sin 2\\pi -\\sin 0 ) =0 $$\nサイン関数\n$\\sin (kx)$の一周期の平均を求めてみると次の通り。 $$ \\int_{0}^\\frac{2\\pi}{k} \\sin(kx)dx = \\dfrac{-1}{k}\\left[ \\cos(kx)\\right]_{0}^{\\frac{2\\pi}{k}} =\\dfrac{-1}{k}(\\cos 2\\pi -\\cos 0 ) =0 $$\n参考 積分の平均値の定理 ","id":983,"permalink":"https://freshrimpsushi.github.io/jp/posts/983/","tags":null,"title":"関数の値の平均"},{"categories":"통계적검정","contents":"仮説検定 定量データ$\\left\\{ x_{i} \\right\\}_{i = 1}^{n}$が与えられているとする。\n$H_{0}$: データ$\\left\\{ x_{i} \\right\\}_{i = 1}^{n}$は正規分布に従う。 $H_{1}$: データ$\\left\\{ x_{i} \\right\\}_{i = 1}^{n}$は正規分布に従わない。 説明 シャピロ・ウィルク検定は、データの正規性を検定するために使用される仮説検定であり、通常は正規性が存在することを示すために使用される。帰無仮説が採択されることが\u0026rsquo;分析者の意図\u0026rsquo;と一致する稀な場合であるため、正確に仮説を理解することが重要である。\nコード 実践 Rでは、shapiro.test()関数を通じて簡単にシャピロ・ウィルク検定を行うことができる。以下の2つのランダムサンプルを生成し、実際にシャピロ・ウィルク検定を行おう。\nNは正規分布からのデータを表し、geoは幾何分布からのデータを表す。\n検定結果は正確に予想通りになる。\n全コード 以下はRコードの例である。\nset.seed(150421)\rN\u0026lt;-rnorm(100)\rwin.graph(4,4); hist(N)\rshapiro.test(N)\rgeo\u0026lt;-rgeom(100,0.5)\rwin.graph(4,4); hist(geo)\rshapiro.test(geo) 参照 ハルケ・ベラ検定 ","id":939,"permalink":"https://freshrimpsushi.github.io/jp/posts/939/","tags":null,"title":"シャピロ-ウィルク検定"},{"categories":"통계적분석","contents":"ビルドアップ 時系列では、時間が経つにつれて分散が大きくなる場合、それに伴う「ペナルティ」を与えて分散を一定にし、定常性を得るために変換が必要である。ルート$\\sqrt{}$やログ$\\log$は、値が大きいほど減少する量が多いためよく使用される。当然だが、分散が減少する場合は、データの傾向がある点に収束するという意味になるので、変換する前に時系列分析は不要である。\nテスト 1 ボックス・カックス変換: $$g(x) := \\begin{cases} \\displaystyle {{ x^{\\lambda} - 1 } \\over { \\lambda }} \u0026amp; , \\lambda \\ne 0 \\\\ \\log x \u0026amp; , \\lambda = 0 \\end{cases}$$\n見た目だけで変換を取るべきか、どんな変換を取るべきか自信が持てないときは、通常ボックス・カックス変換の仮説検定を使用し、逆になぜこれ以上の変換を取る必要がないのかを正当化するのにも使える。\n実践 組み込みデータUKgasを読み込んでみよう。\nUKgasはイギリスのガス消費量を四半期ごとに記録したデータであり、見ての通り、年が経つにつれて変動がさらに激しくなることが分かる。分散が一定でない場合、スムーズな分析が難しい。したがって、ログのような変換を行い、データを扱いやすくすることが必要だ。\n変換を取ると、完璧ではないが、明らかに分散が一定になったことが分かる。\nコード 以下は例示コードである。\nUKgas\rwin.graph(4,4); plot(UKgas,main=\u0026#39;UKgas\u0026#39;)\rlog(UKgas)\rwin.graph(4,4); plot(log(UKgas),main=\u0026#39;log(UKgas)\u0026#39;) 一緒に見よう 回帰分析での変換 Cryer. (2008). Time Series Analysis: With Applications in R(2nd Edition): p101.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":938,"permalink":"https://freshrimpsushi.github.io/jp/posts/938/","tags":["R"],"title":"時系列分析における変換"},{"categories":"정수론","contents":"定義 素数$p$が多くの約数を持っている場合、$(p-1)$であれば$p$をスムースな素数と言う。 $B$以下の素数の積で表される数を**$B$-スムース数**と言う。 $\\psi ( X , B )$は、$X$以下の$B$-スムース数の個数を表す。 説明 スムースな素数の例として$p=37$を考えると、$(p-1)$は$p-1 = 36 = 2^2 3^2$のような細かい素数の積で表される。スムースという概念は、暗号学が発展するにつれて、暗号化に適さない 素数を表すために導入されたものと考えても良い。\n$5$-スムース数の例として $$ 2,3,4,5,6,8,9,10,12,15,16,18 \\cdots $$ があり、$5$-スムース数ではない例として $$ 7,11,13, 14, 17,19,21,22,23,26,28,29,31,33\\cdots $$ がある。\n$\\psi : \\mathbb{N}^2 \\to \\mathbb{N}_{0}$は典型的なカウント関数で、例えば$\\psi (25,5)$を考えると、$25$以下の$5$-スムース数は$2,3,4,5,6,8,9,10,12, $$ 15,16,18 ,20,24,25$で$15$個ある。したがって、$\\psi (25,5) = 15$のように表すことができる。\n","id":927,"permalink":"https://freshrimpsushi.github.io/jp/posts/927/","tags":null,"title":"スムーズ素数"},{"categories":"해석개론","contents":"定義 関数 $f$が区間 $I$で以下の条件を満たすとき、$f$は区間 $I$で片方向に連続piecewise continuousだと言われる。\n有限個の不連続点 $x_{1},\\ x_{2},\\ \\cdots ,\\ x_{n} \\in I$を持つ。\n不連続点で左極限と右極限を持つ。\n$$ \\left|\\lim \\limits_{x\\rightarrow x_{i}^{+}} f(x) \\right| \u0026lt; \\infty \\quad \\text{and} \\quad \\left|\\lim_{x \\rightarrow x_{i}^{-}}f(x)\\right|\u0026lt;\\infty \\quad (i=1,\\ \\cdots ,\\ n) $$\n関数 $f$が有限個の不連続点を除いたすべての場所で無限回微分可能なら片方向に滑らかpiecewise smoothだと言う。\n説明 条件 1.を満たしているだけでも片方向に連続と言う場合もある。簡単に言えば、不連続点を基準に関数を分割したとき、分割された各関数が全部連続なら片方向に連続である。\n片方向に連続、区間ごとに連続、分割的に連続など、様々な訳がある。\n","id":972,"permalink":"https://freshrimpsushi.github.io/jp/posts/972/","tags":null,"title":"各断片ごとの連続性、各断片ごとの滑らかさ"},{"categories":"정수론","contents":"アルゴリズム 1 単位元が$e$であるグループ$G$の元$g$が、オーダー$N$とする。それならば、離散対数問題$g^{x} = h$は次のアルゴリズムに従って、多くても$O \\left( \\sqrt{N} \\log N \\right)$ステップ以内に解ける。\nステップ 1.\n$n: = 1 + \\lfloor \\sqrt{N} \\rfloor $\nステップ 2.\n二つのリスト$A := \\left\\{ e , g , g^{2} , \\cdots , g^{n} \\right\\}$と$B := \\left\\{ h , hg^{-n} , hg^{-2n} , \\cdots , hg^{-n^2} \\right\\}$を作る。\nステップ 3.\n$g^{i} = h g^{-jn} \\in A \\cap B$を見つける。\nステップ 4.\n$x = i + jn$は$g^{x} = h$の解である。\n説明 基本的にグループ$G$の元$g$がオーダー$N$である場合、離散対数問題$g^{x} = h$は多くても$O (N) $ステップ以内に解ける。$g$がオーダー$N$であるとは、少なくとも$g^{N} = e$が成り立つことを意味する。したがって、$ 0, 1, \\cdots , N-1$の中には$g^{x} = h$を満たす$x$が存在することが簡単に確認できる。\nシャンクスのアルゴリズムは、この計算量を$O \\left( \\sqrt{N} \\log N \\right)$まで落としてくれる。暗号に適用される離散対数問題であれば、$N$はかなり大きな数であろうから、半分以上計算を減らすことができるので、大きな意味がある。\nベビーステップとジャイアントステップは、$g$を掛けて作られるリスト$A$と、$g^{-n}$を掛けて作られるリスト$B$から出た言葉だ。\n証明 パート 1. 存在性\n$\\left( A \\cap B \\right) \\ne \\emptyset$であることを示さなければならない。\n$x$を$n$で割った商を$q$、余りを$r$とすると、$x = nq + r$のように表せる。\nすると$\\sqrt{N} \u0026lt; n$なので $$ q = {{ x - r } \\over { n }} \u0026lt; {{ N } \\over { n }} \u0026lt; n $$ そして $$ \\begin{align*} \u0026amp; g^{x} = h \\\\ \\implies \u0026amp; g^{ nq + r } = h \\\\ \\implies \u0026amp; g^{ r } = h g^{ - nq } \\end{align*} $$ である。$r \u0026lt; n$なので$g^{r} \\in A$であり、$q \u0026lt; n$なので$h g^{ - nq } \\in B$である。\nパート 2. 時間複雑性\n$A \\cap B$を見つけるためにはリストをソートする必要があり、最適な比較ソートアルゴリズムを使用すると$O ( n \\log n )$回の計算が必要である。\n$n \\approx \\sqrt{N}$であるため、 $$ \\begin{align*} O \\left( n \\log n \\right) =\u0026amp; O \\left( \\sqrt{N} \\log \\sqrt{N} \\right) \\\\ =\u0026amp; O \\left( {{1} \\over {2}} \\sqrt{N} \\log N \\right) \\\\ =\u0026amp; O \\left( \\sqrt{N} \\log N \\right) \\end{align*} $$\n■\nコード 以下は、R言語で実装されたシャンクスのアルゴリズムである。素因数分解コードとオーダーを求めるコードが使用された。\nprime = read.table(\u0026#34;../attachment\r/cfile8.uf@25411C3C5968BBE322F0D4.txt\u0026#34;); prime = prime[,1]\rfactorize\u0026lt;-function(p)\r{\rq=p\rfactors\u0026lt;-numeric(0)\ri=1; j=1\rwhile(q!=1)\r{\rif(q%%prime[i]) {i=i+1}\relse\r{\rq\u0026lt;-q/prime[i]\rfactors[j]\u0026lt;-prime[i]\ri=1\rj=j+1\r}\r}\rreturn(factors)\r}\rorder\u0026lt;-function(g,p,h=1) #Calculate a order of g in modulo p\r{\rqe\u0026lt;-table(factorize(p-1))\rqe\u0026lt;-rbind(as.numeric(names(qe)),qe)\rdivisor\u0026lt;-qe[1,1]^(0:qe[2,1])\rif((length(qe)/2)==1) {return(qe[1,1]^qe[2,1])}\rfor(i in 2:(length(qe)/2)) {divisor=c(divisor%*%t(qe[1,i]^(0:qe[2,i])))}\rfor(i in divisor) {if((FPM(g,i,p))%%p==1) break;}\rreturn(i)\r}\rshanks\u0026lt;-function(g,h,p)\r{\rN\u0026lt;-order(g,p)\rn\u0026lt;-1+floor(sqrt(N))\rgn\u0026lt;-FPM(g,-n,p) #gn := g^{-n}\rx\u0026lt;-p\rList\\_1\u0026lt;-numeric(n+1)\rList\\_1[1]=1\rfor(i in 1:n) {List\\_1[i+1]=(List\\_1[i]*g)%%p}\rList\\_2\u0026lt;-numeric(n+1)\rList\\_2[1]=h\rfor(i in 1:n) {List\\_2[i+1]=(List\\_2[i]*gn)%%p}\rfor(i in 0:n+1) {\rfor(j in 0:n+1) {\rif (List\\_1[i]==List\\_2[j]) {x[length(x)+1]\u0026lt;-((i-1)+(j-1)*n)}\r}\r}\rreturn(min(x))\r}\rshanks(11,21,71)\rFPM(11,37,71)\rshanks(156,116,593)\rFPM(156,59,593)\rshanks(650,2213,3571)\rFPM(650,319,3571) 上記コードを実行した結果は以下の通りであり、連続べき乗法で検算して、正しく動作していることを確認した。\nも参照 離散対数問題 離散対数問題の難しさを利用したセキュリティアルゴリズム ディフィー・ヘルマン鍵交換アルゴリズム エルガマル公開鍵暗号体系 離散対数問題に対する攻撃アルゴリズム シャンクスのアルゴリズム ポラードのロー・アルゴリズム 離散対数問題が簡単に解ける条件 Hoffstein. (2008). 「数学的暗号化への導入」: p80。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":917,"permalink":"https://freshrimpsushi.github.io/jp/posts/917/","tags":null,"title":"ショアのアルゴリズムの証明"},{"categories":"통계적분석","contents":"定義 1 オペレーター $B$ を $B Y_{t} = Y_{t-1}$ のように定義し、バックシフトと呼ぶ。 オペレーター $\\nabla$ を $\\nabla := 1 - B$ および $\\nabla^{r+1} = \\nabla \\left( \\nabla^{r} Y_{t} \\right)$ のように定義し、差分と呼ぶ。 説明 差分の定義によると、$1$次の差分は $$ \\nabla Y_{t} = Y_{t} - Y_{t-1} $$ のように計算され、$2$次の差分は $$ \\begin{align*} \\nabla^2 Y_{t} =\u0026amp; \\nabla \\left( \\nabla Y_{t} \\right) \\\\ =\u0026amp; \\nabla \\left( Y_{t} - Y_{t-1} \\right) \\\\ =\u0026amp; \\nabla Y_{t} - \\nabla Y_{t-1} \\\\ =\u0026amp; ( Y_{t} - Y_{t-1} ) - ( Y_{t-1} - Y_{t-2} ) \\\\ =\u0026amp; Y_{t} - 2 Y_{t-1} + Y_{t-2} \\end{align*} $$ のように計算される。つまり、$Y_{t}$ に差分を2回適用したからといって、$Y_{t} - Y_{t-2}$ になるわけではない。このように連続して長くなる差分は季節性差分として別に定義される。\n時系列で差分が必要な理由は、トレンドがあるデータを扱う際に便利だからである。時系列分析におけるトレンドとは「データの値が一定期間にわたって増加または減少する傾向」を指し、この場合は定常性に問題がある。したがって、データが定常性を持つように適切に差分を取る前処理を行う。単純に増加または減少する程度であれば、一度で十分であり、複雑な形状を持つ場合はそれだけ多くの差分を取る必要があるかもしれない。\n差分を取るのが適切か、どれくらい取るべきか確信が持てない場合、通常はディッキー-フラー検定を使用し、逆になぜこれ以上差分を取る必要がないのかを正当化するのにも使える。\n実習 TSAパッケージのoil.priceデータを見てみよう。\noil.priceは1986年から2005年までの原油価格に関するデータだ。後期になるほど急騰しているので、定常性が欠けていると言える。このようなデータは分析が難しいため、差分を取ってトレンドを取り除く。\nRで差分を取る方法はとても簡単だ。diff()関数を使えば、最初の観測値を落として差分化されたデータを返す。あまり使用されないが、lag=nというオプションを与えることで、$n$回の差分も簡単に行うことができる。\n差分を取った結果、変動自体は依然として激しいが、平均的には$0$の近くで動いていることが確認できる。\nコード 以下はRの例示コードだ。\nlibrary(TSA)\rdata(oil.price); oil.price\rwin.graph(4,4); plot(oil.price,main=\u0026#39;oil.price\\\u0026#39;)\rdiff(oil.price)\rwin.graph(4,4); plot(diff(oil.price),main=\u0026#39;∇oil.price\\\u0026#39;)\rdiff(oil.price,lag=2) 参照 数値解析における差分 Cryer. (2008). Time Series Analysis: With Applications in R(2nd Edition): p90.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":916,"permalink":"https://freshrimpsushi.github.io/jp/posts/916/","tags":["R"],"title":"時系列分析における差分"},{"categories":"정수론","contents":"ビルドアップ 左から順に、アリス、ボブ、イブとしよう。アリスとボブはメッセージを交換する当事者で、イブはメッセージに興味を持つ消極的な攻撃者だ。オレンジ色の箱はアリスだけが知っている情報を、水色の箱はボブだけが知っている情報を、黒色の箱は公開されている（イブも知っている）情報を示している。\nアリスにはボブから受け取るべきメッセージ$m \\in \\mathbb{N}$がある。\nアルゴリズム 1 $\\mathbb{F}_{p}^{ \\ast } = \\mathbb{F}_{p} \\setminus \\left\\{ 0 \\right\\} = \\left\\{ 1, g , g^2 , \\cdots , g^{p-2} \\right\\}$ は要素数が$(p-1)$個の巡回群としよう。\nキー設定: 非常に大きな素数$p$と$\\text{ord}_{p} (g)$が大きな素数である$g \\in \\mathbb{F}_{p}^{ \\ast }$を選んで公開する。\nアリスは自分だけが知っているキー$1 \\le a \\le p-1$を決めて、$A \\equiv g^{a} \\pmod{p}$を計算して公開する。\n暗号化: 一回用のランダムキー$k$を生成する。ボブは$c_{1} \\equiv g^{k} \\pmod{p}$と$c_{2} \\equiv m A^{k} \\pmod{p}$を計算して$(c_{1} , c_{2})$をアリスに送信する。\n復号化: アリスが$(c_{1} , c_{2})$を使って$x \\equiv (c_{1}^{a} )^{-1} c_{2} \\equiv m \\pmod{p}$を計算すると$x = m$を得る。イブは現実的に$a$を知ることができないため、$m$を知ることができない。\n説明 実際、体系自体で$g$の位数$\\text{ord}_{p} (g)$が大きくなければならない理由はないが、単純に計算する場合の偶然の可能性を下げるためには、位数が大きい数を選ぶ方が良い。\n証明 復号化 アリスは$a$を知っており$c_{1} \\equiv g^{k} \\pmod{p}$と$c_{2} \\equiv m A^{k} \\pmod{p}$であるため$k$を知らなくても $$ \\begin{align*} (c_{1}^{a} )^{-1} c_{2} \u0026amp; \\equiv \\left( g^{ak} \\right)^{-1} \\cdot c_{2} \\\\ \u0026amp; \\equiv \\left( g^{ak} \\right)^{-1} \\cdot \\left( m A^{k} \\right) \\\\ \u0026amp; \\equiv \\left( g^{ak} \\right)^{-1} \\cdot \\left( m \\left( g^{a} \\right)^{k} \\right) \\\\ \u0026amp; \\equiv m \\left( g^{ak} \\right)^{-1} \\cdot g^{ak} \\\\ \u0026amp; \\equiv m \\pmod{p} \\end{align*} $$ を簡単に計算することができる。\n■\n暗号化 イブは$c_{1}$と$c_{2}$しか知らず$a$と$k$を知らないため、離散対数問題${g}^{x} \\equiv c_{1}^{a} \\pmod{p}$を知ることができず、知ったとしても直接解かなければならない。これは非常に難しいため、ボブはメッセージ$m$を安全にアリスに伝えることができる。\n■\n参照 離散対数問題 離散対数問題の困難さを利用したセキュリティアルゴリズム ディフィー・ヘルマン鍵交換アルゴリズム エルガマル公開鍵暗号体系 離散対数問題に対する攻撃アルゴリズム シャンクスのアルゴリズム ポラードのロー アルゴリズム 離散対数問題が簡単に解ける条件 Hoffstein. (2008). An Introduction to Mathematical Cryptography: p70.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":915,"permalink":"https://freshrimpsushi.github.io/jp/posts/915/","tags":null,"title":"エルガマル公開鍵暗号方式の証明"},{"categories":"수리물리","contents":"公式 デル演算子が含まれるベクトル積分について、次の式が成り立つ。\n(a)\n$$ \\int_{\\mathcal{V}}\\mathbf{A} \\cdot (\\nabla f)d\\tau = \\oint_{\\mathcal{S}}f\\mathbf{A} \\cdot d \\mathbf{a}-\\int_{\\mathcal{V}}f(\\nabla \\cdot \\mathbf{A})d\\tau $$\n(b)\n$$ \\int_{\\mathcal{S}} f \\left( \\nabla \\times \\mathbf{A} \\right)\\mathbf{A} \\cdot d \\mathbf{a} = \\int_{\\mathcal{S}} \\left[ \\mathbf{A} \\times \\left( \\nabla f \\right) \\right] \\cdot d\\mathbf{a} + \\oint_{\\mathcal{P}} f\\mathbf{A} \\cdot d\\mathbf{l} $$\n(c)\n$$ \\int_{\\mathcal{V}} \\mathbf{B} \\cdot \\left( \\nabla \\times \\mathbf{A} \\right) d\\tau = \\int_{\\mathcal{V}} \\mathbf{A} \\cdot \\left( \\nabla \\times \\mathbf{B} \\right) d\\tau + \\oint_{\\mathcal{S}} \\left( \\mathbf{A} \\times \\mathbf{B} \\right) \\cdot d \\mathbf{a} $$\n説明 部分積分は、ある関数$(f\\ or\\ \\mathbf{A}$とある関数の導関数$(\\nabla f\\ or\\ \\nabla \\cdot \\mathbf{A})$の積の積分を簡単にする方法だ。\n部分積分 $\\dfrac{d}{dx}\\left( fg \\right) = f\\dfrac{dg}{dx}+g\\dfrac{df}{dx}$ 両辺を定積分すると\n$$ \\int_{a}^b \\dfrac{d}{dx} \\left(fg\\right) = (fg)\\Big|_{a}^b=\\int_{a}^b f\\left(\\dfrac{dg}{dx}\\right)dx+\\int_{a}^bg\\left(\\dfrac{df}{dx}\\right)dx \\\\ \\implies \\int_{a}^b f\\left(\\dfrac{dg}{dx}\\right)dx = (fg)\\Big|_{a}^b-\\int_{a}^bg\\left(\\dfrac{df}{dx}\\right)dx $$\n証明 (a) 乗法則 3を使う\n$$ \\nabla \\cdot (f\\mathbf{A}) = \\mathbf{A} \\cdot (\\nabla f) + f(\\nabla \\cdot \\mathbf{A}) $$\n両辺を体積積分すると\n$$ \\int_{\\mathcal{V}} \\nabla \\cdot (f\\mathbf{A})d\\tau = \\int_{\\mathcal{V}}\\mathbf{A} \\cdot (\\nabla f)d\\tau + \\int_{\\mathcal{V}}f(\\nabla \\cdot \\mathbf{A})d\\tau $$\n左辺に**発散定理**を適用すると\n$$ \\oint_{\\mathcal{S}}f\\mathbf{A} \\cdot d \\mathbf{a} = \\int_{\\mathcal{V}}\\mathbf{A} \\cdot (\\nabla f)d\\tau + \\int_{\\mathcal{V}}f(\\nabla \\cdot \\mathbf{A})d\\tau $$\n整理すると\n$$ \\int_{\\mathcal{V}}f(\\nabla \\cdot \\mathbf{A})d\\tau = \\oint_{\\mathcal{S}}f\\mathbf{A} \\cdot d \\mathbf{a}-\\int_{\\mathcal{V}}\\mathbf{A} \\cdot (\\nabla f)d\\tau $$\nあるいは\n$$ \\int_{\\mathcal{V}}\\mathbf{A} \\cdot (\\nabla f)d\\tau = \\oint_{\\mathcal{S}}f\\mathbf{A} \\cdot d \\mathbf{a}-\\int_{\\mathcal{V}}f(\\nabla \\cdot \\mathbf{A})d\\tau $$\n■\n(b) (c) ","id":959,"permalink":"https://freshrimpsushi.github.io/jp/posts/959/","tags":null,"title":"デル演算子を含む式の部分積分"},{"categories":"통계적분석","contents":"モデル 1 白色雑音 $\\left\\{ e_{t} \\right\\}_{t \\in \\mathbb{N}}$ について $$ Y_{t} := \\phi_{1} Y_{t-1} + \\phi_{2} Y_{t-2} + \\cdots + \\phi_{p} Y_{t-p} +e_{t} - \\theta_{1} e_{t-1} - \\theta_{2} e_{t-2} - \\cdots - \\theta_{q} e_{t-q} $$ として定義される、$(p,q)$次の自己回帰移動平均過程 $ARMA(p,q)$ と呼ばれる。\n説明 アルマモデルは、単純に移動平均過程と自己回帰過程を組み合わせた形をしている。例えば $(1,1)$次であれば、 $$ ARMA(1,1) : Y_{t} = \\phi Y_{t-1} + e_{t} - \\theta e_{t-1} $$ となる式だ。しかし、アルマモデルはまだモデルとして不足している点があるため、差分を通じて改善されたアリマモデルを主に使用する。もちろん、本質的にはすべてアルマモデルとして結論づけられる。\nCryer. (2008). Time Series Analysis: With Applications in R(2nd Edition): p77.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":914,"permalink":"https://freshrimpsushi.github.io/jp/posts/914/","tags":null,"title":"自己回帰移動平均モデル"},{"categories":"통계적분석","contents":"モデル 1 白色雑音 $\\left\\{ e_{t} \\right\\}_{t \\in \\mathbb{N}}$ について、$Y_{t} := \\phi_{1} Y_{t-1} + \\phi_{2} Y_{t-2} + \\cdots + \\phi_{p} Y_{t-p} + e_{t}$ として定義される $\\left\\{ Y_{t} \\right\\}_{ t \\in \\mathbb{N} }$ を $p$次の自己回帰過程 $AR(p)$ と呼ぶ。\n(1): $AR(1) : Y_{t} = \\phi Y_{t-1} + e_{t}$ (2): $AR(2) : Y_{t} = \\phi_{1} Y_{t-1} + \\phi_{2} Y_{t-2} + e_{t}$ (p): $AR(p) : Y_{t} = \\phi_{1} Y_{t-1} + \\phi_{2} Y_{t-2} + \\cdots + \\phi_{p} Y_{t-p} + e_{t}$ (∞): $AR( \\infty ) : Y_{t} = e_{t} + \\phi_{1} Y_{t-1} + \\phi_{2} Y_{t-2} + \\cdots $ $\\mathbb{N}$ は 自然数の集合 $\\left\\{ 1, 2, 3 , \\cdots \\right\\}$ を意味する。 説明 $AR(p)$ を「自己回帰過程」と呼ぶ理由は、文字通り以前の時点の自分自身を独立変数のように見る回帰式の形をとるからだ。変数間の独立性を仮定することは明らかにない。また、定常性を必要としないが、例えば$AR(1) : Y_{t} = \\phi Y_{t-1} + e_{t}$が増加、減少、または振動などの単純な動きを示すことは容易に推測できる。\nCryer. (2008). タイムシリーズ分析: Rの応用(第2版): p66.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":910,"permalink":"https://freshrimpsushi.github.io/jp/posts/910/","tags":null,"title":"自己回帰過程"},{"categories":"정수론","contents":"定義 1 素数 $p$ について、ガロア体 $\\mathbb{F}_{p} := \\mathbb{Z} / p \\mathbb{Z}$ の恒等元が $0$ だとしよう。\n$\\mathbb{F}_{p}$ の原始根 $g \\ne 0$ に関して、巡回群 $\\mathbb{F}_{p} ^{ \\ast } := \\mathbb{F}_{p} \\setminus \\left\\{ 0 \\right\\} = \\left\u0026lt; g \\right\u0026gt;$ 上で定義された関数 $\\log_{g} : \\mathbb{F}_{p}^{ \\ast } \\to \\mathbb{Z} / (p-1) \\mathbb{Z}$ が次の条件を満たすとき、離散対数Discrete Logarithmと呼ぶ。 $$ g^{ \\log_{g} (h) } \\equiv h \\pmod{p} $$\n説明 $\\mathbb{F}_{p} ^{ \\ast }$ の存在は原始根定理によって保証される。\n実際、離散対数はすべての $a,b \\in \\mathbb{F}_{p}^{ \\ast }$ に対して $\\log_{g} (ab) = \\log_{g} (a) + \\log_{g} (b)$ であるため、同型となる。\n通常の対数と異なり、暗号学での対数はその計算の困難さのために有用であるが、与えられた $k$ に対して $$ g^{k} \\equiv x \\pmod{p} $$ を満たす $x$ を見つけることは連続累乗法を使用すればそれほど難しくないが、与えられた $h$ に対して $$ g^{x} \\equiv h \\pmod{p} $$ を満たす $x$ を見つけることは難しい。これを離散対数問題Discrete Logarithm Problemと呼び、暗号の必要不可欠な特性を持ち、暗号学全般で広く使用されている。\n一方、離散対数問題は群 $\\left( G , \\ast\\ \\right)$ 上で一般化することもできる。問題自体は依然として $$ g \\ast\\ \\cdots \\ast\\ g = h $$ を満たすために、$g \\ast\\ g$ を何回見つけなければならないかを問うだけである。これは、暗号学の限界が整数論でないことを意味する。\n同時に見る 離散対数問題 離散対数問題の難しさを利用した暗号アルゴリズム ディフィー・ヘルマン鍵交換アルゴリズム エルガマル公開鍵暗号システム 離散対数問題に対する攻撃アルゴリズム シャンクスのアルゴリズム ポラードのロー・アルゴリズム 離散対数問題が容易に解ける条件 Hoffstein. (2008). 『数学的暗号学への招待』: p63.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":911,"permalink":"https://freshrimpsushi.github.io/jp/posts/911/","tags":null,"title":"離散対数"},{"categories":"통계적분석","contents":"モデル 1 白色雑音 $\\left\\{ e_{t} \\right\\}_{t \\in \\mathbb{N}}$ について$Y_{t} := e_{t} - \\theta_{1} e_{t-1} - \\theta_{2} e_{t-2} - \\cdots - \\theta_{q} e_{t-q}$と同様に定義された$\\left\\{ Y_{t} \\right\\}_{ t \\in \\mathbb{N} }$を**$q$次移動平均過程 $MA(q)$**と呼ぶ。\n(1): $MA(1) : Y_{t} = e_{t} - \\theta e_{t-1}$ (2): $MA(2) : Y_{t} = e_{t} - \\theta_{1} e_{t-1} - \\theta_{2} e_{t-2}$ (q): $MA(q) : Y_{t} = e_{t} - \\theta_{1} e_{t-1} - \\theta_{2} e_{t-2} - \\cdots - \\theta_{q} e_{t-q}$ (∞): $MA( \\infty ) : Y_{t} = e_{t} - \\theta_{1} e_{t-1} - \\theta_{2} e_{t-2} - \\cdots$ $\\mathbb{N}$ は自然数の集合 $\\left\\{ 1, 2, 3 , \\cdots \\right\\}$ を意味する。 説明 次の図の緑、赤、オレンジ、紫の線を移動平均線と呼ぶ。\n移動平均線は、特に株式市場等で広く利用されるグラフで、日々の極端な変動ではなく平均を見ることで全体的なトレンドを見るのに役立つ。しかし、式だけを見て$MA(q)$を「移動平均過程」と呼ぶ理由は理解しにくいが、例えば$MA(2) : Y_{t} = e_{t} - \\theta_{1} e_{t-1} - \\theta_{2} e_{t-2}$とした場合、$\\displaystyle \\theta_{1} = \\theta_{2} = - {{1} \\over {2}}$の時に$\\displaystyle Y_{t} = {{ e_{t-1} + e_{t-2} } \\over {2}} + e_{t}$になることを考えれば良い。\nある変数が順調に成長したり、減退している場合、つまり局所的な変動が少ない場合は、移動平均線を見る意味がない。同様に、移動平均は変化を滑らかにするだけで、スケール自体を変えるわけではない。同様に、$MA(q)$は$q$より短い区間でのパターンを把握することに関心を持ち、具体的な値や大きなトレンドについては知ったことではない。言い換えれば、定常性を持ったデータにのみ使用できる。\nCryer. (2008). Time Series Analysis: With Applications in R(2nd Edition): p57.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":909,"permalink":"https://freshrimpsushi.github.io/jp/posts/909/","tags":null,"title":"移動平均過程"},{"categories":"상미분방정식","contents":"定義 次の微分方程式をチェビシェフChebyshev 微分方程式という。\n$$ (1-x^2)\\dfrac{d^2 y}{dx^2} -x\\dfrac{dy}{dx}+n^2 y=0 $$\n説明 係数に独立変数 $x$が含まれる形式であり、解がべき級数の形であると仮定すると、解くことができる。チェビシェフ方程式の解をチェビシェフ多項式と言い、解は一般的に$T_{n}(x)$と表される。\n解法 $$ \\begin{equation} (1-x^2)y^{\\prime \\prime} -xy^{\\prime}+\\lambda^2 y=0 \\label{1} \\end{equation} $$\n上で示したチェビシェフ微分方程式の解を次のように仮定しよう。\n$$ y=a_{0}+a_{1}(x-x_{0})+a_2(x-x_{0})^2+\\cdots=\\sum \\limits_{n=0}^\\infty a_{n}(x-x_{0})^n $$\nこのとき$x=0$であるとき$y^{\\prime \\prime}$の係数が$(1-x^2)|_{x=0}=1\\ne 0$であるので、$x_{0}=0$としよう。すると\n$$ \\begin{equation} y=a_{0}+a_{1}x+a_2x^2+\\cdots=\\sum \\limits_{n=0}^\\infty a_{n}x^n \\label{2} \\end{equation} $$\nべき級数解として解法を始めるが、解法の最後に実際には$y$の項が有限であることが分かる。今$\\eqref{1}$に代入するために、$y^{\\prime}$と$y^{\\prime \\prime}$を求めよう。\n$$ y^{\\prime}=a_{1}+2a_2x+3a_{3}x^2+\\cdots=\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1} $$\n$$ y^{\\prime \\prime}=2a_2+3\\cdot 2a_{3}x+4\\cdot 3 a_{4}x^2 +\\cdots = \\sum \\limits_{n=2} n(n-1)a_{n}x^{n-2} $$\n$\\eqref{1}$に$y, y^{\\prime}, y^{\\prime \\prime}$を代入すると次のようになる。\n$$ (1-x^2)\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -x\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1}+\\lambda^2 \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\n一番目の項の係数$(1-x^2)$の括弧を外して整理すると\n$$ \\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -x^2\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -x\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1}+\\lambda^2 \\sum \\limits_{n=0}^\\infty a_{n}x^n = 0 $$\n$$ \\implies \\sum \\limits_{n=2} ^\\infty n(n-1)a_{n}x^{n-2} -\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n} -\\sum \\limits_{n=1}^\\infty na_{n}x^{n}+\\lambda^2 \\sum \\limits_{n=0}^\\infty a_{n}x^n = 0 $$\nここでのポイントは$x$の次数を合わせることである。残りはすべて$x^n$として表されるが、最初の級数だけが$x^{n-2}$で表されているので、$n$の代わりに$n+2$を代入すると\n$$ \\sum \\limits_{n=0} ^\\infty (n+2)(n+1)a_{n+2}x^{n} -\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n} -\\sum \\limits_{n=1}^\\infty na_{n}x^{n}+\\lambda^2 \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\n二番目の級数が$x^2$項から始まるので、残りの級数から$n=0,1$の項を取り除いて、定数項は定数項同士、一次項は一次項同士をまとめると\n$$ \\left[ 2\\cdot 1 a_2+\\lambda^2 a_{0} \\right]+\\left[ 3\\cdot 2 a_{3}-a_{1}+\\lambda^2a_{1} \\right]x \\\\ + \\sum \\limits_{n=2}^\\infty \\left[ (n+2)(n+1)a_{n+2}-n(n-1)a_{n}-na_{n}+\\lambda^2a_{n} \\right] x^n=0 $$\n上の式が成り立つためには、すべての係数が$0$でなければならない。\n$$ 2\\cdot 1 a_2+\\lambda^2 a_{0} = 0 $$\n$$ 3\\cdot 2 a_{3}-a_{1}+\\lambda^2a_{1} =0 $$\n$$ (n+2)(n+1)a_{n+2}-n(n-1)a_{n}-na_{n}+\\lambda^2a_{n}=0 $$\nそれぞれを整理すると\n$$ \\begin{align} a_2 \u0026amp;= -\\dfrac{\\lambda^2}{2 \\cdot 1}a_{0} \\label{3} \\\\ a_{3} \u0026amp;=-\\dfrac{\\lambda^2-1^2}{3\\cdot 2} a_{1} \\label{4} \\\\ a_{n+2} \u0026amp;= -\\dfrac{\\lambda^2-n^2}{(n+2)(n+1)}a_{n} \\label{5} \\end{align} $$\n漸化式$\\eqref{5}$を得たので、$a_{0}$と$a_{1}$の値さえ分かれば、すべての係数が分かる。$\\eqref{3}, \\eqref{5}$から偶数次の項の係数を求めると\n$$ \\begin{align*} a_{4} \u0026amp;= -\\dfrac{\\lambda^2-2^2}{4\\cdot 3}a_2=\\dfrac{\\lambda^2(\\lambda^2-2^2)}{4!}a_{0} \\\\ a_{6} \u0026amp;= -\\dfrac{\\lambda^2-4^2}{6\\cdot 5}a_{4}= -\\dfrac{\\lambda^2(\\lambda^2-2^2)(\\lambda^2-4^2)}{6!}a_{0} \\\\ \u0026amp;\\vdots \\end{align*} $$\nここで$n=2m (m=1,2,3,\\cdots)$とすると\n$$ a_{n}=a_{2m}=(-1)^m \\dfrac{\\lambda^2(\\lambda^2-2^2)\\cdots(\\lambda^2-(2m-2)^2)}{(2m)!}a_{0} $$\n同様に$\\eqref{4}, \\eqref{5}$から奇数次の項の係数を求めると\n$$ \\begin{align*} a_{5} \u0026amp;= -\\dfrac{\\lambda^2-3^2}{5\\cdot 4}a_{3}=\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2)}{5!}a_{1} \\\\ a_{7} \u0026amp;= -\\dfrac{\\lambda^2-5^2}{7\\cdot 6 }a_{5}=-\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2)(\\lambda^2-5^2)}{7!}a_{1} \\\\ \u0026amp;\\vdots \\end{align*} $$\nここで$n=2m+1 (m=1,2,3,\\cdots)$とすると\n$$ a_{n}=a_{2m+1}=(-1)^m\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2) \\cdots (\\lambda^2-(2m-1)^2)}{(2m+1)!}a_{1} $$\nこのように求めた係数を$\\eqref{2}$に代入して解を求めると\n$$ \\begin{align*} y = \u0026amp;a_{0}+a_{1}x -\\dfrac{\\lambda^2}{2!}a_{0}x^2-\\dfrac{\\lambda^2-1^2}{3!} a_{1}x^3 + \\dfrac{\\lambda^2(\\lambda^2-2^2)}{4!}a_{0}x^4 \\\\ \u0026amp;+\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2)}{5!}a_{1}x^5+ \\cdots +(-1)^m \\dfrac{\\lambda^2(\\lambda^2-2^2)\\cdots(\\lambda^2-(2m-2)^2)}{(2m)!}a_{0}x^{2m} \\\\ \u0026amp;+(-1)^m\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2) \\cdots (\\lambda^2-(2m-1)^2)}{(2m+1)!}a_{1}x^{2m+1}+\\cdots\\quad(m=1,2,3,\\cdots) \\end{align*} $$\nこのとき偶数次の項は$a_{0}$で、奇数次の項は$a_{1}$でまとめると\n$$ \\begin{align*} y\u0026amp;=a_{0}\\left[1-\\dfrac{\\lambda^2}{2!}x^2+\\dfrac{\\lambda^2(\\lambda^2-2^2)}{4!}x^4+\\sum \\limits_{m=3}^\\infty (-1)^m \\dfrac{\\lambda^2(\\lambda^2-2^2)\\cdots(\\lambda^2-(2m-2)^2)}{(2m)!} x^{2m} + \\cdots \\right] \\\\ \u0026amp; + a_{1}\\left[x-\\dfrac{\\lambda^2-1^2}{3!}x^3+\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2)}{5!}x^5+\\sum \\limits_{m=3}^\\infty (-1)^m\\dfrac{(\\lambda^2-1^2)(\\lambda^2-3^2) \\cdots (\\lambda^2-(2m-1)^2)}{(2m+1)!} x^{2m+1} + \\cdots\\right] \\end{align*} $$\n最初の括弧を$y_{0}$、二番目の括弧を$y_{1}$とすると、チェビシェフ方程式の一般解は次のようになる。\n$$ y=a_{0}y_{0}+a_{1}y_{1} $$\n二つの級数$y_{0}$と$y_{1}$は比率判定法により$|x|\u0026lt;1$の範囲で収束することが分かる。$\\eqref{5}$により$\\dfrac{a_{n+2}}{a_{n}}=\\dfrac{n^2-\\lambda^2}{(n+2)(n+1)}=\\dfrac{n^2-\\lambda^2}{n^2+3n+2}$であるため比率判定法を使うと\n$$ \\lim \\limits_{n \\rightarrow \\infty} \\dfrac{n^2-\\lambda^2}{n^2+3n+2}x^2=x^2\u0026lt;1 $$\n$$ \\implies -1\u0026lt;x\u0026lt;1 $$\nしかし、多くの問題では$x=\\cos \\theta$、$\\lambda$は非負の整数の形で表され、すべての$\\theta$に対して収束する解を求めることが目標である。すなわち、$x=\\pm 1$でも収束する解を見つけることが目標である。幸いにも$\\lambda$が整数の場合、求める解が存在するが、このとき$\\lambda$の値によって必ず$y_{0}, y_{1}$のどちらか一方の解のみが存在する。$\\lambda$が$0$または偶数の場合は$y_{1}$が発散し、$y_{0}$は偶数次の項だけを持つ有限項の多項式となる。$\\lambda$が奇数の場合は$y_{0}$が発散し、$y_{1}$は奇数次の項だけを持つ有限項の多項式となる。表で整理すると以下のようになる。\n$\\lambda$の値 $y_{0}$ $y_{1}$ 方程式の解 $0$または偶数 有限項の多項式 発散 $y=a_{0}y_{0}$ 奇数 発散 有限項の多項式 $y=a_{1}y_{1}$ ケース 1. $\\lambda$が$0$または偶数の場合\n$\\lambda=0$の場合、2次項から$\\lambda^2$を因数として持っており、すべて$0$になるため$y_{0}=1$\n$\\lambda=2$の場合、4次項から$(\\lambda^2-2^2)$を因数として持っており、すべて$0$になるため$y_{0}=1-x^2$\n$\\lambda=4$の場合、6次項から$(\\lambda^2-4^2)$を因数として持っており、すべて$0$になるため$y_{0}=1-8x^2+8x^4$\nそして$\\lambda=0$の場合、$x=1$の$y_{1}=1+\\frac{1}{3!}+\\frac{1\\cdot3^2}{5!}+\\cdots$は発散する。他の偶数の場合も同じである。したがって、$\\lambda$が$0$または偶数の場合は、解が偶数次の項のみを持つ有限項の多項式となる。つまり、級数$y_{0}$の特定の項までのみ残る形の解を得る。$\\lambda$が奇数の場合は、反対の結果を得る。\nケース 2. $\\lambda$が奇数の場合\n$\\lambda=1$の場合、3次項から$(\\lambda^2-1^2)$を因数として持っており、すべて$0$になるため$y_{1}=x$\n$\\lambda=3$の場合、5次項から$(\\lambda^2-3^2)$を因数として持っており、すべて$0$になるため$y_{1}=-3x+4x^3$\n$\\lambda=5$の場合、7次項から$(\\lambda^2-5^2)$を因数として持っており、すべて$0$になるため$y_{1}=5x-20x^3+16x^5$\n$\\lambda=1$の場合、$x^2=1$の$y_{0}$は発散する。他の奇数の場合も同じである。したがって、$\\lambda$が奇数の場合は、解が奇数次の項のみを持つ有限項の多項式となる。つまり、級数$y_{1}$の特定の項までのみ残る形の解を得る。\nそして$\\lambda$が負の場合は、$\\lambda$が正の場合と同じであることが、$y_{0}$と$y_{1}$を見ると分かる。例えば、$\\lambda=2$の場合と$\\lambda=-2$の場合が同じであり、$\\lambda=1$の場合と$\\lambda=-1$の場合が同じである。したがって、$\\lambda$は非負の整数の範囲で考えればよい。$a_{0}$と$a_{1}$の値をうまく選択して、$x=1$のときの解が$y(x)=1$になるようにすれば、これをチェビシェフ多項式Chebyshev polynomialと言い、通常$T_{n}(x)$と表記される。初めのいくつかのチェビシェフ多項式は以下のようである。\n$$ \\begin{align*} T_{0}(x) \u0026amp;= 1 \\\\ T_{1}(x) \u0026amp;= x \\\\ T_2(x) \u0026amp;= 2x^2-1 \\\\ T_{3}(x) \u0026amp;= 4x^3-3x \\\\ T_{4}(x) \u0026amp;= 8x^4-8x^2+1 \\\\ T_{5}(x) \u0026amp;= 16x^5-20x^3+5x \\\\ \\vdots \u0026amp; \\end{align*} $$\n関連項目 チェビシェフ微分方程式とチェビシェフ多項式 ","id":955,"permalink":"https://freshrimpsushi.github.io/jp/posts/955/","tags":null,"title":"チェビシェフ微分方程式の直列解法"},{"categories":"정수론","contents":"ビルドアップ アリスがボブに伝えたいメッセージがあるとしよう。この世に二人しかいなければ、このメッセージは二人だけのもので、隠す必要はない。[ NOTE: 暗号理論では、アリスとボブはそれぞれ$A$と$B$を表す名前としてよく用いられる。] しかし、第三者のイヴがいるとしよう。イヴは特に悪意があるわけではないが、アリスがボブに伝えたいメッセージに興味がある。一方、アリスは自分が送るメッセージがボブだけに知られたいと願っている。[ NOTE: 暗号理論では、イヴは「盗聴者」を意味するEavesdropperの頭文字を取って$E$として表され、メッセージを盗み見するだけの受動的攻撃者の役割を持つ。] だから、アリスはイヴに知られないようにメッセージを送りたい。これは「暗号理論」のモチーフであり、誰もが共感できるものだ。\nアリスがメッセージをボブだけが理解できるように変換することを暗号化とし、ボブが変形されたメッセージを元の形に戻せるようにすることを復号という。また、暗号化される前のメッセージを平文といい、その集合を$\\mathcal{M}$、暗号化された後のメッセージを暗号文といい、その集合を$\\mathcal{C}$と表す。暗号化と復号は何らかのルールに基づいて行われるが、第三者がこのルールを知っていても復号できないようにキーを共有する。このようなキーの集合を$\\mathcal{K}$とするならば、暗号化と復号は以下のように数学的に表現できる。\n定義 1 関数$e : \\mathcal{K} \\times \\mathcal{M} \\to \\mathcal{C}$を暗号化といい、簡単に$e_{k} : \\mathcal{M} \\to \\mathcal{C}$とも表す。 関数$d : \\mathcal{K} \\times \\mathcal{C} \\to \\mathcal{M}$を復号といい、簡単に$d_{k} : \\mathcal{C} \\to \\mathcal{M}$とも表す。 ただし、$e_{k}$と$d_{k}$は互いに逆関数でなければならない。つまり、全ての$m \\in \\mathcal{M}$に対して$d_{k} \\left( e_{k} ( m ) \\right) = m$である必要がある。\nしかし、このような対応関係は非常に多く、その中で使い物になる暗号は次の条件を満たさなければならない。\n暗号システム$( \\mathcal{K} , \\mathcal{M} , \\mathcal{C} , e , d )$は次の性質を持つ時に有用である。\n(i): 全ての$k \\in \\mathcal{K}$と$m \\in \\mathcal{M}$に対して$e_{k} (m)$を計算するのが容易でなければならない。 (ii): 全ての$k \\in \\mathcal{K}$と$c \\in \\mathcal{C}$に対して$d_{k} (c)$を計算するのが容易でなければならない。 (iii): $k$を知らない場合、$c_{1} , \\cdots , c_{n} \\in \\mathcal{C}$が与えられても$d_{k} ( c_{1} ) , \\cdots , d_{k} ( c_{n} )$を計算するのが困難でなければならない。 説明 要約すると、資格のある人だけが簡単にメッセージを見られ、資格のない人は見られないような暗号システムが有用だ。メッセージを傍受する攻撃者がいなければ最も良いが、仮にいたとしても、その内容を漏らしてはならない。さらに、通信を行う当事者同士でも、暗号化と復号に時間がかかりすぎると、セキュリティが良くても通信の機能性を大きく損なってしまう。\nHoffstein. (2008). An Introduction to Mathematical Cryptography: p37.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":908,"permalink":"https://freshrimpsushi.github.io/jp/posts/908/","tags":null,"title":"暗号理論における暗号化と復号화"},{"categories":"통계적분석","contents":"定義 1 時系列データの平均と分散が時間に関して一定の時、定常性Stationarityを持つと言われる。\n説明 普通の正常ではなく、定常性定常だ。\nデータが定常性を持つというのは、平均と分散が安定しているため、分析しやすいという意味だ。データが定常性を持たない場合、分析が難しくなるので、定常性を持たせるための前処理を行う。通常、平均が一定でなければ差分を取り、分散が一定でなければ変換を行う。\n次の四つのグラフを見よう。\n扱いづらいデータ: 複雑な上下を繰り返す形を見せるだけでなく、時間が経つにつれて値が大きくなる傾向があり、その度合いも強くなっている。後の動向を予想するのは難しくないが、数式できれいに表すのは非常に難しい。 一定の平均: 固定された0を中心に徐々に広がる形なので、難しくはないが、その範囲が広がることが問題だ。 一定の分散: 各々のパターンは一定の形を持っているが、時間によって値自体が増加する傾向を説明できなければならない。 定常的なデータ: 平均と分散が一定なので、繰り返される上下だけをうまく説明すればいい。 このようにデータが定常性を持つというのは非常に良いことであり、実際には時系列分析を使用するための必須条件と言える。\nその一方で、すべての時点$t_{1} , t_{2} , \\cdots , t_{n}$およびすべての時差$k$において$Y_{t_{1}} , Y_{t_{2}} , \\cdots , Y_{t_{n}}$と$Y_{t_{1} - k} , Y_{t_{2} - k} , \\cdots , Y_{t_{n} - k}$が同じ結合分布を持つならば、確率過程$\\left\\{ Y_{t} \\right\\}$を厳密に定常的Strictly Stationaryであると言う。しかし、これは非常に理想的な条件であるため、あまり言及されない。\nコード win.graph(); par(mfrow=c(2,2))\rplot(AirPassengers,main=\u0026#39;다루기 어려운 데이터\u0026#39;)\rplot(diff(AirPassengers),main=\u0026#39;일정한 평균\u0026#39;)\rplot(log(AirPassengers),main=\u0026#39;일정한 분산\u0026#39;)\rplot(diff(log(AirPassengers)),main=\u0026#39;정상적 데이터\u0026#39;) 参照 空間過程の定常性 Cryer. (2008). Time Series Analysis: With Applications in R(2版): p16.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":907,"permalink":"https://freshrimpsushi.github.io/jp/posts/907/","tags":["R"],"title":"時系列分析における安定性"},{"categories":"통계적분석","contents":"定義 1 iid (同一分布による独立変数)の確率変数 $e_{t}$ の数列 $\\left\\{ e_{t} \\right\\}_{t = 1}^{\\infty}$ を ホワイトノイズWhite Noiseと呼ぶ。\niidは、independent identically distributed（同一分布による独立）の略で、互いに独立でありながら、同じ分布を共有していることを意味する。 説明 確率変数の数列であるという定義に従って、自然に確率過程となる。特に$E ( e_{t} ) = 0$ならば、$Y_{t} : = \\begin{cases} e_{1} \u0026amp; , t=1 \\\\ Y_{t-1} + e_{t} \u0026amp; , t \\ne 1 \\\\ \\end{cases}$として定義された確率過程$\\left\\{ Y_{t} \\right\\}_{t = 1}^{\\infty}$はランダムウォークになる。\n統計学では、観察された現象に対して100%完全な説明は不可能であると認識されている。問題が完全に説明できるのであれば、統計学を用いて解決する必要もなかったはずだ。どのようなモデルを立てても、避けられない誤差が発生し、統計学ではこれを「情報が不足していること」と受け取る。情報が多ければ多いほどいいが、宇宙の全てを知ることは不可能であり、実際に使用する際にはコストの問題も発生する。\nそういう意味で、ホワイトノイズは時系列分析で発生する「避けられない誤差」と見なされる。データは理想的に作られたものではなく、現実から得られたものなので、必ず存在する。初めは無視できるかもしれないが、時間が経つにつれて蓄積され、かなり大きくなっているかもしれない。そのため、時系列分析における予測は、遠い未来になるほど信頼区間が広がり、その意味を失っていく。\n参照 確率過程から見たランダムウォーク Cryer. (2008). Time Series Analysis: With Applications in R(第2版): p17.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":904,"permalink":"https://freshrimpsushi.github.io/jp/posts/904/","tags":null,"title":"時系列分析におけるホワイトノイズ"},{"categories":"양자역학","contents":"波動関数 波動関数wave functionは、量子力学で時間、位置に依存する粒子の運動状態を表す関数だ。通常、$u$、$\\psi$、$\\Psi$で表される。生エビ寿司店では、位置と時間に関する波動関数を$\\psi (x,t)$で表していて、時間に関係なく、位置についての波動関数は$u(x)$で表される。\n確率的解釈 波動関数で粒子の状態を理解する方法は、マックス・ボルンMax Bornの統計学的（確率的）解釈に基づいている。ここで、波動関数の大きさの二乗をある区間で積分した値に、その区間で粒子を発見する確率という意味を与える。\n$$ \\int _{a} ^b |\\psi (x,t)|^2dx \\\\[1em] = \\text{The probability that a particle exists in the interval } [a,b] \\text{ at time } t $$\nつまり、量子力学では、$\\left| \\psi (x,\\ t) \\right|^2$を時間が$t$の時、どこかの地点$x$で粒子が存在する確率密度関数として扱う。したがって、上の式は時間が$t$の時に区間$[a, b]$で粒子が存在する確率を意味する。そうすると、粒子はどこかには確かに存在するので、全体区間に対する積分値は$1$でなければならない。\n$$ \\int_{-\\infty}^{\\infty} |\\psi (x,\\ t)|^2 dx=1 $$\nこの条件は、波動関数を確率的に解釈する視点から出てきた。\n正規化 しかし、以下の式を見ると、$\\psi$がシュレディンガー方程式を満たす時、その定数倍である$a\\psi$もシュレディンガー方程式を満たすことが分かる。\n$$ H\\psi = E\\psi \\implies aH\\psi = aE\\psi \\implies H(a\\psi) = E(a\\psi) $$\n$a\\psi$に対して上の解釈を適用すると、${\\displaystyle \\int _{-\\infty}^{\\infty}} |a\\psi|^2 dx=a^2 \\ne 1$となってしまい、この値を確率と解釈できなくなる。したがって、波動関数の大きさを調整して、波動関数の全区間に対する積分値を$1$にすることで、確率的な意味を与えなければならない。これを正規化normalizationという。\n量子力学では、波動関数を扱う際には、必ず正規化を行う必要がある。例えば、ある波動関数$\\psi$に対して積分が以下のようだとする。\n$$ \\begin{equation} \\int_{-\\infty}^{\\infty} |\\psi|^2dx=9 \\end{equation} $$\nそのまま扱うのではない。両辺を$9$で割れば、${\\displaystyle \\int_{-\\infty}^{\\infty} }|\\frac{1}{3}\\psi|^2dx=1$となり、確率的な解釈が可能な形になる。ここで$\\psi$を正規化すると$\\frac{1}{3}\\psi$になり、$\\frac{1}{3}\\psi$を正規化された波動関数という。量子力学で扱う関数は、正規化された$\\frac{1}{3}\\psi$だ。\n二乗可積分 一方、波動関数の確率密度の積分値が$(1)$のように$1$でない場合も問題にならない。正規化を通じて大きさを調整すればいいからだ。問題になるのは、積分値が発散する場合だ。したがって、シュレディンガー方程式を満たす波動関数は、以下の数式を満たさなければならない。\n$$ \\int_{-\\infty}^{\\infty} |\\psi (x,\\ t)|^2dx \u0026lt;\\infty $$\nこの条件を満たす波動関数を二乗可積分なsquare-integrable関数という。二乗可積分な波動関数は$x \\rightarrow \\pm \\infty$の時、関数値が$0$に収束しなければならない。そうでない場合、波動関数のグラフの下の面積が収束しないということで、それは二乗可積分ではないということだ。\n","id":945,"permalink":"https://freshrimpsushi.github.io/jp/posts/945/","tags":null,"title":"量子力学における波動関数の確率的解釈と規格化"},{"categories":"통계적분석","contents":"説明 時系列Time Series とは、簡単に言うと、実際のデータから得られる確率過程と見ることができる。株価指数は時間が経つにつれて不確実性を持ち、その価値が変わるため、時系列の良い例となりうる。時系列分析とは、このように時間の流れに沿った従属変数の動きを理解し、予測することを目的とする分析法だ。\n回帰分析との最大の違いは、回帰分析が独立変数が互いに独立していること、そして変数自体も独立していることを前提としているのに対し、時系列分析は変数が自己相関性を持つことを前提としていることだ。また、回帰分析はデータの順序を全く気にしないが、時系列分析は前のデータが後のデータに影響を与えると見て分析にあたるため、その順序が重要だ。\n株も自己相関性の例としても良い。コスダック市場で株の額面が上がるか下がるかは分からないが、今日1株に10,000ウォンなら明日は最大で13,000ウォンまで上がり、7,000ウォンまで下がる。もちろん、これは現行法で30%p以上の変動があってはならないためだが、明日の額面価$Y_{t+1}$は間違いなく今日の額面価格$y_{t} = 10000$に依存している。このように無作為に変動しても、現在までのデータとある程度相関がある変化を捕捉することが時系列分析の目標だ。[ 注：明日の額面価はまだ分からないため確率変数として大文字で表しており、今日の額面価は既に知っているデータであるため小文字で表している。 ]\n","id":900,"permalink":"https://freshrimpsushi.github.io/jp/posts/900/","tags":null,"title":"時系列分析"},{"categories":"수리물리","contents":"まとめ1 $\\mathbf{v}, \\mathcal{S}$をそれぞれ3次元空間におけるあるベクトル、面積だとしよう。$\\mathcal{S}$の面積ベクトルを$d\\mathbf{a}$、$\\mathcal{S}$の境界を$\\mathcal{P}$、$\\mathcal{P}$に沿って動く経路を$d\\mathbf{l}$とする。すると、次の式が成立する。\n$$ \\int_{\\mathcal{S}} (\\nabla \\times \\mathbf{v} )\\cdot d\\mathbf{a} = \\oint_{\\mathcal{P}} \\mathbf{v} \\cdot d\\mathbf{l} $$\nこれはストークスの定理あるいは回転の基本定理と呼ばれる。\nちなみに、定理名に使われる場合を除いて、生さえび寿司屋においては、\u0026lsquo;回転\u0026rsquo;と単独で使われる場合は\u0026lsquo;カール\u0026rsquo;としている。\n説明 定理の式を解いて説明すると、次のようになる。\nある領域$\\mathcal{S}$内でのベクトル$\\mathbf{v}$が回転する量の総和（左辺）は、その領域の境界$\\mathcal{P}$を沿ってベクトル$\\mathbf{v}$の値を全て足したもの（右辺）と同じである。 物理を学ぶ人にとっては、ストークスの定理の証明よりも、どのような意味を持っているかを把握する方がはるかに重要である。\n$d\\mathbf{l}$は閉曲線なので、どの点から始めて、どの方向から始めても、結果には影響しない。したがって、面積ベクトル$d\\mathbf{a}$の方向は右手の法則で決定される。\n面積内での回転の総和と経路を辿る量の総和が同じであるというのは、図を見ずには理解しにくい。下の図を見よう。\n積分値は唯一$\\mathcal{S}$の境界$\\mathcal{P}$によってのみ決定される ストークスの定理は等式であるため、$\\mathcal{S}$の形がどうであれ、境界が$\\mathcal{P}$で同じ曲面であれば、積分値は常に同じだ。つまり曲面がどう見えるかは関係ない。下の図のように、いくつかの曲面があっても、そのエッジが同じであれば、$\\int (\\nabla \\times \\mathbf{v})\\cdot d\\mathbf{a}$は同じ値を持つ。したがって、積分値は$\\mathcal{S}$の境界によって決定される。\n閉じた曲面$\\mathcal{S}$の積分値は常に$0$である 閉じた曲面の場合、境界の長さが$0$であるため、経路の長さが$0$で、右辺の閉曲線積分は常に$0$である。したがって、次の結果を得る。\n$$ \\int_{\\mathcal{S}} (\\nabla \\times \\mathbf{v} )\\cdot d\\mathbf{a} = 0 = \\oint_{\\mathcal{P}} \\mathbf{v} \\cdot d\\mathbf{l} $$\n閉じた曲面の境界の長さが$0$であることが理解しにくい場合は、下の図を見よ。\nDavid J. Griffiths, 기초전자기학(Introduction to Electrodynamics, 김진승 역)(4th Edition). 2014, p37-38\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":937,"permalink":"https://freshrimpsushi.github.io/jp/posts/937/","tags":null,"title":"ストークスの定理"},{"categories":"전자기학","contents":"定義1 定常電流steady currentとは、量や進行方向が変わらずに絶え間なく続く電荷の流れを指す。\n説明 時間によって電流が変わらないため、定常電流によって作られる磁場も時間とともに変わらない。ここで言う「進行方向」とは、一般的に考えるベクトルの方向とは異なる概念だ。曲がった導線を流れても、一方向にだけ続けて流れるならば、進行方向は変わらないということだ。体積電荷密度を$\\rho$、体積電流密度を$\\mathbf{J}$としよう。そして、これによって生じる電流が定常電流であれば、定義により以下の式が成り立つ。\n$$ \\dfrac{\\partial \\rho}{\\partial t} = 0 \\quad \\text{and} \\quad \\dfrac{\\partial \\mathbf{J}}{\\partial t}=0 $$\nしたがって、連続方程式により以下の式が成立する。\n$$ \\nabla \\cdot \\mathbf{J} = 0 $$\nもちろん、定常電流は理論上のもので、実際には存在しないので、定常電流に関する内容は完全に理論的な話である。しかし、物理学の多くの場所で、このような理論は現実とかなり近似する。\n公式 定常電流が作る磁場は次の式で計算でき、これをビオ・サバールの法則Biot-Savart lawという。\n$$ \\mathbf{B}(\\mathbf{r})=\\dfrac{ \\mu_{0}}{4\\pi}\\int \\dfrac{\\mathbf{I} \\times \\crH}{\\cR ^2}dl^{\\prime}=\\dfrac{ \\mu_{0}}{4\\pi} I \\int \\dfrac{d \\mathbf{l}^{\\prime} \\times \\crH}{\\cR ^2} $$\nここで、$\\bcR$は分離ベクトル、定数$\\mu$は透磁率permeabilityである。$\\mu_{0}$は真空中の透磁率である。面電流、体積電流に対するビオ・サバールの法則は表面電流密度と体積電流密度を使って表される。\n$$ \\begin{align*} \\mathbf{B}(\\mathbf{r}) =\u0026amp;\\ \\dfrac{ \\mu_{0}}{4\\pi}\\int \\dfrac{\\mathbf{K}(\\mathbf{r}^{\\prime}) \\times \\crH}{\\cR ^2}da^{\\prime} \\\\ \\mathbf{B}(\\mathbf{r}) =\u0026amp;\\ \\dfrac{ \\mu_{0}}{4\\pi}\\int \\dfrac{\\mathbf{J}(\\mathbf{r}^{\\prime}) \\times \\crH}{\\cR ^2}d\\tau^{\\prime} \\end{align*} $$\n例 定常電流$I$が流れる電線から垂直距離が$s$の地点の磁場を求めよ。\n$$ \\begin{align*} |d\\mathbf{l}^{\\prime} \\times \\crH | =\u0026amp;\\ |d\\mathbf{l}^{\\prime}||\\crH|\\sin \\alpha \\\\ =\u0026amp;\\ dl^{\\prime} \\sin \\alpha \\\\ =\u0026amp;\\ dl^{\\prime} \\sin \\left( \\theta + \\frac{\\pi}{2} \\right) \\\\ =\u0026amp;\\ dl^{\\prime} \\cos \\theta \\end{align*} $$\n$l^{\\prime}=s\\tan \\theta$なので、\n$$ dl^{\\prime}=\\dfrac{s}{\\cos ^2 \\theta}d\\theta $$\n$s=\\cR \\cos \\theta$なので、\n$$ \\dfrac{1}{\\cR ^2}=\\dfrac{\\cos ^2 \\theta}{s^2} $$\nビオ・サバールの法則に代入して、$\\mathbf{B}(\\mathbf{r})$の大きさを計算すると\n$$ \\begin{align*} B =\u0026amp;\\ \\left| \\dfrac{ \\mu_{0}}{4\\pi} I \\int \\dfrac{d \\mathbf{l}^{\\prime} \\times \\crH}{\\cR ^2} \\right| \\\\ =\u0026amp;\\ \\dfrac{ \\mu_{0}}{4\\pi} I \\int \\dfrac{ \\left| d \\mathbf{l}^{\\prime} \\times \\crH \\right| }{\\cR ^2} \\\\ =\u0026amp;\\ \\dfrac{\\mu_{0} I}{4\\pi} \\int \\left( \\dfrac{\\cos ^2 \\theta}{s^2} \\right) \\left( \\dfrac{s}{\\cos^2\\theta} \\right) \\cos \\theta d\\theta \\\\ =\u0026amp;\\ \\dfrac{\\mu_{0} I}{4\\pi s} \\int \\cos \\theta d\\theta \\end{align*} $$ この時、図$(2)$のような電線の断片に関する場合だったら、積分範囲は$\\theta _{1}$から$\\theta_2$までである。例は無限に長い電線に関する場合なので、図$(2)$のように、$\\theta_{1}=-\\dfrac{\\pi}{2}$、$\\theta_2=\\dfrac{\\pi}{2}$の状況と同じである。したがって磁場の大きさは $$ \\begin{align*} B =\u0026amp;\\ \\dfrac{\\mu_{0} I}{4\\pi s} \\int_{-\\frac{\\pi}{2} }^{\\frac{\\pi}{2}} \\cos \\theta d\\theta \\\\ =\u0026amp;\\ \\dfrac{\\mu_{0} I}{4\\pi s} \\left(\\sin {\\textstyle \\frac{\\pi}{2}}- \\sin {\\textstyle \\frac{-\\pi}{2}} \\right) \\\\ =\u0026amp;\\ \\dfrac{\\mu_{0} I}{2\\pi s} \\end{align*} $$\n方向は右手の法則によって、紙を突き抜ける方向である。右側を円筒座標系の$\\hat{\\mathbf{z}}$とすると、\n$$ \\mathbf{B}=\\dfrac{\\mu_{0} I}{2\\pi s} \\hat{\\boldsymbol{\\phi}} $$\n■\nDavid J. Griffiths, 電磁気学の基礎(Introduction to Electrodynamics, 金甚生 訳) (第4版, 2014), 241-245ページ\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":899,"permalink":"https://freshrimpsushi.github.io/jp/posts/899/","tags":null,"title":"定常電流とビオ・サバールの法則"},{"categories":"전자기학","contents":"説明1 静電学では、$\\nabla \\times \\mathbf{E} = \\mathbf{0}$という性質を使って、スカラーポテンシャル $V$を定義する。同様に、磁気静学では、$\\nabla \\cdot \\mathbf{B} = 0$という性質を利用してベクトルポテンシャル $A$を定義して使う。磁場 $\\mathbf{B}$をあるベクトル $\\mathbf{A}$の回転としよう。\n$$ \\mathbf{B}=\\nabla \\times \\mathbf{A} $$\nすると、回転の発散は0だから、自然に次の式が成り立つ。\n$$ \\nabla \\cdot \\mathbf{B} = \\nabla \\cdot (\\nabla \\times \\mathbf{A}) = 0 $$\nしたがって、カールを取ったときに磁場になるベクトル $\\mathbf{A}$を磁場のベクトルポテンシャルと定義する。電場のスカラーポテンシャルを扱う時の要点は、ポテンシャル自体の値ではなく、ポテンシャルの差が重要だった。そこで、定数 $K$の差は、電場を扱う上で影響を与えなかった。同様に、ベクトルポテンシャル $\\mathbf{A}$を発散が$0$になるようなベクトルとして定めることができる。発散が$0$でないベクトルでも構わないが$\\nabla \\cdot \\mathbf{A}=0$を満たす時、式が最も綺麗になる。アンペールの法則の微分形にベクトルポテンシャル $\\mathbf{A}$を代入してみると、次の式を得る。\n$$ \\nabla \\times \\mathbf{B}=\\nabla \\times (\\nabla \\times \\mathbf{A} ) = \\nabla(\\nabla \\cdot \\mathbf{A})-\\nabla ^2 \\mathbf{A} = \\mu_{0} \\mathbf{J} $$\nを参照)$\\nabla \\cdot \\mathbf{A}=0$であれば、アンペールの法則は綺麗に以下のようになる。\n$$ \\begin{equation} \\nabla ^2 \\mathbf{A}=-\\mu_{0} \\mathbf{J} \\label{1} \\end{equation} $$\nなぜ、自由に$\\mathbf{A}$を発散が$0$になる関数として設定してもいいのか確認しよう。発散が$0$でないポテンシャルを$\\mathbf{A}_{0}$としよう。ここに任意のスカラー$\\lambda$の勾配を加えたものを$\\mathbf{A}$としよう。\n$$ \\mathbf{A}=\\mathbf{A}_{0} + \\nabla \\lambda $$\n両辺にカールを取ると、勾配のカールは$\\mathbf{0}$だから、\n$$ \\nabla \\times \\mathbf{A} = \\nabla \\times \\mathbf{A}_{0} + \\nabla \\times (\\nabla\\lambda)=\\nabla \\times \\mathbf{A}_{0} $$\nしたがって、二つのベクトル$\\mathbf{A}, \\mathbf{A}_{0}$のカールは同じで、次が成り立つ。\n$$ \\mathbf{B}=\\nabla \\times \\mathbf{A} = \\nabla \\times \\mathbf{A}_{0} $$\nそれゆえ、ベクトルポテンシャルに任意のスカラーの勾配を足すことは、磁場を表現する上で何の影響も与えない。二つのベクトルポテンシャルに発散を取ると、\n$$ \\nabla \\cdot \\mathbf{A} = \\nabla \\cdot \\mathbf{A}_{0} + \\nabla^2 \\lambda $$\nそこで、$\\nabla ^2 \\lambda=-\\nabla \\cdot \\mathbf{A}_{0}$を満たす$\\lambda$を選べば、ベクトルポテンシャル$\\mathbf{A}$の発散を$0$にすることができる。もし、遠くの地点で$\\nabla \\cdot \\mathbf{A}_{0}=0$が成立すれば、次の式を得る。\n$$ \\lambda=\\dfrac{1}{4 \\pi}\\int \\dfrac{\\nabla \\cdot \\mathbf{A}_{0} } {\\cR} d\\tau^{\\prime} $$\n$(1)$を解いて$\\mathbf{A}$を直接求めると（遠くの地点で$\\mathbf{J}=0$の時）\n$$ \\mathbf{A}(\\mathbf{r})=\\dfrac{\\mu_{0}}{4\\pi} \\int \\dfrac{\\mathbf{J} (\\mathbf{r}^{\\prime}) }{\\cR} d\\tau^{\\prime} $$\n式を見るとわかるが、電流の方向が一定であれば、ベクトルポテンシャルと電流の方向が同じになる。線電流と面電流に対するベクトルポテンシャルは、\n$$ \\mathbf{A}=\\dfrac{\\mu_{0}}{4\\pi} \\int \\dfrac{\\mathbf{I} } {\\cR} dl^{\\prime}=\\dfrac{\\mu_{0} I}{4\\pi} \\int \\dfrac{1}{\\cR} d\\mathbf{l}^{\\prime} $$\n$$ \\mathbf{A}=\\dfrac{\\mu_{0}}{4\\pi}\\int \\dfrac{K}{\\cR} da^{\\prime} $$\nDavid J. Griffiths, 基礎電磁気学(Introduction to Electrodynamics, 金甚生 訳) (4th Edition, 2014), p262-263\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":923,"permalink":"https://freshrimpsushi.github.io/jp/posts/923/","tags":null,"title":"磁場のベクトルポテンシャル"},{"categories":"전자기학","contents":"定義1 動いている電荷（電流）は周りに磁場magnetic field $\\mathbf{B}$を作る。磁場 $\\mathbf{B}$ の中で $\\mathbf{v}$ の速さで動いている電荷 $Q$ が受ける力は次の通りだ。\n$$ \\begin{equation} \\mathbf{F}_{m}=Q(\\mathbf{v} \\times \\mathbf{B}) \\end{equation} $$\nこの力を磁力magnetic forceといい、上の式をローレンツ力の法則Lorentz force lawという。\n説明 電場 の定義と同じように、動いている電荷が電流の周りで$(1)$という力を受けるとき $\\mathbf{B}$を電流が作る磁場という。\nローレンツ力の法則は、実は ヘヴィサイドOliver Heavisideが見つけたと言われている。電場がある時、クーロンの法則によって受ける電気力を足した一般的な式は次の通りだ。\n$$ \\mathbf{F}=Q\\left[ \\mathbf{E} + (\\mathbf{v}\\times\\mathbf{B}) \\right] $$\nこれは電磁気学の根本原理であり、実験法則である。\nDavid J. Griffiths, 基礎電磁気学(Introduction to Electrodynamics, 김진승 訳) (4th Edition, 2014), p227-229\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":896,"permalink":"https://freshrimpsushi.github.io/jp/posts/896/","tags":null,"title":"磁気とローレンツ力の法則"},{"categories":"전자기학","contents":"定義1 導線のどこかの点を単位時間ごとに通過する電荷の量を電流currentと定義し、$I$と表記する。それゆえ、左に動く負の電荷と右に動く正の電荷は同じ符号の電流である。\n単位時間あたりに流れるクーロンの量をアンペアampereと言う。\n$$ 1 [A] = 1 [C/s] $$\n説明 アンペールはフランス人で、実際の発音は[アンペール]に近い。だからアンペールの法則もアンペールの法則だが、単位として使う場合はアンペアと言わなければならない。\n$I$という記号は、currentのintensityの最初の文字を取ったものである。\n線電流密度 上の図は、線電荷密度が$\\lambda$である電荷が導線を$\\mathbf{v}$の速度で移動する状況を示している。距離=速さx時間であるから、単位長さは$v\\Delta t$である。単位長さに含まれる電荷量は、単位長さと線電荷密度をかけて求める。\n$$ \\Delta q=\\lambda v \\Delta t $$\n電流は単位時間あたりに通過する電荷の量なので、$\\Delta t$間に点$P$を通過する電荷量は、\n$$ I=\\dfrac{\\Delta q}{\\Delta t}=\\dfrac{\\lambda v \\Delta t}{\\Delta t}=\\lambda v $$\n電流はベクトルなので、方向まで含めて表記すると、次のようになる。\n$$ \\mathbf{I}=\\lambda \\mathbf{v} $$\n電流が導線を通って流れる際には、その方向が明らかである（導線と平行な方向である）ため、別に言及する必要はない。しかし、表面上や体積内で流れる電流を扱う場合には、その方向をはっきりと言う必要がある。電流が流れる導線が外部磁場$\\mathbf{B}$によって受ける磁力は、\n$$ \\mathbf{F}_{\\text{mag}}=\\int (\\mathbf{v} \\times \\mathbf{B} ) dq=\\int (\\mathbf{v} \\times \\mathbf{B} ) \\lambda dl=\\int (\\mathbf{I} \\times \\mathbf{B}) dl $$\nここで、$\\mathbf{I}$と$d\\mathbf{l}$の方向が同じであるから、\n$$ \\mathbf{F}_{\\text{mag}} = \\int I (d\\mathbf{l} \\times \\mathbf{B}) $$\n導線で流れる電流の大きさが一定であるため、積分の外に出すことができる、\n$$ \\mathbf{F}_{\\text{mag}}=I \\int (d\\mathbf{l} \\times \\mathbf{B}) $$\n表面電流密度 表面で流れる電流は、表面電流密度surface current density $\\mathbf{K}$で説明される。単位長さの幅を通過する電流を表面電流密度と言い、数式では次のように表される。\n$$ \\mathbf{K}=\\dfrac{d \\mathbf{l}} {dl_\\perp} $$\nこの概念をもっと簡単に理解するための説明は、$\\mathbf{I}=\\dfrac{d\\mathbf{q} }{dt}$なので、\n$$ \\dfrac{d \\mathbf{I} }{dl_{\\perp}}=\\dfrac{d^2 \\mathbf{q}}{dl_{\\perp} dt} $$\nしたがって、表面電流密度は単位時間あたり、単位幅あたりに通過する電荷の量である。表面電荷密度が$\\sigma$、電荷の速度が$\\mathbf{v}$の時、表面電流密度は、\n$$ \\mathbf{K}=\\sigma \\mathbf{v} $$\n表面電流が外部磁場によって受ける磁力は、\n$$ \\mathbf{F}_{\\text{mag}}=\\int(\\mathbf{v}\\times \\mathbf{B})\\sigma da=\\int (\\mathbf{K} \\times \\mathbf{B})da $$\n上で見た電流の式から、電流$\\mathbf{I}$の代わりに表面電流密度$\\mathbf{K}$を入れた形である。\n体積電流密度 同様に、電流がある空間で流れる場合は、体積電流密度volume current density $\\mathbf{J}$で説明される。単位面積あたりに流れる電流を体積電流密度と言い、数式では次のように表される。\n$$ \\mathbf{J}=\\dfrac {d\\mathbf{I}} {da_{\\perp}} $$\nしたがって、逆に面$\\mathcal{S}$を通る電流$I$は、一般的に次のように表すことができる。\n$$ I = \\int_{\\mathcal{S}}J da_{\\perp} = \\int_{\\mathcal{S}}\\mathbf{J}\\cdot d\\mathbf{a} $$\nすると、発散定理によって、体積$\\mathcal{V}$から出て行った総電荷量は、次のようになる。\n$$ \\oint_{\\mathcal{S}}\\mathbf{J}\\cdot d\\mathbf{a} = \\int_{\\mathcal{V}} (\\nabla \\cdot \\mathbf{J}) d \\tau $$\n同様に、$\\dfrac {d\\mathbf{I}} {da_{\\perp}}=\\dfrac{d^2 \\mathbf{q} } {da_{\\perp}{dt}}$だから、体積電流密度は単位時間あたり、単位面積あたりに通過する電荷の量である。体積電荷密度が$\\rho$で、電荷の速度が$\\mathbf{v}$の場合、体積電流密度は、\n$$ \\mathbf{J}=\\rho \\mathbf{v} $$\n体積電流が受ける磁力は、\n$$ \\mathbf{F}_{\\text{mag}}=\\int (\\mathbf{v} \\times \\mathbf{B} )\\rho d\\tau = \\int (\\mathbf{J} \\times \\mathbf{B} ) d\\tau $$\nDavid J. Griffiths, 基礎電磁気学(Introduction to Electrodynamics, 金甚成 訳) (第4版, 2014), p234-241\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":898,"permalink":"https://freshrimpsushi.github.io/jp/posts/898/","tags":null,"title":"電流と電流密度"},{"categories":"전자기학","contents":"多重展開1 離れた場所から見た時、集まった電荷分布は点電荷のように見えるだろう。つまり、電荷分布の総電荷が$Q$だとしたら、非常に遠い場所から見ると、電荷量が$Q$の点電荷が一つあるかのように感じるだろう。これは、電圧を$\\dfrac{1}{4\\pi\\epsilon_{0}} \\dfrac{Q}{r}$と見積もっても大体合ってるということになる。\nしかし、総電荷が$0$の場合、電圧を$0$で近似するのが正しいかという疑問が生じる。多重展開は、総電荷が$0$の場合に、どのようにして電圧を近似式で表現するかという解答だ。電圧$V(\\mathbf{r})$を$\\dfrac{1}{r^{n}}$に関する級数で表現することを多重展開という。\n位置$\\mathbf{r}$における電圧は以下の通り。\n$$ \\begin{equation} V(\\mathbf{r}) = \\dfrac{1}{4\\pi\\epsilon_{0}} \\int \\dfrac{1}{\\cR}\\rho (\\mathbf{r}^{\\prime})d\\tau^{\\prime} \\label{1} \\end{equation} $$\nここで$\\bcR = \\mathbf{r} - \\mathbf{r}^{\\prime} (\\cR = \\left| \\bcR \\right|)$は分離ベクトルだ。\n$\\cR$の大きさを求めるために二つのコサイン法則を用いると、\n$$ \\begin{align*} \\cR ^2 =\u0026amp;\\ r^2+(r^{\\prime})^2-2rr^{\\prime}\\cos\\alpha \\\\ =\u0026amp;\\ r^2\\left[1+\\left(\\dfrac{r^{\\prime}}{r}\\right)^2-2\\dfrac{r^{\\prime}}{r}\\cos\\alpha\\right] \\\\ =\u0026amp;\\ r^2\\left[1+\\dfrac{r^{\\prime}}{r}\\left( \\dfrac{r^{\\prime}}{r}-2\\cos\\alpha\\right)\\right] \\end{align*} $$\n便宜上、角かっこ内の第二項を全体として$\\epsilon=\\dfrac{r^{\\prime}}{r}\\left( \\dfrac{r^{\\prime}}{r}-2\\cos\\alpha \\right)$と置換する。それから次が成り立つ。\n$$ \\cR=r\\sqrt{1+\\epsilon} \\implies \\dfrac{1}{\\cR} = \\dfrac{1}{r}(1+\\epsilon)^{-1/2} $$\nこの時、$\\mathbf{r}$が電荷分布から非常に遠い場所であれば、$\\dfrac{r^{\\prime}}{r}$は非常に小さくなり、$\\epsilon \\ll 1$が成り立つ。\n二項級数\n$|x| \u0026lt; 1$ならば、\n$$ (1 + x )^{\\alpha} = 1 + \\alpha x + \\dfrac{\\alpha (\\alpha-1)}{2!}x^{2} + \\dfrac{\\alpha (\\alpha-1)(\\alpha-2)}{3!}x^{3} + \\cdots $$\n従って、$(1+\\epsilon)^{-1/2}$を二項級数で解くことができる。\n$$ \\dfrac{1}{\\cR} = \\dfrac{1}{r}(1+\\epsilon)^{-1/2} = \\dfrac{1}{r}\\left( 1- \\dfrac{1}{2}\\epsilon+\\dfrac{3}{8}\\epsilon ^2 -\\dfrac{5}{16}\\epsilon ^3 +\\cdots \\right) $$\n$\\epsilon$を元に戻して書くと、\n$$ \\dfrac{1}{\\cR}=\\dfrac{1}{r}\\left[ 1- \\dfrac{1}{2}\\dfrac{r^{\\prime}}{r}\\left( \\dfrac{r^{\\prime}}{r}-2\\cos\\alpha \\right) +\\dfrac{3}{8}\\left( \\dfrac{r^{\\prime}}{r} \\right)^2 \\left( \\dfrac{r^{\\prime}}{r}-2\\cos\\alpha \\right) ^2 -\\dfrac{5}{16}\\left( \\dfrac{r^{\\prime}}{r}\\right)^3 \\left( \\dfrac{r^{\\prime}}{r}-2\\cos\\alpha \\right) ^3 +\\cdots \\right] $$\nこれを$\\dfrac{r^{\\prime}}{r}$の各次数に合わせて整理すると、以下のようになる。詳細なプロセスは付録を参照してほしい。\n$$ \\dfrac{1}{\\cR}=\\dfrac{1}{r}\\left[ 1+\\left(\\dfrac{r^{\\prime}}{r}\\right)\\left( \\cos\\alpha \\right) +\\left( \\dfrac{r^{\\prime}}{r} \\right)^2 \\left( \\dfrac{3\\cos^2\\alpha -1 }{2}\\right) +\\left( \\dfrac{r^{\\prime}}{r}\\right)^3 \\left( \\dfrac{5\\cos^2\\alpha-3\\cos\\alpha}{2} \\right) +\\cdots \\right] $$\nここで、各かっこ内は級数$\\sum \\limits_{n=0}^{\\infty} a_{n}\\left( \\dfrac{r^{\\prime}}{r}\\right)^n$形で表現できる。この時、それぞれの係数$a_{n}$は以下の通り。\n$$ \\begin{align*} a_{0} =\u0026amp;\\ 1 \\\\ a_{1} =\u0026amp;\\ \\cos\\alpha \\\\ a_{2} =\u0026amp;\\ \\dfrac{3\\cos^2\\alpha-1}{2} \\\\ a_{3} =\u0026amp;\\ \\left( \\dfrac{5\\cos^2\\alpha-3\\cos\\alpha}{2} \\right) \\\\ \\vdots \u0026amp; \\end{align*} $$\nこれは$\\cos\\alpha$に関するルジャンドル多項式$P_{n}(\\cos \\alpha)$と同じだ。従って、整理すると、\n$$ \\dfrac{1}{\\cR}=\\dfrac{1}{r}\\sum\\limits_{n=0}^{\\infty}\\left( \\dfrac{r^{\\prime}}{r}\\right)^n P_{n}(\\cos\\alpha) $$\nこれを電圧公式$\\eqref{1}$に代入し、積分と無関係な$r$を外に出すと、次を得る。\n$$ V(\\mathbf{r})=\\dfrac{1}{4\\pi\\epsilon_{0}}\\sum \\limits_{n=0}^{\\infty} \\dfrac{1}{r^{n+1}} \\int (r^{\\prime})^nP_{n}(\\cos\\alpha) \\rho (\\mathbf{r}^{\\prime}) d\\tau^{\\prime} $$\nこの級数を再展開すると、\n$$ \\begin{align*} V(\\mathbf{r}) = \\dfrac{1}{4\\pi\\epsilon_{0}} \\bigg[ \u0026amp;\\dfrac{1}{r} \\int r^{\\prime}\\cos\\alpha \\rho (\\mathbf{r}^{\\prime})d\\tau^{\\prime} \\\\ \u0026amp;+ \\dfrac{1}{r^2}\\int r^{\\prime}\\cos\\alpha \\rho (\\mathbf{r}^{\\prime})d\\tau^{\\prime} + \\dfrac{1}{r^3}\\int(r^{\\prime})^2\\dfrac{3\\cos^2\\alpha -1 }{2}\\rho (\\mathbf{r}^{\\prime})d\\tau^{\\prime} + \\cdots \\bigg] \\end{align*} $$\n第一項は単極によって生じる電圧、第二項は双極子による電圧、第三項は四極子による電圧だ。$n$項はそれぞれ$2^{n-1}$極子項と関連している。\n双極子項と双極子モーメント 多重展開式は$r$の逆数に関する級数なので、通常$r$が大きいときは単極項が最も大きくなる。monoは単極monopole, モノポールの略だ。\n$$ V_{\\text{mono}}(\\mathbf{r}) = \\dfrac{1}{4\\pi\\epsilon_{0}}\\dfrac{Q}{r} $$\nもし集まった電荷の総電荷が$0$ならば、単極項は$0$だ。それ以外の場合は、$+$と$-$が$0$になるように組み合わせることができるが、単極項は一つしかできないので、それは不可能だ。従って、この時双極子項が$0$でなければ、双極子項が最も大きくなる。dipは双極子dipole, ダイポールの略だ。\n$$ V_{\\text{dip}}(\\mathbf{r})=\\dfrac{1}{4\\pi\\epsilon_{0}}\\dfrac{1}{r^2}\\int r^{\\prime} \\cos \\alpha \\rho (\\mathbf{r}^{\\prime})d\\tau^{\\prime} $$\nここで、$\\hat{\\mathbf{r}}\\cdot\\mathbf{r}^{\\prime}=r^{\\prime}\\cos\\alpha$であるので、\n$$ V_{\\text{dip}}(\\mathbf{r})=\\dfrac{1}{4\\pi\\epsilon_{0}}\\dfrac{1}{r^2}\\hat{\\mathbf{r}}\\cdot\\int \\mathbf{r}^{\\prime} \\rho (\\mathbf{r}^{\\prime})d\\tau^{\\prime} $$\nこの積分値は$\\mathbf{r}$と無関係で、特に電荷分布の双極子モーメントdipole momentと呼ばれ、$\\mathbf{p}$と表される。\n$$ \\mathbf{p}=\\int\\mathbf{r}^{\\prime}\\rho (\\mathbf{r}^{\\prime})d\\tau^{\\prime} $$\n双極子モーメントを使って双極子の電圧を簡潔に表すことができる。\n$$ V_{\\text{dip}}(\\mathbf{r})=\\dfrac{1}{4\\pi\\epsilon_{0}}\\dfrac{\\mathbf{p}\\cdot\\hat{\\mathbf{r}} } {r^2} $$\n付録 $$ 1- \\dfrac{1}{2}\\dfrac{r^{\\prime}}{r}\\left( \\dfrac{r^{\\prime}}{r}-2\\cos\\alpha \\right) +\\dfrac{3}{8}\\left( \\dfrac{r^{\\prime}}{r} \\right)^{2} \\left( \\dfrac{r^{\\prime}}{r}-2\\cos\\alpha \\right)^2 -\\dfrac{5}{16}\\left( \\dfrac{r^{\\prime}}{r}\\right)^3 \\left( \\dfrac{r^{\\prime}}{r}-2\\cos\\alpha \\right)^3 +\\cdots $$\n上の式の二乗項、三乗項を展開すると、\n$$ 1- \\dfrac{1}{2}\\dfrac{r^{\\prime}}{r}\\left( \\dfrac{r^{\\prime}}{r}-2\\cos\\alpha \\right) +\\dfrac{3}{8}\\left( \\dfrac{r^{\\prime}}{r} \\right)^2 \\left[ \\left( \\dfrac{r^{\\prime}}{r}\\right)^2-\\dfrac{4r^{\\prime}\\cos\\alpha}{r}+4\\cos^2\\alpha \\right] $$\n$$ -\\dfrac{5}{16}\\left( \\dfrac{r^{\\prime}}{r}\\right)^3 \\left[ \\left( \\dfrac{r^{\\prime}}{r}\\right)^3 -3\\left(\\dfrac{r^{\\prime}}{r}\\right)^22\\cos\\alpha + 3\\left( \\dfrac{r^{\\prime}}{r}\\right)4\\cos^2\\alpha-8\\cos^3\\alpha \\right] +\\cdots $$\nこれで、$\\dfrac{r^{\\prime}}{r}$の次数に合わせて整理すると、\n$$ 1+\\left(\\dfrac{r^{\\prime}}{r}\\right)\\cos\\alpha +\\left(\\dfrac{r^{\\prime}}{r}\\right)^2\\left(-\\dfrac{1}{2} +\\dfrac{3}{2}\\cos^2\\alpha \\right)+\\left( \\dfrac{r^{\\prime}}{r} \\right)^3 \\left( -\\dfrac{3}{2}\\cos\\alpha +\\dfrac{5}{2}\\cos^3\\alpha \\right) + \\cdots $$\nこれを整理すると、\n$$ 1+\\left(\\dfrac{r^{\\prime}}{r}\\right)\\left( \\cos\\alpha \\right) +\\left( \\dfrac{r^{\\prime}}{r} \\right)^2 \\left( \\dfrac{3\\cos^2\\alpha -1 }{2}\\right) +\\left( \\dfrac{r^{\\prime}}{r}\\right)^3 \\left( \\dfrac{5\\cos^2\\alpha-3\\cos\\alpha}{2} \\right) +\\cdots $$\nDavid J. Griffiths, 기초전자기학(Introduction to Electrodynamics, 김진승 역) (4th Edition, 2014), p161-167\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":936,"permalink":"https://freshrimpsushi.github.io/jp/posts/936/","tags":null,"title":"ポテンシャルの多重極展開と双極子モーメント"},{"categories":"푸리에해석","contents":"定義 ディリクレカーネルDirichlet kernel $D_{n}$は次のように定義される。\n$$ \\begin{equation} D_{n}(t) := \\dfrac{1}{2}+\\sum \\limits_{k=1}^{n} \\cos kt \\end{equation} $$\n説明 ディリクレカーネルは、デルタ関数、指数関数等と関連しており、フーリエ解析に登場する。関連するいくつかの定理と証明を紹介する。\n定理1 ディリクレカーネルは下の式を満たす。\n$$ D_{n}(t)=\\dfrac{\\sin\\left(n+\\frac{1}{2}\\right) t}{2\\sin \\frac{1}{2}t} $$\n証明 コサイン関数を複素指数関数形で表現すると次のようになる。\n$$ \\begin{align*} D_{n}(t) =\u0026amp;\\ \\dfrac{1}{2}+\\dfrac{1}{2}\\sum \\limits_{k=1}^n( e^{ikt}+e^{-ikt} ) \\\\ =\u0026amp;\\ \\dfrac{1}{2} \\left[ 1+\\sum \\limits_{k=1}^{n} (e^{ikt}+e^{-ikt} ) \\right] \\\\ =\u0026amp;\\ \\dfrac{1}{2} \\sum \\limits_{k=-n}^{n} e^{ikt} \\end{align*} $$\nこの場合、\n等比数列の和の公式\n$$ \\sum_{k=1}^{n} a_{k}= \\dfrac{a (r^{n} -1) }{ r-1 } $$\nを使用すると、初項が$a_{1}=e^{-int}$で、公比が$r=e^{it}$なので、次のように整理することができる。\n$$ \\begin{align*} D_{n}(t) =\u0026amp;\\ \\dfrac{1}{2} \\sum \\limits_{k=-n}^{n} e^{ikt} = \\dfrac{1}{2} \\sum \\limits_{k=1}^{2n+1} e^{i(k-n-1)t} \\\\ =\u0026amp;\\ \\dfrac{1}{2} \\dfrac{ ( e^{-int} ) \\left( e^{i(2n+1)t -1} \\right) }{e^{it}-1} \\\\ =\u0026amp;\\ \\dfrac{1}{2}e^{-int}\\dfrac{e^{i(n+\\frac{1}{2}) t }-e^{-i(n+\\frac{1}{2})t} }{e^{i\\frac{1}{2}t}-e^{-i\\frac{1}{2}t }} \\dfrac{e^{i(n+\\frac{1}{2})t}} {e^{i\\frac{1}{2}t}} \\\\ =\u0026amp;\\ \\dfrac{1}{2}\\dfrac{e^{i(n+\\frac{1}{2}) t }-e^{-i(n+\\frac{1}{2})t} }{e^{i\\frac{1}{2}t}-e^{-i\\frac{1}{2}t }} \\dfrac{e^{i(n+\\frac{1}{2})t}} {e^{i(n+\\frac{1}{2})t}} \\\\ =\u0026amp;\\ \\dfrac{1}{2}\\dfrac{\\sin (n+\\frac{1}{2})t} {\\sin \\frac{1}{2} t} \\end{align*} $$\n最後の等号では$\\sin x = \\dfrac{e^{ix} - e^{-ix}}{2i}$を利用した。\n■\n定理2 以下の式を$2L$-周期関数 $f(t)$のフーリエ級数の部分和とする。\n$$ \\begin{equation} S_{N} ^{f} (t)=\\dfrac{1}{2}a_{0}+\\sum \\limits_{n=1}^{N} \\left( a_{n}\\cos\\dfrac{n\\pi t}{L}+b_{n}\\sin\\frac{n\\pi t}{L} \\right) \\end{equation} $$\nすると、部分和 $S_{N}^{f}(t)$を以下のようなディリクレカーネルを含む積分で表すことができる。\n$$ S_{N}^{f} (t)=\\dfrac{1}{L}\\int_{-L}^{L}f(x)D_{n}\\left(\\dfrac{\\pi (x-t)}{L}\\right)dx $$\n証明 フーリエ係数 $a_{0}$、$a_{n}$、$b_{n}$を求めると次のようになる。\n$$ \\begin{align*} a_{0} \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L} f(x) dx \\\\ a_{n} \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L}f(x)\\cos\\dfrac{n\\pi x}{L} dx \\\\ b_{n} \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L}f(x)\\sin\\dfrac{n\\pi x}{L} dx \\end{align*} $$\nすると、以下の式を得る。\n$$ \\begin{align*} \u0026amp; a_{n} \\cos\\dfrac{n\\pi t}{L} + b_{n} \\sin\\dfrac{n\\pi t}{L} \\\\ =\u0026amp;\\ \\left( \\dfrac{1}{L}\\int_{-L}^{L}f(x)\\cos\\dfrac{n\\pi x}{L} dx \\right) \\cos\\dfrac{n\\pi t}{L} + \\left( \\dfrac{1}{L}\\int_{-L}^{L}f(x)\\sin\\dfrac{n\\pi x}{L} dx \\right)\\sin\\dfrac{n\\pi t}{L} \\\\ =\u0026amp;\\ \\dfrac{1}{L}\\int_{-L}^{L}f(x) \\left[ \\cos\\dfrac{n\\pi x}{L} \\cos\\dfrac{n\\pi t}{L} + \\sin\\dfrac{n\\pi x}{L} \\sin\\dfrac{n\\pi t}{L} \\right] dx \\end{align*} $$\nすると、三角関数の和差公式により、以下の式を得る。\n$$ a_{n}\\cos\\dfrac{n\\pi t}{L} + b_{n}\\cos\\dfrac{n\\pi t}{L} = \\dfrac{1}{L}\\int_{-L}^{L}f(x) \\left[ \\cos\\dfrac{n\\pi (x-t)}{L} \\right] dx $$\nこれを$(2)$に代入すると、以下のようになる。\n$$ \\begin{align*} S_{N} ^{f} (t) =\u0026amp;\\ \\dfrac{1}{2}a_{0}+\\sum \\limits_{n=1}^{N} \\left( a_{n}\\cos\\dfrac{n\\pi t}{L}+b_{n}\\sin\\frac{n\\pi t}{L} \\right) \\\\ =\u0026amp;\\ \\dfrac{1}{2}\\dfrac{1}{L}\\int_{-L}^{L} f(x)dx+\\sum\\limits_{n=1}^{N}\\left( \\dfrac{1}{L}\\int_{-L}^{L}f(x) \\left[ \\cos\\dfrac{n\\pi (x-t)}{L} \\right] dx \\right) \\\\ =\u0026amp;\\ \\dfrac{1}{L} \\int_{-L}^{L} f(x) \\left[ \\dfrac{1}{2} + \\sum\\limits_{n=1}^{N} \\cos \\dfrac{n\\pi (x-t)}{L}\\right]dx \\\\ =\u0026amp;\\ \\dfrac{1}{L}\\int_{-L}^{L}f(x)D_{n}\\left(\\dfrac{\\pi (x-t)}{L}\\right)dx \\end{align*} $$\n最後の等号ではディリクレカーネルの定義を使用した。\n■\n定理3 任意の整数 $n \\in \\mathbb{Z}$に対して、以下の式が成り立つ。\n$$ \\begin{equation} \\dfrac{1}{L}\\int_{-L}^{L}D_{n}\\left( \\dfrac{\\pi (x-t)}{L} \\right)dx=1 \\end{equation} $$\n証明 $\\dfrac{\\pi (x-t)}{L}=y$と置換する。すると、$(3)$の左辺は次のようになる。\n$$ \\begin{align*} \u0026amp;\\dfrac{1}{L}\\int_{-\\pi -\\frac{\\pi}{L}t}^{\\pi-\\frac{\\pi}{L}t} D_{n}(y) \\dfrac{L}{\\pi}dy \\\\ =\u0026amp;\\ \\dfrac{1}{\\pi}\\int_{-\\pi -\\frac{\\pi}{L}t}^{\\pi-\\frac{\\pi}{L}t} \\left( \\dfrac{1}{2} + \\sum \\limits_{n=1}^{N} \\cos ny \\right) dy \u0026amp; \\text{ by } (1) \\\\ =\u0026amp;\\ \\dfrac{1}{2\\pi}\\int_{-\\pi -\\frac{\\pi}{L}t}^{\\pi-\\frac{\\pi}{L}t} dy+ \\sum \\limits_{n=1}^{N} \\int_{-\\pi -\\frac{\\pi}{L}t}^{\\pi-\\frac{\\pi}{L}t} \\cos ny dy \\\\ =\u0026amp;\\ \\dfrac{1}{2\\pi} 2\\pi + \\sum \\limits_{n=1}^{N} \\int_{-\\pi -\\frac{\\pi}{L}t}^{\\pi-\\frac{\\pi}{L}t} \\cos ny dy \\\\ =\u0026amp;\\ 1 \\end{align*} $$\nこの時、二番目の項の積分が$0$になる理由は、$\\cos 0y=1$と$\\cos ny (n\\ne 0)$が互いに直交しているからである。\n■\n","id":932,"permalink":"https://freshrimpsushi.github.io/jp/posts/932/","tags":null,"title":"ディリクレ核"},{"categories":"푸리에해석","contents":"定義 $2L$-周期関数 $f$に対して次のような級数を $f$のフーリエ級数Fourier series of $f$と定義する。\n$$ \\begin{align*} \\lim \\limits_{N \\rightarrow \\infty} S^{f}_{N}(t) \u0026amp;= \\lim \\limits_{N \\to \\infty}\\left[ \\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{N} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L} + b_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right] \\\\ \u0026amp;= \\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{\\infty} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L} + b_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\end{align*} $$\nこの時、各々の係数 $a_{0}, a_{n}, b_{n}$を フーリエ係数Fourier coefficientと言い、値は次のようになる。\n$$ \\begin{align*} \\\\ a_{0} \u0026amp;=\\dfrac{1}{L}\\int_{-L}^{L}f(t)dt \\\\ a_{n} \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{n\\pi t}{L} dt \\\\ b_{n} \u0026amp;=\\dfrac{1}{L}\\int_{-L}^{L}f(t)\\sin \\dfrac{n\\pi t}{L}dt \\end{align*} $$\n説明 フーリエ級数は任意の関数を三角関数の級数展開で表現するもので、フランスの数学者 ジョセフ・フーリエJoseph Fourierが熱方程式を解くために考案したことでよく知られている。任意の関数と表現した理由は、ある区間 $(a,b)$で定義された関数があれば、これをCtrl+C, Ctrl+Vして $(b-a)$-周期関数にすることができるからである。\n核心原理は互いに直交する三角関数たちの線形結合で表現されることであり、3次元ベクトルにたとえると、$(4,-1,7)$を次のように分けることに似ている。\n$$ (4,-1,7) = a_{1}\\hat{\\mathbf{e}}_{1} + a_{2}\\hat{\\mathbf{e}}_{1} + a_{3}\\hat{\\mathbf{e}}_{1} $$\n実際に、$f$のフーリエ級数は$f$との誤差が非常に小さく、条件がよく満たされれば $f$に点ごとに収束する。\n$$ f(t) = \\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{\\infty} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L}t + b_{n}\\sin\\dfrac{n\\pi t}{L} \\right) $$\n導出 回帰分析1 パート 1\n関数 $f(t)$を $1, \\cos \\dfrac{\\pi t}{L}, \\cos\\dfrac{2\\pi t}{L}, \\cdots, \\sin \\dfrac{\\pi t}{L}, \\sin \\dfrac{2\\pi t}{L}, \\cdots $たちの線形結合で表現することが目的である。したがって、$S^{f}_{N}(t)=\\dfrac{1}{2}{\\alpha_{0}}+\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right)$とした時、$f(t)$は以下のように表現できる。\n$$ f(t)=S^{f}_{N}(t)+e_{N}(t) $$\n$e_{N}(t)$は $f(t)$と近似式 $S_{N}^{f} (t)$の差である。この差が最も小さくなる$S_{N}^{f}(t)$を見つければ、それが$f(t)$との差が最も小さい級数展開になる。$e_{N}$を 平均二乗誤差mean square error2としよう。\n$$ e_{N}=\\dfrac{1}{2L}\\int_{-L}^{L} [e_{N}(t) ]^{2}dt=\\dfrac{1}{2L}\\int_{-L}^{L} \\left[ f(t)-S^{f}_{N} (t) \\right]^{2} dt $$\nパート 2\n$$ \\begin{align*} e_{N} \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\left[ f(t)-S^{f}_{N}(t) \\right]^{2} dt \\\\ \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\left[ f(t)-\\dfrac{1}{2}{\\alpha_{0}}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]^{2} dt \\end{align*} $$\n平均二乗誤差 $e_{N}$が最小になる時の係数 $\\alpha_0,\\ \\alpha_{n},\\ \\beta_{n}$をそれぞれ $a_0$, $a_{n}$, $b_{n}$としよう。$e_{N}$を最小化する条件は次のようであり、正規方程式normal equationと言われる。\n$$ \\dfrac{\\partial e_{N}}{\\partial \\alpha_{0}}=0,\\ \\ \\dfrac{\\partial e_{N}}{\\partial \\alpha_{n}}=0,\\ \\ \\dfrac{\\partial e_{N}}{\\partial \\beta_{n}}=0\\quad (m=1,\\ 2,\\ \\cdots,\\ N) $$\nそれでは、$a_{0}$, $a_{n}$, $b_{n}$は以下のように求めることができる。\nパート 2.1 $a_{0}$\n$$ \\begin{align*} \\dfrac{\\partial e_{N}}{\\partial \\alpha_{0}} \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\dfrac{\\partial}{\\partial \\alpha_{0}} \\left[ f(t)-\\dfrac{1}{2} {\\alpha_{0}}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]^{2} dt \\\\ \u0026amp;= 2\\cdot \\dfrac{-1}{2} \\cdot \\dfrac{1}{2L} \\int_{-L}^{L} \\left[ f(t)-\\dfrac{1}{2}{\\alpha_{0}}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_ {N} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac {n\\pi t}{L} \\right) \\right] dt \\\\ \u0026amp;= \\dfrac{-1}{2L}\\int_{-L}^{L}f(t) dt + \\dfrac{1}{2L}\\int_{-L}^{L}\\dfrac{1}{2}\\alpha_{0} dt +\\dfrac{1}{2L}\\int_{-L}^{L} \\sum \\limits_ {n=1}^{N}\\left( \\alpha_{n}\\cos \\dfrac{n\\pi t}{L}+\\beta_{n} \\sin \\dfrac{n \\pi t}{L} \\right) dt \\\\ \u0026amp;= \\dfrac{-1}{2L}\\int_{-L}^{L}f(t) dt + \\dfrac{1}{2L}\\int_{-L}^{L}\\dfrac{1}{2}\\alpha_{0} dt \\\\ \u0026amp;= \\dfrac{-1}{2L}\\int_{-L}^{L}f(t) dt +\\dfrac{1}{2}\\alpha_{0} \\\\ \u0026amp;= 0 \\end{align*} $$\n4番目の等号は三角関数の1周期積分が0であるため成り立つ。したがって\n$$ a_{0} = \\dfrac{1}{L} \\int_{-L}^{L}f(t)dt $$\nパート 2.2 $a_{n}$\nある$m \\in \\left\\{ 1,2,\\dots,N \\right\\}$を一つ選ぼう。\n$$ \\begin{align*} \\dfrac{\\partial e_{N}}{\\partial \\alpha_{m}} \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\dfrac{\\partial}{\\partial \\alpha_{m}} \\left[ f(t)-\\dfrac{1}{2} {\\alpha_{0}}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]^{2} dt \\\\ \u0026amp;= 2\\cdot \\dfrac{1}{2L} \\int_{-L}^{L} \\left( - \\cos \\dfrac{m\\pi t}{L} \\right)\\left[ f(t)-\\dfrac{1}{2}{\\alpha_{0}}-\\sum \\limits_{n=1}^ {N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L} +\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]dt \\\\ \u0026amp;= -\\dfrac{1}{L} \\int_{-L}^{L} f(t)\\cos\\dfrac{m\\pi t}{L} dt +\\dfrac{1}{L}\\int_{-L}^{L} \\dfrac{1}{2}\\alpha_{0}\\cos\\dfrac{m\\pi t}{L} dt \\\\ \u0026amp;\\quad + \\dfrac{1}{L} \\int_{-L}^{L} \\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n\\pi t}{L} + \\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\cos\\dfrac{m\\pi t}{L}dt \\\\ \u0026amp;= -\\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{m\\pi t}{L} dt + \\dfrac{1}{L}\\alpha_{m} \\int_{-L}^{L}\\cos\\dfrac{m\\pi t}{L}\\cos\\dfrac{m\\pi t} {L} dt\\\\ \u0026amp;= -\\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{m\\pi t}{L} dt + \\alpha_{m} \\\\ \u0026amp;= 0 \\end{align*} $$\n4番目、5番目の等号は三角関数の直交性によって成り立つ。したがって\n$$ a_{n}= \\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{n\\pi t}{L} dt \\quad (n=1, 2, \\cdots, N) $$\nパート 2.3 $b_{n}$\nある$m \\in \\left\\{ 1,2,\\dots,N \\right\\}$を一つ選ぼう。\n$$ \\begin{align*} \\dfrac{\\partial e_{N}}{\\partial \\beta_{m}} \u0026amp;= \\dfrac{1}{2L}\\int_{-L}^{L} \\dfrac{\\partial}{\\partial \\beta_{m}} \\left[ f(t)-\\dfrac{1}{2} {\\alpha_{0}}-\\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L}+\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]^{2} dt \\\\ \u0026amp;= 2\\cdot \\dfrac{1}{2L} \\int_{-L}^{L} \\left( - \\sin \\dfrac{m\\pi t}{L} \\right)\\left[ f(t)-\\dfrac{1}{2}{\\alpha_{0}}-\\sum \\limits_{n=1}^ {N} \\left( \\alpha_{n} \\cos \\dfrac{n \\pi t}{L} +\\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\right]dt \\\\ \u0026amp;= -\\dfrac{1}{L} \\int_{-L}^{L} f(t)\\sin\\dfrac{m\\pi t}{L} dt +\\dfrac{1}{L}\\int_{-L}^{L} \\dfrac{1}{2}\\alpha_{0}\\sin\\dfrac{m\\pi t}{L} dt \\\\ \u0026amp;\\quad +\\dfrac{1}{L} \\int_{-L}^{L} \\sum \\limits_{n=1}^{N} \\left( \\alpha_{n} \\cos \\dfrac{n\\pi t}{L} + \\beta_{n}\\sin\\dfrac{n\\pi t}{L} \\right) \\sin\\dfrac{m\\pi t}{L}dt \\\\ \u0026amp;= -\\dfrac{1}{L}\\int_{-L}^{L} f(t)\\sin\\dfrac{m\\pi t}{L} dt + \\dfrac{1}{L}\\beta_{m} \\int_{-L}^{L}\\sin\\dfrac{m\\pi t}{L}\\sin\\dfrac{m\\pi t} {L} dt \\\\ \u0026amp;= -\\dfrac{1}{L}\\int_{-L}^{L} f(t)\\sin\\dfrac{m\\pi t}{L} dt + \\beta_{m} \\\\ \u0026amp;=0 \\end{align*} $$\n4番目、5番目の等号は三角関数の直交性によって成り立つ。したがって\n$$ b_{n}=\\dfrac{1}{L}\\int_{-L}^{L}f(t)\\sin\\dfrac{n\\pi t}{L}dt \\quad (n=1, 2, \\cdots, N) $$\nパート 3 ここで得られた$a_0$, $a_{n}$, $b_{n}$で$f(t)$を表現すると同じになる。\n$$ \\begin{align*} f(t) \u0026amp;= S^{f}_{N}(t)+e_{N}(t) \\\\[1em] \\text{where } S^{f}_{N}(t) \u0026amp;= \\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{N} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L} + b_{n}\\sin\\dfrac{n\\pi t} {L} \\right) \\\\ a_{0} \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L}f(t)dt \\\\ a_{n} \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L} f(t)\\cos\\dfrac{n\\pi t}{L} dt \\\\ b_{n} \u0026amp;= \\dfrac{1}{L}\\int_{-L}^{L}f(t)\\sin\\dfrac{n\\pi t}{L}dt \\end{align*} $$\n$N$に対して極限をとれば\n$$ \\lim \\limits_{N \\rightarrow \\infty} S_{N}^{f} (t)=\\dfrac{a_{0}}{2}+\\sum \\limits_{n=1}^{\\infty} \\left( a_{n} \\cos \\dfrac{n\\pi t}{L} + b_{n} \\sin\\dfrac{n\\pi t}{L} \\right) $$\n上の級数を $f$のフーリエ級数 と呼び、$a_0$, $a_{n}$, $b_{n}$を $f$のフーリエ係数 という。\n■\nチェ・ビョンソン, フーリエ解析入門 (2002), p51-53\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRSSが平均二乗誤差である。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":929,"permalink":"https://freshrimpsushi.github.io/jp/posts/929/","tags":null,"title":"フーリエ級数の導出"},{"categories":"함수","contents":" 🚧 이 포스트는 아직 이관 작업이 완료되지 않았습니다 🚧\n**\n**内積inner product 区間$[a,b]$で定義された二つの複素関数$f$、$g$の内積は、以下のように定義される。 $$ \\left\\langlef, g\\right\\rangle:=\\int_{a}^b f(x) \\overline{g(x)} dx $$ したがって、同じ二つの関数の内積は $$ \\left\\langle f,f \\right\\rangle=\\int_{a}^b f(x) \\overline{f(x)} dx = \\int_{a}^b \\left| f(x) \\right| ^2 dx $$ 関数の内積を定積分で定義する理由\n直交 $(\\mathrm{orthogonal})$ 二つの複素関数$f$、$g$が下記の式を満たす時、「$f$、$g$は区間$[a,b]$で直交する**」と言われる。 $$ \\left\\langle f,g \\right\\rangle=\\int_{a}^b f(x) \\overline{g(x)} dx=0 $$ 二つの関数の内積を積分で定義したから、積分値が$0$の時直交するというのは自然である。\n直交集合と直交性 orthogonal set and orthogonality 関数$\\phi_{1}$、$\\phi_2$、$\\phi_{3}$、$\\cdots$が下記の式を満たす場合、これらの関数の集合$\\left\\{\\phi_{1},\\ \\phi_2,\\ \\phi_{3}, \\cdots \\right\\}$を直交集合と言い、これらの関数の集合が直交性**を持つと言われる。 $$ \\left\\langle \\phi_{m},\\phi_{n} \\right\\rangle = \\int_{a}^b \\phi_{m} (x) \\overline{ \\phi_{n}(x) } dx=0\\ \\ (m\\ne n) $$ 簡単に言えば、直交集合とは他の関数と直交する関数を集めた集合である。\n関数のノルム$(\\mathrm{norm})$ 内積を定義すると、ノルムを定義することができる。複素関数$f$のノルムは以下のように定義される。 $$ | f | = \\left\\langle f,f\\right\\rangle^{ \\frac{1}{2} } := \\left( \\int_{a}^b \\left| f(x) \\right| ^2 dx \\right) ^{ \\frac{1}{2} } $$\n正規化$(\\mathrm{normalization})$ 任意の関数$f$に対して、適切な定数をかけて$f$のノルムが$1$になるようにすることを正規化という。正規化された関数は正規化された$(\\mathrm{normalized})$関数または正規化関数と呼ばれる。従って、$f$の正規化関数を$f_{\\mathrm{normal}}$とすると $$ f_{\\mathrm{normal}}=\\frac{1}{ | f | }f $$\n正規直交集合orthonomal set 直交集合$\\left\\{ \\phi_{1}, \\phi_{2}, \\cdots \\right\\}$の要素が下記の条件を満たす場合、その集合を正規直交集合**と呼ぶ。 $$ \\left\\langle \\phi_{m},\\phi_{n} \\right\\rangle = \\int_{a}^b \\phi_{m} (x) \\overline{ \\phi_{n}(x) } dx=\\delta_{mn} $$ つまり、正規直交集合はすべての要素が正規化された直交集合を指す。$\\delta_{mn}$はクロネッカーデルタである。直交集合から自分自身との内積が1であるという条件が加わっている。例えば、3次元直交座標系で$\\left\\{ \\hat{\\mathbf{x}},\\ \\hat{\\mathbf{y}},\\ \\hat{\\mathbf{z}} \\right\\}$は正規直交集合である。\n**例 $f_{0}(x)=1$、$f_{1}(x)=x$、$f_2(x)=x^2+ax+b$としよう。\n$f_{0}$と$f_{1}$が$[-1,1]$で直交することを示せ解決策** $$ \\begin{align*} \\left\\langle f_{0},f_{1} \\right\\rangle \u0026amp;= \\int_{-1}^{1} x dx \\\\ \u0026amp;= \\left. \\dfrac{1}{2}x^2 \\right]_{-1}^{1} = \\dfrac{1}{2}-\\dfrac{1}{2}=0 \\end{align*} $$ ■\n2. $f_2$が$f_{0}$、$f_{1}$と同時に直交するような定数$a$, $b$を求めよ。解決策 $$ \\begin{align*} \\left\\langle f_2,f_{0} \\right\\rangle \u0026amp;= \\int_{-1}^{1} (x^2+ax+b) dx \\\\ \u0026amp;= \\left. \\frac{1}{3}x^3 +\\frac{a}{2}x^2+bx \\right]_{-1}^{1} \\\\ \u0026amp;= \\left( \\frac{1}{3}+\\frac{a}{2}+b\\right) - \\left( -\\frac{1}{3}+\\frac{a}{2}-b \\right) \\\\ \u0026amp;= \\frac{2}{3}+2b =0 \\end{align*} $$ $$ \\begin{align*} \\left\\langle f_2,f_{1} \\right\\rangle \u0026amp;= \\int_{-1}^{1}( x^3+ax^2+bx) dx \\\\ \u0026amp;= \\left. \\frac{1}{4}x^4 +\\frac{a}{3}x^3+\\frac{b}{2}x^2 \\right]_{-1}^{1} \\\\ \u0026amp;= \\left( \\frac{1}{4}+\\frac{a}{3}+\\frac{b}{2}\\right) - \\left( \\frac{1}{4}-\\frac{a}{3}+\\frac{b}{2} \\right) \\\\ \u0026amp;= \\frac{2}{3}a=0 \\end{align*} $$ 従って、$a=0$、$b=-\\dfrac{1}{3}$■\n**$f_{0}$、$f_{1}$、$f_2$の正規化関数を求めよ。解決策 $$ \\left\\langlef_{0},f_{0} \\right\\rangle=\\int_{-1}^{1} 1 dx =2 $$ $$ \\left\\langlef_{1},f_{1} \\right\\rangle=\\int_{-1}^{1} x^2 dx =\\frac{2}{3} $$\n$$ \\left\\langlef_2,f_2 \\right\\rangle=\\int_{-1}^{1} \\left( x^2-\\frac{1}{3} \\right )\\left( x^2-\\frac{1}{3} \\right) dx =\\frac{8}{45} $$ 従って、$f_{0}$、$f_{1}$、$f_2$の正規化関数はそれぞれ $$ \\frac{1}{\\sqrt{2}}f_{0},\\quad \\sqrt{\\frac{3}{2}}f_{1},\\quad \\sqrt{\\frac{45}{8}}f_2 $$ ■\n","id":926,"permalink":"https://freshrimpsushi.github.io/jp/posts/926/","tags":null,"title":"直交関数と直交集合：正規直交集合と関数のノルム"},{"categories":"확률론","contents":"定義 確率過程 $\\left\\{ X_{n} \\right\\}$ の状態空間が整数の集合 $\\left\\{ \\cdots , -2 , -1, 0 , 1 , 2 , \\cdots \\right\\}$ で、状態 $0$ から始まるとする。次のステップで $1$ だけ減少する確率が$p$、$1$ だけ増加する確率が$(1-p)$ のとき、$\\left\\{ X_{n} \\right\\}$ を一般化されたランダムウォークという。\n説明 ランダムウォークは確率過程の中でも非常に単純な例で、通常、左右に動く確率を同じにする。一般化されたランダムウォークは、その確率を変えるだけのものだ。単純に考えても、左右に動く確率が同じなら、開始状態 $0$ を中心に行ったり来たりすることが難しくないと想像できる。しかし、一方が大きい場合は、時間が経つにつれてその方向へ発散してしまうだろう。\n一方で、状態空間を有限に制限したケースとしては、ギャンブラーの破産問題がある。\n要約 $\\displaystyle p = {{1} \\over {2}}$ ならば状態 $0$ はリカレントで、$\\displaystyle p \\ne {{1} \\over {2}}$ ならば状態 $0$ はトランジェントだ。\n証明 $\\displaystyle \\sum_{n=1}^{\\infty} p_{00}^{(n)}= \\infty$ ならば $0$ はリカレントで、$\\displaystyle \\sum_{n=1}^{\\infty} p_{00}^{(n)} \u0026lt; \\infty$ ならば $0$ はトランジェントだ。まず、状態 $0$ から奇数回だけ動いて$0$ に戻る確率は確実に $0$ なので $$ p_{00}^{ ( 2n - 1 )} = 0 $$ である。$2n $ 回だけ動いて $0$ に戻ったということは正確に左に $n$ 回、右に $n$ 回動いたという意味なので $$\\displaystyle p_{00}^{ ( 2n )} = \\binom{2n}{n} p^{n} (1-p)^{n}$$ である。今、 $$\\displaystyle p_{00}^{ ( 2n )} = {{ ( 2n )! } \\over { ( n! )^2 }} \\left( p ( 1 - p ) \\right)^{n}$$\nこれが発散するか収束するかを確認すれば十分だ。\nスターリングの近似: $$\\lim_{n \\to \\infty} {{n!} \\over { n^{ n + 1/2} e^{- n} \\sqrt{ 2 \\pi } }} = 1$$\n階乗を計算するのが難しく、$n$ は無限大を想定しているので、スターリングの近似を使用する。 $$ \\begin{align*} p_{00}^{ ( 2n )} \\approx\u0026amp; {{ (2n)^{2n + 1/2} e^{-2n} \\sqrt{ 2 \\pi } } \\over { \\left( n^{n + 1/2} e^{-n} \\sqrt{ 2 \\pi } \\right)^{2} }} \\left( p ( 1 - p ) \\right)^{n} \\\\ =\u0026amp; {{ (2n)^{2n } \\sqrt{2n} } \\over { \\left( n^{n } \\right)^{2} n \\sqrt{ 2 \\pi } }} \\left( p ( 1 - p ) \\right)^{n} \\\\ =\u0026amp; {{ 4^{n} n^{2n} \\sqrt{ 2n } } \\over { n^{2n} n \\sqrt{ 2 \\pi } }} \\left( p ( 1 - p ) \\right)^{n} \\\\ =\u0026amp; {{ \\left( 4 p ( 1 - p ) \\right)^{n} } \\over { \\sqrt{ \\pi n } }} \\end{align*} $$\nケース 1. $\\displaystyle p = {{1} \\over {2}}$\np-級数判定法: $\\displaystyle \\sum _{ n=1 }^{ \\infty }{ 1 \\over {n^p} }$ が収束するのは $p\u0026gt;1$ と同値である。\np-級数判定法により、$\\displaystyle \\sum_{n=1}^{\\infty} {{ \\left( 4 p ( 1 - p ) \\right)^{n} } \\over { \\sqrt{ \\pi n } }} = {{1} \\over { \\sqrt{ \\pi } }} \\sum_{n=1}^{\\infty} {{1} \\over { \\sqrt{n} } }$ は発散し、状態$0$ はリカレントだ。\nケース 2. $\\displaystyle p \\ne {{1} \\over {2}}$\n比判定法: $\\displaystyle r = \\lim_{n \\to \\infty} { {|a_{n+1}|} \\over {|a_{n}|} }$ において $r\u0026lt;1$ ならば $\\displaystyle \\sum _{ n=1 }^{ \\infty }{ { a }_{ n }}$ は絶対収束し、$r\u0026gt;1$ ならば $\\displaystyle \\sum _{ n=1 }^{ \\infty }{ { a }_{ n }}$ は発散する。\n$$\\lim_{ n \\to \\infty } \\left| {{ {{ \\left( 4 p ( 1 - p ) \\right)^{n+1} } \\over { \\sqrt{ \\pi ( n + 1 ) } }} } \\over { {{ \\left( 4 p ( 1 - p ) \\right)^{n} } \\over { \\sqrt{ \\pi n } }} }} \\right| = \\lim_{n \\to \\infty } {{ 4 p ( 1 - p ) } \\over { \\sqrt{ (n+1) / n } }} = 4p (1 - p) \u0026lt; 1$$ よって、比判定法により $$\\sum_{n=1}^{\\infty} {{ \\left( 4 p ( 1 - p ) \\right)^{n} } \\over { \\sqrt{ \\pi n } }}$$ は収束し、状態 $0$ はトランジェントだ。\n■\n参照 時系列でのランダムウォーク ","id":870,"permalink":"https://freshrimpsushi.github.io/jp/posts/870/","tags":null,"title":"一般化されたランダムウォーク"},{"categories":"추상대수","contents":"まとめ 1 素数 $p$ と 自然数 $n$ に対して、基数が $p^{n}$ の 有限 体有限体を $p^{n}$ 次のガロア体ガロア体と定義し、$\\text{GF} \\left( p^{n} \\right)$ のように表す。有限体はガロア体だけであり、与えられた $p$ と $n$ に対してガロア体は唯一に存在する。\nここで「唯一である」とは、異なる体であっても同型写像が存在し、実質的に同一の体であるという意味である。 説明 ガウスが最初に有限体の概念を思いついたときは、その実体を信じる人はいなかったが、現在では有限体が存在するだけでなく、その具体的な形まで明らかにされている。すべての有限体の形が解明されたので、無駄な研究をする必要はない。\n例えば、元が $10$ 個の体が存在するかどうかは、考える必要さえなく、$\\text{GF} \\left( p \\right) = \\mathbb{Z}_{p}$ は整数環であるため、すでに多くのことがわかっている。さらに知りたいことがあれば、抽象的な定義に固執する必要はなく、$\\mathbb{Z}_{p}$ を通じてアプローチすればよく、その逆もまた然りである。\n証明 2 パート1. すべての有限体はガロア体である。\n体 $F$ の有限拡大体を $E$ とし、$F$ 上の 次数を $n := \\left[ E : F \\right]$ とする。\n$| F | = q$ とすると、$E$ は $F$ の $n$ 次のベクトル空間であるため、$|E| = q^{n}$ である。体は単位元を持つが、標数が $0$ であれば $\\mathbb{Z}$ と同型の部分環が存在して無限体となる。したがって、有限体の標数は有限の自然数でなければならない。有限体 $E$ の標数を $p \\ne 0$ とすると、$E$ は単位元 $1$ を持つため、$p \\cdot 1 = 0$ でなければならない。体は整域であるため、 $$ p \\cdot 1 = ( p_{1} \\cdot 1 ) ( p_{2} \\cdot 1 ) = 0 $$ を満たす $p_{1}, p_{2} \\in \\mathbb{Z}$ が存在することはなく、$p$ は必ず素数である。したがって、$E$ は素体 $\\mathbb{Z}_{p}$ と同型の部分体を持ち、$\\left| \\mathbb{Z}_{p} \\right| = p$ であるため、$|E| = p^{n}$ である。\nパート2. ガロア体の存在\nパート2-1. $x^{p^{n}} - x$ のゼロ\n$\\left( x^{p^{n}} - x \\right)$ の標数が $p$ の体 $F$ の代数的閉包 $\\overline{F}$ を考える。\n$\\overline{F}$ は代数的に閉じているため、$\\left( x^{p^{n}} - x \\right) \\in \\overline{F} [ x ]$ は $1$ 次の項で因数分解される。すぐにわかる事実は $$ x^{p^{n}} - x = ( x - 0 ) \\left( x^{p^{n}-1} - 1 \\right) $$ であるため、$0$ は $\\left( x^{p^{n}} - x \\right)$ のゼロになる。$f(x) := x^{p^{n}-1} - 1$ の別のゼロ $\\alpha \\ne 0$ を考えると、 $f \\left( \\alpha \\right) = 0$ であるため、 $$ 0 = f \\left( \\alpha \\right) = \\alpha^{p^{n} - 1} - 1 \\implies \\alpha^{p^{n} - 1} = 1 $$ となり、これにより $f(x)$ を $\\left( x - \\alpha \\right)$ の積として表すと、 $$ \\begin{align*} f(x) =\u0026amp; x^{p^{n}-1} - 1 \\\\ =\u0026amp; x^{p^{n}-1} - \\alpha^{p^{n}-1} \\\\ =\u0026amp; (x - \\alpha ) \\left( x^{p^{n} - 2 } + \\alpha x^{p^{n} - 3 } + \\cdots + \\alpha^{p^{n} - 3 } x + \\alpha^{p^{n} - 2} \\right) \\end{align*} $$ である。一方、便宜上第二の因数を、 $$ g(x) := \\left( x^{p^{n} - 2 } + \\alpha x^{p^{n} - 3 } + \\cdots + \\alpha^{p^{n} - 3 } x + \\alpha^{p^{n} - 2} \\right) $$ とすると、$g(x)$ の項の数は $p^{n} - 1$ 個である。したがって、$x = \\alpha$ を代入してみると、 $$ g ( \\alpha ) = \\alpha^{p^{n} - 2} \\cdot \\left( p^{n} - 1 \\right) = {{\\alpha^{p^{n} - 1}} \\over { \\alpha }} \\left( p^{n} - 1 \\right) $$ を得る。上記で $\\alpha \\ne 0$ は $f(x)$ のゼロであるため、$\\alpha^{p^{n}-1} - 1 = 0$ としたし、標数を素数 $p$ と仮定したので、 $$ g ( \\alpha ) = {{1} \\over { \\alpha }} \\cdot (0 - 1) = - {{1} \\over { \\alpha }} \\ne 0 $$ である。したがって、$\\alpha$ は $f(x) = 0$ の重根ではなく、これは $\\alpha$ 以外の他のゼロにも当てはまる。結局、$\\left( x^{p^{n}} - x \\right)$ は正確に $p^{n}$ 個の異なるゼロを持つ。\nパート2-2. 新入生の夢\n一方で、$\\alpha , \\beta \\in F$ に対して $\\left( \\alpha + \\beta \\right)^{p}$ を計算すると、二項定理により、 $$ \\begin{align*} \\left( \\alpha + \\beta \\right)^{p} =\u0026amp; \\sum_{k=1}^{p} \\binom{p}{k} \\alpha^{k} \\beta^{p - k} \\\\ =\u0026amp; \\alpha^{p} + \\sum_{k=2}^{p-1} {{p!} \\over { ( p - k )! ( k )! }} \\alpha^{k} \\beta^{p - k} + \\beta^{p} \\\\ =\u0026amp; \\alpha^{p} + \\beta^{p} + p \\sum_{k=2}^{p-1} {{ ( p - 1 )! } \\over { ( p - k )! ( k )! }} \\alpha^{k} \\beta^{p - k} \\end{align*} $$ $F$ の標数が $p$ であるため、最後の項は $0$ となり、したがって、 $$ \\left( \\alpha + \\beta \\right)^{p} = \\alpha^{p} + \\beta^{p} $$ もう一度両辺に $p$ 乗をすると、 $$ \\left( \\left( \\alpha + \\beta \\right)^{p} \\right)^{p} = \\left( \\alpha^{p} \\right)^{p} + \\left( \\beta^{p} \\right)^{p} $$ 整理すると $\\left( \\alpha + \\beta \\right)^{p^{2}} =\\alpha^{p^2} + \\beta^{p^2}$ であり、これを $n$ 回繰り返すと、次を得る。 $$ \\left( \\alpha + \\beta \\right)^{p^{n}} =\\alpha^{p^n} + \\beta^{p^n} $$\n今度は $\\mathbb{Z}_{p}$ の代数的閉包 $\\overline{ \\mathbb{Z}_{p} }$ を考える。\n$\\left( x^{p^{n}} - x \\right) \\in \\overline{ \\mathbb{Z}_{p} } [ x ]$ のゼロをすべて集めた集合を $K \\subset \\overline{ \\mathbb{Z}_{p} } $、その元を $\\alpha , \\beta \\in K$ とする。\nパート2-3. $K$ はガロア体である。\n(i) 加算に対する閉包: $$ \\begin{cases} \\alpha^{p^{n}} - \\alpha = 0 \\\\ \\beta^{p^{n}} - \\beta = 0 \\end{cases} $$ である。両辺を加えると、パート2-2 $\\left( \\alpha + \\beta \\right)^{p^{n}} =\\alpha^{p^n} + \\beta^{p^n}$ により、 $$ \\left( \\alpha^{p^{n}} + \\beta^{p^{n}} \\right) - ( \\alpha + \\beta ) = \\left( \\alpha + \\beta \\right)^{p^{n}} - ( \\alpha + \\beta ) = 0 $$ であるため、$( \\alpha + \\beta ) \\in K$ である。 (ii) 加算に対する単位元: $0^{p^{n}} - 0 = 0$ であるため、$0 \\in K$ である。 (iii) 加算に対する逆元: $\\left( - \\alpha \\right)^{p^{n}} = \\left( - 1 \\right)^{^{p^{n}}} \\left( \\alpha \\right)^{p^{n}} = \\left( - 1 \\right)^{^{p^{n}}} \\alpha$ である。 $p=2$ の場合、$-1 = 1$ であるため、$\\left( -\\alpha \\right) = \\alpha \\in K$ である。 $p \\ne 2$ は奇数の素数であるため、$\\left( - \\alpha \\right)^{p^{n}} - ( - \\alpha ) = 0$、つまり $( - \\alpha ) \\in K$ である。 (iv) 乗算に対する閉包: $\\left( \\alpha \\beta \\right)^{p^{n}} = \\alpha^{p^{n}} \\beta^{p^{n}} = \\alpha \\beta$ であるため、$\\left( \\alpha \\beta \\right)^{p^{n}} - \\alpha \\beta = 0$、すなわち $\\alpha \\beta \\in K$ である。 (v) 乗算に対する単位元: $1^{p^{n}} - 1 = 0$ であるため、$1 \\in K$ である。 (vi) 乗算に対する逆元: $\\alpha \\ne 0$ に対して $\\displaystyle \\left( \\alpha \\right)^{p^{n}} = \\alpha$ の逆数を取ると、$\\displaystyle {{1} \\over {\\left( \\alpha \\right)^{p^{n}} }} = {{1} \\over { \\alpha }}$、すなわち $$ \\left( {{1} \\over { \\alpha }} \\right)^{p^{n}} - {{1} \\over { \\alpha }} = 0 $$ であるため、$\\alpha^{-1} \\in K$ である。 (vii): $| K | = p^{n}$ : $\\mathbb{Z}_{p}$ の標数は $p$ であるため、パート2-1により $\\left( x^{p^{n}} - x \\right)$ は正確に $p^{n}$ 個の異なるゼロを持つ。 したがって、$K$ は $p^{n}$ 次のガロア体である。\nパート3. ガロア体の一意性\nパート1では、$F$ の標数は素数 $p$ であり、パート2-1では、$F$ の代数的閉包 $\\overline{F}$ での演算が、$F$ の単位元 $1_{F}$ を $1_{\\mathbb{Z}_{p}}$ と見た場合、実際には $\\mathbb{Z}_{p}$ の代数的閉包 $\\overline{\\mathbb{Z}}_{p}$ での演算と変わらないことを指摘しておく。\nパート3-1. 基数が $p^{n}$ の体 $E \\subset \\overline{\\mathbb{Z}}_{p}$ の正体 3\nラグランジュの定理: $H$ が有限群 $G$ の部分群であれば、$|H|$ は $|G|$ の約数である。\n基数が $p^{n}$ の体 $\\left( E , + , \\times \\right)$ において、乗算 $\\times$ に対する群 $\\left( E^{\\ast} , \\times \\right)$ を考えると、$E^{\\ast}$ は $E$ で $+$ に対する単位元 $0 \\in E$ を除く $p^{n} - 1$ 個の元と単位元 $1 \\in E^{\\ast}$ を持つ。$\\alpha \\in E^{\\ast}$ のオーダーOrder、つまり $\\alpha$ によって生成される巡回群の基数である $\\left| \\alpha \\right| = \\left| \\left\u0026lt; \\alpha \\right\u0026gt; \\right|$ はラグランジュの定理により $p^{n} - 1$ の約数であり、したがって、 $$ \\alpha^{p^{n} - 1} = 1 \\implies a^{p^{n}} = \\alpha $$ を得る。つまり、$E$ のすべての元は $x^{p^{n}} - x$ のゼロであり、代数学の基本定理により、$\\mathbb{Z}_{p}$ の代数的閉包 $\\overline{\\mathbb{Z}}_{p}$ に含まれる基数が $p^{n}$ の体 $E$ の元は正確に $\\left( x^{p^{n}} - x \\right) \\in \\mathbb{Z}_{p} [x]$ のゼロである。\nパート3-2. 最小分解体\nパート2-1とパート3-1により、与えられた $p$ と $n$ に対して、すべての元が正確に $\\left( x^{p^{n}} - x \\right)$ のゼロで構成される体 $E$ が存在し、$F$ の標数が $p$ であることにより、その係数に対する演算も素体 $\\mathbb{Z}_{p}$ での演算と同じであったことに注意せよ。パート2-3とパート1により、$E$ は素体 $\\mathbb{Z}_{p}$ を素体として持ち、$|E| = p^{n}$ を満たす必要があるガロア体であり、さらにパート2-1により、$E$ は $\\left( x^{p^{n}} - x \\right)$ の最小分解体であることがわかる。\n最小分解体の性質: $f(x) \\in F [ x ]$ の最小分解体はすべて同型である。\n最小分解体の性質により、与えられた $p$ と $n$ に対して、ガロア体は一意である。\n■\n補助定理: 新入生の夢 単に面白い事実として、パート2-2で登場した等式 $$ \\left( \\alpha + \\beta \\right)^{p^{n}} =\\alpha^{p^n} + \\beta^{p^n} $$ を新入生の夢Freshman\u0026rsquo;s Dreamと呼ぶ。学校に入ったばかりの新入生の立場からすると、累乗が括弧の中に入れば、複雑な展開なしにも難しい問題を解くことができるからである。ちなみに、数論では、標数に関する言及がなくても、同様の方法で合同式 $\\left( \\alpha + \\beta \\right)^{p^{n}} \\equiv \\alpha^{p^n} + \\beta^{p^n} \\pmod{ p }$ を導くことができる。\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p300.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p302~304.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p301\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":820,"permalink":"https://freshrimpsushi.github.io/jp/posts/820/","tags":null,"title":"ガロア体"},{"categories":"함수","contents":"公式 ルジャンドル多項式の明示的explicitな公式は以下の通りです。\n$$ P_{l}(x)=\\dfrac{1}{2^{l} l!} \\dfrac{d^{l}}{dx^{l}}(x^{2}-1)^{l} \\tag{1} $$\n説明 $l$番目のルジャンドル多項式を得る公式であり、これをロドリゲスの公式と言います。元々はルジャンドル多項式の明示的な形を示す言葉でしたが、その後、多項式で表される特殊関数の明示的な形を示す公式の一般的な名称となりました。\n導出 ルジャンドル多項式$P_{l}$は以下のようなルジャンドルの微分方程式の解を指します。\n$$ (1 - x^{2}) \\dfrac{d^{2} y}{d x^{2}} - 2x \\dfrac{d y}{d x} + l(l+1)y = 0 $$\nしたがって、$(1)$が上記の微分方程式の解であることを示せば、証明が完了します。\nまず、$v=(x^2-1)^l$としたとき、$\\dfrac{d^lv}{dx^l}$がルジャンドル方程式の解であることを示すつもりです。その後、$P_{l}(1) = 1$を満たすように正規化して、$(1)$を得ます。\n$$ \\dfrac{dv}{dx}=l(2x)(x^2-1)^{l-1} $$\n両辺に$(x^2-1)$を掛けると、以下の式を得ます。\n$$ (x^2-1)\\dfrac{dv}{dx}=2lx(x^2-1)^l=2lxv $$\n両辺を$l+1$回微分すると、ライプニッツの法則により以下のようになります。\n$$ \\sum \\limits_{k=0}^{l+1} {}_{l+1}\\mathrm{C}_{k} \\dfrac{ d^{l+1-k}}{dx^{l+1-k} } \\left( \\dfrac{dv}{dx} \\right) \\dfrac{d^k}{dx^k} (x^2-1) = 2l\\sum \\limits_{k=0}^{l+1} {}_{l+1}\\mathrm{C} _{k} \\dfrac{d^{l+1-k} v}{dx^{l+1-k}} \\dfrac{d^k x}{dx^k} $$\nこのとき、左辺は$k \\ge 3$のとき$\\dfrac{d^k}{dx^k}(x^2-1)=0$であるため、$k=0,2,3$の項のみが残ります。右辺は$k \\ge 2$のとき$\\dfrac{d^kx}{dx^k}=0$であるため、$k=1,2$の項のみが残ります。したがって、次のように得られます。\n$$ (x^2-1)\\dfrac{d^{l+2} v}{dx^{l+2}} + (l+1)(2x)\\dfrac{d^{l+1}v}{dx^{l+1}}+\\dfrac{l(l+1)}{2!}2\\dfrac{d^l v}{dx^l}=2lx\\dfrac{d^{l+1} v}{dx^{l+1}} + 2l(l+1)\\dfrac{d^lv}{dx^l} $$\n同じ係数項をまとめて整理すると、以下のようになります。\n$$ (1-x^2)\\left( \\dfrac{d^l v}{dx^l} \\right)^{\\prime \\prime} -2x\\left( \\dfrac{d^lv}{dx^l} \\right)^{\\prime} + l(l+1)\\dfrac{d^lv}{dx^l}=0 $$\nこれはルジャンドル方程式と同じ形です。つまり、$\\dfrac{d^l v}{dx^l}$がルジャンドル方程式の解になります。\n$$ P_{l}(x)= \\dfrac{d^l}{dx^l}(x^2-1)^l $$\n$P_{l}(1) = 1$を満たす係数を求めてみましょう。$(x^2-1)^l$を$(x-1)^l(x+1)^l$で因数分解し、ライプニッツの法則で$l$回微分すると、以下のようになります。\n$$ \\begin{align*} \u0026amp;\\quad \\ P_{l}(x) \\\\ \u0026amp;= \\dfrac{d^l}{dx^l} \\left[ (x-1)^l (x+1)^l \\right] \\\\ \u0026amp;= \\sum\\limits_{k=0}^l {}_{l}\\mathrm{C}_{k} \\dfrac{d^{l-k}}{dx^{l-k}}(x-1)^l \\dfrac{d^k}{dx^k}(x+1)^l \\\\ \u0026amp;= {}_{l}\\mathrm{C}_{0} l! (x+1)^l + {}_{l}\\mathrm{C}_{1} l!(x-1) l(x+1)^{l-1}+{}_{l}\\mathrm{C}_2\\dfrac{l!}{2}(x-1)^2l(l-1)(x+1)^{l-2}+\\cdots \\end{align*} $$\n2番目の項からは因数として$(x-1)$を含むため、$x=1$のとき$0$です。したがって、$P_{l}(1)=l! 2^l$であり、この値が$1$になるためには、$\\dfrac{1}{2^l l!}$で割ればよいです。したがって、最終的に以下のようなロドリゲスの公式を得ます。\n$$ P_{l}(x)=\\dfrac{1}{2^l l!}\\dfrac{d^l}{dx^l}(x^2-1)^l $$\n■\n","id":895,"permalink":"https://freshrimpsushi.github.io/jp/posts/895/","tags":null,"title":"ルジャンドル多項式のロドリゲスの公式"},{"categories":"확률론","contents":"定義 状態空間が可算集合で、次を満たす離散的確率過程 $\\left\\{ X_{n} \\right\\}$ を 離散マルコフ連鎖DTMC または簡単に マルコフ連鎖Markov Chain, MCと言う。 $$ p \\left( X_{n+1} = j \\mid X_{n} = i , X_{n-1} = k , \\cdots , X_{0} = l \\right) = p \\left( X_{n+1} = j \\mid X_{n} = i \\right) $$\n参照 連続マルコフ連鎖 説明 $p_{ij}:= p \\left( X_{n+1} = j \\mid X_{n} = i \\right)$ を 遷移確率Transition Probabilityと言い、現在の状態を意味する$i$ を ソースステートSource State、目標状態を意味する$j$ を ターゲットステートTarget Stateという。$k$ ステップ後の遷移確率は $p_{ij}^{(k)}: = p \\left( X_{n+k} = j \\mid X_{n} = i \\right)$ のように表される。\nマルコフ連鎖とは、これまでの歴史を全て知っている時と、現在だけを知っている時の次のステップの確率が同じ確率過程のことを言う。簡単に言えば、現在の状態だけを正確に知っていれば、過去は未来に影響を与えない確率過程だ。通常、このような性質を 無記憶性Memorylessnessと呼ぶ。当然、このような仮定があれば、計算などが非常に便利になる。\n例えば、降水確率に関するモデルをマルコフ連鎖で表すと考えてみよう。昨日雨が降ったかどうかに関わらず、明日の降水確率は今日雨が降ったかどうかだけに影響を受けると仮定する。雨が降った状態を$0$、雨が降らなかった状態を$1$ とし、 $$\\begin{matrix} p_{00} = 0.7 \u0026amp; p_{01} = 0.3 \\\\ p_{10} = 0.4 \u0026amp; p_{11} = 0.6 \\end{matrix}$$ とする。これは、今日雨が降ったならば、明日も雨が降る確率が$70 %$ で、雨が降らない確率が$30 %$、今日雨が降らなかったならば、明日も雨が降らない確率が$60 %$ で、雨が降る確率が$40 %$ という意味だ。\nこれから、 $$P:= \\begin{bmatrix} p_{00} \u0026amp; p_{01} \\\\ p_{10} \u0026amp; p_{11} \\end{bmatrix} = \\begin{bmatrix} 0.7 \u0026amp; 0.3 \\\\ 0.4 \u0026amp; 0.6 \\end{bmatrix} $$ のような行列を通じて、このような確率計算を簡単にしてみよう。このような行列を 遷移確率行列と呼び、$k$ ステップ後の遷移確率行列を$P^{(k)}:= \\left( p_{ij}^{ ( k ) } \\right)$ のように表す。遷移確率行列の有用な性質として、$P^{(n)} = P^{n}$ がチャップマン・コルモゴロフ方程式を通じて証明できる。\nもし今日雨が降ったなら、二日後の降水確率は $$ \\begin{align*} P^{ (2) } =\u0026amp; P^{2} \\\\ =\u0026amp; \\begin{bmatrix} 0.7 \u0026amp; 0.3 \\\\ 0.4 \u0026amp; 0.6 \\end{bmatrix} \\begin{bmatrix} 0.7 \u0026amp; 0.3 \\\\ 0.4 \u0026amp; 0.6 \\end{bmatrix} \\\\ =\u0026amp; \\begin{bmatrix} 0.61 \u0026amp; 0.39 \\\\ 0.52 \u0026amp; 0.48 \\end{bmatrix} \\end{align*} $$ となり、$p_{00}^{(2)} = (0.7)^2 + (0.3) (0.4) = 0.61$ と正確に一致する。通常、私たちが関心を持っている問題はこれ以上に複雑で、比較的遠い未来に関心があるので、このような行列を扱うことが必須であることがわかる。\n遷移確率行列の数式的定義 ちなみに、遷移確率行列の同じ行の成分を全て加えると、必ず$1$ になるが、これは次のステップの確率を全て加えると$1$ になるからである。数式で表すと、$\\displaystyle \\sum_{j} p_{ij} = 1$ となる。確率過程論をどこで学ぶかによって、この性質を定義として受け入れることもある。\n","id":859,"permalink":"https://freshrimpsushi.github.io/jp/posts/859/","tags":null,"title":"離散マルコフ連鎖"},{"categories":"상미분방정식","contents":"定義1 以下の微分方程式をルジャンドルLegendre微分方程式と言う。\n$$ (1-x^2)\\dfrac{d^2 y}{dx^2} -2x\\dfrac{dy}{dx}+l(l+1) y=0 $$\nルジャンドル微分方程式の解をルジャンドル多項式と言い、通常$P_{l}(x)$で示される。最初のいくつかの$l$によるルジャンドル多項式は次のようである。\n$$ \\begin{align*} P_{0}(x) =\u0026amp;\\ 1 \\\\ P_{1}(x) =\u0026amp;\\ x \\\\ P_2(x) =\u0026amp;\\ \\dfrac{1}{2}(3x^2-1) \\\\ P_{3}(x) =\u0026amp;\\ \\dfrac{1}{2}(5x^3-3x) \\\\ P_{4}(x) =\u0026amp;\\ \\dfrac{1}{8}(35x^4-30x^2+3) \\\\ P_{5}(x) =\u0026amp;\\ \\dfrac{1}{8}(63x^5-70x^3+15x) \\\\ \\vdots\u0026amp; \\end{align*} $$\n説明 ルジャンドル微分方程式は、次のような形で紹介されることもある。\n$$ \\dfrac{d}{dx}\\left[ (1-x)^2 \\dfrac{dy}{dx} \\right] +l(l+1)y=0 $$\nこれはシュツルム-リウヴィル理論Sturm-Liouville theoryで表されるものである。第一項を展開して整理すると、同じ式が得られる。ルジャンドル微分方程式を以下のように一般化したものを関連ルジャンドル微分方程式associated Legendre differential equationと言う。\n$$ (1-x^2)\\dfrac{d^2 y}{dx^2} -2x\\dfrac{dy}{dx}+\\left( \\dfrac{-m^2}{1-x^2} +l(l+1) \\right) y=0 $$\nここで$m=0$の場合、ルジャンドル微分方程式となる。\nルジャンドル方程式は物理学や工学などで登場し、特に球面座標系でのラプラス方程式を解く時に見ることができる。物理学科ならば、電磁気学で球面座標系での電位を計算する時、量子力学で球面座標系でのシュレディンガー方程式を解く時に出会うことがある。解法の過程が長いため、教科書では通常、ロドリゲス公式で表される解答のみを記載することが多い。実際、物理学の学生は解法が非常に非常に気になるわけではなければ、知らなくても問題はない。\n解法 係数に独立変数$x$が含まれた形で、解が冪級数の形であると仮定すれば解くことができる。\n$$ \\begin{equation} (1-x^2)y^{\\prime \\prime} -2xy^{\\prime}+l(l+1) y=0 \\label{1} \\end{equation} $$\nルジャンドル微分方程式の解を次のように仮定しよう。\n$$ y=a_{0}+a_{1}(x-x_{0})+a_2(x-x_{0})^2+\\cdots=\\sum \\limits_{n=0}^\\infty a_{n}(x-x_{0})^n $$\nこの時$x=0$の時、$y^{\\prime \\prime}$の係数が$(1-x^2)|_{x=0}=1\\ne 0$であるため、$x_{0}=0$と置く。すると級数解は\n$$ \\begin{equation} y=a_{0}+a_{1}x+a_2x^2+\\cdots=\\sum \\limits_{n=0}^\\infty a_{n}x^n \\label{2} \\end{equation} $$\n解を級数と仮定したが、解法の最後に実際には$y$の項が有限であることがわかる。これで$\\eqref{1}$に代入するために$y^{\\prime}$と$y^{\\prime \\prime}$を求めよう。\n$$ y^{\\prime}=a_{1}+2a_2x+3a_{3}x^2+\\cdots=\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1} $$\n$$ y^{\\prime \\prime}=2a_2+3\\cdot 2a_{3}x+4\\cdot 3 a_{4}x^2 +\\cdots = \\sum \\limits_{n=2} n(n-1)a_{n}x^{n-2} $$\nこれで$\\eqref{1}$に$y, y^{\\prime}, y^{\\prime \\prime}$を代入すると\n$$ (1-x^2)\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -2x\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1}+l(l+1) \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\n第一項の係数$(1-x^2)$の括弧を展開して整理すると\n$$ \\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -x^2\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n-2} -2x\\sum \\limits_{n=1}^\\infty na_{n}x^{n-1}+l(l+1) \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\n$$ \\implies \\sum \\limits_{n=2} ^\\infty n(n-1)a_{n}x^{n-2} -\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n} -2\\sum \\limits_{n=1}^\\infty na_{n}x^{n}+l(l+1) \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\nここでのポイントは**$x$の次数を合わせること**である。他は全て$x^n$で表されるのに対し、最初の級数だけが$x^{n-2}$で表されているため、$n$の代わりに$n+2$を代入すると\n$$ \\sum \\limits_{n=0} ^\\infty (n+2)(n+1)a_{n+2}x^{n} -\\sum \\limits_{n=2}^\\infty n(n-1)a_{n}x^{n} -2\\sum \\limits_{n=1}^\\infty na_{n}x^{n}+l(l+1) \\sum \\limits_{n=0}^\\infty a_{n}x^n=0 $$\n二番目の級数が$x^2$項から始まるので、他の級数から$n=0,1$の項を外して、定数項は定数項同士、1次項は1次項同士をまとめると\n$$ \\left[ 2\\cdot 1 a_2+l(l+1)a_{0} \\right]+\\left[ 3\\cdot 2 a_{3}-2a_{1}+l(l+1)a_{1} \\right]x \\\\ + \\sum \\limits_{n=2}^\\infty \\left[ (n+2)(n+1)a_{n+2}-n(n+1)a_{n}-2na_{n}+l(l+1)a_{n} \\right] x^n=0 $$\n上の式が成り立つためには全ての係数が$0$でなければならない。\n$$ 2\\cdot 1 a_2+l(l+1)a_{0} =0 $$\n$$ 3\\cdot 2 a_{3}-2a_{1}+l(l+1)a_{1} =0 $$\n$$ (n+2)(n+1)a_{n+2}-n(n+1)a_{n}-2na_{n}+l(l+1)a_{n}=0 $$\nそれぞれを整理すると\n$$ \\begin{equation} a_2=-\\dfrac{l(l+1)}{2 \\cdot 1}a_{0} \\label{3} \\end{equation} $$\n$$ \\begin{equation} a_{3}=-\\dfrac{(l+2)(l-1)}{3\\cdot 2} a_{1} \\label{4} \\end{equation} $$\n$$ \\begin{equation} a_{n+2}=-\\dfrac{(l+n+1)(l-n)}{(n+2)(n+1)}a_{n} \\label{5} \\end{equation} $$\n$\\eqref{3}, \\eqref{4}, \\eqref{5}$を利用すると、$a_{0}$と$a_{1}$の値だけを知っていれば全ての係数を知ることができる。$\\eqref{3}$と$\\eqref{5}$で偶数次項の係数を求めると\n$$ \\begin{align*} a_{4} =\u0026amp;\\ - \\dfrac{(l+3)(l-2)}{ 4 \\cdots 3}a_2 = \\dfrac{l(l-2)(l+1)(l+3)}{4!}a_{0} \\\\ a_{6} =\u0026amp;\\ -\\dfrac{(l+5)(l-4)}{6\\cdot5} a_{4} = -\\dfrac{ l(l-2)(l-4)(l+1)(l+3)(l+5)}{6!} a_{0} \\\\ \\vdots\u0026amp; \\end{align*} $$\n$n=2m\\ (m=1,2,3,\\cdots)$とすると\n$$ a_{n}=a_{2m}=(-1)^m \\dfrac{l(l-2)\\cdots (l-2m+4)(l-2m+2)(l+1)(l+3)\\cdots(l+2m-3)(l+2m-1)}{(2m)!}a_{0} $$\n同様に$\\eqref{4}$、$\\eqref{5}$で奇数次項の係数を求めると\n$$ \\begin{align*} a_{5} =\u0026amp;\\ -\\dfrac{(l+4)(l-3)}{5\\cdot 4}a_{3} = \\dfrac{(l+2)(l+4)(l-1)(l-3)}{5!}a_{1} \\\\ a_{7} =\u0026amp;\\ -\\dfrac{(l+6)(l-5)}{7\\cdot 6}a_{5} = -\\dfrac{(l+2)(l+4)(l+6)(l-1)(l-3)(l-5)}{7!}a_{1} \\\\ \\vdots\u0026amp; \\end{align*} $$\n$n=2m+1\\ (m=1,2,3,\\cdots)$とすると\n$$ a_{n}=a_{2m+1}=(-1)^m\\dfrac{(l+2)(l+4)\\cdots(l+2m-2)(l+2m)(l-1)(l-3)\\cdots(l-2m+3)(l-2m+1)}{(2m+1)!}a_{1} $$\nこれで求めた係数を$\\eqref{2}$に代入して解を求めると\n$$ \\begin{align*} y =\u0026amp;\\a_{0}+a_{1}x -\\dfrac{l(l+1)}{2!}a_{0}x^2-\\dfrac{(l+2)(l-1)}{3!}a_{1}x^3 + \\dfrac{l(l-2)(l+1)(l+3)}{4!}a_{0}x^4+\\dfrac{(l+2)(l+4)(l-1)(l-3)}{5!}a_{1}x^5 \\\\ \u0026amp;+ \\cdots +(-1)^m \\dfrac{l(l-2)\\cdots (l-2m+4)(l-2m+2)(l+1)(l+3)\\cdots(l+2m-3)(l+2m-1)}{(2m)!}a_{0}x^{2m} \\\\ \u0026amp;+ (-1)^m\\dfrac{(l+2)(l+4)\\cdots(l+2m-2)(l+2m)(l-1)(l-3)\\cdots(l-2m+3)(l-2m+1)}{(2m+1)!}a_{1}x^{2m+1} +\\cdots \\end{align*} $$\n$(m=1,2,3,\\cdots)$偶数次項は$a_{0}$で、奇数次項は$a_{1}$でまとめると\n$$ \\begin{align*} y =\u0026amp;\\a_{0}\\left[1-\\dfrac{l(l+1)}{2!}x^2+\\dfrac{l(l-2)(l+1)(l+3)}{4!}x^4 \\right. \\\\ \u0026amp;\\left.+\\sum \\limits_{m=3}^\\infty (-1)^m \\dfrac{l(l-2)\\cdots (l-2m+4)(l-2m+2)(l+1)(l+3)\\cdots(l+2m-3)(l+2m-1)}{(2m)!} x^{2m} \\right] \\\\ \u0026amp;+ a_{1}\\left[x- \\dfrac{(l+2)(l-1)}{3!}x^3+\\dfrac{(l+2)(l+4)(l-1)(l-3)}{5!}x^5 \\right. \\\\ \u0026amp; \\left. +\\sum \\limits_{m=3}^\\infty (-1)^m\\dfrac{(l+2)(l+4)\\cdots(l+2m-2)(l+2m)(l-1)(l-3)\\cdots(l-2m+3)(l-2m+1)}{(2m+1)!} x^{2m+1} \\right] \\end{align*} $$\n最初の括弧を$y_{0}$、二番目の括弧を$y_{1}$とすると、ルジャンドル方程式の一般解は次のようになる。\n$$ y=a_{0}y_{0}+a_{1}y_{1} $$\n二つの級数$y_{0}$と$y_{1}$は比率判定法により、$|x|\u0026lt;1$の範囲で収束\nすることがわかる。$\\eqref{5}$により$\\dfrac{a_{n+2}}{a_{n}}=-\\dfrac{(l+n+1)(l-n)}{(n+2)(n+1)}=\\dfrac{(n+l+1)(n-l)}{(n+2)(n+1)}$であるため、比率判定法を使うと\n$$ \\lim \\limits_{n \\rightarrow \\infty} \\dfrac{(n+l+1)(n-l)}{(n+2)(n+1)}x^2=x^2\u0026lt;1 $$\n$$ \\implies -1\u0026lt;x\u0026lt;1 $$\nしかし、多くの問題で$x=\\cos \\theta$、$l$は非負の整数の形で式が現れ、全ての$\\theta$に対して収束する解を得たい。つまり、$x=\\pm 1$でも収束する解を見つけることが目標である。幸いにも$l$が整数の時は、欲しい解が存在し、その時$l$の値によって必ず$y_{0}, y_{1}$のどちらかの解のみが存在する。$l$が$0$か偶数の時は$y_{1}$が発散し、$y_{0}$は偶数次項のみを持つ有限項の多項式となる。$l$が奇数ならば$y_{0}$が発散し、$y_{1}$は奇数次項のみを持つ有限項の多項式となる。表にまとめると以下のようになる。\n$l$の値 $y_{0}$ $y_{1}$ 方程式の解 $0$か偶数 有限項の多項式 発散 $y=a_{0}y_{0}$ 奇数 発散 有限項の多項式 $y=a_{1}y_{1}$ ケース1. $l$が$0$か偶数\n$l=0$の時、2次項から$l$を因数に持ち、全て$0$になるので、$y_{0}=1$\n$l=2$の時、4次項から$(l-2)$を因数に持ち、全て$0$になるので、$y_{0}=1-3x^2$\n$l=4$の時、6次項から$(l-4)$を因数に持ち、全て$0$になるので、$y_{0}= 1-10x^2+\\dfrac{35}{3}x^4$\nそして$l=0$の時、$x^2=1$から$y_{1}=1+\\frac{1}{3}+\\frac{1}{5}+\\cdots$であるが、これは積分判定法により発散する。他の偶数の時も同様である。したがって、$l$が$0$か偶数の時は、解が偶数次項のみを持つ有限項の多項式となる。つまり、級数$y_{0}$の特定の項までのみ残る形の解を得る。\nケース2. $l$が奇数\n偶数の時と反対の結果が現れる。\n$l=1$の時、3次項から$(l-1)$を因数に持ち、全て$0$になるので、$y_{1}=x$\n$l=3$の時、5次項から$(l-3)$を因数に持ち、全て$0$になるので、$y_{1}=x-\\dfrac{5}{3}x^3$\n$l=5$の時、7次項から$(l-5)$を因数に持ち、全て$0$になるので、$y_{1}=x-\\dfrac{14}{3}x^3+\\dfrac{21}{5}x^5$\n$l=1$の時、$x^2=1$から$y_{0}$は発散し、他の奇数の時も同様である。したがって、$l$が奇数の時は、解が奇数次項のみを持つ有限項の多項式となる。つまり、級数$y_{1}$の特定の項までのみ残る形の解を得る。\nそして、$l$が負の場合は、$l$が0ではない整数の場合と同じであることが$y_{0}$と$y_{1}$を見ればわかる。例えば、$l=2$の場合と$l=-3$の場合が同じであり、$l=1$の場合と$l=-2$の場合が同じである。したがって、$l$が非負の整数についてのみ考えれば良い。$a_{0}$と$a_{1}$の値を上手く選んで$x=1$の時の解が$y(x)=1$になるようにすると、これをルジャンドル多項式Legendre polynomialと言い、$P_{l}(x)$と書く。最初のいくつかのルジャンドル多項式は以下の通りである。\n$$ \\begin{align*} P_{0}(x) =\u0026amp;\\ 1 \\\\ P_{1}(x) =\u0026amp;\\ x \\\\ P_2(x) =\u0026amp;\\ \\dfrac{1}{2}(3x^2-1) \\\\ P_{3}(x) =\u0026amp;\\ \\dfrac{1}{2}(5x^3-3x) \\\\ P_{4}(x) =\u0026amp;\\ \\dfrac{1}{8}(35x^4-30x^2+3) \\\\ P_{5}(x) =\u0026amp;\\ \\dfrac{1}{8}(63x^5-70x^3+15x) \\end{align*} $$\nこの結果はロドリゲス公式Rodrigues\u0026rsquo; formulaで直接得ることもできる。\n■\nMary L. Boas, 数理物理学(Mathematical Methods in the Physical Sciences, 최준곤 訳) (3rd Edition, 2008), p577-580\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":889,"permalink":"https://freshrimpsushi.github.io/jp/posts/889/","tags":null,"title":"ルジャンドル微分方程式の直列解法：ルジャンドル多項式"},{"categories":"해석개론","contents":"要約 $$ \\dfrac{d}{dx} (fg)=\\dfrac{df}{dx}g+f\\dfrac{dg}{dx} $$\n$$ \\begin{align*} \\dfrac{d^n}{dx^n}(fg)\u0026amp;=\\sum \\limits_{k=0}^{n}\\frac{n!}{(n-k)!k!}\\dfrac{d^{n-k}f}{dx^{n-k}}\\dfrac{d^k g}{dx^k} \\\\ \u0026amp;=\\sum \\limits_{k=0}^{n}{}_{n}\\mathrm{C}_{k} \\dfrac{d^{n-k}f}{dx^{n-k}}\\dfrac{d^k g}{dx^k} \\\\ \u0026amp;=\\sum \\limits_{k=0}^{n} \\binom{n}{k} \\dfrac{d^{n-k}f}{dx^{n-k}}\\dfrac{d^k g}{dx^k} \\end{align*} $$\n説明 ライプニッツの法則Leibniz\u0026rsquo;s ruleとしても知られている。\n最初の式は、微分の積の法則、または積の規則としてよく知られた式だ。二つの関数の積を一度微分したときの結果を簡単に表したものだ。ここでさらに一般化して$n$回微分したときの結果を示すのが下の式だ。多項式は繰り返し微分されると$0$になる可能性があるため、直接$n$回微分せずに簡単に結果を計算できる。\nこれ以外にも、ライプニッツの名がついた微分と積分に関する定理や公式が多数ある。\n証明 $D$を次のような微分演算子としよう。\n$$ D=\\dfrac{d}{dx} $$\n例えば$Df(x)=\\dfrac{df(x)}{dx}$だ。$D$を使って$fg$の微分を表すと、以下のようになる。\n$$ \\dfrac{d}{dx}(fg)=gDf+fDg $$\nこの時$D_{f}$を$f$にのみ適用される演算子とし、$D_{g}$を$g$にのみ適用される演算子としよう。すると、上の式は以下のように表される。\n$$ (D_{f}+D_{g})(fg)=gDf+fDg $$\nすると次を得る。\n$$ \\dfrac{d}{dx}(fg)=(D_{f}+D_{g})(fg) $$\n$$ \\dfrac{d^2}{dx^2}(fg)=D(D_{f}+D_{g})(fg) $$\nこの時$D$は微分演算子なので、操作の順序は関係ない。つまり$DD_{f}f=D_{f}Df$という意味だ。すると、上の式は以下のようになる。\n$$ \\begin{align*} \\dfrac{d^2}{dx^2}(fg) \u0026amp;= D(D_{f}+D_{g})(fg) \\\\ \u0026amp;= (D_{f}+D_{g})D(fg) \\\\ \u0026amp;= (D_{f}+D_{g})(D_{f}+D_{g})(fg) \\\\ \u0026amp;= (D_{f}+D_{g})^2 (fg) \\end{align*} $$\n上で話したように、$D$は積の交換が成立するので、最後の行のように表現できる。微分回数を$n$回に拡張すると、次のようになる。\n$$ \\dfrac{d^n}{dx^n} (fg)=(D_{f}+D_{g})^n(fg) $$\n交換法則が成立するので、二項定理を適用できる。二項定理を使うと、次を得る。\n$$ \\begin{align*} \\dfrac{d^n}{dx^n} (fg) \u0026amp;= (D_{f}+D_{g})^n(fg) \\\\ \u0026amp;= \\sum \\limits_{k=0} ^n {}_{n}\\mathrm{C} _{k} {D_{f}}^{n-k} {D_{g}}^{k}(fg) \\\\ \u0026amp;=\\sum \\limits_{k=0} ^n {}_{n}\\mathrm{C} _{k} {D_{f}}^{n-k} f{D_{g}}^{k}g \\\\ \u0026amp;= \\sum \\limits_{k=0} ^n {}_{n}\\mathrm{C} _{k} \\dfrac{d^{n-k}f}{dx^{n-k}} \\dfrac{d^k g}{dx^k} \\\\ \u0026amp;= \\sum \\limits_{k=0} ^n {}_{n}\\mathrm{C} _{k} f^{(n-k)} g^{(k)} \\end{align*} $$\n■\n例 1 $\\dfrac{d^7}{dx^7}( x \\sin x)$を求めよ。 $x$、$\\sin x$をそれぞれ上の証明の$g$、$f$とすると、ライプニッツの法則により\n$$ \\dfrac{d^7}{dx^7}( x \\sin x)=\\sum \\limits_{k=0} ^7 {}_{7} \\mathrm{C}_{k} \\dfrac{d ^{n-k} } {dx^{n-k} }(\\sin x) \\dfrac{d^k}{dx^k} (x) $$\nこの時$k \\ge 2$の場合、$\\dfrac{d^k}{dx^k}(x)=0$であるため、$k=0,1$の二項だけが残る。したがって、\n$$ \\begin{align*} \\dfrac{d^7}{dx^7} ( x \\sin x ) \u0026amp;= {}_{7} \\mathrm{C} _{0} \\dfrac{d^7}{dx^7}(\\sin x) x + {}_{7}\\mathrm{C}_{1} \\dfrac{d^6}{dx^6} (\\sin x) \\\\ \u0026amp;= -x \\cos x -7\\sin x \\end{align*} $$\n■\n2 $\\dfrac{d^{10}}{dx^{10}} ( x^2 e^{-x} )$を求めよ。 $x^2$、$e^{-x}$をそれぞれ上の証明の$g$、$f$とすると、ライプニッツの法則により\n$$ \\dfrac{d^{10}}{dx^{10}} (x^2 e^{-x}) = \\sum \\limits _{k=0} ^{10} {}_{10} \\mathrm{C} _{k} \\dfrac{d^{10-k}}{dx^{10-k}}(e^{-x}) \\dfrac{d^k}{dx^k} ( x^2) $$\nこの時、$k \\ge 3$の場合は$\\dfrac{d^k}{dx^k} (x^2)=0$であるため、$k=0,1,2$の三項だけが残る。したがって、\n$$ \\begin{align*} \\dfrac{d^{10} } {dx^{10} } (x^2 e^{-x}) \u0026amp;= {}_{10} \\mathrm{C}_{0} \\dfrac{d^{10}}{dx^{10}} (e^{-x}) x^2 + {}_{10} \\mathrm{C} _{1} \\dfrac{d^9}{dx^9}(e^{-x}) \\dfrac{d}{dx}(x^2) + {}_{10}\\mathrm{C}_2 \\dfrac{d^8}{dx^8} ( e^{-x} ) \\dfrac{d^2}{dx^2} (x^2) \\\\ \u0026amp;= x^2 e^{-x} -20 x e^{-x} + 90e^{-x} \\end{align*} $$\n■\n参照 ライプニッツの積分法則 ","id":884,"permalink":"https://freshrimpsushi.github.io/jp/posts/884/","tags":null,"title":"ライプニッツの定理の証明"},{"categories":"확률론","contents":"定義 確率変数 $X: \\Omega \\to E$ の値域を状態空間という。 確率変数の集合 $\\left\\{ X_{t} \\mid t \\in [ 0 , \\infty ) \\right\\}$ を連続的確率過程という。 確率変数の数列 $\\left\\{ X_{n} \\mid n = 0, 1, 2, \\cdots \\right\\}$ を離散的確率過程という。 説明 過程Processという言葉が含まれているため、確率過程を理解するのは難しい、典型的には言葉が難しいために難しい概念だ。「プロセス」とは通常、あるアルゴリズムや、言葉そのままの「過程」を意味するため、上の定義と全く合わないためである。高校を卒業すると、数列を「定義域が自然数の関数」と定義するため、「確率変数の数列」や「確率変数の集合」という説明を敢えてする。\n関数？数列？集合？ この意味で確率過程とは結局のところ、「時間的な変数 $t$ か $n$ に対して確率変数を対応させる関数」である。重要なのは結局「いつ、どのように確率が出るか？」であり、集合だの数列だの複雑に考える必要はない。人々は今日雨が降る確率も気になるし、明日の雨の確率も気になるし、明後日の雨の確率も気になる。今日をD+0、明日をD+1、明後日をD+2として、$p ( X_{n} = \\text{ rain } )$ をD+nの降水確率と表せるなら、確率過程の概念を素晴らしく理解したことになる。\n確率過程論は、その性質上、多くの学問分野で様々なレベルで学ばれるため、教科書によって言葉が異なる。少なくとも確率情報論を学び始めるときは、正確な定義よりも直観的な概念をうまく受け入れることがさらに重要だ。\n例 ギャンブル 例として、コイン投げゲームCash Processを考えてみよう。このゲームでは、コインを投げて表が出れば$1$ポイントを得て、裏が出れば$1$ポイントを失う。プレイヤーが終了を宣言した時点でゲームは終わり、最後にスコアが正ならばポイント一つにつき千円を受け取り、負ならばポイント一つにつき千円を支払わなければならないゲームである。スコアは$0$ポイントから始まる。\nまずこのゲームの状態空間はプレイヤーのスコアであり、整数の集合$\\left\\{ \\cdots, (- 2) , (-1) , 0 , 1 , 2 , \\cdots \\right\\}$になるだろう。プレイヤーが$n$回コインを投げた時のスコアが$x$ポイントである確率は$p( X_{n} = x)$のように表せる。特に、コインを一度も投げていない場合、私のスコアは必ず$0$ポイントであり、$p ( X_{0} = 0 ) = 1$を確信することができる。\nここで$X_{1}$は$(-1)$か$1$であり、$X_{2}$は$(-2) , 0 , 2$のうちの1つであることが確実だ。このように、試行回数$n$が変わると、確率変数$X_{n}$も変わっている。\n上述のゲームのシミュレーションを行うと、スコアの波は上のようにランダムに現れる。このようなランダムな波をブラウン運動Brownian Motionという。$10$回繰り返し、それでやめたなら2千円の賞金を手にしていただろうし、$400$回くらいでやめたならかなりの損失があっただろうし、$900$回くらいでやめたならかなりの大金を手にしていただろう。\n確率過程論は「では、いつやめるのがよいか」に対する答えも提供することができる。適切な目標を達成した時、次の機会はいつか、どれほどのリスクを負うかについても誰もが疑問に思う。\n株 もう一つの例は株式である。\nもちろん、株は完全にランダムではない。しかし、上のようなチャートを見て1年後の動向を予測するのは非常に難しい。確率過程論を勉強することは、チャーチストになることではなく、むしろその逆である。何が価格の変動に影響を与えるかを把握し、迅速に情報を取得し、正確なモデルを作成し、それを自分だけが知っていれば、一生懸命にならずとも生計を立てることができる（もちろん、不可能だが）。\n一方、数学的には、確率過程は非決定論的な動力学系とも見なすことができる。\nコード 以下はRを通じてキャッシュプロセスをシミュレーションした例のコードである。\nset.seed(150421)\rtoss\u0026lt;-sample(c(-1,1),10,replace=T)\rwin.graph(4,4)\rplot(cumsum(toss),type=\u0026#39;l\u0026#39;,main=\u0026#39;10회 반복\u0026#39;)\rabline(h=0)\rtoss\u0026lt;-sample(c(-1,1),1000,replace=T)\rwin.graph(4,4)\rplot(cumsum(toss),type=\u0026#39;l\u0026#39;,main=\u0026#39;1000회 반복\u0026#39;)\rabline(h=0) ","id":857,"permalink":"https://freshrimpsushi.github.io/jp/posts/857/","tags":["R"],"title":"確率過程とは何か？"},{"categories":"해석개론","contents":"定義1 数列 $\\left\\{ a_{n} \\right\\}$が与えられたとしよう。そして、以下の記法を定義しよう。\n$$ \\sum \\limits_{n=p}^{q} a_{n} = a_{p} + a_{p+1} + \\cdots + a_{q}\\quad (p \\le q) $$\n$\\left\\{ a_{n} \\right\\}$の部分和partial sum $s_{n}$を次のように定義する。\n$$ s_{n} = \\sum \\limits_{k=1}^{n} a_{k} $$\nそうすると、$s_{n}$の数列 $\\left\\{ s_{n} \\right\\}$を考えることができる。数列 $\\left\\{ s_{n} \\right\\}$の極限を$\\left\\{ a_{n} \\right\\}$の無限級数infinite series、または単に級数と呼び、次のように記される。\n$$ \\sum \\limits_{n = 1}^{\\infty} a_{n} = \\lim \\limits_{n \\to \\infty} s_{n} = \\lim\\limits_{n \\to \\infty}\\sum \\limits_{k=1}^{n} a_{k} $$\n$\\left\\{ s_{n} \\right\\}$が$s$に収束する場合、次のように示し、級数が収束するという。\n$$ \\sum \\limits_{n = 1}^{\\infty} a_{n} = s $$\n$\\left\\{ s_{n} \\right\\}$が収束しない場合、級数が発散するという。級数が発散する場合に、\nすべての$M \\in \\mathbb{R}$に対し、$n \\ge N \\implies s_{n} \u0026gt; M$を満たす$N \\in \\mathbb{N}$が存在する場合 $$ \\sum \\limits_{n = 1}^{\\infty} a_{n} = \\infty $$ と記される。\nすべての$M \\in \\mathbb{R}$に対し、$n \\ge N \\implies x_{n} \u0026lt; M$を満たす$N \\in \\mathbb{N}$が存在する場合 $$ \\sum \\limits_{n = 1}^{\\infty} a_{n} = -\\infty $$ と記される。\n説明 級数は、無限に多くの項を加えるという曖昧な概念を数学的に厳密に定義したものである。$\\sum a_{n}$のように単純に記されることもある。\nWalter Rudin, Principles of Mathmatical Analysis (3rd Edition, 1976), p59\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":886,"permalink":"https://freshrimpsushi.github.io/jp/posts/886/","tags":null,"title":"級数、無限級数"},{"categories":"동역학","contents":"定義1 定義域と値域が同じ関数$f : X \\to X$をマップと言う。$f$を$k$回合成したマップを$f^{k}$と表す。 $f(p) = p$を満たす$p \\in X$を固定点と言う。 全ての$x \\in N_{ \\epsilon } ( p )$に対して$\\displaystyle \\lim_{k \\to \\infty} f^{k} (x) = p$を満たす$\\epsilon \u0026gt; 0$が存在する場合、固定点$p$をシンクとする。 $p$を除く全ての$x \\in N_{\\epsilon } (p)$に対して$f^{ \\infty } (x) \\notin N_{\\epsilon } (p)$を満たす$\\epsilon \u0026gt; 0$が存在する場合、固定点$p$をソースとする。 $N_{ \\epsilon } ( p ) = B ( p ; \\epsilon )$は、$p$の半径$\\epsilon$内にある全ての点を含むネイバーフッドを意味する。 例 $X$で定義されたマップは、各点$x_{t-1}$を$x_{t}$へマッピングすることで動力学系を形成する。例えば、時間$t$が$1$だけ変わるたびに、$60$だけ$x$方向へ移動する点がある場合、この点の位置は次のように表される。 $$ x_{t} = f(x_{t-1}) = x_{t-1} + 60 $$ 別のマップの例として$f(x) = x^3$を考えると、$$f(0) = 0 \\\\ f( \\pm 1) = \\pm 1$$であるので、$0$と$\\pm 1$は固定点である。 特に$0$を含む十分に小さい区間$( - 1, 1)$の全ての数は、二乗するたびに小さくなり、最終的には$0$に収束するので、シンクである。 $\\pm 1$を含むどんな区間を考えても、その大きさが$1$より大きい数は、三乗するたびにその大きさが大きくなるので、ソースである。 シンクは近くの点が集まる一種の「収束点」、ソースは近かった点が徐々に離れていく一種の「発散点」と見ることができる。だから、シンクを安定した固定点、ソースを不安定な固定点とも呼ぶ。\nこれはグラフ理論のシンク、ソースと似ている。\n参照 マップによって表される動力学系 微分方程式によって表される動力学系 動力学系の厳密な定義 Yorke. (1996). CHAOS: An Introduction to Dynamical Systems: p5, 9.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":856,"permalink":"https://freshrimpsushi.github.io/jp/posts/856/","tags":null,"title":"地図で表される動力学系と不動点"},{"categories":"통계적검정","contents":"仮設検定 ロジスティック回帰分析で得られたモデルを$M$としよう。\n$H_{0}$：$M$は適切だ。 $H_{1}$：$M$は適切ではない。 説明 ホスマー・レメショー適合度検定はロジスティック回帰モデルの適合性を判断する代表的な仮説検定だ。\nとても単純なテストでもあるが、帰無仮説と対立仮説が混同されがち。仮説検定に良し悪しはないのは事実だが、正直に言うと回帰分析を行うのはどんな相関関係を理解するためだから、通常はF検定の帰無仮説を棄却したいと思う。この点はt検定でも同じで、ロジスティック回帰分析を学ぶほどに回帰分析に慣れた学習者は「p値が小さいことが成功だ」という直感的でない直感を持つようになる。\nだからちゃんと分析したのに、ホスマー・レメショー適合度検定の結果がその「直感」と違って戸惑うことがある。「だから帰無仮説が何で、対立仮説が何か、正確にチェックする必要がある。\n注意事項 最近ではホスマー・レメショー適合度検定の弱点が指摘され、あまり推奨されないと言われている1。\n関連項目 ロジスティック回帰分析 Rでホスマー・レメショー適合度検定をする方法 https://twitter.com/f2harrell/status/1228423023834718208\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":852,"permalink":"https://freshrimpsushi.github.io/jp/posts/852/","tags":null,"title":"ホスマー・レメショー適合度検定"},{"categories":"추상대수","contents":"定義 1 整域 $D$ で以下の二つの条件を満たす ユークリッドノルム $\\nu : D \\setminus \\left\\{ 0 \\right\\} \\to \\mathbb{N}_{0}$ が存在する場合、$D$ をユークリッド整域と言う。\n(i): すべての $a,b \\in D (b \\ne 0 )$ に対して $$ a = bq + r $$ を満たす $q$ と $r$ が存在する。この時、$r = 0$ または $\\nu (r) \u0026lt; \\nu (b)$ のどちらかでなければならない。 (ii): すべての $a,b \\in D (b \\ne 0 )$ に対して $\\nu ( a ) \\le \\nu ( ab )$ $\\mathbb{N}_{0}$ は自然数の集合に $0$ を含む集合を意味する。 定理 ユークリッド整域 $D$ の単位元を $0$、単位元を $1$, ユークリッドノルムを $\\nu$ としよう。\n[3]: $0$ ではないすべての $d \\in D$ に対して $\\nu (1) \\le \\nu (d)$ [4]: $u \\in D$ は単位元 $\\iff$ $\\nu ( u ) = \\nu (1)$ PIDは主理想整域を、UFDは一意分解整域を指す。 単位元は乗算の単位元 $1$ であり、単位元は乗算の逆元を持つ元である。 説明 「ユークリッド整域」という言葉はそれほど長くないが、通常EDという略称がよく使われる。\n条件 (i) と (ii) は整数環 $\\mathbb{Z}$ では自然に満たされている条件で、ユークリッドノルム $\\nu ( n ) := | n |$ が存在して $\\mathbb{Z}$ はユークリッド整域になる。もともとユークリッドノルムという言葉自体が数論のユークリッドの互除法から来ているのだ。\n一方、体 $F$ に対して $F [ x ]$ を考えると、ユークリッドノルム $\\nu ( f(x) ) : = \\deg ( f(x) )$ を定義することによってユークリッド整域になる。もともと割り算の定理がこの条件に該当する。\n上のように様々な整域を図示すると、EDがどれだけ多くの良い性質を持っているか簡単にわかる。\n証明 1 $D$ のイデアルを $N$ としよう。\n$N = \\left\\{ 0 \\right\\} = \\left\u0026lt; 0 \\right\u0026gt;$ は自然に主理想なので、$N \\ne \\left\\{ 0 \\right\\}$ を考えよう。\n$0$ ではないすべての $n \\in N$ に対して、 $$ \\nu (b) \\le \\nu (n) $$ を満たす $b \\ne 0$ を見つけることができる。これを$a \\in N$ とすると、条件 (i) により $$ a = b q + r $$ を満たす $q,r \\in D$ が存在しなければならない。$N = Nq$ はイデアルなので、$r = a - bq$ も $N$ に存在する元であることがわかる。$b$ は$\\nu (b)$ を最小にする元だったので、条件 (ii) により $r=0$ でなければならない。すべての元 $a \\in N$ が $a = bq$ として表されるということはつまり $N = \\left\u0026lt; b \\right\u0026gt;$ であり、すべてのイデアル $N$ は主理想である。\n■\n2 EDはPIDであり、PIDはUFDなので、EDもUFDである。\n■\n[3] 条件 (ii) により $$ \\nu (1) \\le \\nu ( 1 d) = \\nu (d) $$\n■\n[4] $( \\implies )$\n$u$ が単位元なので、その逆元 $u^{-1}$ が存在して $$ \\nu ( u ) \\le \\nu ( u u^{-1} ) = \\nu (1) $$ そして、定理 [3] により $\\nu (1) \\le \\nu (1)$ よって $$ \\nu ( u ) = \\nu (1) $$\n$( \\impliedby )$\n$1 = uq + r$ とすると、定理 2 により、$\\nu ( u) = \\nu (1)$ は $\\nu (0)$ を除いて最小である。定理 [3] により、$\\nu ( r) \u0026lt; \\nu (u)$ を満たす場合は $r=0$ のみで、$1 = uq$ となり、$u$ は単位元となる。\n■\n参照 ユークリッド整域 $\\implies$ 主理想整域 $\\implies$ 一意分解整域 $\\implies$ 整域 ユークリッド整域 $\\implies$ 主理想整域 $\\implies$ ネーター環 Fraleigh. (2003). 「抽象代数入門(第7版)」: p401.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":838,"permalink":"https://freshrimpsushi.github.io/jp/posts/838/","tags":null,"title":"ユークリッド幾何学"},{"categories":"통계적분석","contents":"ビルドアップ $Y \\gets X_{1} , \\cdots, X_{p}$ をやってみようと思う。ここで、$Y$ は質的変数で、中でもクラスが2つしかない場合がある。例えば、男性と女性、成功と失敗、陽性と陰性、$0$ と$1$ などがあり、便宜上、単に$Y=0$ や$Y=1$ と呼ぼう。このように従属変数が2値の場合、興味があるのは\u0026rsquo;独立変数 $ X_{1} , \\cdots X_{p}$ を見たときに$Y$ が何か\u0026rsquo;である。\nしかし、$Y$ は質的変数なので、通常の回帰分析とは異なり、回帰係数と変数の線形結合 $y = \\beta_{0} + \\beta_{1} x_{1} + \\cdots \\beta_{p} x_{p}$ で表現することができない。そのため、$Y=1$ になる確率を計算する方向でアプローチしようとする。\n与えられた $X=x$ に対して、$Y=1$ になる確率を以下のように設定する。 $$\\displaystyle \\pi := P ( Y = 1 | X = x ) = {{ e^{ \\beta_{0} + \\beta_{1} x_{1} + \\cdots \\beta_{p} x_{p}} } \\over { 1+ e^{ \\beta_{0} + \\beta_{1} x_{1} + \\cdots \\beta_{p} x_{p}} }}$$\n(i) 指数関数は常に $0$ より大きく、$\\pi$ の分母が分子より大きいので $ 0 \u0026lt; \\pi \u0026lt; 1$ である。 (ii) 自然に、$Y = 0$になる確率は $$ \\begin{align*} 1 - \\pi =\u0026amp; P ( Y = 0 | X = x ) \\\\ =\u0026amp; 1 - {{ e^{ \\beta_{0} + \\beta_{1} x_{1} + \\cdots \\beta_{p} x_{p}} } \\over { 1+ e^{ \\beta_{0} + \\beta_{1} x_{1} + \\cdots \\beta_{p} x_{p}} }} \\\\ =\u0026amp; {{ 1 } \\over { 1+ e^{ \\beta_{0} + \\beta_{1} x_{1} + \\cdots \\beta_{p} x_{p}} }} \\end{align*} $$ であり、したがって $$\\displaystyle { { \\pi } \\over { 1 - \\pi } } = { { \\displaystyle {{ e^{ \\beta_{0} + \\beta_{1} x_{1} + \\cdots \\beta_{p} x_{p}} } \\over { 1+ e^{ \\beta_{0} + \\beta_{1} x_{1} + \\cdots \\beta_{p} x_{p}} }} } \\over { \\displaystyle {{ 1 } \\over { 1+ e^{ \\beta_{0} + \\beta_{1} x_{1} + \\cdots \\beta_{p} x_{p}} }} } } = e^{ \\beta_{0} + \\beta_{1} x_{1} + \\cdots \\beta_{p} x_{p}} $$ だ。両辺に自然対数を取ると $$\\displaystyle \\ln \\left( { { \\pi } \\over { 1 - \\pi } } \\right) = \\beta_{0} + \\beta_{1} x_{1} + \\cdots \\beta_{p} x_{p}$$ になる。 このように対数を取ることをロジット変換と呼び、$\\displaystyle \\ln \\left( { { \\pi } \\over { 1 - \\pi } } \\right)$ をロジットと呼ぶ。\nモデル 1 ロジットを従属変数とした多重回帰分析 $\\displaystyle \\ln \\left( { { \\pi } \\over { 1 - \\pi } } \\right) \\gets X_{1} , \\cdots, X_{p}$ をロジスティック回帰分析と呼ぶ。\nロジスティックモデルで得られた値にロジット変換の逆変換を適用することで、もともと知りたかった確率 $\\pi$ を得ることができる。この時、$X_{i}$ の係数 $\\beta_{i}$ が正であるということは、$X_{i}$ が大きくなるにつれて、$Y=1$ になる確率も大きくなることを意味し、負であることは、$X_{i}$ が大きくなるにつれて、$Y=0$ になる確率も大きくなることを意味する。\nまた、ロジスティック回帰分析は与えられた条件に対して結果が起こる確率を教えてくれるので予測技術であると同時に、確率に対して適切な閾値を提案することで分類技術にもなり得る。\n一緒に見る ロジスティックという名前が付いた理由は、ロジスティック関数を使用するためである。 Rでのロジスティック回帰分析結果 ホスマー・レムショー適合度検定 多重回帰分析 Hadi. (2006). Regression Analysis by Example(4th Edition): p318~320.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":832,"permalink":"https://freshrimpsushi.github.io/jp/posts/832/","tags":null,"title":"ロジスティック回帰分析"},{"categories":"추상대수","contents":"定義 1 整域 $D$ の$0$でもなく単元もない全ての要素に対して有限素因数分解が一意に存在する場合、$D$を一意素因数分解整域UFDという。 一意素因数分解整域 $D$ の $a_{1} , \\cdots , a_{n}$ に対して$d \\mid a_{i}$であり、$a_{i}$の全ての約数が$d$を割る場合、$d$を$a_{1} , \\cdots , a_{n}$の最大公約数Greatest Common Divisorといい、$\\gcd$と書く。 一意素因数分解整域 $D$ のある多項式を$f(x) := a_{0} + a_{1} x + \\cdots + a_{n} x^{n}$とする。$\\gcd ( a_{0} , a_{1} , \\cdots , a_{n} ) = 1$の場合、$f(x) \\in D [ x ]$を原始的Primitiveという。 単位元は乗算に対する恒等元$1$であり、単元は乗算に対する逆元を持つ元である。 定理 2 [2] 算術の基本定理：$\\mathbb{Z}$はUFDである。 [3] ガウスの補助定理：$D$がUFDであれば、$D [ x ]$の原始多項式たちの積も原始的である。 [4]: $D$がUFDであれば、$D [ x ]$もUFDである。 [5]: $F$が体であれば、$F[ x_{1} , \\cdots , x_{n} ]$はUFDである。 説明 「一意素因数分解整域」という言葉は通常、長いためによくUFDという略語が使われる。\nUFD 要素の有限素因数分解が存在することは、与えられた要素が有限数の既約元の積で表されることを意味する。UFDが便利である理由は、より大きなオブジェクトを分割して考えることができるようになるためである。定義上その要素が何であるかは指摘できなくても、そのような素因数分解が存在するだけで大いに役立つ。これにより、我々が考える「常識的な」計算が成り立つ整域となる。\nUFDの例は非常に多い。例えば、定理 [2] で言及されているように、整数環$\\mathbb{Z}$がそうである。しかし、整数環に$\\sqrt{-5}$を加えた単純拡大体$\\mathbb{Z} ( \\sqrt{ - 5 } )$を考えてみよう。ここで、$21 \\in \\mathbb{Z} ( \\sqrt{ - 5 } )$は素因数分解$21 = 3 \\cdot 7$を持つ一方で、$21 = ( 1 + 2 \\sqrt{-5}) ( 1 - 2 \\sqrt{-5}) $も可能であり、一意ではないため、$\\mathbb{Z} ( \\sqrt{ - 5 } )$が一意素因数分解整域でないことが容易に確認できる。\n原始的関数？ 関数が原始的であるとは、積分学における原始関数とは全く関係なく、$(3 x^2 + 6 x + 3) \\in \\mathbb{Z} [ x ]$が$3 ( x^2 + 2x + 1)$のように全体を$3$で囲むこととは異なり、係数を囲むことができない関数を指す。\n算術の基本定理 整数論におけるステートメントとは異なり、整数環$\\mathbb{Z}$がUFDであることを要約したものである。もちろん、この宣言のためには無数の概念が動員されているが、高度な整数論ではこのように代数の言葉で表現されることが多いため、代数学の学習は不可欠である。代数学を専攻しなくても、代数学の知識がなければ理解が難しい。\nガウスの補助定理 ガウスの補助定理は思っているよりも面白い定理である。例えば、$(5x + 1) , (2x^2 + 3x + 1) \\in \\mathbb{Z} [ x ]$を考えると、その積は$( 10 x^3 + 17 x^2 + 8 x + 1 )$であり、一見するといかなる最大公約数$a \\in \\mathbb{Z}$で囲むこともできない。一つくらいは反例が見つかりそうだが、ガウスの補助定理のおかげで、そうした無駄な努力をする必要はなくなる。\n証明 1 Part 1. 存在性\n$D$がPIDである場合、$d \\in D$は既約元$p_{1} , \\cdots , p_{r}$たちの有限積$a = p_{1} \\cdots p_{r}$として表現される。\nPart 2. 一意性\n別の既約元$q_{1} , \\cdots , q_{s}$に対して$a = q_{1} \\cdots q_{s}$も可能であるとしよう。\nPIDの既約元は素元であるため、ある$1 \\le j \\le s$に対して$p_{1} \\mid q_{j}$でなければならない。 $$ p_{1} p_{2} \\cdots p_{r} = p_{1} u_{1} q_{2} \\cdots q_{s} $$ 両辺から$p_{1}$を取り除くと $$ p_{2} \\cdots p_{r} = u_{1} q_{2} \\cdots q_{s} $$ 同じ方法で$i=r$まで繰り返すと $$ 1 = u_{1} \\cdots u_{r} q_{r+1} \\cdots q_{s} $$ を得る。$q_{r+1} \\cdots q_{s}$は既約元であるため、$r=s$を割る必要がある。\n■\n[2] $\\mathbb{Z}$の全てのイデアルは$\\left\u0026lt; n \\right\u0026gt; = n \\mathbb{Z}$の形であるためPIDであり、定理 1 によってUFDである。\n■\n[3] $$ \\begin{align*} f(x) \u0026amp;:= a_{0} + a_{1} x + \\cdots + a_{n} x^n \\\\ g(x) \u0026amp;:= b_{0} + b_{1} x + \\cdots + b_{m} x^m \\end{align*} $$ 原始多項式$f(x) , g(x) \\in D[x]$を上記のように表わそう。\n$p \\in D$を既約元としよう。\n$f(x)$は原始的であるため$\\gcd ( a_{0} , \\cdots , a_{n} ) = 1$であり、$p$が$a_{0} , \\cdots , a_{n}$を全て割ることはできない。それに$i = 0, 1 , \\cdots , n$に対して$p$が$a_{i}$を割ることができない最初の係数を$a_{r}$としよう。 $g(x)$も原始的であるため$\\gcd ( b_{0} , \\cdots , b_{m} ) = 1$であり、$p$が$b_{0} , \\cdots , b_{m}$を全て割ることはできない。それに$j = 0, 1 , \\cdots , m$に対して$p$が$b_{j}$を割ることができない最初の係数を$b_{s}$としよう。 すると、$f(x)g(x)$の$( r + s)$次の項の係数は $$ c_{r+s} = ( a_{0} b_{r+s} + \\cdots + a_{r-1} b_{s+1} ) + a_{r} b_{s} + ( a_{r+1} b_{s-1} + \\cdots + a_{r+s} b_{0} ) $$ となり、\n$a_{r}$の定義によれば $$ p \\mid ( a_{0} b_{r+s} + \\cdots + a_{r-1} b_{s+1} ) $$ $b_{s}$の定義によれば $$ p \\mid ( a_{r+1} b_{s-1} + \\cdots + a_{r+s} b_{0} ) $$ である。しかし、$p \\nmid a_{r} b_{s}$であるため、与えられた$p$は$f(x) g(x)$を割ることができない。これは全ての既約元についても同様であるため、$f(x) g(x)$は原始的である。\n■\n[4] $f(x) \\in D[x]$の次数を$n$としよう。\nすると、$f(x)$は $$ f (x) = g_{1} (x) \\cdots g_{r} (x)) $$ のように因数分解できる。また、$i = 1 , \\cdots , r$に対して、それぞれの因数を原始関数$h_{i} (x) \\in D[x]$と$c_{i} \\in D$の積である $$ g_{i} (x) = c_{i} h_{i} (x) $$ として表わすことができる。このような$c_{i}$を$g_{i} (x)$のコンテントContentと\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p390, 395~396.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p394~399。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":827,"permalink":"https://freshrimpsushi.github.io/jp/posts/827/","tags":null,"title":"一意因数分解整域"},{"categories":"통계적분석","contents":"概要 変数を選択する問題は、必然的に分析者の主観が介入するが、できる限り客観的な結論を導けるよう数値的指標が必要だった。そのような値を計算できれば、変数選択手順をいつ停止させるかについて明確な答えになる。ただし、この基準には様々な種類があり、基準を異なって適用すると結果も変わる可能性がある。\n指標 [^1] 説明力R-squared $R^2$ 説明力は、$\\displaystyle R^2 = 1 - {{ \\text{ SSE } } \\over { \\text{ SST} }}$で計算され、$1$に近づけば近づくほど、モデルがデータをよく説明していると解釈できる。\nしかし、変数選択基準としては、比較されるモデル間で独立変数の数が異なると意味がなくなるため、適切ではない。\n修正説明力Adjusted R-squared $R_{a}^2$ 回帰分析では、変数の数が増えるとそれだけ使用できるデータが増えることを意味し、その時の説明力$R^{2}$は必ず増加する。それに対して、修正説明力は、説明力と異なり$\\displaystyle R^{2}_{a} = 1 - {{ \\text{ SSE } / (n - p - 1) } \\over { \\text{ SST} / (n - 1) }}$で計算されて変数の数が反映される。\n修正説明力$R^{2}_{a}$は、変数の数に対してペナルティを適用することで、変数の数が異なると使用できない説明力の欠点を克服している。また、他の変数選択基準が相対的な指標であるため、モデルとモデルを比較するときのみ意味があるのとは異なり、修正説明力はそれ自体でモデルがどれだけデータをよく説明しているかも教えてくれるため、どんな基準を使っても参考になる指標でもある。最適なモデルではなく、最も説明力の高いモデルを求めているなら、役に立つだろう。\nアカイケ情報量基準Akaike Information Criteria $\\text{AIC}_{p}$ 独立変数$p$で、アカイケ情報量基準は$\\displaystyle \\text{AIC}_{p} := n \\ln \\left( {{ \\text{SSE}_{p} } \\over {n}} \\right) + 2(p+1) $で計算される。AICは実際の分析で最も好まれる尺度として、AICが小さい方がより良いと判断される。\n式の第二項は、$p$が大きくなると、つまり変数が多くなるほどペナルティを与えると考えられる。AICの欠点は、標本$n$が異なる場合、比較が不正確になることである。同じデータを使って変数だけを変えて分析するのに、どうして$n$が異なるのか不思議に思うかもしれないが、特定の変数が多くの欠損値を持つ場合、これが致命的な問題になる可能性がある。\nベイズ情報量基準Bayes Information Criteria $\\text{ BIC }_{p}$ 独立変数$p$で、ベイズ情報量基準は$\\displaystyle \\text{BIC}_{p} := n \\ln \\left( {{ \\text{SSE}_{p} } \\over {n}} \\right) + ( p +1 ) \\ln n $で計算される。AICと似ているが、最後の項を修正することでAICを補完し、同様に小さい方が良いとされる。\nマローズMallows $C_{p}$ 独立変数$p$に対して、マローズ$C_{p}$は$\\displaystyle C_{p} := {{ \\text{SSE}_{p} } \\over { \\hat{\\sigma}^2 }} + ( 2p - n )$で計算される。\n$C_{p}$は、偏りが少ない方向への変数を選択し、$C_{p} \\approx p$に近づくほど良いと判断される。つまり、偏りが少ないということだ。偏りを気にしなければならない分析があるなら、役に立ち、数学的にもきれいだが、最近では、ある程度の偏りがあっても分散を大幅に減らして精密に合わせる技術が評価を受けているため、人気はない。\n","id":826,"permalink":"https://freshrimpsushi.github.io/jp/posts/826/","tags":null,"title":"統計分析における変数選択基準"},{"categories":"추상대수","contents":"定義 1 整域 $D$ の $p \\ne 0$ が単元でないとする。\nPID $D$ の全てのイデアルが主イデアルである場合、$D$ を主イデアル整域PIDと呼ぶ。\n従属定義 可換環 $R$ が単位元 $1$ を持つとする。$a,b \\in R$ に対して $b=ac$ を満たす $c \\in R$ が存在する場合、$a$ が $b$ を割るDivideまたは$a$ が $b$ の因子Factorであると言い、$a \\mid b$ のように表す。 $a \\mid b$ かつ $b \\mid a$ の場合、$a,b$ が連想Associatesであると言う。 $\\forall a,b \\in D$ と $p=ab$ に対して、$a$ か $b$ のいずれかが単元である場合、$p$ を既約元Irreducible Elementと言う。 $\\forall a,b \\in D$ に対して、$p \\mid ab$ の場合、$p \\mid a$ または $p \\mid b$ の $p$ を素元Prime Elementと言う。 単位元は乗算に対する単位元 $1$、単元は乗算に対する逆元を持つ要素である。 定理 2 $D$ が主イデアル整域であるとする。\n[1]: $D$ はネーター環である。 [2]: $0$ でも単元でもない $d \\in D$ は、$D$ の既約元の積として表される。 [3]: $\\left\u0026lt; p \\right\u0026gt;$ が $D$ の極大イデアルである場合、$p$ は $D$ の既約元である。 [4]: $D$ の既約元は素元である。 説明 「主イデアル整域」という言葉は長いため、通常はPIDという略語がよく使用される。\n連想は結合則とスペルは同じだが名詞形であることに注意し、$-3,3 \\in \\mathbb{Z}$ のように互いに単元の積で表すことができる関係である。\n例 整数環 $\\mathbb{Z}$ 整数環 $\\mathbb{Z}$ は全てのイデアルが $n \\mathbb{Z} = \\left\u0026lt; n \\right\u0026gt;$ のように主イデアルとして表される。\n全ての体 $\\mathbb{F}$ ガウス整数環 $\\mathbb{Z} [i]$ とアイゼンシュタイン整数環 $\\mathbb{Z} [\\omega]$ ガウス整数環とアイゼンシュタイン整数環はそれぞれ整数環 $\\mathbb{Z}$ に純虚数 $i := \\sqrt{-1}$ または $\\omega := (-1)^{1/3}$ を加えた環である。\n証明 [1] ネーター環の定義: $N$ を環とする。\n$N$ のイデアルが $S_{1} \\le S_{2} \\le \\cdots$ を満たす場合、これを昇鎖Ascending Chainという。 昇鎖 $\\left\\{ S_{i} \\right\\}_{i \\in \\mathbb{N} }$ に対して、$S_{n} = S_{n+1} = \\cdots$ を満たす $n \\in \\mathbb{n}$ が存在する場合、定常Stationaryであるという。つまり、定常昇鎖では、ある時点からイデアルがこれ以上大きくならない。 すべての昇鎖が定常である環をネーター環という。 $D$ のイデアルの昇鎖 $N_{1} \\le N_{2} \\le \\cdots$ とその和集合 $\\displaystyle N := \\bigcup_{k=1}^{ \\infty } N_{k}$ を考える。ある $i, j \\in \\mathbb{N}$ に対して $$ a \\in N_{i} \\\\ b \\in N_{j} \\\\ N_{i} \\le N_{j} $$ とすると、$( N_{j} , + , \\cdot )$ はイデアルによって定義されるため、部分環であり、$b$ の加算に対する逆元 $(-b) \\in N_{j}$ が存在する。また、$ab \\in N_{j}$ であるため、$(a-b), ab \\in N$ であり、部分環判定法により、$N$ は $D$ の部分環である。それだけでなく、$N_{i}$ がイデアルである\nため、全ての $d \\in D$ に対して $d a = a d$ であり、$da \\in N$ であるため、$N$ は $D$ のイデアルである。\n$D$ はPIDであるため、全てのイデアルが主イデアルであり、ある $c \\in N$ に対して $N = \\left\u0026lt; c \\right\u0026gt;$ のように表せる。ここで、$\\displaystyle N = \\bigcup_{k=1}^{ \\infty } N_{k}$ であるため、$c \\in N$ であれば、$c \\in N_{r}$ を満たす自然数 $r \\in \\mathbb{N}$ が存在しなければならない。$c \\in N_{r}$ は、$N_{r}$ より小さいイデアルの中に $c$ を生成元とする主イデアルが存在することを意味する。数式で表すと $$ \\left\u0026lt; c \\right\u0026gt; \\le N_{r} \\le N_{r+1} \\le \\cdots \\le N = \\left\u0026lt; c \\right\u0026gt; $$ となり、$N_{r} = N_{r+1} = \\cdots$ である。したがって、$D$ はネーター環である。\n■\n[2] $d$ が既約元であれば証明する必要はないため、単元でない $d_{1}, c_{1} \\in D$ に対して、$d = d_{1} c_{1}$ のように表されるとする。\nすると、$\\left\u0026lt; d \\right\u0026gt; \\le \\left\u0026lt; d_{1} \\right\u0026gt;$ であり、$d_{i} := d_{i+1} c_{i+1}$ を続けて定義すると、昇鎖 $$ \\left\u0026lt; d \\right\u0026gt; \\le \\left\u0026lt; d_{1} \\right\u0026gt; \\le \\left\u0026lt; d_{2} \\right\u0026gt; \\le \\cdots $$ を得る。しかし、定理[1]により、この鎖が終わる $a_{r}$ が存在し、$a_{r}$ は同時に $a$ の因子である既約元となる。このように、$d$ を割る既約元を $p_{1}$ とし、単元でない $f_{1}$ に対して、$d = p_{1} f_{1}$ とすると、$\\left\u0026lt; d \\right\u0026gt; \\le \\left\u0026lt; f_{1} \\right\u0026gt;$ となり、$f_{i} := p_{i+1} f_{i+1}$ を続けて定義すると、昇鎖 $$ \\left\u0026lt; d \\right\u0026gt; \\le \\left\u0026lt; f_{1} \\right\u0026gt; \\le \\left\u0026lt; f_{2} \\right\u0026gt; \\le \\cdots $$ を得る。これも定理[1]により、この鎖が終わる $f_{s}$ が存在し、$f_{s}$ は同時に $f_{i}$ の因子である既約元となる。\nこのプロセスを有限回繰り返すことで、$d$ が既約元の積として表されることが確認できる。\n■\n[3] $( \\implies )$\n$D$ の極大イデアル $\\left\u0026lt; p \\right\u0026gt;$ の $p$ が、$D$ の単元でない $a,b$ に対して、$p=ab$ のように表されると仮定する。\nすると、$\\left\u0026lt; p \\right\u0026gt; \\le \\left\u0026lt; a \\right\u0026gt;$ であり、$\\left\u0026lt; p \\right\u0026gt; = \\left\u0026lt; a \\right\u0026gt;$ の場合、$b$ は単元でなければならないため、実際には $\\left\u0026lt; p \\right\u0026gt; \\lneq \\left\u0026lt; a \\right\u0026gt;$ を得る。しかし、$\\left\u0026lt; p \\right\u0026gt;$ が極大イデアルであるため、$\\left\u0026lt; a \\right\u0026gt; = D = \\left\u0026lt; 1 \\right\u0026gt;$ でなければならず、$a$ と $1$ は連想される。要約すると、\n$\\left\u0026lt; p \\right\u0026gt; \\ne \\left\u0026lt; a \\right\u0026gt;$ の場合、$a$ は単元であり、 $\\left\u0026lt; p \\right\u0026gt; = \\left\u0026lt; a \\right\u0026gt;$ の場合、$b$ は単元であるため、 $p$ は既約元である。\n$( \\impliedby )$\n既約元 $p=ab$ に対して、$\\left\u0026lt; p \\right\u0026gt; \\le \\left\u0026lt; a \\right\u0026gt;$ と仮定する。\n$a$ が単元であれば、$\\left\u0026lt; a \\right\u0026gt; = D$ で問題はないが、$a$ が単元でない場合、$b$ は必ず単元でなければならない。\n$b$ が単元であるということは、ある $u \\in D$ に対して、$bu =1$ という意味であるが、 $$ pu = abu = a $$ となるため、$\\left\u0026lt; p \\right\u0026gt; \\ge \\left\u0026lt; a \\right\u0026gt;$、つまり$\\left\u0026lt; p \\right\u0026gt; = \\left\u0026lt; a \\right\u0026gt;$ でなければならない。要約すると、\n$\\left\u0026lt; a \\right\u0026gt; = D$ であるか、 $\\left\u0026lt; a \\right\u0026gt; = \\left\u0026lt; p \\right\u0026gt;$ である必要があるため、 $\\left\u0026lt; p \\right\u0026gt;$ は極大イデアルとなる。\n■\n[4] $p$ が既約元であるとすると、$\\left\u0026lt; p \\right\u0026gt;$ は定理[3]により極大イデアルであり、$1 \\in D$ であるため、素イデアルである。\n$p$ が $ab$ を割るとすると、$(ab) \\in \\left\u0026lt; p \\right\u0026gt;$ であり、$\\left\u0026lt; p \\right\u0026gt;$ が素イデアルであるため、$a \\in \\left\u0026lt; p \\right\u0026gt;$ または $b \\in \\left\u0026lt; p \\right\u0026gt;$ である。これを別の形で表すと、$p \\mid ab$ の場合、$p \\mid a$ または $p \\mid b$ であるため、$p$ は素元となる。\n■\n関連項目 ユークリッド整域 $\\implies$ 主イデアル整域 $\\implies$ 一意分解整域 $\\implies$ 整域 ユークリッド整域 $\\implies$ 主イデアル整域 $\\implies$ ネーター環 Fraleigh. (2003). A First Course in Abstract Algebra(7th Edition): p389~391, 394.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFraleigh. (2003). A First Course in Abstract Algebra(7th Edition): p392~393.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":825,"permalink":"https://freshrimpsushi.github.io/jp/posts/825/","tags":null,"title":"主イデアル整域"},{"categories":"추상대수","contents":"定義 $N$ を環としよう。\n$N$ のイデアルたちが $S_{1} \\le S_{2} \\le \\cdots$ を満たすとき、これを昇鎖Ascending Chainという。 昇鎖 $\\left\\{ S_{i} \\right\\}_{i \\in \\mathbb{N} }$ に対して $S_{n} = S_{n+1} = \\cdots$ を満たす $n \\in \\mathbb{n}$ が存在するなら、定常的Stationaryという。つまり、定常的な昇鎖ではイデアルがある点からこれ以上大きくならない。 全ての昇鎖が定常的な環をネーター環と言う。 逆に、徐々に小さくなる鎖に関しては、降鎖Descendingという言葉を使う。 説明 鎖はイデアルにのみ当てはまる概念ではなく、集合論では部分順序集合や関係を定義しながら、より厳密かつ一般的に扱われる。しかし、通常昇鎖が必要な場所は代数学であり、代数学ではそれほど複雑に理解する必要はない。\nある繰り返し構造の中で、有限の場所に最も大きなものが存在するというのは、思ったよりも自明ではない。だから、ネーター環であるという条件は、非常に好ましい条件であり、以下の有名な定理は多くの分野で応用されている。\nヒルベルトの基底定理 $N$ がネーター環であれば、$N [ x_{1} , \\cdots , x_{n} ]$ もネーター環である。\n参照 ユークリッド整域 $\\implies$ 主イデアル整域 $\\implies$ ネーター環 ","id":823,"permalink":"https://freshrimpsushi.github.io/jp/posts/823/","tags":null,"title":"脳の脳室拡大"},{"categories":"통계적분석","contents":"概要 多重回帰分析 $Y \\gets X_{1} , \\cdots, X_{p}$ を行うとしよう。主成分分析、英語では PCA は、簡単に言えば量的変数が きちんと独立しているように 「再構成」して分析する方法だ。多変量データの分析という観点から見ると、より少ない変数で現象を説明しようとする「次元削減」の意味を持つ。\n主成分分析の理論的導出をしっかり理解するためには、線形代数、可能ならば数値線形代数についての知識まで必要だ。全く分からないなら、Step 3, 4も読んで理解してみるといい。数理統計学にある程度自信があるなら、数理統計学での主成分分析 のポストを読むのもいい。\n導出 1 Step 1. $p$ 個の独立変数と $n$ 個のサンプルがあるデータを標準化\n$$ \\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; x_{11} \u0026amp; \\cdots \u0026amp; x_{1p} \\\\ 1 \u0026amp; x_{21} \u0026amp; \\cdots \u0026amp; x_{2p} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 1 \u0026amp; x_{n1} \u0026amp; \\cdots \u0026amp; x_{np} \\end{bmatrix} \\begin{bmatrix} \\beta_{0} \\\\ \\beta_{1} \\\\ \\vdots \\\\ \\beta_{p} \\end{bmatrix} $$\nは、設計行列を使って $Y = X \\beta + \\varepsilon$ として表せる。この $X$ を標準化した行列を $Z$ というのは、$j$ 番目の独立変数 $X_{j}$ の標本平均 $\\overline{ x_{j} }$ と標本標準偏差 $s_{ X_{j} }$ に対し $(i,j)$ 成分が\n$$ \\left( Z \\right)_{ij}: = {{ x_{ij} } - \\overline{ x_{j} } \\over { s_{ X_{j} } }} $$\nの行列である。そうすると、新しい回帰係数\n$$ \\Theta := \\begin{bmatrix} \\theta_{1} \\\\ \\theta_{2} \\\\ \\vdots \\\\ \\theta_{p} \\end{bmatrix} $$\nについて、定数項がない回帰分析の計画行列式 $Y = Z \\Theta + \\varepsilon$ を得ることができる。この $Z = \\begin{bmatrix} Z_{1} \u0026amp; \\cdots \u0026amp; Z_{p} \\end{bmatrix}$ は、ベクトル $X_{1} , \\cdots , X_{p}$ を標準化した $Z_{1} , \\cdots , Z_{p}$ で構成された$( n \\times p )$ 行列となる。\nStep 2.\nスペクトル分解について、$Z^{T} Z$ は、$( p \\times p )$ が対称行列であるが、その定義を考えると、$\\displaystyle {{1} \\over {n-1}} Z^{T} Z$ は $Z_{1} , \\cdots , Z_{p}$ に対する共分散行列となる。特に $Z$ は標準化された行列であり、同時に相関係数行列ともなる。スペクトル理論によれば、\n$$ \\begin{cases} Z^{T} Z = Q \\Lambda Q^{T} \\\\ Q^{T} Q = Q Q^{T} = I \\end{cases} $$\nを満たす直交行列\n$$ Q = \\begin{bmatrix} q_{11} \u0026amp; q_{12} \u0026amp; \\cdots \u0026amp; q_{1p} \\\\ q_{21} \u0026amp; q_{22} \u0026amp; \\cdots \u0026amp; q_{2p} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ q_{p1} \u0026amp; q_{p2} \u0026amp; \\cdots \u0026amp; q_{pp} \\end{bmatrix} $$\nと、$Z^{T} Z$ の固有値からなる対角行列\n$$ \\Lambda = \\text{diag} ( \\lambda_{1} , \\lambda_{2} , \\cdots , \\lambda_{p} ) = \\begin{bmatrix} \\lambda_{1} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\lambda_{2} \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; \\lambda_{p} \\end{bmatrix} $$\nが存在する。ここで、便宜上 $\\lambda_{1} \\ge \\lambda_{2} \\ge \\cdots \\ge \\lambda_{p}$ になるようにし、$Z$ もそれに対応して再編成した行列と考えよう。\nStep 3. 主成分の構成\n$I = QQ^{T}$ なので、 $$ Y = Z \\Theta + \\varepsilon = Z Q Q^{T} \\Theta + \\varepsilon $$ ここで、$C := ZQ $ と $\\alpha := Q^{T} \\Theta$ とすると、 $$ Y = C \\alpha + \\varepsilon $$ これが$C = \\begin{bmatrix} C_{1} \u0026amp; \\cdots \u0026amp; C_{p} \\end{bmatrix}$ に対する$C_{1} , \\cdots , C_{p}$ の 主成分PCsだ。$j$ 番目の主成分の形は\n$$ C_{j} = q_{1j} Z_{1} + \\cdots + q_{pj} Z_{p} = \\sum_{i=1}^{p} q_{ij} Z_{j} $$\n元の独立変数を線形結合で再構成したものだ。\nStep 4.\n主成分の独立性も次の計算で確認できる： $$ \\begin{align*} \u0026amp; Z^{T} Z = Q \\Lambda Q^{T} \\\\ \\implies\u0026amp; Q^{T} Z^{T} Z Q = \\Lambda \\\\ \\implies\u0026amp; \\left( Z Q \\right) ^{T} \\left( Z Q \\right) = \\Lambda \\\\ \\implies\u0026amp; C^{T} C = \\Lambda \\end{align*} $$ つまり、 $$ C_{j}^{T} C_{j} = \\begin{cases} \\lambda_{j} \u0026amp; , i=j \\\\ 0 \u0026amp; , i \\ne j \\end{cases} $$ これにより、主成分は必ず独立していて、固有値 $\\lambda_{j}$ が $0$ に近い程度に小さいことは、$\\displaystyle C_{j} = \\sum_{i=1}^{p} q_{ij} Z_{j}$ がゼロベクトルに近いことを意味し、したがって$Z_{1} , \\cdots , Z_{p}$ が多重共線性を持つと見なすことができる。\n■\n限界 主成分回帰分析 $Y \\gets C_{1} , \\cdots , C_{p}$ は、固有値に問題のある変数を除去することで多重共線性の問題を回避する。さらに、元の回帰分析と比べるとずっと少ない変数を使うため、次元が削減されたと言える。\nしかし、主成分分析は万能のように見えるが、必ずしもそうではない。まず、$Z$ を作るために標準化するということは、質的変数や変換に対して手を付けにくい点が多いことを意味し、このように「再構成」する過程で分析自体が理解しにくくなる。\n統計が統計学を理解していない人にも必要だと考えると、この点はかなり致命的だ。例えば、韓国経済に対する分析に主成分分析を使うとしたら、失業率$X_{2}$ や平均初任給$X_{7}$ など理解しやすい数字ではなく、「総合雇用指数」$C_{4}$ などの変わった言葉で表されることになる。分析者でさえ、使える回帰式を作り出したとしても、その真の意味を掴むことが出来ない大惨事が起こる可能性がある。（コンピュータ科学の分野では、データの理解より予測と分類が重要であるため、この欠点にあまり神経を使わない。）\nまた、どの主成分も除外せずに$Y \\gets C_{1} , \\cdots , C_{p}$ をそのまま使う場合、元の$Y \\gets X_{1} , \\cdots , X_{p}$ と変わらないが、ここでいくつかの主成分を除外すること自体が、元々あったデータを諦めることを意味する。それでも必要ならば使うべきだが、必要なければわざわざ使う理由はない。使う時は、どんな欠点や限界があるかをはっきりと理解して使うべきだ。\n条件数 1 一方、導出過程で得られる固有値を通じて多重共線性を診断する数値的指標である条件数Condition Number\n$$ \\kappa := \\sqrt{ {{ \\lambda_{1} } \\over { \\lambda_{p} }} } $$\nを計算できる。経験的に$\\kappa \u0026gt; 15$ ならば、元のデータに多重共線性があると推測できるが、それほど広く使われてはいない。\n参照 多重共線性 Rで主成分回帰分析をする方法 数理統計学での主成分分析 Hadi. (2006). Regression Analysis by Example(4th Edition): p255~257.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":812,"permalink":"https://freshrimpsushi.github.io/jp/posts/812/","tags":null,"title":"統計学における主成分分析"},{"categories":"해석개론","contents":"=\nこの記事はリーマン・スティルチェス積分を基準に書かれている。$\\alpha=\\alpha (x)=x$と設定した場合、リーマン積分と同じである。\n定理 関数$f$が$[a,b]$で連続であれば、$[a,b]$でリーマン（-スティルチェス）積分可能である。\n証明 $\\epsilon \u0026gt;0$が与えられたとしよう。そして、$\\left[ \\alpha (b) - \\alpha (a) \\right] \\eta \u0026lt; \\epsilon$を満たす$\\eta\u0026gt;0$を選んだとしよう。$[a,b]$は閉じており有界なのでコンパクトであり、コンパクト集合上の連続関数は一様連続であるから、$f$は一様連続である。したがって、一様連続の定義により、下記の式が成立する$\\delta \u0026gt;0$が存在する。\n$$ |x-t|\u0026lt;\\delta \\implies |f(x)-f(t)|\u0026lt;\\eta\\quad \\forall x, t \\in [a,b] $$\n一様連続の定義により、$\\eta$の位置にどんな正数を入れても満たすので、私たちが上で選んだ$\\eta$も当然満たす。\n$[a,b]$の分割 $P$が$\\Delta x_{i} \u0026lt;\\delta (i=1,\\cdots,n)$を満たすように与えられたとしよう。また、次のようにしよう。\n$$ M_{i}=\\sup\\limits_{[x_{i-1},x_{i}]}f(x) \\quad \\text{and} \\quad m_{i}=\\inf\\limits_{[x_{i-1},x_{i}]}f(x) $$\nすると、$f$が一様連続であるという条件によって、次が成立する。\n$$ M_{i}-m_{i} \\le \\eta \\quad (i=1,\\cdots,n) $$\nすると、次の式を得る。\n$$ \\begin{align*} U(P,f,\\alpha) - L(P,f,\\alpha) \u0026amp;= \\sum \\limits_{i=1}^n (M_{i}-m_{i})\\Delta \\alpha_{i} \\\\ \u0026amp; \\le \\sum \\limits _{i=1} ^n \\eta \\Delta \\alpha_{i} \\\\ \u0026amp;= \\eta \\sum \\limits_{i=1}^n \\Delta \\alpha_{i} \\\\ \u0026amp;= \\eta \\left[ \\big( \\alpha ( x_{2}) -\\alpha (a) \\big) + \\cdots + \\big( \\alpha ( b) -\\alpha (x_{n-1}) \\big)\\right] \\\\ \u0026amp;=\\eta \\left[ \\alpha ( b) - \\alpha (a) \\right] \\\\ \u0026amp;\u0026lt; \\epsilon \\end{align*} $$\n証明の最初の部分で、$\\eta$を選ぶ際、最後の式を満たすように$\\eta$を選んだので、最後の行が成立するのは当然である。この式は積分可能である同値条件なので、$f$は積分可能である。\n■\n","id":847,"permalink":"https://freshrimpsushi.github.io/jp/posts/847/","tags":null,"title":"連続関数はリーマン-スティルチェス可積分である"},{"categories":"전자기학","contents":"説明1 電界は常にカール(回転)が$\\mathbf{0}$になる特別なベクトル関数だ。この特性から、電場$\\mathbf{E}$に関連する電位electric potentialというスカラー関数を導入する。電位は$V$と表記され、電場$\\mathbf{E}$と以下の関係が成り立つ。\n$$ \\mathbf{E} = -\\nabla V $$\n従って、電位$V$を知れば、電場$\\mathbf{E}$を知ることができる。電位はスカラー関数なので、ベクトル関数である電場を直接求めるよりも、電位を求める方が簡単である。電場と電位の関係は、重力と位置エネルギーの関係に似ている。ただし、電場の単位は力ではないため、電位は正確にはポテンシャルエネルギーではなく、単にポテンシャルである。\n以下の結果は、電場が正確に電位の勾配であり、電位を知っていればその勾配を計算して電場を知ることができるという意味である。また、この内容は電場だけでなく、カールが$\\mathbf{0}$になるすべてのベクトル関数に適用される。\n導出 $\\nabla \\times \\mathbf{E}=0$であることを証明する過程で、電場の閉路に対する線積分が$0$であることがわかった。上の図で、$1$の経路と$2$の経路を組み合わせると、点$\\mathbf{a}$から点$\\mathbf{a}$へ戻る閉路が形成される。したがって、\n$$ \\int _{1} \\mathbf{E} \\cdot d\\mathbf{l} + \\int_2 \\mathbf{E} \\cdot d\\mathbf{l} =0 $$\nとなるので、$1$の経路と$2$の経路の積分値は、大きさは同じで符号は逆である。$1$の経路を反転させ、両方とも点$\\mathbf{a}$から点$\\mathbf{b}$へ行く経路とすると、両者は同じ値を持つ。\n電場の線積分は経路に依存しないので、ある基準点$\\mathcal{O}$から位置$\\mathbf{r}$までの積分は常に同じ値を持つ。従って、スカラー関数$V$を以下のように定義しよう。\n$$ V(\\mathbf{r} ) \\equiv - \\int _\\mathcal{O} ^{\\mathbf{r}} \\mathbf{E} \\cdot d \\mathbf{l} $$\nすると、電位の定義により、次の式が成り立つ。\n$$ \\begin{align*} V(\\mathbf{b} )- V( \\mathbf{a} ) =\u0026amp;\\ -\\int _\\mathcal{O} ^{\\mathbf{b}} \\mathbf{E} \\cdot d\\mathbf{l} +\\int_\\mathcal{O} ^{\\mathbf{a}} \\mathbf{E} \\cdot d \\mathbf{l} \\\\ =\u0026amp;\\ -\\int_\\mathbf{a} ^\\mathbf{b} \\mathbf{E} \\cdot d\\mathbf{l} \\end{align*} $$\n勾配の基本定理\n$$ T(b)-T(a) = \\int _{a}^{b} (\\nabla T) \\cdot d\\mathbf{l} $$\nまた、勾配の基本定理により、次の式が成り立つ。\n$$ V( \\mathbf{b} ) - V (\\mathbf{a} ) = \\int_{\\mathbf{a}}^{\\mathbf{b}}\\left( \\nabla V \\right) \\cdot d\\mathbf{l} $$\n従って、$\\displaystyle \\int_\\mathbf{a} ^ \\mathbf{b} \\left( \\nabla V \\right) \\cdot d\\mathbf{l} = -\\int_\\mathbf{a} ^\\mathbf{b} \\mathbf{E} \\cdot d\\mathbf{l}$なので、\n$$ \\mathbf{E} = -\\nabla V $$\n■\nDavid J. Griffiths, 基礎電磁気学(Introduction to Electrodynamics, 金珍勝訳) (4th Edition, 2014), p86-87\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":845,"permalink":"https://freshrimpsushi.github.io/jp/posts/845/","tags":null,"title":"ポテンシャル"},{"categories":"통계적분석","contents":"定義 1 多重回帰分析 $Y \\gets X_{1} , \\cdots, X_{p}$ をするとしよう。このとき、独立変数 $ X_{1} , \\cdots, X_{p}$ の中で独立変数同士が強い相関関係を持っている場合、多重共線性Multicollinearityがあるとされる。\n実践 もともと、独立変数同士が依存していること自体が回帰分析の前提に反する話であり、実際には数値的な問題を引き起こし分析結果を信頼できなくする。データによっては多重共線性があるかどうかを見つけ出すことからが仕事となる。\nデータ探索 組み込みデータから MplsDemo データを 読み込んでみよう。\nMplsDemoはアメリカ ミネアポリス地域を区別して、population(総人口)、white(白人比率)、black(黒人比率)、foreignBorn(外国生まれ)、hhIncome(世帯所得)、poverty(貧困)、collegeGrad(大学卒業者率)を推定したデータだ。\n回帰分析後の仮説検定をみると、特に問題はなさそうで説明力もまあまあ良く、残差図も良好だ。しかし、回帰係数をよく見ると、白人が多いほど大卒者が増え、外国生まれが多いほど大卒者が増えるように見える。もちろんこのデータがすべての人種について正確な情報を持っているわけではないが、これは何かおかしい。また、所得が多いほど大卒者が増えるのはそうだとしても、貧困率が全く影響を与えないのもなんとなく気持ち悪い。\n散布図を見ると、誰が見ても白人、世帯所得が正の相関関係、黒人、外国生まれが負の相関関係を持つのが正常に見える。貧困はあいまいだけれど、無理に言えば弱い負の相関関係を示す。しかし、黒人と外国生まれに対する回帰係数が正であることは、何かこのデータが正しく説明されていないことを示唆している。もちろん説明力自体は0.8を超える程度にはまあまあだが、どこか納得いかない点が明らかにある。多重共線性を考えると、白人が他の変数と強い相関関係があるのが引っかかる。\nモデル修正 白人比率を表す独立変数 whiteを削除して、再度回帰分析をやってみよう。\n新しい分析結果は、説明力が約10%近く落ちたものの、散布図から予想された回帰関係を比較的まともに説明していることが確認できる。外国生まれは少し係数が変だが、回帰係数が有意でないので気にする必要はなさそうだ。ここら辺から、より洗練された分析のためにデータをどう扱うかは完全に分析者にかかっている。\n多重共線性の検出 多重共線性がある可能性が高い状況は以下のような場合がある：\nF検定は合格したが、各々の回帰係数が t検定を通過しない場合 予想したこととは異なり、回帰係数の符号が反対でずれが大きい場合 データを追加または削除するときに、既存の回帰係数が極端に変化する場合 1の場合は、それでも多重共線性を発見できたという点では幸運なことだ。データをどう扱ってどう問題を解決するかは別として、多重共線性があるという事実自体は把握されたからである。\n2の場合は、直感と違うから発見しやすいが、「予想されること」とデータによっては多重共線性を把握するのが非常に難しくなる場合がある。例えば、植物の生長に影響を与える原因を調べている場合、独立変数として日光や水の量、土壌の質などが生長に役立つかどうか大まかな予測は可能だ。しかし、社会科学で未知の問題を解決しなければならない場合は、分析者の直感自体が信じがたくなる。多重共線性があっても分析は正しくされたように見えるため、適切なレビューがなければ実際の現象を全く説明できない分析結果を出すかもしれない。\n3の場合は、独立変数が非常に多い場合、回帰係数一つ一つに気を使うのが難しくなり、直接見ても見過ごすことがある。データとは必ずしも独立変数だけでなく、何かの異常値になることもある。\nどのケースでも、分析が誤っていて問題が発生したことから、説明力 $R^2$が低いため多重共線性を疑うことができる。しかし、説明力にはどれくらいから良いという基準もなく、間違った分析でも説明力が高く出ることがあり、それすらも大部分は主観的で、参考にならないことがある。もともと、現実の中で得られるデータはすべての変数が完全に独立である場合がより珍しい。多重共線性と言うほどではないけれど、ある程度は関係があり、あいまいな影響を与えるのが普通だ。\n例のように散布図を通じて図で把握することも万能とは言えない。もちろん散布図を見れば二つの変数間の関係はすぐにわかるが、$X_{1}+ X_{2} + X_{3} = 1$のように複数の変数が複雑な関係を持っている場合、目で見つけるのは難しい。\n数値的指標 そうなると、当然ながら数値的な指標を考えるしかない。このために最も好んで使われるのが分散膨張因子(VIF)であり、多重共線性を見つけるのに役立っている。ただしVIFはどのような確率分布に従うわけでもないため、仮説検定を行うことができない。だから経験的に定められた基準を超えると多重共線性があると主張するしかない。その経験的基準もあいまいな時があり、悩みの種だ。回帰分析をするということは実際にはこの多重共線性との戦いと言える程度である。\nVIF以外にも、主成分分析を通じて得られる条件数のような指標もあるが、あまり使われない。\nコード 以下は例示コードです。\ninstall.packages(\u0026#39;car\u0026#39;)\rlibrary(car)\rDATA=MplsDemo; head(DATA)\rwin.graph()\rplot(DATA[,-1])\rout0\u0026lt;-lm(collegeGrad~.-neighborhood,data=DATA)\rsummary(out0)\rwin.graph(4,4)\rplot(out0$residuals, main=\u0026#34;잔차\u0026#34;)\rout1\u0026lt;-lm(collegeGrad~.-neighborhood-white,data=DATA)\rsummary(out1) Hadi. (2006). Regression Analysis by Example(4th Edition): p222.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":808,"permalink":"https://freshrimpsushi.github.io/jp/posts/808/","tags":["R"],"title":"多重共線性"},{"categories":"전자기학","contents":"定理 電場のカール（回転）は常に$\\mathbf{0}$である。\n$$ \\nabla \\times \\mathbf{E} = \\mathbf{0} $$\n証明1 点電荷が原点にある特別な場合の結果から一般的な結果を導くことにする。原点から距離$r$の場所での点電荷による電場は以下の通りである。\n$$ \\mathbf{E}=\\dfrac{1}{4 \\pi \\epsilon_{0} } \\dfrac{q}{r^2} \\hat{\\mathbf{r}} $$\n点$\\mathbf{a}$から点$\\mathbf{b}$までの球座標系における電場の経路積分を行うと、次のようになる。\n$$ \\begin{align*} \\int_\\mathbf{a} ^\\mathbf{b} \\mathbf{E} \\cdot d\\mathbf{l} =\u0026amp;\\ \\int_\\mathbf{a}^\\mathbf{b} \\left( \\dfrac{1}{4 \\pi \\epsilon_{0}} \\dfrac{q}{r^2} \\hat{\\mathbf{r}} \\right) \\cdot \\left( dr \\hat{\\mathbf{r}} + rd\\theta\\hat{\\boldsymbol{\\theta}} + r\\sin\\theta d\\phi \\hat{\\boldsymbol{\\phi}} \\right) \\\\ =\u0026amp;\\ \\int_\\mathbf{a}^\\mathbf{b} \\dfrac{1}{4\\pi\\epsilon_{0}}\\dfrac{q}{r^2}dr \\\\ =\u0026amp;\\ \\dfrac{q}{4 \\pi \\epsilon_{0}} \\int_\\mathbf{a}^\\mathbf{b} \\dfrac{1}{r^2} dr = \\dfrac{q}{4 \\pi \\epsilon_{0}} \\left[ -\\dfrac{1}{r} \\right]_{r_{a}}^{r_{b}} \\\\ =\u0026amp;\\ \\dfrac{q}{4 \\pi \\epsilon_{0}} \\left( \\dfrac{1}{r_{a}}-\\dfrac{1}{r_{b}} \\right) \\end{align*} $$\nここで、$r_{a}$と$r_{b}$は原点から点$\\mathbf{a}$、点$\\mathbf{b}$までの距離である。上の積分結果からわかるように、閉じた経路に対する積分は$0$である。\n$$ \\oint \\mathbf{E} \\cdot d \\mathbf{l} = 0 $$\nストークスの定理\n$$ \\int_{\\mathcal{S}} \\left( \\nabla \\times \\mathbf{v} \\right) \\cdot d\\mathbf{a} = \\oint _{\\mathcal{P} }\\mathbf{v} \\cdot d \\mathbf{l} $$\nストークスの定理を用いると\n$$ \\int \\left( \\nabla \\times \\mathbf{E} \\right) \\cdot d\\mathbf{a} =\\oint \\mathbf{E} \\cdot d\\mathbf{l}=0 $$\nしたがって、$\\nabla \\times \\mathbf{E} = \\mathbf{0}$であることがわかる。任意の面積に対する積分でも結果が$\\mathbf{0}$でなければならないので、$\\nabla \\times \\mathbf{E} = \\mathbf{0}$であるしかない。\n複数の点電荷に対する電場は、各点電荷に対する電場を足し合わせるのと同じである。連続的に分布した電荷に対しては、$\\sum$を$\\int$に変えるだけで良い。したがって、$\\mathbf{E}=\\mathbf{E}_{1} + \\mathbf{E}_2+\\mathbf{3}+\\cdots$であり、各電場のカールが$\\mathbf{0}$であるため、その合計も当然$\\mathbf{0}$である。\n$$ \\begin{align*} \\nabla \\times \\mathbf{E} =\u0026amp;\\ \\nabla \\times (\\mathbf{E}_{1} + \\mathbf{E}_2+\\mathbf{3}+\\cdots ) \\\\ =\u0026amp;\\ (\\nabla \\times \\mathbf{E}_{1}) +(\\nabla \\times \\mathbf{E}_2 )+(\\nabla \\times \\mathbf{E}_{3})+\\cdots \\\\ =\u0026amp;\\ \\mathbf{0} \\end{align*} $$\n■\nDavid J. Griffiths, 기초전자기학(Introduction to Electrodynamics, 김진승 역) (4th Edition, 2014), p84-85\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":844,"permalink":"https://freshrimpsushi.github.io/jp/posts/844/","tags":null,"title":"電場の回転"},{"categories":"전자기학","contents":"定義1 面 $\\mathcal S$を通過する電場 $\\mathbf{E}$のフラックスを以下のように定義する。\n$$ \\Phi_{E} \\equiv \\int_{\\mathcal S} \\mathbf{E} \\cdot d\\mathbf{a} $$\nこれから、$\\mathcal{S}$をある閉じた曲面としよう。閉じた曲面内の総電荷量を$Q_{\\text{in}}$としよう。すると、次の式が成り立つ。\n$$ \\oint_{\\mathcal{S}} \\mathbf{E} \\cdot d\\mathbf{a} = \\frac{1}{\\epsilon_{0}}Q_{\\mathrm{in}} $$\nこれをガウスの法則という。\nフラックス フラックスとは、ある物理量が特定の面に対して垂直に通過する量をいう。例えば、管を流れる水やガスは、その管の垂直面に対して平行に流れるため、流れる量自体がフラックスと同じである。しかし、電場は管に沿って流れない。そのため、電場のフラックスは内積を使って求める。内積を取ると、平行でない成分はすべて0として計算されるからである。\nある面を通過する電場線が上の図のようだとしよう。ここで私たちが知りたいのは、面に垂直に通過する程度がどれくらいかということだ。みんな知っての通り、ベクトルは分解が可能だ。電場線を面に垂直な方向と平行な方向に分解しよう。すると下の図のようになる。 私たちの目的は、図で示された$\\mathbf{E}_\\parallel$を求めることである。面ベクトル$d\\mathrm{a}$の大きさは$1$であるため、内積を使って以下のような式で求めることができる。\n$$ \\mathbf{E}_\\parallel=\\mathbf{E} \\cdot d\\mathbf{a} $$\n上で行った方法に従って、与えられた面に対する電場のフラックスを以下のように定義する。\n$$ \\Phi_{E} \\equiv \\int_{\\mathcal S} \\mathbf{E} \\cdot d\\mathbf{a} $$\nガウスの法則（積分形） ガウスの法則の核心は、閉じた曲面の外にある電荷はフラックスに何の影響も与えないということである。\n図を見れば、閉じた曲面を外から貫通する電場線によるフラックスは、面の両端で2回計算されることがわかる。閉じた曲面における面ベクトルの方向は常に面の外側に定義される。両端の面ベクトル方向が正反対であるため、両端を通過するフラックスの大きさは同じで、方向は違う。この2つを加えると$0$になるため、閉じた曲面の外の電荷はフラックスに影響を与えない。\n導出 これから、点電荷$Q$が半径$r$の球の中心にあるとしよう。このとき、球面を通過する電場$\\mathbf{E}$のフラックスを求めてみよう。クーロンの法則で電場を表すと、次のようになる。\n$$ \\begin{align*} \\Phi_{E} \u0026amp;= \\oint \\mathbf{E} \\cdot d\\mathbf{a} \\\\ \u0026amp;= \\int_{0}^{2\\pi} \\int_{0}^\\pi \\left( \\frac{1}{4\\pi\\epsilon_{0} q r^{2}} \\hat{\\mathbf{r}} \\right) \\cdot \\left( r^{2}\\sin\\theta d\\theta d \\phi \\hat{\\mathbf{r}} \\right) \\\\ \u0026amp;= \\frac{1}{4\\pi\\epsilon_{0}}Q \\int_{0}^{2\\pi}d\\phi \\int_{0}^\\pi \\sin\\theta d\\theta \\\\ \u0026amp;= \\frac{1}{4\\pi\\epsilon_{0}}Q (2\\pi)(2) \\\\ \u0026amp;= \\frac{1}{\\epsilon_{0}}Q \\end{align*} $$\n結果を見ると、半径$r$に対して無関係であることがわかる。これは球の表面積が$r^{2}$に比例し、電場が$r^{2}$に反比例するからである。お互いに打ち消しあって、結果に影響を与えない。曲面内に複数の点電荷がある場合は、単純に加えればよい。複数の点電荷による電場が重ね合わせの原理に従って単純に加えられるからである。例えば、点電荷$Q_{1}$、$Q_{2}$があり、$Q=Q_{1}+Q_{2}$とすると、結果は下のようになる。\n$$ \\begin{align*} \\oint \\mathbf{E} \\cdot d\\mathbf{a} \u0026amp;= \\oint \\left( \\sum \\limits_{i=1}^{2} \\mathbf{E}_{i} \\right) \\cdot d\\mathbf{a} \\\\ \u0026amp;= \\oint \\left( \\mathbf{E}_{1} + \\mathbf{E}_{2} \\right) \\cdot d\\mathbf{a} \\\\ \u0026amp;= \\oint \\mathbf{E}_{1} \\cdot d\\mathbf{a} + \\oint \\mathbf{E}_{2} \\cdot d\\mathbf{a} \\\\ \u0026amp;= \\frac{1}{\\epsilon_{0}}Q_{1}+\\frac{1}{\\epsilon_{0}}Q_{2} \\\\ \u0026amp;= \\frac{1}{\\epsilon_{0}}(Q_{1}+Q_{2})=\\frac{1}{\\epsilon_{0}}Q \\end{align*} $$\n当然、点電荷が3つ以上の場合も結果は同じである。したがって、閉じた曲面内の総電荷量を$Q_{\\text{in}}$とし、各電荷が作る電場の合計である総電場を$\\mathbf{E}$とすると、次の式が成り立つ。\n$$ \\begin{equation} \\oint \\mathbf{E} \\cdot d\\mathbf{a} = \\dfrac{1}{\\epsilon_{0}}Q_{\\text{in}} \\label{1} \\end{equation} $$\n微分形 発散定理\n$$ \\int_\\mathcal{V} \\nabla \\cdot \\mathbf{v} d\\tau = \\oint _{S} \\mathbf{v} \\cdot d \\mathbf{a} $$\n$\\eqref{1}$に発散定理を適用すると、以下の式を得る。\n$$ \\int_{\\mathcal{V}} \\nabla \\cdot \\mathbf{E} d\\tau = \\oint _{\\mathcal{S}} \\mathbf{E} \\cdot d \\mathbf{a} = \\frac{1}{\\epsilon_{0}}Q_{\\text{in}} $$\nこのとき、単位体積当たりの電荷量を体積電荷密度$\\rho$としよう。すると、体積内部の総電荷量$Q_\\mathrm{in}$と$\\rho$の関係は以下のようになる。\n$$ Q_\\mathrm{in}=\\int_\\mathcal{V} \\rho d\\tau $$\n上の2つの式の結果を組み合わせると、以下のようになる。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\int_\\mathcal{V} \\nabla \\cdot \\mathbf{E} d\\tau \u0026amp;= \\int_\\mathcal{V} \\frac{1}{\\epsilon_{0}}\\rho d\\tau \\\\ \\implies \u0026amp;\u0026amp; \\nabla \\cdot \\mathbf{E} \u0026amp;= \\frac{1}{\\epsilon_{0}}\\rho \\end{align*} $$\nこれをガウスの法則の微分形といい、マクスウェル方程式のひとつである。\nDavid J. Griffiths, 基礎電磁気学(Introduction to Electrodynamics, 金晋昇訳)(4th Edition). 2014, p73-77\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":635,"permalink":"https://freshrimpsushi.github.io/jp/posts/635/","tags":null,"title":"電気フラックスとガウスの法則"},{"categories":"추상대수","contents":"定義 1 体 $F$ と アーベル群 $V$ が以下の条件を満たす場合、$V$ を$F$ 上の ベクトル空間Vector Spaceと呼ぶ。$F$ の元を スカラーScalar、$V$ の元を ベクターVectorと呼ぶ。\n(i): $\\alpha x \\in V$ (ii): $\\alpha ( \\beta x) = ( \\alpha \\beta ) x$ (iii): $\\alpha (x + y) = \\alpha x + \\alpha y$ (iv): $1 x = x$ 添字集合 $I$ に対して、$\\left\\{ x_{i} \\right\\}_{i \\in I} \\subset V$ とする。\nある$\\left\\{ \\alpha_{i} \\right\\}_{ i \\in I} \\subset F$ に対して、$\\displaystyle \\sum_{i \\in I} \\alpha_{i} x_{i}$ を$\\left\\{ x_{i} \\right\\}_{i \\in I}$ の 線形結合と呼ぶ。 $V$ の全ての元が$M$ の線形結合で表せる場合、$ \\left\\{ x_{i} \\right\\}_{i \\in I}$ が$V$ を 生成すると言い、$\\text{span} \\left\\{ x_{i} \\right\\}_{i \\in I} = V$ と表す。 有限集合$I$ に対して、$\\text{span} \\left\\{ x_{i} \\right\\}_{i \\in I} = V$ を満たす$\\left\\{ x_{i} \\right\\}_{i \\in I}$ が存在する場合、$V$ は 有限次元であるという。 全ての$\\left\\{ x_{i} \\right\\}_{i \\in I}$ に対して $\\displaystyle \\sum_{i \\in I} \\alpha_{i} x_{i} = 0$ を満たすのが $\\alpha_{i} = 0$ のみの場合、$\\left\\{ x_{i} \\right\\}_{i \\in I}$ は$F$ 上で 線形独立であるという。そうでない場合は 線形従属である。 $\\text{span} \\left\\{ x_{i} \\right\\}_{i \\in I} = V$ の時、$\\left\\{ x_{i} \\right\\}_{i \\in I}$ が線形独立であれば、その$\\left\\{ x_{i} \\right\\}_{i \\in I}$ を$V$ の基底と呼ぶ。 有限次元ベクトル空間$V$ の基底を$M$ とする場合、$M$ の基数を$V$ の次元と呼び、$\\dim V$ と表す。 説明 通常、線形代数ですでに馴染みのある概念で、\u0026lsquo;代数\u0026rsquo;という名前が付いているものの抽象代数で説明できないわけではない。簡単な例として、多項式関数の環 $\\mathbb{R} [ x ]$ は$1 , x , \\cdots , x^{n}$ を基底とするベクトル空間であることが容易に確認できる。\n参照 線形代数学のベクトル空間 抽象代数学のベクトル空間 以下の文書で話される$F$-ベクトルスペースは、基本的に上記のベクトル空間と変わらない。ただ、見方が少し異なっており、線形代数学のベクトル空間は直感的なユークリッド空間の抽象化で、抽象代数学のベクトル空間はそれを真の意味での\u0026rsquo;代数\u0026rsquo;として持ち込んでいると考えられる。\n逆に、$R$-モジュールは、$F$-ベクトルスペースの スカラー体 $F$ を スカラー環 $R$ に一般化することで、その意味があるとされている。従って、$F$-ベクトルスペースの歴史や意味に無関心な命名であり、そのアイデンティティを示している。グループ $G$ の観点から見れば、環 $R$ へ新しい操作 $\\mu$ が追加されているため、それも 加群加群の一種である。\n抽象代数学のR-モジュール 抽象代数学の$F$-ベクトルスペース Fraleigh. (2003). A first course in abstract algebra(7th Edition): p274~280.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":806,"permalink":"https://freshrimpsushi.github.io/jp/posts/806/","tags":null,"title":"抽象代数学におけるベクトル空間"},{"categories":"전자기학","contents":"クーロンの法則1 固定された点電荷 $q$から距離$\\cR$だけ離れたところにある試験電荷$Q$が受ける力をクーロン力といい、その式は次の通りである。\n$$ \\mathbf{F} = \\dfrac{1}{4\\pi \\epsilon_{0}} \\dfrac{qQ}{\\cR ^2} \\crH $$\nこれをクーロンの法則Coulomb\u0026rsquo;s lawという。\n説明 クーロンの法則は繰り返しの実験から得られた実験法則である。だから数学的に証明することはできない。数学の公理みたいに考えると、理解しやすいだろう。$\\epsilon_{0}$は真空中の誘電率permittivity of free spaceで、その値は$8.85 \\times 10^{-12} \\dfrac{\\mathrm C^2}{\\mathrm N \\cdot \\mathrm m^2}$である。一方、文の上部の式は国際単位系système international, SIで表されている。ガウス単位系Gaussian systemで表すと、以下のようになる。\n$$ \\mathbf{F} = \\dfrac{qQ}{\\cR ^2} \\crH $$\nこれは、国際単位系の前に比例定数を$1$に置き換えるものである。つまり、$\\dfrac{1}{4\\pi\\epsilon_{0}} \\equiv 1$ということである。言い換えると、国際単位系をガウス単位系に簡単に変換する方法は、$\\epsilon_{0}$を$\\dfrac{1}{4\\pi}$に置き換えればいい。\n電場 点電荷分布 今、試験電荷$Q$の周りにいくつかの点電荷があるとしよう。その場合、$Q$が受ける力は単純に各点電荷から受ける力を線形に足すだけでよい。つまり$Q$と$q_{1}$の相互作用は$q_{2}, q_{3}, \\dots$に影響されないという意味である。これを重ね合わせの原理superposition principleという。\n$$ \\begin{align*} \\mathbf{F} \u0026amp;= F_{1}+F_{2}+\\cdots + F_{n} \\\\ \u0026amp;= \\dfrac{1}{4\\pi\\epsilon_{0}}\\dfrac{q_{1}Q}{{\\cR_{1}}^2}\\crH_{1} +\\dfrac{1}{4\\pi\\epsilon_{0}}\\dfrac{q_{2}Q}{{\\cR_{2}}^2}\\crH_{2}+\\cdots +\\dfrac{1}{4\\pi\\epsilon_{0}}\\dfrac{q_{n}Q}{{\\cR_{n}}^2}\\crH_{n} \\\\ \u0026amp;= Q\\left( \\dfrac{1}{4\\pi\\epsilon_{0}}\\dfrac{q_{1}}{{\\cR_{1}}^2}\\crH_{1} +\\dfrac{1}{4\\pi\\epsilon_{0}}\\dfrac{q_{2}}{{\\cR_{2}}^2}\\crH_{2}+\\cdots +\\dfrac{1}{4\\pi\\epsilon_{0}}\\dfrac{q_{n}}{{\\cR_{n}}^2}\\crH_{n} \\right) \\\\ \u0026amp;= Q\\mathbf{E} \\end{align*} $$\nここで、括弧内の部分を源電荷$q_{1},\\ q_{2},\\ \\cdots ,\\ q_{n}$たちが作る電場electric fieldと定義し、$\\mathbf{E}$と表示する。\n$$ \\mathbf{E}(\\mathbf {r}) =\\dfrac{1}{4\\pi \\epsilon_{0}} \\sum \\limits_{i=1}^n \\dfrac{q_{i}}{{\\cR_{i}}^2}\\crH_{i} $$\n連続電荷分布 電荷が連続的に分布している場合は、合計の代わりに積分で表される。\n$$ \\sum \\rightarrow \\int \\\\ \\mathbf{E}(\\mathbf {r}) =\\dfrac{1}{4\\pi \\epsilon_{0}} \\int \\dfrac{1}{\\cR^2}\\crH dq $$\n線電荷の場合は$dq=\\lambda dl^{\\prime}$。ここで$\\lambda$は線電荷密度である。線電荷が作る電場は以下のようである。\n$$ \\mathbf{E}(\\mathbf {r}) =\\dfrac{1}{4\\pi \\epsilon_{0}} \\int _\\mathcal{P} \\dfrac{\\lambda (\\mathbf{r}^{\\prime})}{\\cR^2} \\crH dl^{\\prime} $$\n面電荷の場合は$dq=\\sigma da^{\\prime}$。ここで$\\sigma$は面電荷密度である。面電荷が作る電場は以下のようである。\n$$ \\mathbf{E}(\\mathbf {r}) =\\dfrac{1}{4\\pi \\epsilon_{0}} \\int _\\mathcal{S} \\dfrac{\\sigma (\\mathbf{r}^{\\prime})}{\\cR^2} \\crH da^{\\prime} $$\n体積電荷の場合は$dq=\\rho d\\tau^{\\prime}$。ここで$\\rho$は体積電荷密度である。体積電荷が作る電場は以下のようである。\n$$ \\mathbf{E}(\\mathbf {r}) =\\dfrac{1}{4\\pi \\epsilon_{0}} \\int _\\mathcal{V} \\dfrac{\\rho (\\mathbf{r}^{\\prime})}{\\cR^2} \\crH d\\tau^{\\prime} $$\nDavid J. Griffiths, 基礎電磁気学（Introduction to Electrodynamics, 金進世訳）(第4版). 2014, p65-70\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":836,"permalink":"https://freshrimpsushi.github.io/jp/posts/836/","tags":null,"title":"クーロンの法則と電場"},{"categories":"정수론","contents":"定義 1 素数$p \\ne 2$ および $a \u0026lt; p$ に関して、合同方程式 $x^{2} \\equiv a \\pmod{p}$ の解が存在すれば、$a$ を $p$ の二次剰余 QRと呼ぶ。$a$ が二次剰余でない場合は、非二次剰余 NRと呼ばれる。\n説明 簡単に言えば、二次剰余とは $\\pmod{p}$で平方根が存在する数を意味する。\n例えば、素数 $7$ を考えると $$ 1^2 \\equiv 1 \\pmod{7} \\\\ 2^2 \\equiv 4 \\pmod{7} \\\\ 3^2 \\equiv 2 \\pmod{7} \\\\ 4^2 \\equiv 2 \\pmod{7} \\\\ 5^2 \\equiv 4 \\pmod{7} \\\\ 6^2 \\equiv 1 \\pmod{7} $$ $1,2,4$ はQRであり、$3,5,6$ はNRである。素数$11$ を考えると $$ 1^2 \\equiv 1 \\pmod{11} \\\\ 2^2 \\equiv 4 \\pmod{11} \\\\ 3^2 \\equiv 9 \\pmod{11} \\\\ 4^2 \\equiv 5 \\pmod{11} \\\\ 5^2 \\equiv 3 \\pmod{11} \\\\ 6^2 \\equiv 3 \\pmod{11} \\\\ 7^2 \\equiv 5 \\pmod{11} \\\\ 8^2 \\equiv 9 \\pmod{11} \\\\ 9^2 \\equiv 4 \\pmod{11} \\\\ 10^2 \\equiv 1 \\pmod{11} $$ $1,3,4,5,9$ がQRであり、残りの$2,6,7,8,10$ はNRである。面白いことに、QRは対象的に現れるが、事実 $$ (p-q)^2 \\equiv p^2-2pq+q^2 \\equiv q^2 \\pmod{p} $$ であるため当然である。また、常にQRとNRは正確に同じ数だけ現れる。\n要約 $2$ より大きな素数$p$ に対して、QRとNRは正確に$\\displaystyle {(p-1) \\over 2}$ 個存在する。\n証明 $1$ から$p-1$ までの全ての数を平方したリストは次のようになる。 $$ 1^2, 2^2, \\cdots , (p-1)^2 $$ しかし、先に見たように $$ (p-q)^2 \\equiv p^2-2pq+q^2 \\equiv q^2 \\pmod{p} $$ であるため、$1$ を見ても$p-1$ を見ても同じであり、$2$ を見ても$p-2$ を見ても同じである。従って、元の数の半分だけを見れば十分である。 $$ 1^2, 2^2, \\cdots , \\left( {{ p-1 } \\over { 2 }} \\right)^2 $$ これらの数は全てQRの定義によりQRであるため、これらの数が全て異なることを示せば、QRが正確に$\\displaystyle {(p-1) \\over 2}$ 個存在すると言えるだろう。同様に、$\\pmod{p}$ で$p$ より小さい自然数はQRでなければNRであるため、QRが正確に$\\displaystyle {(p-1) \\over 2}$ 個存在するならば、NRも正確に$\\displaystyle {(p-1) \\over 2}$ 個存在するだろう。本格的な証明は背理法を用いる。$b_1$ と$b_2$ を$\\displaystyle {{(p-1)} \\over {2}}$ より小さい異なる二つの自然数とする。\n${b_1}^2 \\equiv {b_2}^2 \\pmod{p}$ が成り立つと仮定すると $$ {b_1}^2 - {b_2}^2 \\equiv 0 \\pmod{p} $$ したがって、ある整数$k$ に対して${b_1}^2 - {b_2}^2 = pk$ であり、素数$p$ は$({b_1} + {b_2})({b_1} - {b_2})$ の約数である。しかし、$b_1$ と$b_2$ が$\\displaystyle {{(p-1)} \\over {2}}$ より小さいとしたので、$({b_1} + {b_2})$ は$(p-1)$ より小さい。従って、$p$ が$({b_1} + {b_2})$ の約数になることはできず、必然的に$({b_1} - {b_2})$ の約数でなければならない。しかし、同じ理由で$|{b_1} - {b_2}|$ は$(p-1)$ より小さいため、$0$ になって初めて$p$ で割り切れる。つまり、${b_1} = {b_2}$ なのだが、$b_1$ と$b_2$ を$\\displaystyle {{(p-1)} \\over {2}}$ より小さい異なる二つの自然数と仮定したので、これは矛盾である。従って、${b_1}^2 \\neq {b_2}^2 \\pmod{p}$ であり、QRは正確に$\\displaystyle {(p-1) \\over 2}$ 個存在する。\n■\nSilverman. (2012). 『Number Theoryへのやさしい導入 (第4版)』: p143.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":137,"permalink":"https://freshrimpsushi.github.io/jp/posts/137/","tags":null,"title":"二次剰余と非二次剰余"},{"categories":"해석개론","contents":"この記事は、リーマン-シュティルチェス積分を基に書かれている。$\\alpha=\\alpha (x)=x$とすれば、リーマン積分と同じだ。\n定理1 関数$f$が$[a,b]$でリーマン(-シュティルチェス)積分可能であるための必要十分条件は、全ての$\\epsilon \u0026gt;0$に対して、$U(P,f,\\alpha) - L(P,f,\\alpha) \u0026lt; \\epsilon$を満たす$[a,b]$の分割$P$が存在することである。\n$$ \\begin{equation} f \\in \\mathscr{R} (\\alpha) \\text{ on } [a,b] \\\\ \\iff \\forall\\epsilon \u0026gt;0, \\exists P\\text{ s.t. } U(P,f,\\alpha) - L(P,f,\\alpha) \u0026lt; \\epsilon \\end{equation} $$\n積分可能性を示す際に実際に使われる条件である。\n証明 以下が与えられているとしよう。\n$f : [a,b] \\to \\mathbb{R}$が有界である。 $\\alpha : [a,b] \\to \\mathbb{R}$は単調増加関数である。 $P$を$[a,b]$の分割としよう。 $(\\implies)$\n$f$がリーマン(-シュティルチェス)積分可能な関数であると仮定しよう。$\\epsilon \u0026gt; 0$が与えられたとする。すると、下積分と積分の定義によって、全ての分割$P$に対して次が成り立つ。\n$$ L(P,f,\\alpha) \\le \\underline{\\int _{a}^{b}} f d\\alpha = \\int _{a} ^{b} f d\\alpha $$\nよって、次を満たす分割$P_{1}$が存在する。\n$$ \\begin{equation} \\int _{a} ^{b} f d\\alpha - L(P_{1},f,\\alpha) \u0026lt; \\frac{\\epsilon}{2} \\end{equation} $$\n同様に次も成り立つ。\n$$ \\int _{a} ^{b} f d\\alpha = \\overline {\\int _{a} ^{b}} f d\\alpha \\le U(P,f,\\alpha) $$\nよって、次を満たす分割$P_{2}$が存在する。\n$$ \\begin{equation} U(P_2,f,\\alpha) - \\int _{a}^{b} f d\\alpha \u0026lt; \\frac{\\epsilon}{2} \\end{equation} $$\n今、$P^{\\ast}$を$P_{1}$と$P_{2}$の共通細分としよう。すると、細分の上(下)合は分割よりも小さく(大きく)、$(2)$、$(3)$によって次が成り立つ。\n$$ \\begin{align*} U(P^{\\ast},f,\\alpha) \u0026amp;\\le U(P_2,f,\\alpha) \\\\ \u0026amp;\\lt {\\color{blue}\\int _{a} ^{b} f d\\alpha} + \\frac{\\epsilon}{2} \\\\ \u0026amp;\\lt {\\color{blue} L(P_{1},f,\\alpha) + \\frac{\\epsilon}{2} } + \\frac{\\epsilon}{2} \\\\ \u0026amp;= L(P_{1},f,\\alpha) + \\epsilon \\\\ \u0026amp;\\le L(P^{\\ast},f,\\alpha) + \\epsilon \\end{align*} $$\nよって、$U(P^{\\ast},f,\\alpha)-L(P^{\\ast},f,\\alpha) \u0026lt; \\epsilon$を満たす分割$P^{\\ast}$が存在する。\n■\n$(\\impliedby)$\n全ての$\\epsilon \u0026gt;0$に対して、$U(P,f,\\alpha) - L(P,f,\\alpha) \u0026lt; \\epsilon$を満たす$[a,b]$の分割$P$が存在すると仮定しよう。上積分、下積分の定義によって次の式が成り立つ。\n$$ L(P,f,\\alpha) \\le \\underline {\\int _{a} ^{b}} f d\\alpha \\le \\overline{ \\int _{a} ^{b}}f d\\alpha \\le U(P,f,\\alpha) $$\nこの時$A\u0026lt;B\u0026lt;C\u0026lt;D$であれば$C-B\u0026lt;D-A$であるので、仮定と上の式を利用して次のような式を得る。\n$$ 0 \\le \\overline {\\int _{a}^{b}} f d\\alpha -\\underline{\\int _{a} ^{b}} f d\\alpha \u0026lt; \\epsilon $$\n全ての正数$\\epsilon$に対してこの式が満足されるならば、次が成立しなければならない。\n$$ \\overline {\\int _{a}^{b}} f d\\alpha -\\underline{\\int _{a} ^{b}} f d\\alpha=0 $$\nよって、次が成り立ち、これは$f$が積分可能であるという定義であるので、$f$は積分可能である。\n$$ \\overline {\\int _{a}^{b}} f d\\alpha =\\underline{\\int _{a} ^{b}} f d\\alpha $$\n■\n系2 (a) ある分割$P$と$\\varepsilon \u0026gt;0$に対して$(1)$が成立するならば、$P$の全ての細分についても$(1)$が成立する。\n(b) 分割$P=\\left\\{ x_{0},\\cdots,x_{n} \\right\\}$に対して$(1)$が成立し、それを$s_{i},t_{i}\\in [x_{i-1},x_{n}]$とする。すると、以下の不等式が成立する。 $$ \\sum \\limits _{i=1} ^{n} \\left| f(s_{i}) -f(t_{i}) \\right| \\Delta \\alpha_{i} \u0026lt;\\varepsilon $$\n(c) $f$が積分可能で**(b)**の仮定が成立するならば、以下の式が成立する。 $$ \\left| \\sum \\limits _{i=1} ^{n} f(t_{i})\\Delta \\alpha_{i} - \\int _{a} ^{b}f (x)d\\alpha (x) \\right| \u0026lt; \\varepsilon $$\n証明 (a) $P^{\\ast}$を$P$の細分としよう。すると、細分の性質により次が成り立つ。\n$$ U(P^{\\ast},f,\\alpha) -L(P^{\\ast},f,\\alpha)\u0026lt;U(P,f,\\alpha) -L(P,f,\\alpha) \u0026lt;\\varepsilon $$\nよって、**(a)**が成立する。\n■\n(b) $x\\in[x_{i-1},x_{i}]$に対して次のようにしよう。\n$$ M_{i}=\\sup f(x) \\quad \\text{and} \\quad m_{i}=\\inf f(x) $$\nすると、全ての$s_{i},t_{i}\\in [x_{i-1},x_{i}]$に対して次が成り立つ。\n$$ \\left| f(s_{i})-f(t_{i}) \\right| \u0026lt; M_{i}-m_{i},\\quad i=1,\\cdots,n $$\nよって、上合、下合の定義により次が成立する。\n$$ \\begin{align*} \\sum \\limits _{i=1} ^{n} \\left| f(s_{i})-f(t_{i}) \\right| \\Delta \\alpha_{i} \u0026amp;\u0026lt; \\sum \\limits _{i=1} ^{n}(M_{i}-m_{i})\\Delta \\alpha_{i} \\\\ \u0026amp;=U(P,f,\\alpha)-L(P,f,\\alpha) \\\\ \u0026amp;\u0026lt; \\varepsilon \\end{align*} $$\n■\n(c) 先の証明で使用した記法を続けよう。上合、下合の定義により以下の式が成立することは自明だ。\n$$ L(P,f,\\alpha) \\le \\sum \\limits _{i=1} ^{n} f(t_{i})\\Delta \\alpha_{i} \\le U(P,f,\\alpha) $$\nまた、積分の定義により以下の式も明白に成立する。\n$$ L(P,f,\\alpha) \\le \\int _{a} ^{b} f(x)d\\alpha (x) \\le U(P,f,\\alpha) $$\nよって、上の二つの式により次が成立する。\n$$ \\left| \\sum \\limits _{i=1} ^{n} f(t_{i})\\Delta \\alpha_{i} - \\int _{a} ^{b}f (x)d\\alpha (x) \\right| \u0026lt; \\varepsilon $$\n■\nワルター・ルーディン, 数学分析の原理 (第3版, 1976), p124-125\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nワルター・ルーディン, 数学分析の原理 (第3版, 1976), p125\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":833,"permalink":"https://freshrimpsushi.github.io/jp/posts/833/","tags":null,"title":"リーマン（-スティルチェス）積分可能の必要十分条件"},{"categories":"해석개론","contents":"この投稿はリーマン-スティルチェス積分を基準に書かれている。$\\alpha=\\alpha (x)=x$と設定すれば、リーマン積分と同じだ。\n定義 $P^{\\ast}$と$P$が$[a,b]$の分割であり、$P \\subseteq P^{\\ast}$を満たす場合、$P^{\\ast}$を$P$の細分refinementという。従って、$P$の全ての点は$P^{\\ast}$の点である。\n任意の二つの分割$P_{1}$と$P_{2}$に対して、$P_{3}=P_{1} \\cup P_{2}$を$P_{1}$と$P_{2}$の共通細分という。\n高等学校で積分を定義する時、与えられたグラフを$n$等分し、$n$が無限大になる極限を取っていたことを思い出してみると、細分の役割がすぐに理解できるだろう。\n定理 $P^{\\ast}$が$P$の細分であるとする。すると、以下の二つの式が成立する。\n$$ \\begin{align} L(P,f,\\alpha) \u0026amp;\\le L(P^{\\ast},f,\\alpha) \\label{eq1} \\\\ U(P^{\\ast},f,\\alpha) \u0026amp;\\le U(P,f,\\alpha) \\label{eq2} \\end{align} $$\nこの時、$L$と$U$はそれぞれリーマン(-スティルチェス)上和、下和である。\nつまり、分割が細分化されるほど、下和は大きくなり、上和は小さくなるということだ。\n証明 証明に先立って、以下のように与えられたとする。\n$f : [a,b] \\to \\mathbb{R}$が有界である。 $\\alpha : [a,b] \\to \\mathbb{R}$は単調増加関数である。 $P$を$[a,b]$の分割とする。 $P^{\\ast}$が$P$よりもちょうど一点多い細分であるとし、その点を$x^{\\ast}$とし、ある$i=1,\\cdots ,n$に対して$x_{i-1} \u0026lt; x^{\\ast} \u0026lt; x_{i}$とする。\n$\\eqref{eq1}$ $P$に対するリーマン(-スティルチェス)下和は次のようになる。\n$$ \\begin{align*} L(P,f,\\alpha) \u0026amp;= \\sum \\limits _{i=1} ^n m_{i} \\Delta \\alpha_{i} \\\\ \u0026amp;= m_{1}\\Delta \\alpha_{1} + \\cdots + m_{i} \\left[ \\alpha (x_{i}) - \\alpha (x_{i-1}) \\right] + \\cdots + m_{n}\\Delta \\alpha_{n} \\\\ \u0026amp;= m_{1}\\Delta \\alpha_{1} + \\cdots + m_{i} \\left[ \\alpha (x_{i}) -\\alpha (x^{\\ast}) \\right] + m_{i} \\left[ \\alpha (x^{\\ast})- \\alpha (x_{i-1}) \\right] + \\cdots + m_{n}\\Delta \\alpha_{n} \\end{align*} $$\nそして、以下のように設定する。\n$$ \\begin{align*} w_{1} \u0026amp;= \\inf f(x) \u0026amp;(x_{i-1} \\le x \\le x^{\\ast}) \\\\ w_2\u0026amp;= \\inf f(x) \u0026amp;(x^{\\ast} \\le x \\le x_{i}) \\end{align*} $$\nすると、$m_{i}=\\inf f(x)\\ \\ (x_{i-1} \\le x \\le x_{i})$であるため、次が成り立つ。\n$$ m_{i} \\le w_{1} \\quad \\text{and} \\quad m_{i} \\le w_2 $$\n従って、次を得る。\n$$ \\begin{align*} m_{i} \\left[ \\alpha (x_{i}) - \\alpha (x^{\\ast}) \\right] + m_{i}\\left[ \\alpha (x^{\\ast}) - \\alpha (x_{i-1}) \\right] \u0026amp;\\le w_2 \\left[ \\alpha (x_{i}) - \\alpha (x^{\\ast}) \\right] + w_{1}\\left[ \\alpha (x^{\\ast}) - \\alpha (x_{i-1}) \\right] \\\\ \u0026amp;= w_{1}\\left[ \\alpha (x^{\\ast}) - \\alpha (x_{i-1}) \\right] + w_2 \\left[ \\alpha (x_{i}) - \\alpha (x^{\\ast}) \\right] \\end{align*} $$\nしたがって、次が成り立つ。\n$$ \\begin{align*} L(P,f,\\alpha) \u0026amp;= m_{1}\\Delta \\alpha_{1} + \\cdots + m_{i} \\left[ \\alpha (x_{i}) -\\alpha (x^{\\ast}) \\right] + m_{i} \\left[ \\alpha (x^{\\ast})- \\alpha (x_{i-1}) \\right] + \\cdots + m_{n}\\Delta \\alpha_{n} \\\\ \u0026amp;\\le w_{1}\\Delta \\alpha_{1} + \\cdots + w_{1}\\left[ \\alpha (x^{\\ast}) - \\alpha (x_{i-1}) \\right] + w_2 \\left[ \\alpha (x_{i}) - \\alpha (x^{\\ast}) \\right] + \\cdots + m_{n}\\Delta \\alpha_{n} \\\\ \u0026amp;= L(P^{\\ast},f,\\alpha) \\end{align*} $$\n■\n$\\eqref{eq2}$ $\\eqref{eq1}$と同じ方法で証明する。$P$に対するリーマン(-スティルチェス)上和は次のようになる。\n$$ \\begin{align*} U(P,f,\\alpha) \u0026amp;= \\sum \\limits _{i=1} ^n M_{i} \\Delta \\alpha_{i} \\\\ \u0026amp;= M_{1}\\Delta \\alpha_{1} + \\cdots + M_{i} \\left[ \\alpha (x_{i}) - \\alpha (x_{i-1}) \\right] + \\cdots + M_{n}\\Delta \\alpha_{n} \\\\ \u0026amp;= M_{1}\\Delta \\alpha_{1} + \\cdots + M_{i} \\left[ \\alpha (x_{i}) -\\alpha (x^{\\ast}) \\right] + M_{i} \\left[ \\alpha (x^{\\ast})- \\alpha (x_{i-1}) \\right] + \\cdots + M_{n}\\Delta \\alpha_{n} \\end{align*} $$\nそして、以下のように設定する。\n$$ \\begin{align*} W_{1} \u0026amp;= \\sup f(x)\u0026amp; (x_{i-1} \\le x \\le x^{\\ast}) \\\\ W_2\u0026amp;= \\sup f(x)\u0026amp;(x^{\\ast} \\le x \\le x_{i}) \\end{align*} $$ すると、$M_{i}=\\sup f(x)\\ \\ (x_{i-1} \\le x \\le x_{i})$であるため、次が成り立つ。\n$$ W_{1} \\le M_{i} \\quad \\text{and} \\quad W_2 \\le M_{i} $$\n従って、次を得る。\n$$ \\begin{align*} M_{i} \\left[ \\alpha (x_{i}) - \\alpha (x^{\\ast}) \\right] + M_{i}\\left[ \\alpha (x^{\\ast}) - \\alpha (x_{i-1}) \\right] \u0026amp; \\ge W_2 \\left[ \\alpha (x_{i}) - \\alpha (x^{\\ast}) \\right] + W_{1}\\left[ \\alpha (x^{\\ast}) - \\alpha (x_{i-1}) \\right] \\\\ \u0026amp;= W_{1}\\left[ \\alpha (x^{\\ast}) - \\alpha (x_{i-1}) \\right] + W_2 \\left[ \\alpha (x_{i}) - \\alpha (x^{\\ast}) \\right] \\end{align*} $$\nしたがって、次が成り立つ。\n$$ \\begin{align*} U(P,f,\\alpha) \u0026amp;= M_{1}\\Delta \\alpha_{1} + \\cdots + M_{i} \\left[ \\alpha (x_{i}) -\\alpha (x^{\\ast}) \\right] + M_{i} \\left[ \\alpha (x^{\\ast})- \\alpha (x_{i-1}) \\right] + \\cdots + M_{n}\\Delta \\alpha_{n} \\\\ \u0026amp;\\ge W_{1}\\Delta \\alpha_{1} + \\cdots + W_{1}\\left[ \\alpha (x^{\\ast}) - \\alpha (x_{i-1}) \\right] + W_2 \\left[ \\alpha (x_{i}) - \\alpha (x^{\\ast}) \\right] + \\cdots + M_{n}\\Delta \\alpha_{n} \\\\ \u0026amp;= U(P^{\\ast},f,\\alpha) \\end{align*} $$\n■\n","id":830,"permalink":"https://freshrimpsushi.github.io/jp/posts/830/","tags":null,"title":"細分화"},{"categories":"해석개론","contents":"概要 リーマン・スティルチェス積分は、リーマン積分を一般化したもので、簡単にスティルチェス積分とも呼ばれる。リーマン積分はリーマン・スティルチェス積分の中で$\\alpha (x)=x$の特別な場合に該当する。\nリーマン・スティルチェス積分を定義するプロセスは、リーマン積分を定義するプロセスと同じなので、表記と構築に関する具体的な説明は省略する。\n定義 $\\alpha : [a,b] \\to \\mathbb{R}$を単調増加関数とし、$\\Delta \\alpha_{i}=\\alpha (x_{i})-\\alpha (x_{i-1})$とする。すると$\\alpha$が単調増加関数であるため$\\Delta \\alpha_{i} \\ge 0$が成り立つ。\n有界な関数$f : [a,b] \\to \\mathbb{R}$と$[a,b]$の分割$P$に対して$U, L$を以下のように定義する。\n$$ \\begin{align} U(P,f,\\alpha) \u0026amp;:= \\sum \\limits _{i=1} ^n M_{i} \\Delta \\alpha_{i} \\\\ L(P,f,\\alpha) \u0026amp;:= \\sum \\limits_{i=1} ^n m_{i} \\Delta \\alpha_{i} \\end{align} $$\n$(1), (2)$を**$[a,b]$での$\\alpha$に対する$f$のリーマン・スティルチェス上積分と下積分**upper and lower Riemann-Stieltjes sumとする。\n$(1), (2)$に区間$[a,b]$の全ての任意の分割$P$に対する$\\inf, \\sup$を取ったものをそれぞれ**$[a,b]$での$\\alpha$に対する$f$のリーマン・スティルチェス上積分と下積分**upper and lower Riemann-Stieltjes integralとする。\n$$ \\begin{align*} \\overline {\\int _{a} ^b} f d\\alpha \u0026amp;:= \\inf\\limits_{P} U(P,f,\\alpha) \\\\ \\underline {\\int _{a} ^b} f d\\alpha \u0026amp;:= \\sup\\limits_{P} L(P,f,\\alpha) \\end{align*} $$\n上積分と下積分が等しい場合、これを**$[a,b]$での$\\alpha$に対する$f$のリーマン・スティルチェス積分**Riemann-Stieltjes integralと呼び、以下のように表記する。\n$$ \\int _{a} ^b f d\\alpha = \\int _{a}^b f(x) d\\alpha (x) = \\overline {\\int _{a} ^b} f d\\alpha = \\underline {\\int _{a} ^b} f d\\alpha $$\n$f$のスティルチェス積分が存在する場合、$f$は$[a,b]$で$\\alpha$に対してリーマン・スティルチェス積分可能Riemann-Stieltjes integrableであり、以下のように表記する。\n$$ f \\in \\mathscr{R}(\\alpha) = \\left\\{ f : f \\text{ is Riemann-Stieltjes integrable} \\right\\} $$\n","id":829,"permalink":"https://freshrimpsushi.github.io/jp/posts/829/","tags":null,"title":"リーマン・スティルチェス積分"},{"categories":"추상대수","contents":"定義 1 体 $F$ の拡張体を $E$ としよう。定数関数ではない $f(x) \\in F [ x ]$ について、$\\alpha \\in E$ が $f( \\alpha ) = 0$ を満たす時、$F$ 上で代数的Algebraicと言い、代数的でなければ超越的Transcendentalという。$F = \\mathbb{Q}$、$E = \\mathbb{C}$ とする場合、$\\alpha \\in \\mathbb{C}$ が代数的であれば代数的数、超越的であれば超越数という。\n説明 例えば、多項式 $ f(x) = x^2 - 2 $ があるとして、$f(x) = 0$ を満たす有理数の解は存在しないが、$\\mathbb{Q}$ から拡張された $\\mathbb{R}$ では、$\\sqrt{2}$ という解が存在する。しかし、$\\pi$ のような数の場合には、このような方法では導くことができない。したがって、$\\sqrt{2}$ と $\\pi$ はどちらも無理数だが、$\\sqrt{2}$ は代数的数、$\\pi$ は超越数と言われる。\n意外と、代数的数と超越数の概念は高校生から馴染み深いものだ。通常、高校で文系と理系を分けるのは超越関数の微積分だと言われるためだ。そこでは通常、代数的数、超越数についての説明も伴う。\n高校レベルでは、整数を係数とする多項方程式の解になれるなら代数的数、そうでなければ超越数と説明される。このことを抽象代数の言葉で言うときは $F = \\mathbb{Q}$、$E = \\mathbb{C}$ としてきれいにまとめることができる。\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p267.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":799,"permalink":"https://freshrimpsushi.github.io/jp/posts/799/","tags":null,"title":"代数的な数と超越数"},{"categories":"정수론","contents":"定義 1 定理 $a^{n} \\equiv 1 \\pmod{p}$ とすると、$\\text{ord}_{p} (a) \\mid n$ である。\n説明 例えば、$p=7$ を考える。 $$ \\begin{align*} 1^{1} \\equiv \u0026amp; 1 \\pmod{ 7 } \\\\ 2^{3} \\equiv \u0026amp; 1 \\pmod{ 7 } \\\\ 3^{6} \\equiv \u0026amp; 1 \\pmod{ 7 } \\\\ 4^{3} \\equiv \u0026amp; 1 \\pmod{ 7 } \\\\ 5^{6} \\equiv \u0026amp; 1 \\pmod{ 7 } \\\\ 6^{2} \\equiv \u0026amp; 1 \\pmod{ 7 } \\end{align*} $$ ここで、$6$ の位数は $2$ であり、$2, 4$ の位数は $3$ で、$3,5$ の位数は $6$ である。\n上の定理では、とくに $n=p-1$ とすると、$2,3,6$ が $p-1= 6$ を割ることを容易に確認できる。また、フェルマーの小定理によれば、素数 $p$ に対して常に $a^{p-1} \\equiv 1 \\pmod{p}$ が成り立つので、$\\text{ord}_{p} (a) \\mid (p-1)$ であることがわかる。\n証明 $G := \\gcd ( \\text{ord}_{p} (a) , n )$ とすると、$G = \\text{ord}_{p}(a) \\cdot s + n \\cdot t$ を満たす $s,t$ が存在する。\n位数の定義と仮定により、 $$ a^{G} = a^{ \\text{ord}_{p}(a) \\cdot s + n \\cdot t} = \\left( a^{ \\text{ord}_{p}(a) } \\right)^s \\cdot \\left( a^{n} \\right)^{t} \\equiv 1 \\cdot 1 \\pmod{p} $$ $\\text{ord}_{p}(a)$ は $a^{e} \\equiv 1 \\pmod{p}$ を満たす最小の自然数 $e$ として定義されたので、$G = \\text{ord}_{p}(a)$ であり、$\\text{ord}_{p}(a) \\mid p$ である。\n■\nコード 以下は、R言語で位数を計算するためのコードである。素因数分解のコードが使われた。\nprime = read.table(\u0026#34;../attachment\r/cfile8.uf@25411C3C5968BBE322F0D4.txt\u0026#34;); prime = prime[,1]\rfactorize\u0026lt;-function(p)\r{\rq=p\rfactors\u0026lt;-numeric(0)\ri=1; j=1\rwhile(q!=1)\r{\rif(q%%prime[i]) {i=i+1}\relse\r{\rq\u0026lt;-q/prime[i]\rfactors[j]\u0026lt;-prime[i]\ri=1\rj=j+1\r}\r}\rreturn(factors)\r}\rorder\u0026lt;-function(g,p,h=1) #Calculate a order of g in modulo p\r{\rqe\u0026lt;-table(factorize(p-1))\rqe\u0026lt;-rbind(as.numeric(names(qe)),qe)\rdivisor\u0026lt;-qe[1,1]^(0:qe[2,1])\rif((length(qe)/2)==1) {return(qe[1,1]^qe[2,1])}\rfor(i in 2:(length(qe)/2)) {divisor=c(divisor%*%t(qe[1,i]^(0:qe[2,i])))}\rfor(i in divisor) {if((FPM(g,i,p))%%p==1) break;}\rreturn(i)\r}\rorder(1,7)\rorder(2,7)\rorder(3,7)\rorder(4,7)\rorder(5,7)\rorder(6,7) 以下は上記のコードを実行した結果である。\nSilverman. (2012). A Friendly Introduction to Number Theory (4th Edition): p211. $\\gcd (a, p) = 1$ としよう。$a^{e} \\equiv 1 \\pmod{p}$ を満たす最小の自然数 $e$ を $\\text{ord}_{p} (a)$ と書き、法 $p$ での $a$ の位数Orderと定義する。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":798,"permalink":"https://freshrimpsushi.github.io/jp/posts/798/","tags":null,"title":"数論における位数"},{"categories":"해석개론","contents":"分割1 区間$[a,b]$が与えられたとしよう。$[a,b]$の分割partition$P$を下のように定義する。\n$$ P := \\left\\{ x_{0},\\ x_{1},\\ \\cdots, x_{n}\\right\\},\\quad a=x_{0} \u0026lt;x_{1}\u0026lt;\\cdots \u0026lt; x_{n} =b $$\nそして、$\\Delta x_{i}$を次のように定義する。\n$$ \\Delta x_{i} :=x_{i}-x_{i-1},\\quad i=1,2,\\cdots,n $$\n説明 簡単に言えば、分割とはある区間を分割した時、区間の両端と区間内のすべての境界点を要素として持つ集合のことだ。重要な点は、分割について話す場合、必ずどの区間についてのものかが必要だということだ。つまり、単に分割と言うことはできず、ある区間の分割と言うべきだ。\nリーマン和 $f$を$[a,b]$で定義された有界関数、$P$を$[a,b]$の分割としよう。そして、$M_{i}$, $m_{i}$を以下のようだとしよう。\n$$ \\begin{align*} M_{i} \u0026amp;=\\sup f(x),\u0026amp;(x_{i-1} \\le x \\le x_{i}) \\\\ m_{i}\u0026amp;=\\inf f(x), \u0026amp;(x_{i-1} \\le x \\le x_{i}) \\end{align*} $$\nすると、$U(P,f), L(P,f)$を以下のように定義し、それぞれを**$P$に対する$f$のリーマン上和、下和**upper and lower Riemann sumという。\n$$ \\begin{align*} U(P,f) \u0026amp;:=\\sum \\limits _{i=1} ^n M_{i} \\Delta x_{i} \\\\ L(P,f) \u0026amp;:= \\sum \\limits _{i=1} ^{n} m_{i}\\Delta x_{i} \\end{align*} $$\n説明 リーマン和は、関数の面積を区間を分割して近似するもので、区分求積法と同じだ。与えられた分割$P$に対して、上和は最大値を、下和は最小値を意味する。上和と下和の差がないほど近似した場合、それを$f$のグラフの下の面積と見なしてもいいだろう。\nリーマン積分 区間$[a,b]$のすべての分割$P$に対して$\\inf$を取ったものを**$[a,b]$上での$f$のリーマン上積分**upper Riemann integralという。\nそれぞれの$P$に対するリーマン上和の最小上界として定義し、以下のように示す。\n$$ \\begin{equation} \\overline{\\int _{a}^{b}} f dx := \\inf \\limits_{P} U(P,f) \\label{eq1} \\end{equation} $$\n同様に、区間$[a,b]$のすべての分割$P$に対して$\\sup$を取ったものを**$[a,b]$上での$f$のリーマン下積分**lower Riemann integralという。\n$$ \\begin{equation} \\underline {\\int _{a}^b } f dx := \\sup \\limits_{P} L(P,f) \\label{eq2} \\end{equation} $$\n$f$のリーマン上積分とリーマン下積分が同じである場合、$f$は$[a,b]$でリーマン積分可能Riemann integrableであると言い、以下のように表記する。\n$$ f \\in \\mathscr{R}= \\left\\{ f : f \\text{ is Riemann integrable} \\right\\} $$\n$\\mathscr R$はリーマン積分可能な関数の集合である。そして、$(1)$と$(2)$の共通値を以下のように表記し、これを**$[a,b]$上での$f$のリーマン積分**Riemann integralという。\n$$ \\underline {\\int _{a}^b } f dx = \\int _{a} ^b f dx = \\overline {\\int _{a}^b} f dx $$\nまたは\n$$ \\int _{a} ^b f(x) dx $$\n説明 上積分は$f$の面積を少し大きく近似したもの(上和)の中で最小のものであり、下積分は$f$の面積を少し小さく近似したもの(下和)の中で最大のものだ。だから、この二つが同じである時、$f$のグラフの下の面積を正確に近似したと言えるだろう。\nさらに、$f$が有界であるため、次を満たす二つの定数$M$、$m$が存在する。\n$$ m \\le f(x) \\le M \\ \\ \\ (a\\le x\\le b) $$\nしたがって、すべての分割$P$に対して次が成り立つ。\n$$ m(b-a) \\le L(P,f) \\le U(P,f) \\le M(b-a) $$\nWalter Rudin, Principles of Mathematical Analysis (3rd Edition, 1976), p120-121\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":828,"permalink":"https://freshrimpsushi.github.io/jp/posts/828/","tags":null,"title":"分割、リーマン和、リーマン積分"},{"categories":"추상대수","contents":"拡大体の定義 1 体 $F$ にたいして $F \\le E$ が存在する場合、$E$ を $F$ の拡大体Extension Fieldという。\nクロネッカーの定理 $f(x) \\in F [ x ]$ が定数でないとすると、$F$ の拡大体 $E$ と $f ( \\alpha ) = 0$ を満たす $\\alpha \\in E$ が存在する。\n説明 拡大体の例として$\\mathbb{C}$は$\\mathbb{R}$の拡大体だ。クロネッカーの定理は、$F$ で多項式の根がすぐには存在しないかもしれないけれども、定義域を $E$ に拡大することができ、そして拡大すれば根が存在することを意味している。$F$ がどのように見えるかも知らずに拡大すれば根が存在するだろうというステートメント自体が、とても数学的な定理だ。\n証明 パート1. $f(x)$ は定数関数ではないので、$F$ 上で既約元に一意に因数分解され、その既約元の一つを $p(x)$ とする。 すると、主イデアル $\\left\u0026lt; p(x) \\right\u0026gt;$ は $F [ x ]$ の極大イデアルであり、$F [ x ] / \\left\u0026lt; p(x) \\right\u0026gt;$ は体となる。\nパート2. 拡大体 $E$ の存在性\n写像 $\\psi : F \\to F [ x ] / \\left\u0026lt; p(x) \\right\u0026gt;$ を次のように定義すると、$\\psi$ は自然に準同型写像となる。 $$ \\psi (a) := a + \\left\u0026lt; p(x) \\right\u0026gt; $$\nある $a,b \\in F$ に対して $\\psi (a) = \\psi (b)$ の場合 $$ a + \\left\u0026lt; p(x) \\right\u0026gt; = b + \\left\u0026lt; p(x) \\right\u0026gt; $$ となるので、$(b-a) \\in \\left\u0026lt; p(x) \\right\u0026gt;$ であり、これは $(b-a)$ が $p(x)$ の定数倍であることを意味する。しかし初めから $a, b \\in F$ なので、$(b-a) \\in F$ であり、$(b-a)$ が $p(x)$ の定数倍になるには $(b-a) = 0$ でなければならない。したがって $\\psi$ は単射であり、$\\psi$ は$F$ の各要素を $F [ x ] / \\left\u0026lt; p(x) \\right\u0026gt;$ のある部分体に送る同型写像となる。具体的に $E := F [ x ] / \\left\u0026lt; p(x) \\right\u0026gt;$ と定義すると $E$ が $F$ の拡大体となる。\nパート3. 根 $\\alpha \\in E$ の存在性\n$\\alpha : = x + \\left\u0026lt; p(x) \\right\u0026gt;$ とすると、まず $\\alpha$ $\\in E$ である。この $\\alpha$ に対して代入関数 $\\phi_{\\alpha} : F [ x ] \\to E$ を定義しよう。 選んだ既約元を具体的に$p(x) := a_{0} + a_{1} x + \\cdots + a_{n} x^{n}$ と表すと\n$$ \\begin{align*} p ( \\alpha) =\u0026amp; \\phi_{\\alpha} ( p(x) ) \\\\ =\u0026amp; a_{0} + a_{1} ( x + \\left\u0026lt; p(x) \\right\u0026gt; ) + \\cdots + a_{n} ( x + \\left\u0026lt; p(x) \\right\u0026gt; )^n \\\\ =\u0026amp; a_{0} + a_{1} ( x + \\left\u0026lt; p(x) \\right\u0026gt; ) + \\cdots + a_{n} ( x^n + \\left\u0026lt; p(x) \\right\u0026gt; ) \\\\ =\u0026amp; \\left( a_{0} + a_{1} x + \\cdots + a_{n} x^n \\right) + \\left\u0026lt; p(x) \\right\u0026gt; \\\\ =\u0026amp; p(x) + \\left\u0026lt; p(x) \\right\u0026gt; \\\\ =\u0026amp; 0 + \\left\u0026lt; p(x) \\right\u0026gt; \\end{align*} $$\nしたがって、$F [ x ] / \\left\u0026lt; p(x) \\right\u0026gt;$ で $p( \\alpha ) = 0$ が成り立ち、$\\alpha$は$f ( \\alpha ) = 0$ を満たす。 ■\nFraleigh. (2003). 「抽象代数入門(第7版)」: p265。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":797,"permalink":"https://freshrimpsushi.github.io/jp/posts/797/","tags":null,"title":"拡大体の定義とクロネッカーの定理の証明"},{"categories":"정수론","contents":"定義 1 整数 $n$ が全ての $1 \\le a \\le n$ に対して $a^{n} \\equiv a \\pmod{n}$ を満たす場合、カーマイケル数と呼ばれる。\n定理 全てのカーマイケル数は$2$ を除く異なる素数の積で表される。\n説明 カーマイケル数は合成数でありながらフェルマーの小定理を通過する、つまり素数のように見える数だ。例えば、$561=3 \\cdot 11 \\cdot 17$ は合成数だが$a^{561} \\equiv a \\pmod{561}$ は常に成り立つ。\nこれらのカーマイケル数を捕まえるためにミラー-ラビン素数判定法がある。\n証明 $n$ をカーマイケル数とする。\nパート1。 $a = n-1$ とすると $n-1 \\equiv -1 \\pmod{n}$ であるため $$ (n-1)^{n} \\equiv (-1)^{n} \\equiv -1 \\pmod{n} $$ 上記の合同式が成り立つのは$n$ が奇数のときだけである。\nパート2。 $n$ が$2$ を除く素数 $p_{1}, \\cdots , p_{m}$ に対して$n = p_{1}^{r_{1} + 1} \\cdots p_{m}^{r_{m} + 1}$ と表されるとする。ここで $\\displaystyle r : = \\max \\left\\{ r_{1}, \\cdots , r_{m} \\right\\}$ と置き$r=0$ であることを示せば証明は完了である。この$r$ に対応する素数を$p$ とする。\n$n$ はカーマイケル数であるので$p^{rn} \\equiv p^{r} \\pmod{n}$ が成り立ち、$n$ は$\\left( p^{rn} - p^{r} \\right)$ を割る必要がある。また、$n$ の約数の中には$p^{r+1}$ もあるため、$p^{r+1}$ は$\\left( p^{rn} - p^{r} \\right)$ を割る必要がある。つまり $$ {{ p^{rn} - p^{r} } \\over { p^{r+1} }} = {{ p^{rn - r} - 1 } \\over { p }} $$ これは整数であることを意味し、これが可能なのは分子が$0$ となる$r=0$ のみである。\n■\nSilverman. (2012). A Friendly Introduction to Number Theory (4th Edition): p133.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":794,"permalink":"https://freshrimpsushi.github.io/jp/posts/794/","tags":null,"title":"カーマイケル数"},{"categories":"힐베르트공간","contents":"証明 $f = 0$とする。$f ( \\cdot ) = \\left\\langle \\cdot , \\mathbf{0} \\right\\rangle$と$\\| f \\| = 0$を満たす$\\mathbf{y} = \\mathbf{0}$が存在するので証明することはないので$f \\ne 0$としよう。\n第1部. 存在性\n直交分解定理\nヒルベルト空間$H$の閉部分空間$W$に対して、\n$$ H = W \\oplus W^{\\perp} $$\n$W= \\ker (f) = \\left\\{ \\mathbf{x} \\in H : f(\\mathbf{x}) = 0 \\right\\}$としよう。$\\ker (f)$は閉部分空間だから直交分解定理によって\n$$ H = W \\oplus W^{\\perp} $$\nで$W^{\\perp} \\ne \\left\\{ \\mathbf{0} \\right\\}$である。だから$\\left\\| \\mathbf{y} \\right\\|=1$な$\\mathbf{y} \\in W^{\\perp}$を一つ選ぼう。$W^{\\perp}$もベクトル空間なので$\\mathbf{0}$でない要素が少なくとも一つあれば、そのような$\\mathbf{y}$の存在は必ず保証される。そして今$\\mathbf{x} \\in H$に対して、次のようなベクトル$\\mathbf{z} \\in H$を考える。\n$$ \\mathbf{z} := f(\\mathbf{x})\\mathbf{y} - f(\\mathbf{y})\\mathbf{x} $$\nここで$f(\\mathbf{x}), f(\\mathbf{y})$は定数であることに注意しよう。この$\\mathbf{z}$に線形汎函数$f$を適用すると、線形性により次を得る。\n$$ f(\\mathbf{z}) = f\\left( f(\\mathbf{x})\\mathbf{y} - f(\\mathbf{y})\\mathbf{x} \\right) = f(\\mathbf{x})f(\\mathbf{y}) - f(\\mathbf{y})f(\\mathbf{x}) = 0 $$\nしたがって$\\mathbf{z} \\in W$である。$\\mathbf{y} \\in W^{\\perp}$と言ったので、両者の内積は$0$である。\n$$ \\left\\langle \\mathbf{z}, \\mathbf{y} \\right\\rangle = \\left\\langle f(\\mathbf{x})\\mathbf{y} - f(\\mathbf{y})\\mathbf{x}, \\mathbf{y} \\right\\rangle = 0 $$\n内積を展開すると次を得る。\n$$ f(\\mathbf{x}) \\left\\langle \\mathbf{y}, \\mathbf{y} \\right\\rangle - f(\\mathbf{y})\\left\\langle \\mathbf{x}, \\mathbf{y} \\right\\rangle = f(\\mathbf{x}) \\left\\| \\mathbf{y} \\right\\|^{2} - f(\\mathbf{y})\\left\\langle \\mathbf{x}, \\mathbf{y} \\right\\rangle = f(\\mathbf{x}) - f(\\mathbf{y})\\left\\langle \\mathbf{x}, \\mathbf{y} \\right\\rangle=0 $$\n$$ \\implies f(\\mathbf{x}) = f(\\mathbf{y})\\left\\langle \\mathbf{x}, \\mathbf{y} \\right\\rangle = \\left\\langle \\mathbf{x}, \\overline{f(\\mathbf{y})}\\mathbf{y} \\right\\rangle $$\n$\\mathbf{w} = \\overline{f(\\mathbf{y})}\\mathbf{y}$とすると$f(\\mathbf{x}) = \\left\\langle \\mathbf{x} , \\mathbf{w} \\right\\rangle$である。またコーシー・シュワルツの不等式によって $$ \\| f \\| = \\sup_{ \\| \\mathbf{x} \\| =1} | f(\\mathbf{x}) | = \\sup_{ \\| \\mathbf{x} \\| =1} \\left| \\left\\langle \\mathbf{x} , \\mathbf{w} \\right\\rangle \\right| \\le \\sup_{ \\| \\mathbf{x} \\| =1} \\| \\mathbf{x} \\| \\cdot \\| \\mathbf{w} \\| = \\| \\mathbf{w} \\| $$\nであり、$\\left\\| \\dfrac{\\mathbf{w}}{\\left\\| \\mathbf{w}\\right\\|} \\right\\|=1$であるので\n$$ \\| f \\| = \\sup_{ \\| \\mathbf{x} \\| =1} | f(\\mathbf{x}) | = \\sup_{ \\| \\mathbf{x} \\| =1} \\left| \\left\\langle \\mathbf{x} , \\mathbf{w} \\right\\rangle \\right| \\ge \\left\\langle {{ \\mathbf{w} } \\over { \\| \\mathbf{w} \\| }} , \\mathbf{w} \\right\\rangle = {{ \\| \\mathbf{w} \\|^2 } \\over { \\| \\mathbf{w} \\| }} = \\| \\mathbf{w} \\| $$\nが成立して$\\left\\| f \\right\\| = \\left\\| \\mathbf{w} \\right\\|$である。従って$\\mathbf{w}$が定理の条件を満たしていることがわかる。\n第2部. 一意性\n今$\\mathbf{w}^{\\prime}$も\n$$ f(\\mathbf{x}) = \\left\\langle \\mathbf{x}, \\mathbf{w}^{\\prime} \\right\\rangle $$\nを満たすとしよう。\n補助定理\n$\\left( H, \\left\\langle \\cdot,\\cdot \\right\\rangle \\right)$をヒルベルト空間とし、$x, y \\in H$に対して次が成り立つとしよう：\n$$ \\left\\langle x, y \\right\\rangle = \\left\\langle x, z \\right\\rangle, \\quad \\forall x \\in H $$\nすると$y = z$である。\nそれなら、上の補助定理によって、このような$\\mathbf{w}=\\mathbf{w}^{\\prime}$であることがわかる。\n■\n","id":786,"permalink":"https://freshrimpsushi.github.io/jp/posts/786/","tags":null,"title":"リウヴィルの定理の証明"},{"categories":"수치해석","contents":"定義 $$U_{n} (x) := {{1} \\over {n+1} } T_{n+1} \u0026rsquo; (x) = {{\\sin \\left( ( n +1 ) \\theta \\right)} \\over { \\sin \\theta }} $$を第2種チェビシェフ多項式と言うんだ。\n基本性質 再帰公式 [0]: $$U_{n+1} (x) = 2x U_{n} (x) - U_{n-1} (X)$$ 直交集合 [1] 関数の内積: $\\displaystyle \\left\u0026lt;f, g\\right\u0026gt;:=\\int_a^b f(x) g(x) w(x) dx$ に対して重み $w$ を $\\displaystyle w(x) := \\sqrt{1 - x^2}$ のようにすると、$\\left\\{ U_{0} , U_{1}, U_{2}, \\cdots \\right\\}$ は直交集合になるんだ。 チェビシェフノード [2]: $\\displaystyle U_{n} (X)$ の根は $k=1, \\cdots , n$ に対して次のようだ。 $$x_{k} = \\cos \\left( {{k} \\over {n+1}} \\pi \\right)$$ 偶関数と奇関数 [3]: $$U_{n} (-x) = (-1)^{n} U_{n} (x)$$ 普通、$0 \\le \\theta \\le \\pi$ に対して $\\theta := \\cos^{-1} x $ と設定するんだ。 同じく見る 第1種チェビシェフ多項式 第2種チェビシェフ多項式 第1種、第2種チェビシェフ多項式の関係 チェビシェフ微分方程式の解としてのチェビシェフ多項式 説明 $n = 0, \\cdots , 3$ に対する第2種チェビシェフ多項式は次のように表されるんだ。\n$$ \\begin{align*} U_{0} (x) =\u0026amp; 1 \\\\ U_{1} (x) =\u0026amp; 2x \\\\ U_{2} (x) =\u0026amp; 4x^{2} - 1 \\\\ U_{3} (x) =\u0026amp; 8x^{3} - 4x \\end{align*} $$\n$T_{n} (X)$ は第1種チェビシェフ多項式だ。\n$\\displaystyle {{1} \\over {n+1} } T_{n+1} \u0026rsquo; (x) = {{\\sin \\left( ( n +1 ) \\theta \\right)} \\over { \\sin \\theta }}$ が成り立つことは 逆三角関数の微分法 を使えば次のように示せるんだ。 $$ \\begin{align*} \\displaystyle U_{n} (x) =\u0026amp; {{1} \\over {n+1} } \\left[ \\cos \\left( ( n +1 ) \\cos^{-1} x \\right) \\right]\u0026rsquo; \\\\ \u0026amp;= {{n+1} \\over {n+1} } {{ - 1} \\over { \\sqrt{ 1 - x^{2} } }} \\left[ - \\sin \\left( ( n +1 ) \\cos^{-1} x \\right) \\right] \\\\ =\u0026amp; {{\\sin \\left( ( n +1 ) \\cos^{-1} x \\right)} \\over { \\sqrt{ 1 - x^{2} } }} \\\\ =\u0026amp; {{ \\sin \\left( (n+1) \\theta \\right) } \\over {\\sin \\theta }} \\end{align*} $$ 第2種チェビシェフ多項式は数値解析だけでなく、応用数学全般で非常に役立つ関数で、第1種チェビシェフ多項式と共に面白い性質をたくさん持っているんだ。\n一方で、第2種チェビシェフ多項式は逆に $U_{0} (x) = 1$、$U_{1} (x) = 2x$ そして再帰式 [0] を使って定義することもできる。これは第1種チェビシェフ多項式にも当てはまることだし、第1種と第2種を呼ぶ理由は $T_{1} (x) = 1 \\cdot x$ と $U_{1} (x) = 2 \\cdot x$ だと考えてもいいんだ。\n証明 [0] 第1種チェビシェフ多項式の再帰式 $T_{n+1} (x) = 2x T_{n} (x) - T_{n-1} (X)$ の両辺を微分すると $$ T_{n+1} ' (x) = 2 T_{n} (x) + 2x T_{n} ' (x) - T_{n-1} ' (x) $$ $T_{n+1} ' (x) = ( n+1 ) U_{n} (x) $ なので $$ (n+1) U_{n} (x) = 2 T_{n} (x) + 2x n U_{n-1} (x) - (n-1) U_{n-2} (x) $$ $n$ とまとめると $$ n \\left[ U_{n} (x) - 2x U_{n-1} (x) + U_{n-2} (x) \\right] = 2 T_{n} (x) + U_{n-2} (x) - U_{n} (x) $$\n第1種、第2種チェビシェフ多項式の関係:\n[1]: $$U_{n} (x) - U_{n-2} (x) = 2 T_{n} (X)$$ $$ n \\left[ U_{n} (x) - 2x U_{n-1} (x) + U_{n-2} (x) \\right] = 0 $$ 両辺を $n$ で割って整理すると $$ U_{n+1} (x) = 2x U_{n} (x) - U_{n-1} (x) $$\n■\n[1] $dx = - \\sin \\theta d \\theta = - \\sqrt{1 - x^2} d \\theta$ かつ $\\sin \\theta = \\sqrt{1 - x^2}$ だから $$ \\begin{align*} \\displaystyle \\left\u0026lt; U_{n}, U_{m} \\right\u0026gt; =\u0026amp; \\int_{-1}^{1} U_{n} (x) U_{m} (x) \\sqrt{1 - x^2} d x \\\\ =\u0026amp; - \\int_{\\pi}^{0} {{ \\sin \\left( (n + 1 ) \\theta \\right) \\sin \\left( (m + 1 ) \\theta \\right) \\sin^2 \\theta } \\over { \\sin^2 \\theta}} d \\theta \\\\ =\u0026amp; \\int_{0}^{\\pi} \\sin \\left( (n + 1 ) \\theta \\right) \\sin \\left( (m + 1 ) \\theta \\right) d \\theta \\\\ =\u0026amp; \\begin{cases} \\pi/2 \u0026amp;, n=m \\\\ 0 \u0026amp;, n \\ne m \\end{cases} \\end{align*} $$ だから、$\\left\\{ U_{0} , U_{1}, U_{2}, \\cdots \\right\\}$ は直交集合だ。\n■\n[2] 定義により自明だ。\n■\n[3] ケース 1. $n=0,1$\n$$ \\begin{align*} U_{0} (-x) =\u0026amp; 1 = U_{0} (x) \\\\ U_{1} (-x) =\u0026amp; 2(-x) = -2x = - U_{1} (x) \\end{align*} $$\nケース 2. $n \\ge 2$ が偶数\n$U_{n}(x)$ で、係数が $0$ でないすべての項の次数は偶数だから、$U_{n}(-x) = U_{n}(x)$ だ\nケース 3. $n \\ge 2$ が奇数\n$U_{n}(x)$ で、係数が $0$ でないすべての項の次数は奇数だから、$U_{n}(-x) = - U_{n}(x)$ だ\n■\n実装 下はRで書かれたチェビシェフ多項式のコードだ。\n多項式そのものを返すから、直接計算に使うことができるんだ。nは次数で、kindで種類を指定し、printオプションを真にすれば係数を表示してくれるんだ。\n表示される係数は定数項から高次の項の順に出力され、第2種チェビシェフ多項式は$U_{3} (x) = 8x^{3} - 4x$ だから、正しく求まったことがわかる。関数値も $U_{3} (3) = 8 \\cdot 3^{3} - 4 \\cdot 3 = 216-12 = 204$ と正確に計算されたんだ。\nChebyshev\u0026lt;-function(n,kind=1,print=F)\r{\rp\u0026lt;-NA\rif((round(n)-n)!=0 | n\u0026lt;0) {stop(\u0026#34;Wrong Degree!!\u0026#34;)} #degree must be nonnegative integer\rif(!kind%in%(1:2)) {stop(\u0026#34;Wrong Kind!!\u0026#34;)} #kind must be 1 or 2\rif(n==0)\r{\rif(print) {print(1)}\rp\u0026lt;-function(x) {return(1)}\rreturn(p)\r}\rif(n==1)\r{\rif(print) {print(c(0,kind))}\rp\u0026lt;-function(x) {return(kind*x)}\rreturn(p)\r}\rcoef0\u0026lt;-c(1)\rcoef1\u0026lt;-c(0,kind)\rfor(i in 1:(n-1))\r{\rcoef2\u0026lt;- ( c(0,2*coef1) - c(coef0,0,0) )\rcoef0\u0026lt;-coef1\rcoef1\u0026lt;-coef2\r}\rif(print) {print(coef2)}\rp\u0026lt;-function(x) {return(sum(coef2*x^(0:n)))}\rreturn(p)\r}\rp\u0026lt;-Chebyshev(1,2); p(2)\rp\u0026lt;-Chebyshev(3,2,T); p(3) ","id":779,"permalink":"https://freshrimpsushi.github.io/jp/posts/779/","tags":null,"title":"第二種チェビシェフ多項式"},{"categories":"수치해석","contents":"定義 1 $T_{n} (x) = \\cos \\left( n \\cos^{-1} x \\right)$ を第1種チェビシェフ多項式という。\n基本性質 再帰公式 [0]: $$T_{n+1} (x) = 2x T_{n} (x) - T_{n-1} (X)$$ 直交集合 [1] 関数の内積: 重み$w$ を$\\displaystyle w(x) := {{1} \\over { \\sqrt{1 - x^2} }}$ のように与えると、$\\left\\{ T_{0} , T_{1}, T_{2}, \\cdots \\right\\}$ は直交集合になる。 チェビシェフ・ノード [2]: $T_{n} (x)$ の根は、$k=1, \\cdots , n$ に対して以下のようなチェビシェフ・ノードだ。 $$x_{k} = \\cos \\left( {{2k-1} \\over {2n}} \\pi \\right)$$ 奇関数と偶関数 [3]: $$T_{n} (-x) = (-1)^{n} T_{n} (x)$$\n通常、$0 \\le \\theta \\le \\pi$ に対して、$\\theta := \\cos^{-1} x $ とする。 参照 第1種チェビシェフ多項式 第2種チェビシェフ多項式 第1種と第2種チェビシェフ多項式の関係 チェビシェフ微分方程式の解としてのチェビシェフ多項式 説明 $n = 0, \\cdots , 3$ に対する第1種チェビシェフ多項式は、次のように表される。\n$$ \\begin{align*} T_{0} (x) =\u0026amp; 1 \\\\ T_{1} (x) =\u0026amp; x \\\\ T_{2} (x) =\u0026amp; 2x^{2} - 1 \\\\ T_{3} (x) =\u0026amp; 4x^{3} - 3x \\end{align*} $$\n第1種チェビシェフ多項式は、数値解析だけでなく、応用数学全般で非常に役立つ関数であり、第2種チェビシェフ多項式と合わせて、興味深い性質を豊富に持つ。\n一方、第1種チェビシェフ多項式は、逆に$T_{0} (x) = 1$、$T_{1} (x) = x$ そして再帰式 [0] を使って定義することもできる。これは第2種チェビシェフ多項式も同様であり、第1種と第2種を呼ぶ理由は、$T_{1} (x) = 1 \\cdot x$ と $U_{1} (x) = 2 \\cdot x$ のためと考えても良い。\n証明 [0] $T_{n} (x) = \\cos \\left( n \\theta \\right)$ であるから、三角関数の加法定理により $$ T_{n \\pm 1} (x) = \\cos (n \\pm 1) \\theta = \\cos (n \\theta ) \\cos \\theta \\mp \\sin ( n \\theta ) \\sin \\theta $$ 両辺を足すと $$ T_{n+1} (x) + T_{n-1} (x) = 2 \\cos (n \\theta ) \\cos \\theta = 2 T_{n} (x) x $$ $T_{n-1} (x)$ を整理すると $$ T_{n+1} (x) = 2x T_{n} (x) - T_{n-1} (x) $$\n■\n[1] $dx = - \\sin \\theta d \\theta = - \\sqrt{1 - x^2} d \\theta$ であるから $$ \\begin{align*} \\displaystyle \\left\u0026lt; T_{n}, T_{m} \\right\u0026gt; =\u0026amp; \\int_{-1}^{1} T_{n} (x) T_{m} (x) {{1} \\over { \\sqrt{1 - x^2} }} dx \\\\ =\u0026amp; - \\int_{\\pi}^{0} \\cos n \\theta \\cos m \\theta d \\theta \\\\ =\u0026amp; \\int_{0}^{\\pi} \\cos n \\theta \\cos m \\theta d \\theta \\\\ =\u0026amp; \\begin{cases} \\pi/2 \u0026amp;, n=m \\\\ 0 \u0026amp;, n \\ne m \\end{cases} \\end{align*} $$ 従って、$\\left\\{ T_{0} , T_{1}, T_{2}, \\cdots \\right\\}$ は直交集合である。\n■\n[2] 定義より自明である。\n■\n[3] ケース 1. $n=0,1$\n$$ T_{0} (-x) = 1 = T_{0} (x) $$\n$$ T_{1} (-x) = (-x) = -x = - T_{1} (x) $$\nケース 2. $n \\ge 2$ が偶数\n$T_{n}(x)$ の、係数 $0$ 以外の全ての項が偶数次であるから、$T_{n}(-x) = T_{n}(x)$\nケース 3. $n \\ge 2$ が奇数\n$T_{n}(x)$ の、係数 $0$ 以外の全ての項が奇数次であるから、$T_{n}(-x) = - T_{n}(x)$\n■\n実装 以下は、Rで書かれたチェビシェフ多項式のコードだ。\n多項式自体が返されるので、直接計算に使うことができる。nは次数、kindで種類を指定し、printオプションをtrueにすると、係数が出力される。\n出力される係数は、定数項から高次項順に出力され、第1種チェビシェフ多項式が$T_{3} (x) = 4x^{3} - 3x$ であるため、正しく得られたことがわかる。関数値も$T_{3} (3) = 4 \\cdot 3^{3} - 3 \\cdot 3 = 108-9 = 99$ で正確に計算された。\nChebyshev\u0026lt;-function(n,kind=1,print=F)\r{\rp\u0026lt;-NA\rif((round(n)-n)!=0 | n\u0026lt;0) {stop(\u0026#34;Wrong Degree!!\u0026#34;)} #degree must be nonnegative integer\rif(!kind%in%(1:2)) {stop(\u0026#34;Wrong Kind!!\u0026#34;)} #kind must be 1 or 2\rif(n==0)\r{\rif(print) {print(1)}\rp\u0026lt;-function(x) {return(1)}\rreturn(p)\r}\rif(n==1)\r{\rif(print) {print(c(0,kind))}\rp\u0026lt;-function(x) {return(kind*x)}\rreturn(p)\r}\rcoef0\u0026lt;-c(1)\rcoef1\u0026lt;-c(0,kind)\rfor(i in 1:(n-1))\r{\rcoef2\u0026lt;- ( c(0,2*coef1) - c(coef0,0,0) )\rcoef0\u0026lt;-coef1\rcoef1\u0026lt;-coef2\r}\rif(print) {print(coef2)}\rp\u0026lt;-function(x) {return(sum(coef2*x^(0:n)))}\rreturn(p)\r}\rp\u0026lt;-Chebyshev(1,1); p(2)\rp\u0026lt;-Chebyshev(3,1,T); p(3) Atkinson. (1989). 数値解析入門(第2版): p211.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":777,"permalink":"https://freshrimpsushi.github.io/jp/posts/777/","tags":null,"title":"第一種チェビシェフ多項式"},{"categories":"힐베르트공간","contents":"定義1 完備 内積空間をヒルベルト空間Hilbert spaceと言う。ヒルベルトの名前から、主に$H$と表記される。\n説明 完備空間とは、すべてのコーシー数列が収束する空間のことだ。バナッハ空間も完備空間なので、内積が定義されたバナッハ空間としてヒルベルト空間を説明することもできる。例えば、以下のような空間がある。\nルベーグ空間 $L^{2}$ $\\ell^{2}$ 空間 実数空間 $\\mathbb{R}^{n}$ 複素数空間 $\\mathbb{C}^{n}$ 性質 ヒルベルト空間は一様に凸である 最短ベクトル定理 直交分解定理 リース表現定理 Ole Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010), p65\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":776,"permalink":"https://freshrimpsushi.github.io/jp/posts/776/","tags":null,"title":"関数解析学におけるヒルベルト空間"},{"categories":"바나흐공간","contents":"定義 二つの距離空間 $(X,\\ d_X), (Y,\\ d_Y)$に対して、下の条件を満たす写像 $f : X \\to Y$が存在するなら、$X$と$Y$が等距離isometricだと言い、$X \\approx Y$と記される。また、その写像 $f$を等距離写像isomertic map, isometryという。\n$$ d_X(x_1,\\ x_2) =d_Y\\big( f(x_1),\\ f(x_2) \\big),\\quad \\forall\\ x_1,x_2\\in X $$\n説明 名前が示す通り、等距離写像は距離を保持する写像である。従って、等距離写像が存在する二つの空間は「事実上」同じであると見なせる。また、等距離写像は定義から自然に単射になる。\nノルム空間で もし$X$と$Y$がノルム空間であれば、以下の距離が定義されているため、等距離写像はノルムを保持する写像になる。\n$$ d_X(x_1,x_2) = \\|x_1-x_2\\|_X $$\n定義1 $(X, \\left\\| \\cdot \\right\\|_{X}), (Y, \\left\\| \\cdot \\right\\|_{Y})$をノルム空間としよう。$X$と$Y$に対して、下の条件を満たす線形作用素 $L\\ : X \\to Y$が存在するならば、$L$を等距離同型写像isometric isomorphismという。また、$X$と$Y$が等距離同型isometrically isomorphicであるという。\n$$ \\|x\\|_X = \\|L(x)\\|_Y, \\quad \\forall\\ x\\in X $$\n性質 等距離写像について、以下の事実が成り立つ。\n等距離写像は単射である。 等距離写像はホメオモルフィズムである。 $\\approx$は同値関係である。 等距離写像は埋め込みである。 証明 $x_1,x_2\\in X$であり、$f(x_1)=f(x_2)$とする。すると、距離の定義から$d_Y\\big( f(x_1),\\ f(x_2) \\big)=0$が成立する。$f$は距離を保持するので、$d_X(x_1,\\ x_2)=0$が成立し、同様に距離の定義から$x_1=x_2$が成立する。$f(x_1)=f(x_2)$ならば、$x_1=x_2$なので、$f$は単射である。\n■\nRobert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003), p5\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":756,"permalink":"https://freshrimpsushi.github.io/jp/posts/756/","tags":null,"title":"等距離写像"},{"categories":"선형대수","contents":"双対空間 定義11 ベクトル空間 $X$ の全ての連続する線形汎関数の集合を $X^{ \\ast }$ と標記し、これを $X$ の双対空間dual space、簡単に $X$ のデュアルと呼び、以下のように表記する。\n$$ X^{ \\ast }:=\\left\\{ x^{ \\ast }:X\\to \\mathbb{C}\\ |\\ x^{ \\ast } \\text{ is continuous and linear} \\right\\} $$\n$$ X^{ \\ast }:=B(X,\\mathbb{C}) $$\n$B \\left( X, \\mathbb{C} \\right)$ は、定義域が $X$ で値域が $\\mathbb{C}$ の有界線形作用素の集合である。\n定義22 体 $F$ 上のベクトル空間 $X$ に対して、$X$ 上の線形汎関数の集合を $X$ の双対空間dual spaceと呼び、$X^{\\ast}$ と表記する。\n$$ X^{\\ast} = L(X, F) $$\n$L(X, F)$ は、$X$ から $F$ への全ての線形変換の集合である。\n説明 線形作用素の性質により、連続という条件は有界という条件と同値である。 双対空間の記号として $\\ast$ ではなく $^{\\prime}$ も使われることがある。 双対空間の双対空間についても話すことができる。この場合は、$X^{\\ast \\ast}=\\left( X^{ \\ast } \\right)^{ \\ast }$ のように表記し、バイデュアルbidual、ダブルデュアルdouble dual、セカンドデュアルsecond dualなどと呼ばれる。\nオペレーターのノルム $\\displaystyle \\| f \\| = \\sup_{\\substack{x \\in X \\\\ \\| x \\| =1}} | f(x) |$ について、$(X^{ \\ast } , \\| \\cdot \\| )$ は バナッハ空間となる。これに対して、次の定理が成立する。\n定理 $X$ が有限次元 ベクトル空間である場合、次が成立する。\n$$ \\dim X^{ \\ast } = \\dim X $$\n証明 方法11 戦略: $\\dim X$ の基底を使って、$\\dim X^{ \\ast }$ が有限次元になるような基底を作る。\n$\\dim X = n$ とすると、$X$ は有限次元なので基底 $\\left\\{ \\tilde{ e_{1} } , \\cdots , \\tilde{ e_{n} } \\right\\}$ を持つ.$\\displaystyle e_{j} : = {{ \\tilde{e_{j} } } \\over { \\| \\tilde{ e_{j} } \\| }} \\in X$ とすると、$\\| e_{j} \\| = 1$ であり、$\\left\\{ e_{1} , \\cdots , e_{n} \\right\\}$ は依然として $X$ の基底である。今、$e_{j}^{ \\ast } : (X , \\| \\cdot \\| ) \\to ( \\mathbb{C} , | \\cdot | )$ を以下のように定義しよう。\n$$ e_{j}^{ \\ast } (e_{i}) := \\delta_{ij} $$\n線形作用素の性質\n$T : (X , \\| \\cdot \\|_{X}) \\to ( Y , \\| \\cdot \\|_{Y} )$ が線形作用素だとしよう。$X$ が有限次元空間であれば、$T$ は連続である。\n$\\dim X = n$ と仮定したので、$e_{j}^{ \\ast }$ は連続する線形汎関数である。\n線形汎関数が線形独立組合せで表されるための必要十分条件\n$f_{1} , \\cdots , f_{n}$ が定義域が $X$ の線形汎関数だとしよう。\n$f_{1} , \\cdots , f_{n}$ が線形独立 $\\iff$ $f_{j} (x_{i} ) = \\delta_{ij}$ を満たす $x_{1} , \\cdots , x_{n}$ が存在する\n上記の定理により、$\\beta^{\\ast} = \\left\\{ e_{1}^{\\ast}, \\dots, e_{n}^{\\ast} \\right\\}$は線形独立である。$f \\in X^{ \\ast }$ を任意の $\\displaystyle x = \\sum_{i=1}^{n} t_{i} e_{i} \\in X$ に作用させると\n$$ f(x) = f\\left( \\sum_{i=1}^{n} t_{i} e_{i} \\right) = \\sum_{i=1}^{n} t_{i} f(e_{i} ) = \\sum_{i=1}^{n} f(e_{i} ) t_{i} $$\n$\\displaystyle t_{i} = e_{i}^{ \\ast } \\left( \\sum_{k=1}^{n} t_{k} e_{k} \\right) = e_{i}^{ \\ast } (x)$ であるから、\n$$ f(x) = \\sum_{i=1}^{n} f(e_{i} ) e_{i}^{ \\ast } (x) = \\left[ \\sum_{i=1}^{n} f(e_{i} ) e_{i}^{ \\ast } \\right] (x) $$\n従って、\n$$ f = \\sum_{i=1}^{n} f(e_{i} ) e_{i}^{ \\ast } \\in \\text{span} \\left\\{ e_{1}^{ \\ast } , \\cdots , e_{n}^{ \\ast } \\right\\} $$\nつまり、$\\beta^{\\ast} = \\left\\{ e_{1}^{ \\ast } , \\cdots , e_{n}^{ \\ast } \\right\\}$ は線形独立であり、$X^{\\ast}$ を生成するので、$X^{ \\ast }$ の基底である。\n$$ \\dim X^{ \\ast } = n $$\n■\n方法22 $\\dim(L(X,F)) = \\dim(X)\\dim(F)$であるから、\n$$ \\dim(X^{\\ast}) = \\dim(L(X,F)) = \\dim(X)\\dim(F) = \\dim(X) $$\n■\n定理の証明はこれで終わりだが、$X^{\\ast}$ の基底を具体的に見つけよう。$X$ の順序基底を $\\beta = \\left\\{ x_{1}, \\dots, x_{n} \\right\\}$ としよう。そして、$f_{i}$ を $i$ 番目の座標関数としよう。\n$$ f_{i}(x_{j}) = \\delta_{ij} $$\nすると、$f_{i}$ は $X$ 上で定義された線形汎関数である。今、$\\beta^{\\ast} = \\left\\{ f_{i}, \\dots, f_{n} \\right\\}$ としよう。\nClaim: $\\beta^{\\ast}$ は $X^{\\ast}$ の（順序）基底である\n$\\dim (X^{\\ast}) = n$ であることは既にわかっているので、$\\span(\\beta^{\\ast}) = X^{\\ast}$ を示せばいい。つまり、任意の $f \\in X^{\\ast}$ が $f_{i}$ の線形組合せで表されることを示さなければならない。与えられた $f$ に対して、$g = \\sum_{i=1}^{n}f(x_{i})f_{i}$ とする。すると、実際にこの $g$ がまさに $f$ であり、$f$ が $f_{i}$ の線形組合せで表されることがわかる。$1 \\le j \\le n$ について、\n$$ g(x_{j}) = \\left( \\sum_{i=1}^{n}f(x_{i})f_{i} \\right) (x_{j}) = \\sum_{i=1}^{n}f(x_{i})f_{i}(x_{j}) = \\sum_{i=1}^{n}f(x_{i})\\delta_{ij} = f(x_{j}) $$\nよって、$g=f$ であり、$f = \\sum_{i=1}^{n}f(x_{i})f_{i}$ である。したがって、$\\beta^{\\ast}$ は $X^{\\ast}$ を生成する。\n双対基底 上記の記法に従い、$X^{\\ast}$ の順序基底 $\\beta^{\\ast} = \\left\\{ f_{1}, \\dots, f_{n} \\right\\}$ を $\\beta$ の双対基底dual basis、相互基底reciprocal basisと呼ぶ。\n$$ f_{i} : X \\to \\mathbb{F} \\quad \\text{ by } \\quad f_{i}(x_{j}) = \\delta_{ij} $$\nこの場合、$\\delta_{ij}$ はクロネッカーデルタである。\nKreyszig. (1989). Introductory Functional Analysis with Applications: p106.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStephen H. Friedberg, Linear Algebra (4th Edition, 2002), p119-120\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":753,"permalink":"https://freshrimpsushi.github.io/jp/posts/753/","tags":null,"title":"双対空間"},{"categories":"통계적분석","contents":"診断法 標準化残差図 を使って、回帰分析が正しく行われたか確認できる。\n正規性については、残差の散らばりよりも、ヒストグラムで確認したり、正規性検定を行う方が良い。\n左側は中央から上下に向かって密度が小さくなるが、右は上下問わず均等に広がっている。\nしかし、実際の解析では、このように残差が正規分布以外の知られた分布に従うケースはほとんど見られない。分析で正規性が問題になるのは大体、異常値Outlier のせいだ。\nサンプルに比べて異常値が多過ぎる場合や、6~7に迫るようなおかしい数字が出る場合に正規性に問題が生じる。このような異常値を盲目的に除去するのではなく、分析者が一つ一つのデータをしっかりと確認する必要がある。\n参照 線形性 独立性 等分散性 ","id":683,"permalink":"https://freshrimpsushi.github.io/jp/posts/683/","tags":null,"title":"モデル診断による残差の正規性の確認"},{"categories":"통계적분석","contents":"診断法 直感的パターン認識 標準化残差図を使って回帰分析が正しく行われたかを確認できる。独立性を確かめるためには、残差図にはっきりした傾向が現れていないことが条件だ。残念ながら、独立性の診断は他の回帰分析の仮定に比べて非常に主観的になるしかない。\n独立性が欠けている一番よく見られる例は、上のように特定できない直線が現れることだ。偶然の可能性もあるけれど、通常はデータを理解していないか、重要なデータが抜けている時に現れる現象だ。\n一見、何の問題もなさそうだけど、よく見ると$9$ごとに同じパターンが繰り返されている。この程度の規則性が現れているならば、自信を持って独立性が欠けていると言える。この時、残差は自己相関Autocorrelationがあるとされ、時系列を含む分析を考慮するのが良い。\n統計的検定 このような問題は、分析者自身がこれらのデータを見る目が悪いと思ったらダービン・ワトソン検定を使えば簡単に見つけられる。問題は、だからといってダービン・ワトソン検定を完全に信頼してはいけないということだ。ダービン・ワトソン検定は自己相関を見つけるだけで、独立性そのものについては保証できないからだ。下の図は理解を助けるために非常に作為的に作られたものだが、ダービン・ワトソン検定を通過しても独立性が欠けている例はいくらでもある。\n図では、残差が$0$にほぼ正確に当たる場合、次の二回も$0$に当たるが、最初の$0$がいつ現れるかはわからない。このような場合、残差の独立性には深刻な問題があると言えるが、自己相関があると言うには不規則すぎる。偶然の可能性もあるが、それを判断するのは完全に分析者の役割だ。\n併せて見る 線形性 等分散性 正規性 ","id":679,"permalink":"https://freshrimpsushi.github.io/jp/posts/679/","tags":null,"title":"モデル診断による残差の独立性の確認"},{"categories":"추상대수","contents":"定義 1 環 $(R , + , \\cdot )$ の全ての $a,b \\in R$ に対して $a I \\subset I$ と $I b \\subset I$ を満たす部分群 $(I, +)$ をアイディアルIdealと呼ぶ。\n説明 簡単な例として、$n \\mathbb{Z}$ は $\\mathbb{Z}$ のアイディアルになる。アイディアルという名前は文字通り 理想的なIdealから来たものだ。抽象代数で扱うには理想的な部分群だから、実際にそう呼ぶわけだ。\n特に$R$ が可換環である場合、$I$ が $R$ の正規部分群になる意味で、ただの $I \\triangleleft R$ とも呼ばれる。正規部分群が群論で重要だったように、アイディアルも環論のあらゆる定理で重要に登場することだろう。特に環論と呼ぶのは、アイディアルが実質的に環の概念に限定されるからだ。\nアイディアル $I$ は $R$ の部分環だ。\n定義では、条件を満たす「部分群」として群との対比を強調したが、実際には自然に部分環にもなる。ここでは証明はしないが、理解に苦しむ場合は、$a I \\subset I$ と $I b \\subset I$ という条件をよく考えればいい。感じとして、$I$ は $R$ の全ての元に対して「乗算を施した時」に依然として代数構造として存在できる「耐えうる元の集まり」だ。常識的にこのように作り出した $(I , \\cdot )$ は少なくとも $(R , + , \\cdot)$ に対して半群くらいは成り立つだろう。もちろん、この説明は数学的ではないので、疑問が残る場合は部分環の判定法を使って直接確認してみよう。実際には、教科書によっては最初から部分環として定義していることもある。\nFraleigh. (2003). 「抽象代数入門」第7版: p241.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":739,"permalink":"https://freshrimpsushi.github.io/jp/posts/739/","tags":null,"title":"抽象代数学におけるイデアル"},{"categories":"행렬대수","contents":"整理 正方行列 $A_{n \\times n} = (a_{ij})$ が与えられているとしよう。\n[1]: 選択された $i$ 行について $$ \\det A = \\sum_{j=1}^{n} a_{ij} C_{ij} $$ [2]: 選択された $j$ 列について $$ \\det A = \\sum_{i=1}^{n} a_{ij} C_{ij} $$ 正方行列 $A_{n \\times n} = (a_{ij})$ の $i$ 番目の行と $j$ 番目の列を除いた行列の行列式 $M_{ij}$ を小行列式といい、これに対し$C_{ij} := (-1)^{i + j} M_{ij}$ を余因子という。 説明 ラプラス展開は 余因子展開とも呼ばれる理論で、その便利さは言葉では表せない。ただ定義だけを持って行列式を計算するよりずっと簡単だ。特別な条件が整った行列の行列式を求める時、その利点はさらに増すので、このファクトは絶対に知っておくべきだ。\n例示 すぐにでもある行列が可逆行列かどうかを判断する際に使える例として、次のラプラス展開を見てみよう。\n$$ \\begin{align*} \\displaystyle \\det \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 4 \u0026amp; 5 \u0026amp; 6 \\\\ 7 \u0026amp; 8 \u0026amp; 9 \\end{bmatrix} =\u0026amp; 1 \\cdot \\begin{vmatrix} 5 \u0026amp; 6 \\\\ 8 \u0026amp; 9 \\end{vmatrix} - 2 \\cdot \\begin{vmatrix} 4 \u0026amp; 6 \\\\ 7 \u0026amp; 9 \\end{vmatrix} + 3 \\cdot \\begin{vmatrix} 4 \u0026amp; 5 \\\\ 7 \u0026amp; 8 \\end{vmatrix} \\\\ =\u0026amp; 1 \\cdot (-3) - 2 \\cdot (-6) + 3 \\cdot (-3) \\\\ =\u0026amp; 0 \\end{align*} $$\nそれゆえ、$\\displaystyle \\det \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 4 \u0026amp; 5 \u0026amp; 6 \\\\ 7 \u0026amp; 8 \u0026amp; 9 \\end{bmatrix}$ は逆行列が存在しないことを容易に確認できる。\nコード 次はJuliaでラプラス展開を実装して検証する内容のコードだ。実質的には数式をそのまま移したレベルで、実際にはとても非効率的に実装されている状態だ。これについては、次のポストを参照するといい:\n再帰関数を使うときに注意すべきこと ","id":781,"permalink":"https://freshrimpsushi.github.io/jp/posts/781/","tags":null,"title":"ラプラス展開"},{"categories":"바나흐공간","contents":"【要約】1 $T : (X , \\left\\| \\cdot \\right\\|_{X}) \\to ( Y , \\left\\| \\cdot \\right\\|_{Y} )$を線形作用素と呼ぼう。\n(a) $T$が有界ならば、すべての$x \\in X$に対して$\\left\\| T(x) \\right\\|_{Y} \\le \\left\\| T \\right\\| \\left\\| x \\right\\|_{X}$\n(b) $T$は連続$\\iff$$T$は有界\n(c) $X$が有限次元空間ならば、$T$は連続である。\n(d) $Y$がバナッハ空間ならば、$( B(X,Y) , \\| \\cdot \\| )$はバナッハ空間である。\n【説明】 $B(X,Y)$は有界線形作用素の空間だから、**(b)**によってこの空間の作用素はすべて連続だとわかる。線形なだけでなく連続でさえあり、それが完備であれば非常に良い空間であることは確かだ。\n**(a)**はよく使われ、大きな問題がなければ、通常は$\\| Tx \\| \\le \\| T \\| \\| x \\| $と書く。\n**(d)**ではノルム$\\| \\cdot \\|$は作用素ノルムだ。\n【証明】 (a) 戦略: $\\| x \\|_{X}$がスカラーであることを利用して$T$の内外を行き来する。\n$T$が有界であるから、ある$c\u0026gt; 0$に対して\n$$ {{ \\| T(x) \\|_{Y} } \\over { \\| x \\|_{X} }} \\le c $$\n$\\| x \\|_{X}$はスカラーであり$T$は線形であるから\n$$ {{ \\| T(x) \\|_{Y} } \\over { \\| x \\|_{X} }} =\\left\\| {{1} \\over {\\| x \\|_{X} }} T \\left( x \\right) \\right\\|_{Y} = \\left\\| T \\left( {{x} \\over {\\| x \\|_{X} }} \\right) \\right\\|_{Y} $$\n作用素ノルムの定義から$\\left\\| T \\right\\| = \\sup \\limits_{\\substack{x\\in X \\\\ \\left\\| x \\right\\|=1 }} \\left\\| T(x) \\right\\|_{Y}$だから\n$$ {{ \\| T(x) \\|_{Y} } \\over { \\| x \\|_{X} }} = \\left\\| T \\left( {{x} \\over {\\| x \\|_{X} }} \\right) \\right\\|_{Y} \\le \\sup \\limits_{\\substack{x\\in X \\\\ \\left\\| x \\right\\|=1 }} \\left\\| T(x) \\right\\|_{Y} = \\| T \\| $$\n両辺にスカラー$\\| x \\|_{X}$を掛けると\n$$ \\| T(x) \\|_{Y} \\le \\| T \\| \\| x \\|_{X} $$\n■\n(b) 戦略: イプシロン-デルタ論法で直接演繹する。$(\\implies)$は背理法を使用し、連続性によって、仮定に反する数列を作り出す。\n$(\\impliedby)$\n$T = 0$ならば自然に連続だから、$T \\ne 0$の場合を考えよう。任意の$x_{0} \\in X$に対して$\\| x - x_{0} \\| \u0026lt; \\delta$とする。\n$T$は有界線形作用素だから、**(a)**によって\n$$ \\| Tx - Tx_{0} \\| = \\| T ( x - x_{0} ) \\| \\le \\| T \\| \\| x - x_{0} \\| \u0026lt; \\| T \\| \\delta $$\n任意の$\\varepsilon \u0026gt; 0$に対して$\\displaystyle \\delta = {{ \\varepsilon } \\over { \\| T \\| }}$とすると$\\| Tx - Tx_{0} \\| \u0026lt; \\varepsilon$だから$T$は連続だ。\n$(\\implies)$\n$\\| T \\| = \\infty$と仮定すると\n$$ \\| x_{n} \\| = 1 $$\n$$ \\lim_{n \\to \\infty} \\| T x_{n} \\| = \\infty $$\n$X$の数列$\\left\\{ x_{n} \\right\\}_{ n \\in \\mathbb{N} }$が存在し、$\\displaystyle z_{n} := {{x_{n}} \\over { \\sqrt{ \\| Tx_{n} \\| } }}$を定義すると\n$$ \\lim_{n \\to \\infty} z_{n} = 0 $$\n$T$は連続だから\n$$ 0 = \\lim_{n \\to \\infty} \\| T( 0 ) \\| = \\left\\| T \\left( \\lim_{n \\to \\infty} z_{n} \\right) \\right\\| = \\lim_{n \\to \\infty} \\| T( z_{n} ) \\| = \\lim_{n \\to \\infty} \\left\\| T \\left( {{x_{n}} \\over { \\sqrt{ \\| Tx_{n} \\| } }} \\right) \\right\\|= \\lim_{n \\to \\infty} \\sqrt{ \\| T(x_{n} ) \\| } = \\infty $$\nこれは仮定に反するから、$T$は有界だ。\n■\n(c) 戦略: **(b)**によって連続性を示すには、有界であることを示すことで十分だ。有限次元空間の性質を利用すると、$T$が有界であることを示すのは比較的簡単だ。\n$\\dim X = n$とすると、$X$は基底$\\left\\{ e_{1} , \\cdots , e_{n} \\right\\}$を持って任意の$x \\in X$は$t_{i} \\in \\mathbb{C}$に対して\n$$ x = \\sum_{i=1}^{n} t_{i} e_{i} $$\n$T$は線形作用素であるから\n$$ Tx = T \\left( \\sum_{i=1}^{n} t_{i} e_{i} \\right) = \\sum_{i=1}^{n} | t_{i} | T \\left( e_{i} \\right) $$\n各辺にノルム$\\| \\cdot \\|_{Y}$を取ると\n$$ \\begin{equation} \\| Tx \\|_{Y} = \\left\\| \\sum_{i=1}^{n} t_{i} T \\left( e_{i} \\right) \\right\\|_{Y} \\le \\sum_{i=1}^{n} | t_{i} | \\| T ( e_{i} ) \\|_{Y} \\le \\max_{1 \\le i \\le n} \\| T ( e_{i} ) \\|_{Y} \\sum_{i=1}^{n} | t_{ i} | \\end{equation} $$\n新たにノルム$\\displaystyle \\left\\| \\sum_{i=1}^{n} t_{i} e_{i} \\right\\|_{1} := \\sum_{i=1}^{n} | t_{ i} |$を定義しよう。有限次元ベクトル空間で定義されたノルムはすべて等価であるから\n$$ C \\left\\| \\sum_{i=1}^{n} t_{i} e_{i} \\right\\|_{1} \\le \\left\\| \\sum_{i=1}^{n} t_{i} e_{i} \\right\\|_{X} $$\n$C\u0026gt;0$を満たすものが存在する。したがって\n$$ \\sum_{i=1}^{n} | t_{ i} | = \\left\\| \\sum_{i=1}^{n} t_{i} e_{i} \\right\\|_{1} \\le {{1} \\over {C}} \\left\\| \\sum_{i=1}^{n} t_{i} e_{i} \\right\\|_{X} = {{1} \\over {C}} \\| x \\|_{X} $$\n$(1)$に適用すると\n$$ \\| T x \\|_{Y} \\le {{1} \\over {C}} \\max_{1 \\le i \\le n} \\| T(e_{i} ) \\|_{Y} \\cdot \\| x \\|_{X} $$\nしたがって$\\displaystyle \\| T \\| \\le {{1} \\over {C}} \\max_{1 \\le i \\le n} \\| T(e_{i} ) \\|_{Y} \u0026lt; \\infty$だが、$T$は有界線形作用素であるから、**(b)**によって連続だ。\n■\n(d) 戦略: バナッハ空間$Y$で完備性を引き出して$T(x) \\in T(X) \\subset Y$についての議論に変える。\nパート1. ノルム空間$( B(X,Y) , \\| \\cdot \\| )$に対して、$\\| \\cdot \\|$は以下の条件を満たす。$T \\in B(X,Y)$に対して、\n(i): $$ \\| T \\| = \\sup\\limits_{\\substack{x\\in X \\\\ \\left\\| x \\right\\|=1 }} \\| T(x) \\| \\ge 0 $$\n(ii): $$ \\| T \\| = \\sup\\limits_{\\substack{x\\in X \\\\ \\left\\| x \\right\\|=1 }} \\| T(x) \\| = 0 \\iff T = 0 $$\n(iii): $$ \\| \\lambda T \\| = \\sup\\limits_{\\substack{x\\in X \\\\ \\left\\| x \\right\\|=1 }} \\| \\lambda T(x) \\| =\\sup\\limits_{\\substack{x\\in X \\\\ \\left\\| x \\right\\|=1 }} \\lambda \\| T(x) \\| = \\lambda \\sup\\limits_{\\substack{x\\in X \\\\ \\left\\| x \\right\\|=1 }} \\| T(x) \\| $$\n(iv): $$ \\begin{align*} \\| T_{1} + T_{2} \\| =\u0026amp; \\sup\\limits_{\\substack{x\\in X \\\\ \\left\\| x \\right\\|=1 }} \\| (T_{1} + T_{2})(x) \\| \\\\ \\le \u0026amp; \\sup\\limits_{\\substack{x\\in X \\\\ \\left\\| x \\right\\|=1 }} \\left( \\| T_{1} (x) \\| + \\| T_{2}(x) \\| \\right) \\\\ \\le \u0026amp; \\sup\\limits_{\\substack{x\\in X \\\\ \\left\\| x \\right\\|=1 }} \\| T_{1}(x) \\| + \\sup\\limits_{\\substack{x\\in X \\\\ \\left\\| x \\right\\|=1 }} \\| T_{2}(x) \\| \\end{align*} $$\nパート2. 完備性\n$B(X,Y)$のコーシー数列$\\left\\{ T_{n} \\right\\}_{n \\in \\mathbb{N}}$を定義すると、任意の$\\varepsilon \u0026gt; 0$に対して\n$$ \\| T_{n} - T_{m} \\| \u0026lt; \\varepsilon $$\n**(a)**によって、すべての$x \\in X$に対して\n$$ \\| T_{n} x - T_{m} x \\| = \\| ( T_{n} - T_{m} ) x \\| \\le \\| T_{n} - T_{m} \\| \\| x \\| \u0026lt; \\varepsilon \\| x \\| $$\nしたがって、$\\left\\{ T_{n}x \\right\\}$は$Y$のコーシー数列だ。仮定から$Y$は完備だから、何らかの$Tx \\in Y$に対して\n$$ \\lim_{m \\to \\infty } T_{m}x = Tx $$\n再び、**(a)**によって、すべての$x \\in X$に対して\n$$ \\| T_{n} x - T x \\| = \\left\\| T_{n} x - \\lim_{m \\to \\infty} T_{m} x \\right\\| = \\lim_{m \\to \\infty} \\left\\| T_{n} x - T_{m} x \\right\\| \u0026lt; \\varepsilon \\| x \\| $$\nすべての$x \\in X$に対して$\\displaystyle {{ \\| ( T_{n} - T ) x \\| } \\over { \\| x \\| }} \u0026lt; \\epsilon$だから\n$$ ( T_{n} - T ) \\in B(X,Y) $$\n一方、パート1で$B(X,Y)$がベクタースペースであることを示したから\n$$ T = T_{n} - ( T_{n} - T ) \\in B(X,Y) $$\n今、$\\| x \\| = 1$について考えると、すべての$x \\in X$に対して$\\displaystyle {{ \\| ( T_{n} - T ) x \\| } \\over { \\| x \\| }} \u0026lt; \\epsilon$だから\n$$ \\| T_{n} - T \\| = \\sup\\limits_{\\substack{x\\in X \\\\ \\left\\| x \\right\\|=1 }} {{ \\| ( T_{n} - T ) x \\| } \\over { \\| x \\| }} \u0026lt; \\varepsilon $$\nすべてのコーシー数列$\\left\\{ T_{n} \\right\\}_{n \\in \\mathbb{N}} $が、$n \\to \\infty$のとき何らかの$T \\in B(X,Y)$に収束するので、$B(X,Y)$は完備だ。\nパート3.\n$B(X,Y)$は完備なノルムドスペースであるから、バナッハ空間である。\n■\nKreyszig. (1989). Introductory Functional Analysis with Applications: p92~97, 118~119.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":730,"permalink":"https://freshrimpsushi.github.io/jp/posts/730/","tags":null,"title":"線形作用素の性質"},{"categories":"추상대수","contents":"定義 1 $$ f(x) : = \\sum_{i=0}^{n} a_{i} x^{i} = a_{0} + a_{1} x + \\cdots + a_{k} x^{k} $$ 多項式関数 $f \\in F [x]$と体 $F \\le E$において、$\\alpha \\in E$の評価関数Evaluation $\\phi_{\\alpha} : F [ x ] \\to E$を以下のように定義する。 $$ \\phi_{\\alpha} ( f(x) ) : = a_{0} + a_{1} \\alpha + \\cdots + a_{n} \\alpha^n = f (\\alpha) $$ $f( \\alpha ) = 0$を満たす$\\alpha \\in E$を$f(x)$のゼロZeroという。\n説明 評価関数 事実として、$\\phi_{\\alpha}$は準同型写像になる。\n定義があいまいに感じる場合、簡単な例$\\phi_{i} : \\mathbb{R} [ x ] \\to \\mathbb{C}$を考えてみよう。例えば$\\phi_{i} ( x^2 - 3 x + 2)$だとすると、単純に $$ f(x) = x^2 - 3 x + 2 $$ に$i$を代入した $$ i^2 - 3 i + 2= 1 -i2 \\in \\mathbb{C} $$ になる。もちろん、$\\mathbb{R} \\le \\mathbb{C}$であることは問題ない。\nゼロのモチーフ カーネルを考えると、$\\ker ( \\phi_{\\alpha} )$は$f(\\alpha) = 0$を満たす関数の集まりになる。上の例を続けると、$\\ker ( \\phi_{i} ) \\subset \\mathbb{R} [ x ]$の要素は$(x-i)$を因数に持つ多項式関数である。\nこのように$f( \\alpha ) = 0$を満たすとき、$\\alpha$を$f(x)$のゼロと呼ぶことには疑いの余地がない。同様に、$\\phi_{\\alpha} ( f(x) ) = 0$ならば$\\alpha$を$f(x)$のゼロと呼んでいた。\n「方程式の解」という概念を厳密に定義するためわざわざ関数まで話を持ち込む理由はまさにそこにある。 $$ f(x) = g(x) $$ 例えば上のような方程式を考える。こんな方程式の集合を考えない理由はないが、関係を集めるよりも$f(x)$と$g(x)$自体を別々に扱う方がずっと簡単で明瞭だ。もし方程式の集合$X$が上のような方程式の集まりだった場合 $$ \\left( f(x) = g(x) \\right) \\in X $$ のように表せるはずだが、どうせ化学反応をして $$ f(x) = g(x) \\iff f(x) - g(x) = 0 $$ になるから、右辺をわざわざ汚く自由にする必要がない。集合 $X$が方程式ではなく関数を持つ構造に従った場合、右辺が$0$である方程式を集めて、それらがいつ成立するかに関心を持つのと同じことである。\nこのような考えの拡張により、「実数系係数を持つ多項式関数でも虚根が出ることがある」などの事実が抽象化・一般化されるだろう。\nフレーリー。(2003)。初等代数学入門(第7版)：p201、204。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":723,"permalink":"https://freshrimpsushi.github.io/jp/posts/723/","tags":null,"title":"多項式の零点"},{"categories":"바나흐공간","contents":"定義1 $(X, \\left\\| \\cdot \\right\\|_{X}), (Y, \\left\\| \\cdot \\right\\|_{Y})$をノルム空間と呼ぶ。\nノルム空間からノルム空間への写像を作用素と呼ぶ。\n$x,x_{1},x_{2}\\in X$に対して、$T : X \\to Y$が $$ T( x_{1} + x_{2} ) = T( x_{1} ) + T( x_{2} ) \\quad \\text{and} \\quad T( a x ) = a T( x ) $$ を満たす場合、線形作用素と呼ぶ。\nすべての$x \\in X$に対して、$\\left\\| T(x) \\right\\|_{Y} \\le C \\left\\| x \\right\\|_{X}$を満たす$C \\ge 0$が存在する場合、$T$は有界であるという。\n3.を満たす$C$の中で最も小さい$C$を$T$の作用素ノルムと定義し、以下のように記す。 $$ \\left\\| T \\right\\| :=\\min \\left\\{ C : \\left\\| T(x) \\right\\|_{Y} \\le C \\left\\| x \\right\\|_{X} \\right\\} $$\n有界線形作用素$T : X \\to Y$をすべて集めた集合を$B(X,Y)$のように表す。\n説明 ベクトル空間からベクトル空間への写像を特に変換と呼ぶように、ノルム空間からノルム空間への関数を特に作用素と呼ぶ。\n便宜上、多くの教科書では、ベクトル空間間の任意の関数$X \\to Y$を変換と呼び、$X \\to X$のような変換を作用素と呼ぶ。\n4.の定義から次のことが得られる。\n$$ \\left\\| T \\right\\| = \\sup \\limits_{\\substack{x\\in X \\\\ \\left\\| x \\right\\|=1 }} \\left\\| T(x) \\right\\|_{Y} $$\nこれは$T$のノルムとも定義される。$\\left\\| x \\right\\|_{X}=1$という条件がなぜ存在するのか理解できなければ、3.を考えればいい。\n作用素は代数的には演算を保持するホモモルフィズムであり、当然、これに関する定理もすべて使うことができる。\n「演算」という表現の代わりに「作用素」という表現を使うのは、過去のように「演算」に焦点を当てるのではなく、ある空間での「作用」に興味を持ち、数学的に抽象化して扱うためである。回転変換のようなものを考えると、空間上である点を回転させて移動させると見ることができる。座標をベクトルとしてとり、行列を乗算して「計算」した結果を得るという説明も正しいが、点の位置を「移動させる」という行動として考えれば、作用素という表現も十分適切である。\nこのように、与えられた空間内でベクトルとして表される数学的な対象に対して、ある「作用$T$を加える」という表現を使うことができるようになった。その中でも特に私たちが関心を持つのは線形作用素であり、例として次のようなものがある。\n恒等作用素 $I : X \\to X, Ix = x$ その名の通り、作用を加えても変わらない、あるいは作用を加えないのと同じである作用である。$1$や${\\rm id}$とも記される。\n零作用素 $\\mathbb{0} (x) : = 0$\nどんな元も$0$にする作用で、作用素のベクトルスペースでゼロベクトルの役割を果たす。\n微分作用素 $D : C^{1} \\to C^{1}, Df = \\dfrac{d f}{d x}=f^{\\prime}$\n微分を行う作用素であり、実際には高校から誰もが知らず知らずのうちに使ってきた事実である。\n積分作用素 $T : C[0, 1] \\to C[0, 1]$, $\\displaystyle y(t) = Tx(t) = \\int_{0}^{1}K(t, s)x(s) ds$ 積分もまた一つの作用素であり、このとき$K$をカーネルという。積分変換とも言う。\n行列 $T_{A} ( \\mathbb{x} ) := A \\mathbb{x}$ $m \\times n$行列$A$は、$\\mathbb{C}^{n}$から$\\mathbb{C}^{m}$への関数と考えることができる。\nOle Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering (2010), p37\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":728,"permalink":"https://freshrimpsushi.github.io/jp/posts/728/","tags":null,"title":"関数解析学における作用素"},{"categories":"추상대수","contents":"定義 1 $$ f(x) : = \\sum_{i=0}^{n} a_{i} x^{i} = a_{0} + a_{1} x + \\cdots + a_{k} x^{k} $$ 環 $R$ 上の多項式Polynomial $f(x)$ を上記のように定義する。\n$a_{i} \\in R$ を$f(x)$ の係数Coefficientという。 $n \u0026lt; \\infty$ の場合、$n$ を$f(x)$ の次数Degreeという。 $R[x]$ は$R$ の元を係数とする有限多項式の集合である。 $$ R[x] := \\left\\{ a_{0} + a_{1} x + \\cdots + a_{n} x^{n} \\ | \\ a_{0}, \\cdots , a_{n} \\in R \\right\\} $$ $R[[x]]$ は$R$ の元を係数とする無限多項式の集合である。 $$ R[[x]] := \\left\\{ a_{0} + a_{1} x + \\cdots + a_{n} x^{n} + \\cdots \\ | \\ a_{0}, \\cdots , a_{n} , \\cdots \\in R \\right\\} $$ 説明 遠回りをして、ついに中学・高校で習った「代数学」に戻ってきた。多項式を再定義する理由は、多項式\u0026rsquo;式\u0026rsquo;を群、環、体のある元として見て扱うためである。\nこのことについて、次の重要な定理を知るべきである。たいしたことないように見えるかもしれないが、多項式の環が元の環の有用な性質を保存することを保証する事実である。\n定理 [1]: $R$ が環ならば、$R[x]$ も多項式の加算および乗算に関して環である。 [2]: $R$ が可換環ならば、$R[x]$ も可換環である。 [3]: $R$ が単位元 $1 \\ne 0$ を持てば、$R[x]$ も単位元 $1 \\ne 0$ を持つ。 これらの定理は$R[x]$ について成り立つ場合、$R[[x]]$ にも適用される。\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p199.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":721,"permalink":"https://freshrimpsushi.github.io/jp/posts/721/","tags":null,"title":"多項式環"},{"categories":"추상대수","contents":"定義 1 環$R$において、$ab = 0$を満たす$0$ではない$a,b \\in R$を零因子Zero Divisorと呼ぶ。 単位元$1 \\ne 0$を持つ$D$が零因子を持たない場合、その環を整域Integral Domainという。 説明 零因子 $0$ではない要素同士の積が$0$になる例には $$ \\begin{bmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{bmatrix} = \\begin{bmatrix} 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\end{bmatrix} $$ や $2 \\cdot 3 \\equiv 0 \\pmod{6}$がある。このように環では、常に計算が便利に行えるわけではないため、注意が必要である。これは、$xy = 0$で、$x \\ne 0$である場合、$y = 0$とは言えないということである。\n整域 整域は、IDともよく略される。\n整域の例としては、単純に整数の集合$\\mathbb{Z}$を挙げることができる。もともと整域のIntegralという言葉は、整数Integerから来ているので当然である。整域の良い点は、$0$以外のもので割り算をする際に心配する必要がない点である。整域では、$x y = 0$ならば$x = 0$または$y = 0$であることが保証され、代数構造として大変便利である。\n$R$が整域であることは、$R$において乗法に関する消去法Cancellation lawが成立することを保証し、零因子を持たない環であることから、体と密接な関係がある。以下の有用な定理を見てみよう。\n定理 [3] $p$が素数ならば$\\mathbb{Z}_{p}$は体である。 証明 1 体$F$に対して、$a \\ne 0$かつ$ab = 0$ならば $$ \\left( {{1} \\over {a}} \\right) (ab) = \\left( {{1} \\over {a}} \\right) 0 = 0 $$ かつ、同時に $$ \\left[ \\left( {{1} \\over {a}} \\right) a \\right] b =1 b = b $$ が成り立つ。これは、$ab= 0$ならば、どちらかが必ず$0$でなくてはならないということであり、従って体の元は零因子になることができず、$F$は整域である。\n■\n2 有限整域$D$の元で$0$を除く残りの元を$1, a_{1} , \\cdots , a_{n}$としよう。それらに$a \\ne 0$を乗じた $$ a, aa_{1} , \\cdots , aa_{n} $$ を考えると、$D$が整域であるため、これらの中に$0$は存在しない。\nまた、整域では消去法が成立するため、$aa_{i} = aa_{j}$ならば$a_{i} = a_{j}$であることがわかる。つまり $$ a_{i} \\ne a_{j} \\implies aa_{i} \\ne aa_{j} $$ となり、 $$ \\left\\{ 1, a_{1} , \\cdots , a_{n} \\right\\} = \\left\\{ a, aa_{1} , \\cdots , aa_{n} \\right\\} $$ を得る。従って、$a \\ne 0$に対しては常に$ab=1$を満たす$b \\in \\left\\{ 1, a_{1} , \\cdots , a_{n} \\right\\}$が存在することがわかる。$b$は$a$の乗法に関する逆元であるため、$D$は体である。\n■\n[3] 自明のように、$\\mathbb{Z}_{p} = \\left\\{ 0 , 1, \\cdots , p-1 \\right\\}$は有限集合である。しかし、$p$が素数であるため、 $$ ab \\equiv 0 \\pmod{p} $$ を満たす$0$ではない$a,b \\in \\mathbb{Z}_{p}$は存在しないため、$\\mathbb{Z}_{p}$は整域であり、定理2により体である。\n■\n4 体$F$において、$0^2 = 0$かつ$1^2 = 1$であるため、$0$と$1$は$F$の冪等元である。$0$でも$1$でもない冪等元$a \\in F$が存在すると仮定すると、$a^2 = a$であるため、$a( a-1) = 0$でなければならない。しかし、定理1により、$F$は整域であり、零因子を持たないため、この仮定は矛盾している。\n■\n参照 ユークリッド整域 $\\implies$主イデアル整域 $\\implies$一意分解整域 $\\implies$ 整域 Fraleigh. (2003). A first course in abstract algebra(7th Edition): p178~179.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":719,"permalink":"https://freshrimpsushi.github.io/jp/posts/719/","tags":null,"title":"反射と屈折"},{"categories":"상미분방정식","contents":"公式1 関数 $f(t)$ のラプラス変換 $F(s)=\\mathcal{L} \\left\\{ f(t) \\right\\}$ が $s\u0026gt;a$ として存在するとする。すると、定数 $c$ に対して次が成立する。\n$$ \\begin{align*} \\mathcal{L} \\left\\{ e^{ct}f(t) \\right\\}\u0026amp;=F(s-c), \u0026amp;s\u0026gt;a+c \\\\ \\mathcal{L^{-1}} \\left\\{ F(s-c) \\right\\}\u0026amp;=e^{ct}f(t) \u0026amp; \\end{align*} $$\n説明 $f$ に指数関数をかけることと、$F$ を平行移動することが同じという意味だ。\n導出 $$ \\begin{align*} \\mathcal{L} \\left\\{ e^{ct}f(t) \\right\\} \u0026amp;=\\int_{0}^\\infty e^{-st}e^{ct}f(t)dt \\\\ \u0026amp;= \\int_{0}^\\infty e^{-(s-c)t}f(t)dt \\\\ \u0026amp;= F(s-c) \\end{align*} $$\n■\n結論 $$ \\begin{align*} \\mathcal{L} \\left\\{ e^{ct} t^p \\right\\} \u0026amp;=\\dfrac{\\Gamma (p+1)}{(s-c)^{p+1}} \\\\ \\mathcal{L} \\left\\{ e^{ct} \\sin (at) \\right\\} \u0026amp;=\\dfrac{a}{(s-c)^2+a^2} \\\\ \\mathcal{L} \\left\\{ e^{ct} \\cos (at) \\right\\} \u0026amp;= \\dfrac{s-c}{(s-c)^2+a^2} \\\\ \\mathcal{L} \\left\\{ e^{ct} \\sinh (at) \\right\\} \u0026amp;= \\dfrac{a}{(s-c)^2-a^2} \\\\ \\mathcal{L} \\left\\{ e^{ct} \\cosh (at) \\right\\} \u0026amp;= \\dfrac{s-c}{(s-c)^2-a^2} \\end{align*} $$\n参照 ラプラス変換表 William E. Boyce, Boyce\u0026rsquo;s Elementary Differential Equations and Boundary Value Problems (11th Edition, 2017), p262\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":764,"permalink":"https://freshrimpsushi.github.io/jp/posts/764/","tags":null,"title":"ラプラス変換の平行移動"},{"categories":"추상대수","contents":"定義 1 $R$ を環としよう。\n$r \\in R$ が $r^2 = r$ を満たすなら、$r$ を冪等元Idempotent Elementという。 $R$ の全ての元が冪元なら、$R$ をブーリアン環Boolean Ringという。 説明 ブーリアン環は韓国語でそのまま\u0026rsquo;ブール環\u0026rsquo;となるが、聞こえがあまりよくないため英語発音をそのまま用いた。\n線形代数学での射影が非常に有用な性質を多く持っているが、一般化された抽象代数学では言うまでもない。\nブーリアン環の例として最も有名なのは、\u0026lsquo;ブール代数\u0026rsquo;とも呼ばれる $$ (\\left\\{ \\text{True}, \\text{False} \\right\\} , \\text{OR}, \\text{AND} ) $$ である。よく知られているように $$ \\text{True AND True} = \\text{True} \\\\ \\text{Flase AND Flase} = \\text{Flase} $$ なのでこの環はブーリアン環となる。数学者にとってより馴染み深い例は$\\mathbb{Z}_{2}$があり、当然ブーリアン環と$\\mathbb{Z}_{2}$は同型である。\nその一方で、ブーリアン環の性質として次のことが知られている。\n定理 ブーリアン環は可換環である。\n証明 ブーリアン環 $R$ において $a, b \\in R$ ならば $(a+b) \\in R$ で $$ (a + b)^2 = (a+b) $$ 分配法則によって $$ (a + b)^2 = (a+b)a + (a+b)b = a^2 + ba + ab + b^2 = (a+b) $$ $a^2 = a$ そして $b^2 = b$ なので $$ a+ ba + ab + b = a+ b $$ $a$ と $b$ は加法に関する逆元が存在するので、整理すると $$ ba +ab = 0 $$ 両辺に $ba$ の逆元 $(-ba)$ を加えると $ab = -ba$ となるので $$ ab = (ab)^2 = (-ba)^2 = (ba)^2 = ba $$\n■\n関連項目 線形代数学での射影 Fraleigh. (2003). A first course in abstract algebra(7th Edition): p176.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":717,"permalink":"https://freshrimpsushi.github.io/jp/posts/717/","tags":null,"title":"ブーリアン環"},{"categories":"상미분방정식","contents":"証明 仮定2. により、$t \\ge M$について$|f(t)| \\le Ke^{at}$が成り立つ。両辺に$e^{-st}$を掛けると\n$$ |e^{-st}f(t)| \\le Ke^{-(s-a)t} $$\n補助定理1\n以下の条件を仮定する。\n関数$f$が$t \\ge \\tau$の時に部分的に連続である。 ある正の数$M$に対して、$t \\ge M$が$|f(t)| \\le g(t)$を満たす。 $\\displaystyle \\int _{M} ^\\infty g(t) dt$が収束する。 すると、$\\displaystyle \\int _\\tau ^\\infty f(t)dt$も収束する。\n補助定理によれば、$\\displaystyle \\int _{M} ^\\infty Ke^{-(s-a)t}dt$が収束する時、$\\displaystyle \\int _{0} ^\\infty e^{-st} f(t) dt$も収束する。証明で使う条件を補助定理に代入してみると、\n$$ g(t)=Ke^{-(s-a)t},\\quad f(t)=e^{-st}f(t),\\quad \\tau=0 $$\n今、$\\displaystyle \\int _{M} ^\\infty Ke^{-(s-a)t}dt$が収束するかだけを確認すれば、証明は完了である。\n$$ \\begin{align*} \\int _{M} ^\\infty Ke^{-(s-a)t}dt \u0026amp;= K\\lim _{B \\to \\infty} \\int _{M} ^B e^{-(s-a)t}dt \\\\ \u0026amp;= K\\lim _{B \\to \\infty} \\frac{1}{a-s}\\left[ e^{-(s-a)t} \\right]_{M}^B \\\\ \u0026amp;= K \\lim _{B \\to \\infty} \\frac{1}{a-s} \\left( e^{-(s-a)B }- e^{-(s-a)M} \\right) \\end{align*} $$\nここで、$s-a\u0026gt;0$ならば、$\\displaystyle \\lim _{B \\to \\infty} e^{-(s-a)B }=0$となり、広義積分$\\displaystyle \\int _{M} ^\\infty Ke^{-(s-a)t}dt$が収束する。従って、補助定理により、$\\displaystyle \\int _{0} ^\\infty e^{-st} f(t) dt$も収束する。\n■\nWilliam E. Boyce , Boyceの初等微分方程式及び境界値問題 (第11版, 2017), p243\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":761,"permalink":"https://freshrimpsushi.github.io/jp/posts/761/","tags":null,"title":"ラプラス変換の定義と存在証明"},{"categories":"함수","contents":"定義 部分ごとに定数関数である関数をステップ関数と呼ぶ。\n説明 上の図のように階段のような形をしているのでステップ関数と呼ばれる。最初に提案した人がヘビサイドと知られており、ヘビサイド関数とも呼ばれる。ヘビサイドは電気回路の微分方程式を解く方法を作った人で、その方法がラプラス変換である。ステップ関数の代表例にガウス関数がある。\nヘビサイドの名前にちなんで主に$H, h$と表示される。特に説明がなければ、ステップ関数と言った場合、以下の単位ステップ関数を指すことが多い。\n単位ステップ関数 次のような$H$を単位ステップ関数と呼ぶ。\n$$ H(x) = \\begin{cases} 1 \u0026amp; x \\gt 0 \\\\ 0 \u0026amp; x \\le 0\\end{cases} $$\n関連項目 ディラックのデルタ関数 ランプ関数 ReLU ","id":757,"permalink":"https://freshrimpsushi.github.io/jp/posts/757/","tags":null,"title":"階段関数"},{"categories":"추상대수","contents":"定義 1 環 $(R , + , \\cdot)$が乗算$\\cdot$に対する単位元$1 \\in R$を持っている時、$1$を単位元Unityと言う。 単位元を持つ環$R$で、乗算に対する逆元が存在する元素$r \\ne 0$を単元Unitと言う。 単位元を持つ環$R$で、$0$以外の全ての元が単元であれば、それを除算環Division Ringと言う。 除算環$R$が乗算に関して可換ならば、その環を体Fieldと言う。 説明 簡単に言えば、体$(F , + , \\cdot )$とは、加算に対する単位元$0 \\in F$を除くすべての元が逆元を持つ可換環である。抽象代数の観点から考えると難しそうだが、解析学で学んだ$\\mathbb{R}$を思い出してみれば、実際にはこれが「代数構造」らしいと見ることができるだろう。\nなぜ逆元を持つ元がユニットと呼ばれるのか 単位元の英語表現であるUnityは簡単に受け入れられるが、なぜ逆元を持つ元素をUnitと呼ぶのか理解し難い人も多いのではないだろうか。通常、Unitは「単位」と訳され、「ある量を測るときの基準」として良く使われるからだ。逆元が存在することと単位は関係ないように見えるが、なぜUnitと定義したのか？ここで面白い脳内提案をしてみたい。\n代数学の発展初期には、当然ながら整数に関する研究が活発だった。実際、私たちが整数集合を$\\mathbb{Z}$と書くのも、ドイツ語のZahlringからきており、\u0026lsquo;Zahl-\u0026lsquo;が「数」を意味し、\u0026rsquo;-ring\u0026rsquo;はご存知の通り環に翻訳されている。代数学で使われる多くの概念が数論のセンスから出てきたと受け入れるのはそう難しくないだろう。\nここで整数体$\\mathbb{Z}$を考えてみよう。\n$\\mathbb{Z}$は無限に多くの整数を元として持つ。ここで、乗算に対して単位元となるのは$1$のみで、逆元を持つ元素は$-1$と$1$のみである。抽象代数を学ぶだけの数学に親しんでいれば、$-1$と$1$が「ユニット」と呼ばれるのに違和感を感じないはずだ。この背景から、整数を超えて様々な代数構造を見ていくうちに、これらをユニットと呼ぶのが適切だったのではないかと思われる。\n$\\mathbb{R}$に至って、$0$を除く全ての$r \\in \\mathbb{R}$に対して乗算に対する逆元$\\displaystyle {{1} \\over {r}} \\in \\mathbb{R}$が存在するので、$0$を除く全ての元がユニットである。考えてみれば、ある数$a$を$r$に掛けて欲しい数である$x$を作ることができるので、$r \\ne 1$も単位としての役割を果たす理由が全くない。そして、そのある数$a$は当然$a = r^{-1}x$であり、$r^{-1}$の存在無しには確信できないことだ。\n参照 解析学での体の公理 Fraleigh. (2003). 「抽象代数入門」(第7版): p173。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":715,"permalink":"https://freshrimpsushi.github.io/jp/posts/715/","tags":null,"title":"抽象代数学における体"},{"categories":"상미분방정식","contents":"公式1 これはラプラス変換の表です。\n$f(t)=\\mathcal{L^{-1}}$ $F(s)=\\mathcal{L} \\left\\{ f(t) \\right\\}$ 導く정 $1$ $\\dfrac{1}{s}$ link $e^{at}$ $\\dfrac{1}{s-a}$ link $t^n$ $\\dfrac{n!}{s^{n+1}}$ link $t^{p}$ $\\dfrac{ \\Gamma (p+1) }{ s^{p+1}}$ link $t^{p}e^{at}$ $\\dfrac{ \\Gamma (p+1) }{ (s-a)^{p+1}}$ link $\\sin (at)$ $\\dfrac{a}{s^2+a^2}$ link $\\cos (at)$ $\\dfrac{s}{s^2+a^2}$ link $e^{at}\\sin(bt)$ $\\dfrac{b}{(s-a)^2 +b^2}$ link $e^{at}\\cos(bt)$ $\\dfrac{s-a}{(s-a)^2+b^2}$ link $\\sinh (at)$ $\\dfrac{a}{s^2-a^2}$ link $\\cosh (at)$ $\\dfrac{s}{s^2-a^2}$ link $e^{at} \\sinh (bt)$ $\\dfrac{b}{(s-a)^2-b^2}$ link $e^{at} \\cosh (bt)$ $\\dfrac{s-a}{(s-a)^2-b^2}$ link $u_{c}(t)= \\begin{cases} 0 \u0026amp; t\u0026lt;c \\\\ 1 \u0026amp; t\\ge c\\end{cases}$ $\\dfrac{e^{-cs}}{s}$ link $u_{c}(t)f(t-c)$ $e^{-cs}F(s)$ link $f^{\\prime}(t)$ $s\\mathcal{L} \\left\\{ f(t) \\right\\} -f(0)$ link $f^{(n)}$ ${s^n\\mathcal {L}\\left\\{ f(t) \\right\\} -s^{n-1}f(0) - \\cdots -f^{(n-1)}(0) }$ link $f(t)=f(t+T)$ $\\dfrac{\\displaystyle \\int_{0}^T e^{-st}f(t)dt}{1-e^{-st}}$ link $\\delta (t-t_{0})$ $e^{-st_{0}}$ link $f(ct)$ $\\frac{1}{c}F \\left( \\frac{s}{c} \\right)$ link $\\frac{1}{k}f (\\frac{t}{k} )$ $F(ks)$ link $\\frac{1}{a} e^{-\\frac{b} {a}t}f\\left(\\frac{t}{a}\\right)$ $F(as+b)$ link $t^{n}f(t)$ $(-1)^{n}F^{(n)}(s)$ link William E. Boyce , Boyce\u0026rsquo;s Elementary Differential Equations and Boundary Value Problems (11th Edition, 2017), Chapter6 The Laplace Transform\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":743,"permalink":"https://freshrimpsushi.github.io/jp/posts/743/","tags":null,"title":"ラプラス変換の表"},{"categories":"바나흐공간","contents":"要約 1 全ての有限次元ノルム空間は基底を持つ。\n説明 特定の条件を満たす基底ではなく、基底の存在を明らかにすることに不慣れかもしれないが、実際に基底の定義で全てのベクトル空間が基底を持つとは一度も言っていない。有限次元を定義するにあたっては、別途証明が不要なほど自明な事実でもある。\n証明 戦略: 有限次元であることを利用して具体的に基底を構築する。\n$(X, | \\cdot | )$ は有限次元なので$\\text{span} \\left\\{ x_{1} , \\dots , x_{n} \\right\\} = X$ を満たす$\\left\\{x_{1} , \\dots , x_{n} \\right\\}$ が存在する。$y_{1} : = x_{1}$ とする。もし$x_{2} \\in \\text{span} \\left\\{ y_{1} \\right\\}$ なら$x_{3}$ を考える。もし$x_{2} \\notin \\text{span} \\left\\{ y_{1} \\right\\}$ なら$y_{2} := x_{2}$ とする。もし$x_{3} \\in \\text{span} \\left\\{ y_{1}, y_{2} \\right\\}$ なら$x_{4}$ を考える。もし$x_{3} \\notin \\text{span} \\left\\{ y_{1}, y_{2} \\right\\}$ なら$y_{3} := x_{3}$ とする。このように$M = \\left\\{ y_{1} , \\dots , y_{k} \\right\\}$ を定義することにより$1 \\le j \\le k$ に対して\n$$ y_{j} \\notin \\text{span} \\left\\{ y_{1} , \\dots , y_{j-1} \\right\\} $$\n$M$ が線形独立でないと仮定すると、ある$\\lambda_{j} \\ne 0$たちについて\n$$ \\lambda_{1} y_{1} + \\dots + \\lambda_{k} y_{k} = 0 $$\nそのような$j$たちの中で最も大きい$j$を$j_{0}$ とする。\n$$ y_{j_{0}} = - {{1} \\over { \\lambda_{j_{0}} }} \\sum_{j \u0026lt; j_{0}} \\lambda_{j} y_{j} - {{1} \\over { \\lambda_{j_{0}} }} \\sum_{j \u0026gt; j_{0}} \\lambda_{j} y_{j} = - {{1} \\over { \\lambda_{j_{0}} }} \\sum_{j \u0026lt; j_{0}} \\lambda_{j} y_{j} $$\nしたがって$\\displaystyle y_{j_{0}} = - {{1} \\over { \\lambda_{j_{0}} }} \\sum_{j \u0026lt; j_{0}} \\lambda_{j} y_{j} \\in \\text{span} \\left\\{ y_{1} , \\dots , y_{j_{0}-1} \\right\\}$で、これは矛盾である。$M \\subset \\left\\{ x_{1} , \\dots , x_{n} \\right\\}$は線形独立であり$\\text{span} M = X$を満たすので、$M$は$X$の基底となる。\n■\n証明過程をよく見ると、$\\left\\{ x_{1} , \\dots , x_{n} \\right\\}$はかなり余裕を持って$X$を生成することがわかる。線形独立になることを妨げるものをすべて捨てて$M$だけを取り、具体的にそれが基底であることを示したのである。\nKreyszig. (1989). Introductory Functional Analysis with Applications: p55.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":707,"permalink":"https://freshrimpsushi.github.io/jp/posts/707/","tags":null,"title":"有限次元のノルム空間には基底が存在することの証明"},{"categories":"바나흐공간","contents":"定義 1 ベクトル空間$X$が与えられているとする。\n$X$のベクトル$x_{1} , \\dots , x_{n}$とスカラー$\\alpha_{1} , \\dots , \\alpha_{n}$に対して、$\\alpha_{1} x_{1} + \\cdots + \\alpha_{n} x_{n}$をベクトル$x_{1} , \\dots , x_{n}$の線形結合という。\n$M =\\left\\{ x_{1} , \\dots , x_{n} \\right\\}$とするとき、$M$の全てのベクトルの線形結合の集合を$\\text{span} M$と呼び、$M$によって生成された$X$の部分空間とする。\n$\\alpha_{1} x_{1} + \\cdots + \\alpha_{n} x_{n} = 0$を満たすケースが$\\alpha_{1} = \\cdots = \\alpha_{n} = 0$のみのとき、$M$は線形独立であるという。\n有限集合$K \\subset X$が$\\text{span} K = X$を満たすならば、$X$は有限次元であるという。\n線形独立の集合$M$が$\\text{span} M = X$を満たすとき、$M$を$X$の基底という。\n基底の濃度$\\dim X := | M|$を$X$の次元という。\n説明 ベクトル空間の基底は特に「有限」な線形結合について話すとき、ハメル基底と呼ばれる。有限次元のノルム空間は名前が長いからといって、線形代数を始めるところから知っている親しみやすい空間でもある。だから、普通はこれらがどれほど良い性質を持っているかについての何か真剣な考察をしていなかったはずだ。\n有限次元ノルム空間には基底がある。 有限次元ベクトル空間上で定義された全てのノルムは同値である。 有限次元ノルム空間は完備性を持つ。 ユークリッド空間を考えるなら、上記の事実は事実として受け入れられるが、一般的な空間では必ずしもそうではない。それぞれ証明が必要で、想像以上に難しいことがある。\n参照 線形代数学におけるベクトルの線形独立と基底、次元 無限次元ベクトル空間の基底：シャウダー基底 Kreyszig. (1989). Introductory Functional Analysis with Applications: p54~55.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":705,"permalink":"https://freshrimpsushi.github.io/jp/posts/705/","tags":null,"title":"有限次元ベクトル空間のハメル基底"},{"categories":"바나흐공간","contents":"定義1 完備なノルム空間をバナッハ空間Banach spaceと言う。\n説明 完備空間とは、すべてのコーシー列が収束する空間のことを言う。\nバナッハ空間は以下の各項をすべて満たす空間として、非常に便利な空間である。距離関数が定義されている上に完備性を備えている。\nベクトル空間である。 ノルム空間である。$\\implies$ 距離空間である。 完備空間である。 また、バナッハ空間の例としては、定義域が閉区間である連続関数の集合が考えられる。これは非常に簡単な例でありながら、様々な重要な定理を支えているため、非常に重要な事実でもある。バナッハ空間の例には以下のものがある。\n$C[a,b]$ $\\R^{n}$ $\\mathbb{C}^{n}$ $C[a,b]$に関する証明を紹介する。\n証明 1 パート 1. ベクトル空間\n閉区間で定義された連続関数は、定数関数 $f(x) = 0$ を単位元として、$f(x) = - f(x)$ を逆元として持つ。また、$C[a,b]$ はスカラーフィールド $\\mathbb{R}$上でベクトル空間の条件をよく満たす。\nパート 2. ノルム空間\n$f \\in C [a,b]$ に対して $\\| \\cdot \\|$ を $\\displaystyle \\| f \\| := \\sup_{ a \\le t \\le b } | f (t) |$ として定義すると、ノルムの条件をよく満たす。\nパート 3. 完備性\n$\\left\\{ f_{n} \\right\\}_{n \\in \\mathbb{N}}$ を $C [a,b]$ のコーシー列としよう。つまり、すべての $\\varepsilon / 3 \u0026gt; 0$ に対して、$n,m \u0026gt; N_{1}$ の時いつでも $\\| f_{n} (t) - f_{m} (t) \\| \u0026lt; \\varepsilon / 3$ を満たす $N_{1} \\in \\mathbb{N}$ が存在する。\n$\\mathbb{R}$ は完備空間なので、固定された $t_{0} \\in [a,b]$ が与えられるたびに、$\\displaystyle \\lim_{n \\to \\infty} f_{n} (t_{0})$ はある $f : [a,b] \\to \\mathbb{R}$ に関して\n$$ \\lim_{n \\to \\infty} f_{n} ( t_{0} ) = f ( t_{0} ) $$\nと表せる。すると、$f_{n}$ はコーシー列なので、任意の $t \\in [a,b]$ に対して、$m \\ge N_{2}$ の時に\n$$ \\begin{align*} | f(t) - f_{m} (t) | =\u0026amp; \\left| \\lim_{n \\to \\infty} f_{n} (t) - f_{m} (t) \\right| \\\\ =\u0026amp; \\lim_{n \\to \\infty} | f_{n} (t) - f_{m} (t) | \\\\ \\le \u0026amp; \\lim_{n \\to \\infty} \\sup_{t \\in [a,b] } | f_{n} (t) - f_{m} (t) | \\\\ =\u0026amp; \\lim_{n \\to \\infty} \\| f_{n} - f_{m} \\| \\\\ \u0026lt;\u0026amp; \\varepsilon / 3 \\end{align*} $$\nを満たす $N_{2}$ が存在する。もちろん、関数 $f$ が連続である保証はまだなく、単にすべての $t \\in [a,b]$ に対して最終的に $f_{n} (t)$ に収束する値を関数値として定義されただけである。しかし、この定義から、$f_{n}$ が $f$ に一様収束する、つまりすべての $x,y \\in [a,b]$ および $\\varepsilon / 3 \u0026gt; 0$ に対して $n \\ge N_{3}$ の時に同時に満たされる $N_{3} \\in \\mathbb{N}$ が存在することを保証することができる。\n$$ \\left| f_{n} (x) - f(x) \\right| \u0026lt; \\varepsilon / 3 \\\\ \\left| f_{n} (y) - f(y) \\right| \u0026lt; \\varepsilon / 3 $$\nこれで、$f$ が連続関数であることを示すだけである。\n空集合でない $E \\subset \\mathbb{R}$ について、$f : E \\to \\mathbb{R}$ としよう。\nコンパクト距離空間\n$f$ が連続で、$E$が有界閉区間であれば、$f$ は一様連続である。\n$f_{n} : [a,b] \\to \\mathbb{R}$ は連続であり、$[a,b] \\subset \\mathbb{R}$ はコンパクトなので、$f_{n}$ は $[a,b]$ で一様連続である。つまり、すべての $x,y \\in [a,b]$ および $\\varepsilon / 3 \u0026gt; 0$ に対して、$|x-y| \u0026lt; \\delta$ の時に満たされる $\\delta \u0026gt; 0$ が存在する。\n$$ \\left| f_{n}(x) - f_{n}(y) \\right| \u0026lt; \\varepsilon / 3 $$\n以上の結果を組み合わせると、$|x-y| \u0026lt; \\delta$ および $n \\ge N_{3}$ の場合に\n$$ \\begin{align*} |f(x) - f(y)| \\le \u0026amp; \\left| f (x) - f_{n} (x) \\right| + \\left| f_{n}(x) - f_{n}(y) \\right| + \\left| f_{n} (y) - f(y) \\right| \\\\ =\u0026amp; \\varepsilon / 3 + \\varepsilon / 3 + \\varepsilon / 3 \\\\ =\u0026amp; \\varepsilon \\end{align*} $$\nを満たす $\\delta \u0026gt; 0$ および $N_{3} \\in \\mathbb{N}$ が存在するため、$f$ は $[a,b]$ で一様連続であり、$f \\in C[a,b]$ に収束する。したがって、任意の連続関数のコーシー列 $\\left\\{ f_{n} \\right\\}_{n \\in \\mathbb{N}}$ が一様に何らかの $f \\in C[a,b]$ に収束するため、$C[a,b]$ は完備性を有する。\n■\nKreyszig. (1989). Introductory Functional Analysis with Applications: p36.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":703,"permalink":"https://freshrimpsushi.github.io/jp/posts/703/","tags":null,"title":"バナッハ空間"},{"categories":"수리통계학","contents":"ビルドアップ 統計学とは、「母数を把握する方法を研究する学問」と言える。物理量を測定するように、公式や法則を通じて正確に母数を推定できれば言うことないが、現実的にそれが不可能なため、仮定と標本を利用して「母数と思われるもの」を見つけ出すだけだ。「我が国の男性の平均身長 $X$ に興味があるなら $X \\sim N ( \\theta , \\sigma^2 )$ と仮定して $\\displaystyle \\hat{\\theta} = \\overline{x} = {{1} \\over {n}} \\sum_{k = 1}^{ n } x_{k}$ を求め $\\theta = \\hat{\\theta}$ と判断するといった具合だ。このような推定法は、かなり簡単で基本的な概念に基づいている。\nフリークエンティスト 私たちが持っている標本は、母集団から無作為に得られたものであり、その標本を得る方法が公正であれば、同じ大きさの標本同士を特別に区分けすることはない。もちろん、実際には異なる標本だが、標本が母集団をよく代表しているかどうかの問題は純粋に運にかかっているからだ。確かなのは、小さい標本より大きい標本の方が良いということだけだ。当然ながら、私たちが得られなかった観測値が現在の標本と大きく異なるとは考えていない。それが異なるなら統計的な分析をする意味がないからだ。このような推論は、標本と母集団が大きく異ならないという期待から出発し、標本が多ければ多いほど、その期待は確信に近づく。これまでに得られたデータだけでなく、これから得る、またはまだ得ていないデータまで考えるこの推論をフリークエンティスト推論と呼ぶ。標本の大きさ(Frequency)が大きければ大きいほど正確になるという観点から、この名前は妥当だと言えるだろう。\nベイジアン 一方、ベイジアン推論は、これまでに得た標本だけを考える。ベイズの定理を通じて、事前分布が事後分布に変わるだけだ。母数がある分布を持っていると考えるが、それが正確だとは特に仮定しない。分析を始める前に、専門家の意見や主観的な経験をもとに何らかの事前分布を想定しても構わない。新しい標本を得てその分布が変わっても気にしない。確かなのは、分析を終えたときの事後分布が、事前分布に標本を反映して得られた結果であることだけだ。\nベイジアンパラダイムとは？1 ベイジアンパラダイムの構成は次のとおりだ：\n(1): 母数の事前分布の決定 (2): ベイズの定理を通じた計算 (3): 事後分布を利用した母数の推定 母数 $\\theta$ の事前分布を $\\pi (\\theta)$、観測値を $y$ とすると、ベイズの定理により $$ p ( \\theta | y ) = {{ p(y | \\theta ) \\pi (\\theta ) } \\over { p(y) }} $$ である。このとき、データが反映された母数の確率分布 $p ( \\theta | y )$ を事後分布と呼ぶ。\n例 簡単な例を挙げてみよう。よく約束に遅れる友人アダムがいるとする。\nアダムが約束に遅れる時間が平均で10分、標準偏差が5分の正規分布に従うならば $N ( 10 , 5^2 )$、フリークエンティストとベイジアンはアダムが約束に遅れたとき、次のように言うだろう：\nフリークエンティスト:「アダムはもともと10分遅れる奴だよ。」 ベイジアン:「アダムはいつも見ていると、だいたい10分くらい遅れるね。」 フリークエンティストはアダムが平均的に10分遅れると推論し、それがアダムの本質であるため、これまで10分くらい遅れてきて、これからも10分くらい遅れると期待する。ベイジアンは、今まで見た限りアダムが遅れる時間が10分である確率が最も高いと思うため、この度の遅れる時間も10分であろうと期待する。\n一見、その話はその話だ。それもそのはず、フリークエンティストとベイジアンは、視点が異なるだけで、統計的な推論を導出している点に違いはないからだ。違いが生じるのは、例えば次の約束でアダムが時間通りに来たときだ：\nフリークエンティスト:「アダムが時間通りに来るのは、その確率が3％しかないほど珍しいケースだ。」 ベイジアン:「アダムが早く来ることもあるんだね。次もこの時に来るかな？」 そして、次の約束でアダムが時間通りに出られるかを尋ねた場合、二人の答えは確かに異なるだろう：\nフリークエンティスト:「アダムが変わったとは思えないな。今回早く出たのは、十分に起こりうることだった。」 ベイジアン:「アダムが遅れてくる確率は依然として高いが、時間通りに来る確率が上がったのも事実だ。」 フリークエンティストは、新しく得られた観測値が、すでに導き出された結論と合致するかのみを確認するが、ベイジアンは、既存の結論に即座にアップデートすることで、新しい事後分布を得ると見なせる。このように、順次分析が容易であることは、フリークエンティストと区別される最大の特徴であり、同時にベイジアン推論の独自の利点でもある。\n김달호. (2013). R과 WinBUGS를 이용한 베이지안 통계학: p89.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":702,"permalink":"https://freshrimpsushi.github.io/jp/posts/702/","tags":null,"title":"ベイジアン・パラダイム"},{"categories":"선형대수","contents":"定義 $V$上のベクトル空間として$\\mathbb{F}$を定義しよう。\n$\\left\\| \\cdot \\right\\| : V \\to \\mathbb{F}$が$\\mathbf{u}, \\mathbf{v} \\in V$と$k \\in \\mathbb{F}$に対して次の三つの条件を満たす場合、$\\left\\| \\cdot \\right\\|$を**$V$上のノルム**と定義する。\n(i) 正定性: $\\left\\| \\mathbf{u} \\right\\| \\ge 0$であり$\\mathbf{u} = \\mathbb{0} \\iff \\left\\| \\mathbf{u} \\right\\| = 0$ (ii) 斉次性: $\\left\\|k \\mathbf{u} \\right\\| = | k | \\left\\| \\mathbf{u} \\right\\| $ (iii) 三角不等式: $\\left\\| \\mathbf{u} + \\mathbf{v}\\right\\| \\le \\left\\|\\mathbf{v} \\right\\| + \\left\\| \\mathbf{u} \\right\\|$ 説明 ノルムは絶対値から始まって抽象化された概念だ。韓国語にはそのまま当てはまる言葉がなく、そのまま読む方式で翻訳された。個人的にはちょっと変だと思うので、可能な限り[nɔ:m]に近い発音で読むことにしている。\n線形代数学ではノルムの定義は上記の通りだ。(つまり、他の分野ではノルムが別の方法で定義されることを意味する。)読んでみれば分かるが、ノルムの定義に必要な条件は基本的に「測定」または「比較」を可能にする要素だ。「測定」と「比較」を可能にする​。単に三次元空間$\\mathbb{R}^3$を考えると、これらの概念は直観的な定義で十分だが、複素数を考えるだけでも抽象化が必要になる。世の中には思っているよりも多くのノルムの種類があり、ベクトル空間においてのノルムも必ずしも一つである必要はない。これらの定義を満たしてくれさえすれば、ノルムについて無限に考え出すことができるだろう。\nベクトル空間$\\mathbb{C}^n$とそのベクトル$\\mathbf{u} = ( u_1 , u_2 , \\cdots , u_n ) \\in \\mathbb{C}^n$に対して次のノルムを紹介する：\nマンハッタンノルム $$ \\left\\| \\mathbf{u} \\right\\|_1 = \\sum_{k=1}^{n} |u_k| $$\n$\\mathcal{l}^1$ノルムとも呼ばれ、タクシー幾何学で距離を定義する際に使われるノルムだ。実際の都市マンハッタンから名前が来ており、単純な直線距離ではなく実際の移動距離を表すために考案された。正確には同じ概念ではないが、理解を助けるための図を見ると、なぜこのノルムにマンハッタンという名前が付けられたのか納得できるはずだ。上の図では青い線に相当し、緑色の正方形の一辺を$1$とすると、AとBの距離は$6+2 = 8$になる。\nユークリッドノルム $$ \\left\\| \\mathbf{u} \\right\\|_2 = \\sqrt{\\sum_{k=1}^{n} |u_k|^2} $$\n我々がよく知っている距離、大きさの概念で、寸法に関係なく絶対値の二乗の和の平方根で求められる。上の図では赤い線に相当し、よく知っている通り、AとBの距離は$\\sqrt{6^2 + 2^2} =6.32\u0026hellip;$になる。\n$\\infty$-ノルム、最大ノルム $$ \\left\\| \\mathbf{u} \\right\\|_\\infty = \\max_{1\\le k \\le n} |u_k| $$\nスプレマムノルムsupremum normとも呼ばれ、単純に最大値だけを取るノルムだ。\n$p$-ノルム $$ \\left\\| \\mathbf{u} \\right\\|_p = \\left( \\sum_{k=1}^{n} |u_k|^p \\right) ^ {{1} \\over {p} } $$\n$p$は$1$以上で、必ずしも自然数である必要はない。上でマンハッタンノルムとユークリッドノルムは$p$-ノルムの特別な例で、それぞれ$1$-ノルム、$2$-ノルムに該当する。特に$p = \\infty$の場合は最大ノルムになり、上述の記法をすべてカバーする。\n注目に値するかどうかは分からないが、一つ興味深い点は、$p$-ノルムの形が統計学の$p$-次モーメントの形に似ているということだ。絶対値があるとか、中央値が0に固定されているなどの違いはあるが、形だけを見た場合、$1$-ノルムは平均を、$2$-ノルムは分散を連想させる。\n","id":257,"permalink":"https://freshrimpsushi.github.io/jp/posts/257/","tags":null,"title":"線形代数学においてノルムとは何か？"},{"categories":"추상대수","contents":"定義 1 二つの二項演算、足し算$+$と掛け算$\\cdot$に関して以下のルールを満たす集合$R$を環と定義する。\n$a$、$b$、$c$が$R$の元の時、\n足し算に対して交換法則が成り立つ。$$a+b=b+a$$ 足し算に対して結合法則が成り立つ。$$(a+b)+c=a+(b+c)$$ 足し算に対する単位元が存在する。$$\\forall a \\ \\exists 0\\ \\ \\mathrm{s.t} \\ a+0=a$$ すべての元の足し算に対する逆元が存在する。$$\\forall a \\ \\exists -a\\ \\ \\mathrm{s.t}\\ a+(-a)=0$$ 掛け算に対して結合法則が成り立つ。$$(ab)c=a(bc)$$ 足し算と掛け算に対して分配法則が成り立つ。$$a(b+c)=ab+ac\\ \\mathrm{and} \\ (b+c)a=ba+ca$$ 説明 要するに、集合$R$が足し算に対して可換群であり、掛け算に対して半群であり、二つの演算に対して分配法則が成り立つ時、$R$を環という。\n特に、掛け算に対しても交換法則が成り立つ場合、可換環またはアーベル環と呼ばれる。また、環の定義からわかる通り、掛け算に対する単位元や逆元が存在する必要はない。単位元が存在しても、逆元が存在する必要もない。上記の6つの条件を満たせば、環と言える。\n群を扱う時、演算に対する単位元を$e$と表わす。環では演算が二つあるため、どちらの演算に対する単位元か簡単に分かるように異なる記号を使う。足し算に対する単位元は$0$とし、単位元と呼ぶ。掛け算に対する単位元が存在すれば$1$とし、単位と呼ぶ。ある元$a$の掛け算に対する逆元が存在する時、$a$を環$R$の単位と呼ぶ。\n群と同じく、環の掛け算に対する単位元も存在するならば、その存在は一意である。各元の逆元も存在すれば、それも一意である。この証明は群で行った方法と同じなので、ここでは書かないが、詳細はこちらを参照。\n例 整数の集合$\\mathbb{Z}$を考える。上記の6つの条件を満たすため、足し算、掛け算に対する環だ。また、掛け算に対して交換法則も満たすため、可換環だ。単位$1$が存在し、その元は整数の1であり、単位は1と-1だ。（それぞれの逆元として1と-1を持つ）\n注意 環では、掛け算に対する単位元や逆元が存在する「必要」がない。だから、群でのように安易に消去することができない。つまり、$a,\\ b,\\ c$が環$R$の元の時、$ab=ac$として$b=c$と結論づけることはできないのだ。$a$の逆元が必ず存在するわけではないからだ。\n同様に、$a^2=a$としても、安易に$a=0$や$a=1$という結論を出すことはできない。環を扱う際には、この点に注意しよう。\nFraleigh. (2003). 「抽象代数入門(第7版)」: p167.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":587,"permalink":"https://freshrimpsushi.github.io/jp/posts/587/","tags":null,"title":"抽象代数学における環"},{"categories":"바나흐공간","contents":"定義 1 $1 \\le p \u0026lt; \\infty$に対して、距離空間$( \\ell^{p} , d^{p} )$は次のように定義される。\n(i) 収束する数列の集合:\n$$ \\ell^{p} := \\left\\{ \\left\\{ x_{n} \\right\\}_{n \\in \\mathbb{N}} \\subset \\mathbb{C} \\left| \\left( \\sum_{i=1}^{\\infty} | x_{i} |^{p} \\right)^{{1} \\over {p}} \u0026lt; \\infty \\right. \\right\\} $$\n(ii) 距離関数:\n$$ d^{p} ( x_{n} , y_{n} ) := \\left( \\sum_{i = 1}^{\\infty} | x_{i} - y_{i} |^{p} \\right)^{ {{1} \\over {p}} },\\quad \\left\\{ x_{n} \\right\\} , \\left\\{ y_{n} \\right\\} \\in \\ell^{p} $$\n$p = \\infty$に対して、距離空間$( \\ell^{\\infty} , d^{\\infty} )$は次のように定義される。\n(i)' 有界数列の集合:\n$$ \\ell^{\\infty} := \\left\\{ \\left\\{ x_{n} \\right\\}_{n \\in \\mathbb{N}} \\ \\left| \\ \\sup_{i \\in \\mathbb{N}} | x_{i} | \u0026lt; \\infty \\right. \\right\\} $$\n(ii)' 距離関数:\n$$d^{\\infty} ( x_{n} , y_{n} ) := \\sup_{i \\in \\mathbb{N}} | x_{i} - y_{i} |,\\quad \\left\\{ x_{n} \\right\\} , \\left\\{ y_{n} \\right\\} \\in \\ell^{\\infty} $$\n説明 $\\ell^{p}$は数列空間sequence spaceと呼ばれ、$\\ell^{p}$は[エルピー]と読む。$\\ell$のTeXコードは\\ellだ。\n$\\ell^{p}$空間が$L^{p}$空間と異なる点は、数列か関数か、級数か積分か、という点だけだ。ヤングの不等式、コーシー・シュワルツの不等式、ヘルダーの不等式、ミンコフスキーの不等式や完全性も同様だ。事実が似ているので、証明方法もほとんど同じであるため、一方を十分に学んだなら、もう一方をわざわざする必要はない。\n一方で$\\ell^{\\infty}$は、実際には$p \\to \\infty$のときと同じで、証明可能であるため、別に定義する必要はない。$\\ell^{p}$や$\\ell^{\\infty}$のほとんどの性質を見ると、ほぼ同じであり、別に考える必要はない。\nただし、典型的な例外として可分性がある。\n定理 1 $1 \\le p_{0} \u0026lt; \\infty$とする。\n(a): $\\ell^{p_{0}}$は可分空間である。 (b): $\\ell^{\\infty}$は不可分空間である。 この差は$\\ell^{p_{0}}$が収束性を条件としているのに対し、$\\ell^{\\infty}$が有界性のみを条件としているために生じる。\n証明 (a) ストラテジー: 収束する数列は、$\\displaystyle \\left| \\sum_{i = i_{0}}^{\\infty} a_{i} \\right|$を十分小さくする$i_{0}$を常に選択できる。この$i_{0}$を基準に有限部分と無限部分に分けた後、有限数列が必ず収束点になることを利用して、$l^{p_{0}}$が可分空間になるような部分集合を具体的に見つけ出す。\n主張: $\\overline{M} = \\ell^{p_{0}}$を満たす可算集合$M \\subset \\ell^{p_{0}}$が存在する。\n特定の$j_{0}$からはすべて$0$のみが繰り返される複素数列の集合\n$$ M : = \\left\\{ \\left\\{ m_{j} \\right\\} \\in \\ell^{p_{0}} \\ | \\ m_{j} \\in \\mathbb{Q} + i \\mathbb{Q} , m_{j} = 0 , \\ j\u0026gt;j_{0} , \\ j_{0} \\in \\mathbb{N} \\right\\} $$\nを考えれば、$M$は可算集合であり、$\\overline{M} \\subset \\ell^{p_{0}}$は当然成立するので、$\\ell^{p_{0}} \\subset \\overline{M}$を示せば十分だ。$l^{p_{0}}$の定義により、すべての数列$x : = ( x_{1} , x_{2} , \\cdots ) \\in \\ell^{p_{0}}$は任意の$\\varepsilon \u0026gt; 0$に対して\n$$ \\left( \\sum_{j \u0026gt; N} | x_{j} |^{p_{0}} \\right)^{ {{1} \\over {p_{0}}} } \u0026lt; {{ \\varepsilon } \\over {2}} $$\nを満たす$N \\in \\mathbb{N}$が存在しなければならない。すると、各々の$x$に対して\n$$ \\left( |x_{1} - m_{1}|^{p_{0}} + \\cdots + |x_{N} - m_{N}|^{p_{0}} \\right)^{ {{1} \\over {p_{0}}} } \u0026lt; {{\\varepsilon} \\over {2}} $$\nを満たす$m : = (m_{1} , m_{2} , \\dots , m_{N} , 0, \\dots ) \\in M$も存在するので\n$$ d^{p_{0}} ( x, m) = \\left( \\sum_{j \\le N} |x_{j} - m_{j}|^{p_{0}} + \\sum_{j \u0026gt; N} |x_{j}|^{p_{0}} \\right)^{{1} \\over {p_{0}}} \u0026lt; {{ \\varepsilon} \\over {2}} + {{ \\varepsilon} \\over {2}} = \\varepsilon $$\nすべての$\\varepsilon \u0026gt;0$に対して$B^{d^{p_{0}}} (x ; \\varepsilon ) \\cap M \\ne \\emptyset$なので$x \\in \\overline{M}$\n■\n(b) ストラテジー: 扱いやすい有界関数$e_{I} \\in \\ell^{\\infty}$とこれらに関する関数$\\psi$を定義し、それらの単射性を利用して基数を計算する。\n主張 : $\\overline{M} = \\ell^{\\infty}$を満たす可算集合$M \\subset \\ell^{p_{0}}$が存在しない。\n$\\overline{ M} = \\ell^{\\infty}$を満たすすべての$M \\subset \\ell^{\\infty}$が非可算であることを示せば十分だ。\nパート1.\n定義域$I \\subset \\mathbb{N}$を持つ関数$e_{I} : I \\to \\left\\{ 0, 1 \\right\\}$を\n$$ e_{I} (j) := \\begin{cases} 1 \u0026amp; , j \\in I \\\\ 0 \u0026amp; , j \\in ( \\mathbb{N} \\setminus I ) \\end{cases} $$\nとして定義しよう。例えば$I = 2 \\mathbb{N} = \\left\\{ 2, 4, 6 , \\cdots \\right\\} $の場合、関数値は$e_{2 \\mathbb{N}} (1) = 0$、$e_{2 \\mathbb{N}} (2) = 1$、$e_{2 \\mathbb{N}} (3) = 0$、$e_{2 \\mathbb{N}} (4) = 1 $となる。\nこの関数たちの集合$A: = \\left\\{ e_{I} \\ | \\ I \\subset \\mathbb{N} \\right\\}$を考えると、関数値が$[0,1]$を超えることがないので、$A \\subset \\ell^{\\infty}$が成り立つ。\nパート2.\n関数$\\phi : \\mathscr{P} ( \\mathbb{N} ) \\to A$を$\\phi (I) : =e_{I}$として定義しよう。すると、$I , I\u0026rsquo; \\subset \\mathbb{N}$が$I \\ne i '$ならば$\\phi (I) = e_{I} \\ne e_{I\u0026rsquo;} = \\phi (I\u0026rsquo;)$であるため、$\\phi$は単射であり、したがって\n$$ |A| \\ge | \\mathscr{P} ( \\mathbb{ N} ) | = 2^{\\aleph_{0}} = \\aleph_{1} $$\n任意の$x \\in \\ell^{\\infty} = \\overline{M}$と$\\varepsilon \u0026gt;0$に対して$B_{d^{\\infty}} (x ; \\varepsilon ) \\cap M \\ne \\emptyset$なので\n$$ B_{d^{\\infty}} \\left( e_{I} ; {{1} \\over {3}} \\right) \\cap M \\ne \\emptyset $$\nパート3.\n$\\displaystyle B_{d^{\\infty}} \\left( e_{I} ; {{1} \\over {3}} \\right) \\cap M \\ne \\emptyset$であるため\n$$ \\psi ( e_{I} ) \\in \\left( B_{d^{\\infty}} \\left[ e_{I} ; {{1} \\over {3}} \\right] \\cap M \\right) $$\nが成立するような関数$\\psi : A \\to M$を定義できる。$\\psi$が単射でないと仮定してみると、$\\psi ( e_{I}) = \\psi ( e_{I\u0026rsquo; })$に対して\n$$ \\psi ( e_{I}) = \\psi ( e_{I\u0026rsquo; }) \\in \\left[ B_{d^{\\infty}} \\left( e_{I} ; {{1} \\over {3}} \\right) \\cap B_{d^{\\infty}} \\left( e_{I\u0026rsquo;} ; {{1} \\over {3}} \\right) \\right] $$\n三角不等式により\n$$ 1 = d^{\\infty} ( e_{I} , e_{I\u0026rsquo;} ) \\le d^{\\infty} ( e_{I} , \\psi (e_{I}) ) + d^{\\infty} ( \\psi (e_{I}) , e_{I\u0026rsquo;} ) \\le {{1} \\over {3}} + {{1} \\over {3}} = {{2} \\over {3}} $$\nまとめると$\\displaystyle 1 \\le {{2} \\over {3}}$であり、これは矛盾なので、$\\psi$は単射である。また$\\psi : A \\to M$が単射であるため、$|M| \\ge |A| = \\aleph_{1}$であり$M$は可算集合であることができない。\n■\nKreyszig. (1989). Introductory Functional Analysis with Applications: p11.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":695,"permalink":"https://freshrimpsushi.github.io/jp/posts/695/","tags":null,"title":"数列空間（ℓp 空間）"},{"categories":"위상수학","contents":"定義 1 位相空間 $X$が次の三つの条件を満たす時、$X$を**$n$次元多様体**Manifoldと言う。\n(i): 第二可算である。 (ii): ハウスドルフである。 (iii): $X$の全ての点が$\\mathbb{R}^{n}$の開集合と位相同型な近傍を持つ。 $n$次元多様体$X$が以下の二つのタイプの点を持つ時、$X$は境界を持つと言われる。\n(1) 内部点: 全ての$x \\in X^{\\circ}$の近傍が$\\mathbb{R}^{n}$と位相同型である。 (2) 境界点: 全ての$x \\in \\partial X$の近傍が$U^{n} := \\left\\{ \\mathbb{x} \\ | \\ \\mathbb{x} \\in (\\mathbb{R}^{+})^{n} \\right\\}$と位相同型である。 説明 条件(iii)と局所的にユークリッド空間であることは、同じ意味である。つまり、多様体は局所的にユークリッド空間に似た位相空間を指す。特に、$1$次元多様体をカーブCurve、$2$次元多様体をサーフェスSurfaceと言う。\n上の例では、最初と二番目は$1$次元多様体だが、三番目はねじれた部分があるため、$1$次元多様体ではない。\n特に、境界を持つ$n$次元多様体$X$と境界を持たない$m$次元多様体$Y$について、以下のことが成り立つ。 $$ \\partial (X \\times Y) = X \\times \\partial Y $$\nMunkres. (2000).『位相幾何学』(第2版): p225。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":673,"permalink":"https://freshrimpsushi.github.io/jp/posts/673/","tags":null,"title":"多様体とは何か"},{"categories":"통계적분석","contents":"診断法 1 標準化残差グラフを使って回帰分析が正しく行われているか確認できる。等分散性を確認するためには、残差の散らばりが全体的に均一かを確認すればいい。等分散性がない一般的な例として、以下の二つのケースが代表的だ。\n後ろに行くほど分散が大きくなる形で、このような場合には変換や重みを導入することで解決しなければならない。本当に簡単に解決されるかは別として、モデル診断で見つかった問題の中で最も典型的で簡単な解決策がある。\n中央部分のみ分散が信じられないほど小さいが、データ収集の段階から問題があると疑われる状況だ。極端な差を正確に説明できる別の変数がある可能性が高いので、データセットをもう一度見直すことをお勧めする。\n参照 線形性 独立性 正規性 Hadi. (2006). Regression Analysis by Example(4th Edition): p98.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":681,"permalink":"https://freshrimpsushi.github.io/jp/posts/681/","tags":null,"title":"モデル診断により確認される残差の等分散性"},{"categories":"통계적분석","contents":"診断法 1 標準化残差グラフを通じて、回帰分析が適切に行われたかを確認できる。\n線形性があるか確認するには、$0$ を中心に残差が対称的になっているかを確認すればいい。\n右の図を見れば、誰が見ても線形性が欠けていることが確認できる。\nもし単純回帰分析だったら、上のようにデータの傾向を全く説明できない結果になる。\n注意すべき形を以下の例で見てみよう。\n左は、緑の残差が回帰分析の様々な前提に反しているが、線形性自体は満たしている状態だ。\n右は、残差がどうにか$0$ を平均として分布しているが、対称的と言うには問題があるように見える。残差の図がこうなっていたら、ほぼ確実に何か重要な条件やデータが欠落していると見ていい。\nもし単純回帰分析だったら、データの傾向はなんとなく合っているが、常に全か無かの大きな誤差が付きまとうだろう。\n参照 等分散性 独立性 正規性 Hadi. (2006). 回帰分析の例(第4版): p91。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":677,"permalink":"https://freshrimpsushi.github.io/jp/posts/677/","tags":null,"title":"モデル診断によって確認される残差の線形性"},{"categories":"통계적분석","contents":"必要性 単純回帰分析の場合は、独立変数と従属変数を考慮しても$2$の次元であるため、分析が適切に行われたか一目で確認することができる。しかし、多重回帰分析の場合$3$の次元を超えると図で描きにくくなり、分析が実際に適切であるかを確認するのが難しくなる。回帰分析の仮定を満たさないが仮説検定は通過する場合があるが、この場合分析は間違っているとするしかない。\n分析が間違っているのは主に(1)データが線形モデルに合わない場合や(2)分析結果と実際のデータに対する理解との乖離が激しい場合だ。モデル診断はデータが線形モデルに適合しているかどうかを確認するために行われる。\n診断法 1 データが線形モデルに合わないとは、簡単に言えばデータが直線状にないということだ。データが線形モデルに合っているかどうかは、標準化された残差グラフを見てモデル診断を行うことで判断する。この残差分析は、高次元で直線を描くのが難しいために考案されたかなり独創的な方法である。残差を計算する理由がそもそもこれであるとも言える。\n残差グラフで下記の四つの条件が満たされた場合、モデル診断はパスしたと見なされる。\n(i) 線形性Linearity: 残差が$0$を中心に対称的に分布していればよい。 これは回帰分析の本質とも言える前提であり、回帰分析の目的が直線を求めることであるため、線形性を満たさないと意味がない。もともと回帰分析を使用する理由自体が線形性があると推測することにあるので、実際の分析では非常に簡単に満たされる。 (ii) 等分散性Homoscedasticity: 残差の分布が均等であればよい。 特定の区間で変動が急に小さくなったりすると、データが同じプロセスで得られたとは言い難くなる。調査員の違い、ミスなどの問題を考慮せざるを得ない。データが後になるほど分散が大きくなったり小さくなったりする場合、変数の変換によって部分的に解決することができる。 (iii) 独立性Independecy: 残差同士にどんな傾向もなければよい。 残差に何らかの傾向があるということは、誤差が完全に偶然であるという回帰分析の仮定に反する。独立性が欠けているということは、逆に言えば、まだ私たちが知らない規則、たとえば自己相関がある可能性があるという意味になる。この場合、遠回りして解決しようとするよりも、時系列分析など、より適したツールを探すほうが良い。程度がひどい場合は一目で分かるほど明らかになるが、そうでなければ大きな問題ではないとも言える。独立性をチェックするためにダービン-ワトソン検定を安易に使わないこと。ダービン-ワトソン検定は、厳密に言えば、一定の間隔で離れた残差の自己相関を見つけるものであり、独立性を確認するものではない。明らかな傾向があるのにダービン-ワトソン検定を通過したからと言って独立と信じてはいけない。 (iv) 正規性Normality: 残差が標準正規分布に従っているように見えればよい。 他の前提と異なり、正規性はシャピロ-ウィルク検定やハルケ-ベラ検定のように客観的な診断が可能である。しかし、問題が単純であるとは限らず、主に正規性に大きな影響を与えるのが異常値であることが多い。分析者が異常値に該当するデータを直接見て、その現象を直接説明できれば大きな問題にはならない。異常値異常値といっても安易に除外してはいけないこと。例えば、標本が$300$個で、上下にシックスシグマ($\\pm 3 \\sigma$)を超える異常値が$3$個程度あれば、それは正常であるとされる。異常値が多すぎるのも問題だが、分布理論と異なり異常値が過剰に少なすぎるのも正規分布に適切に従っているわけではない。 これら四つの条件は無作為に並べられたのではなく、重要な順に配置されており、回帰係数に対する仮説検証の理論的導出過程を見れば、この順序を理解することができる。2実際の統計分析に臨むと、すべてのデータがきれいに現れるわけではなく、時にはいくつかの条件と妥協しなければならない場面もある。そのような場合、異常値が多かったり、わずかに偏っていても、ある程度正規性から逸脱しても許容できる場合がある。\nこのようなモデル診断はかなりの部分が目視に依存しており、データに対する理解が不可欠である。間違った部分を見つけることが第一の問題であり、どのように解決するかが第二の問題である。この能力を養う方法は、可能な限り実際の分析に取り組み、多くのタイプを見ることが最善である。\nコード 以下は、残差グラフを出力するRコードである。\nout\u0026lt;-lm(rating~.,data=attitude); summary(out)\rwin.graph(5,5); plot(rstudent(out),main=\u0026#34;표준화된 잔차\u0026#34;) Hadi. (2006). Regression Analysis by Example(4th Edition): p86~88.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n残念ながら、学部レベルで理解するのはかなり難しい。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":675,"permalink":"https://freshrimpsushi.github.io/jp/posts/675/","tags":null,"title":"回帰分析のモデル診断"},{"categories":"통계적검정","contents":"仮説検定 線形多重回帰モデルのモデル診断で、残差が線形性、等分散性、独立性、正規性を満たすとする。 $n$ 個の観測値と $p$ 個の独立変数を対象とした多重回帰分析における仮説検定は次の通りです。\n$H_{0}$: $\\beta_{1} = \\beta_{2} = \\cdots = \\beta_{p} = 0$ つまり、すべての独立変数が従属変数と相関関係を持たない。 $H_{1}$: $\\beta_{1} , \\beta_{2} , \\cdots , \\beta_{p}$ のうち少なくとも1つは $ 0$ ではない。つまり、少なくとも1つの独立変数が意味のある相関関係を持つ。 導出 SST, SSR, SSE: 6. 総平方和(Total Sum of Squares)またはSST(Sum of Squares Total): $$ \\text{TSS} =\\text{SST} := \\sum_{i=1}^{n} ( y_{i} - \\overline{y} )^2 $$ 7. 回帰平方和(Explained Sum of Squares)またはSSR(Sum of Squares due to Regression): $$ \\text{ESS} = \\text{SSR} := \\sum_{i=1}^{n} ( \\hat{y}_{i} - \\overline{y} )^2 $$ 8. 残差平方和(Residual Sum of Squares)またはSSE(Sum of squared Error): $$ \\text{RSS} = \\text{SSE} := \\sum_{i=1}^{n} ( y_{i} - \\hat{y}_{i} )^2 $$\nSSTは平均1つを用いるため、自由度が$(n-1)$であり、SSEは$p$個の独立変数に基づいて計算されるため、定数項を含む$(p+1)$個の回帰係数を使用し、自由度が$\\left( n-(p-1) \\right)$である。これらは残差の等分散性と独立性、正規性に従って$\\sigma^{2}$で分けたとき、それぞれの自由度を持つカイ二乗分布に従う。一方 $$ SST = SSR + SSE \\iff SSR = SST - SSE $$ であるため、$SSR$の自由度を持ち、カイ二乗分布$\\chi^{2} (p)$に従う。\nF分布の導出: ２つの確率変数 $U,V$が独立であり、$U \\sim \\chi^{2} ( r_{1})$、$V \\sim \\chi^{2} ( r_{2})$であるとすると $$ {{ U / r_{1} } \\over { V / r_{2} }} \\sim F \\left( r_{1} , r_{2} \\right) $$\n検定統計量 $F$を $$ F := {{ \\text{SSR} / p } \\over { \\text{SSE} / (n-p-1 ) }} $$ として定義すると自由度が$(p , n-p-1)$のF分布に従う。\n方程式で再度表すと$\\displaystyle F = {{ \\text{SSR} / p } \\over { \\text{SSE} / (n-p-1 ) }} \\sim F(p, n-p-1)$であり、これを利用して仮説検定を行う。\n■\nこのF検定は回帰係数のt検定が見られるのであれば無意味であり、真の本質はモデルの比較にある。統計学に避けられない「主観性」や「あいまいさ」を払拭して、統計的に意味のある結果を出せるようになることにある。（もちろん、実際の分析ではもっと便利で簡単な統計量を使ってモデルを比較する。）\n縮小モデルに対する仮説検定 $n$ 個の観測値と $p$ 個の独立変数を対象とした多重回帰分析について、$i=0,1,\\cdots,p$とする。この回帰モデルを全体モデルFMと呼び、FMから$k$個の独立変数を除去したモデルを縮小モデルRMと呼ぶ。\n$H_{0}$: RMで十分である。つまり、多くの変数を使ってFMを使用する必要はない。 $H_{1}$: RMは不十分である。つまり、変数を増やしてでもFMを使用する方が良い。 $$ F = {{ [ \\text{SSR} (RM) - \\text{SSR} (FM) ] / (p +1 - k) } \\over { \\text{SSE} ( FM ) / (n-p-1 ) }} $$ は、自由度が$(p + 1 - k , n-p-1)$のF分布に従う。有意水準 $\\alpha$で$F \\le F_{ ( p+1-k , n-p-1 ; \\alpha ) }$であれば、$H_{0}$が採用され、縮小RMが使用できるようになる。\n関連項目 多重回帰分析 Rでの多重回帰分析結果 ","id":672,"permalink":"https://freshrimpsushi.github.io/jp/posts/672/","tags":null,"title":"回帰係数のF検定"},{"categories":"통계적분석","contents":"データ探索 tail(attitude) Rで、組み込みデータattitudeを読み込んで、tail()関数で確認してみよう。このデータで重回帰分析を行うつもりだ。\nratingを従属変数として、他の独立変数がratingにどれくらい影響を与えるかに関心がある。データだけ見てもratingと他の変数との間に線形関係があるか確かめるのは難しいので、図を描いて確認してみよう。\nwin.graph()\rplot(attitude) 単にplot()関数にデータを入れて実行すると、変数ごとに比較した点の図を出力してくれる。\n一見すると、ratingはcomplaintsとはっきりとした線形関係があるようだ。他のlearningやraisesも線形関係にあるようだが、complaintsと比べると散らばり具合が激しい。単回帰分析と同様に、lm()関数に線形モデルを入力して、summary()関数で結果を見ることができる。\n結果の解釈 以下のコードでは、1～2列目と3列目は完全に同じ表現だ。線形モデルを入力するときに記述される点(.)は「その他のすべての変数」を意味する。[注意：ここでrating~.-privilegesと入力してprivilegesだけを除外する方法も使える。]\nout\u0026lt;-lm(rating~complaints+privileges+learning\r+raises+critical+advance,data=attitude)\rout\u0026lt;-lm(rating~.,data=attitude)\rsummary(out) 「知らなくてもいい」というのは、本当に知らなくてもいいというわけではなく、急いで勉強して結果を読み取る必要がある立場で、すぐに参考にすべき指標ではないという意味だ。 (1) 残差 中央値(中央値)を中心に他の分位数が対称的に現れるかどうかの程度だけ見ても構わない。分位数だけでは適切なモデル診断ができないため、意味はない。\n(2) 推定値 各変数による回帰係数で、(Intercept)は回帰線の$y$切片を表し、残りは各変数毎の単位変化率を示す。単回帰分析と違って様々な独立変数があるため、有意水準によって結果が変わる。有意水準$5 \\%$ではlearningの回帰係数が$0$であるという帰無仮説は棄却されないため、 $$ \\text{(rating)} = 0.61319 \\cdot \\text{(complaints)} + \\varepsilon $$ その一方で、有意水準$10 %$では次のようにモデルを縮小することもできる。 $$ \\text{(rating)} = 0.61319 \\cdot \\text{(complaints)} + 0.32033 \\cdot \\text{(learning)} + \\varepsilon $$\n(3) 標準誤差 知らなくてもいい。推定値の標準誤差であり、これを使用して回帰係数の信頼区間を求めることができる。\n(4) t値 知らなくてもいい。推定値を標準誤差で割った値であり、自由度$n-p-1$のt分布に従う検定統計量になる。これを通じて回帰係数が統計的に有意かどうか仮説検定を行うことができる。\n(5) p値 pr(\u0026gt;|t|) これが小さいということは、回帰係数が有意であり、相関関係があることを示したい場合は、小さいほど良いということだ。この値がどれだけ小さいかによって、直下のSignif. codesに従って点が打たれる。通常、有意水準は5%とされるので、点が1つでも打たれれば、相関関係があると見なしても構わない。この値が大きく回帰関係がないことが明らかになった場合、回帰係数がどのように決定されたとしても、統計的には意味がない。このp値はt分布から出たものなので、pr(\u0026gt;|t|)という表現が適切であることがわかる。\n(6) 修正決定係数 この分析が変数の数に対してデータをどれだけよく説明しているかを示す尺度であり、高ければ高いほど良い。回帰分析では、変数の数が増えることは、それだけ使用できるデータが増えることを意味し、そのときに説明力$R^{2}$は必ず増加する。修正決定係数と決定係数の違いは、$\\displaystyle R^2 = 1 - {{ \\text{ SSE } } \\over { \\text{ SST} }}$と違って $$ \\displaystyle R^{2}_{a} = 1 - {{ \\text{ SSE } / (n - p - 1) } \\over { \\text{ SST} / (n - 1) }} $$ 変数の数が反映されるように計算されるところだ。実際の応用数学で変数の数を増やすということは、コストの増大を意味し、当然これを減らす方法を考えなければならない。修正決定係数$R^{2}_{a}$は変数の数に対するペナルティを適用し、決定係数を補正し、モデルとモデルを比較するのに使用される。変数をたくさん増やしても修正決定係数があまり増加しない場合、それだけ無駄なことをしたと見なされる。\n(7) F統計量 これのp値が小さいということは、有意な回帰係数が存在するということであり、相関関係を示したい場合は、小さいほど良い。回帰係数のt検定が回帰係数一つ一つに対する検定であれば、F検定は回帰分析自体に対する検定だ。p値を別にして、F統計量自体について語れば、モデルを比較するのに便利に使われる。\nただ、これで結果を見ることができたとしても終わったわけではないことに注意。重回帰分析が単回帰分析よりもずっと難しいのは、独立変数が増えることでなかったさまざまな問題が生じるからだ。数値計算だけでは説明が難しいさまざまな確認手順が残っており、実際はこれをうまくできることが、学部レベルの回帰分析をうまくできることだ。\nコード 以下はRで書かれた全体の例題コードだ。\ntail(attitude)\rwin.graph()\rplot(attitude)\rout\u0026lt;-lm(rating~complaints+privileges+learning\r+raises+critical+advance,data=attitude)\rout\u0026lt;-lm(rating~.,data=attitude)\rsummary(out) 参照 重回帰分析 回帰係数のF検定 Juliaでの回帰分析のやり方 ","id":670,"permalink":"https://freshrimpsushi.github.io/jp/posts/670/","tags":["R"],"title":"Rでの多重回帰分析結果の見方"},{"categories":"통계적분석","contents":"概要 回帰分析とは、変数間の関係を解明する方法であり、特に線形関係を明らかにするのに役立つ。多重線形回帰分析Multiple Linear Regressionは、一つの従属変数（応答変数）に複数の独立変数（説明変数）がどのように影響を及ぼすかを把握する回帰分析を指す。\nモデル 1 $$Y = \\beta_{0} + \\beta_{1} X_{1} + \\cdots + \\beta_{p} X_{p} + \\varepsilon $$\n私たちは、変数が上記のような線形関係を持っているかに興味がある。各変数は互いに独立していると仮定され、同様に、回帰係数は他の変数が固定されたときのその変数の単位変化率を意味する。設計行列で表すと $$ \\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; x_{11} \u0026amp; \\cdots \u0026amp; x_{p1} \\\\ 1 \u0026amp; x_{12} \u0026amp; \\cdots \u0026amp; x_{p2} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 1 \u0026amp; x_{1n} \u0026amp; \\cdots \u0026amp; x_{pn} \\end{bmatrix} \\begin{bmatrix} \\beta_{0} \\\\ \\beta_{1} \\\\ \\vdots \\\\ \\beta_{p} \\end{bmatrix} + \\begin{bmatrix} \\varepsilon_{1} \\\\ \\varepsilon_{2} \\\\ \\vdots \\\\ \\varepsilon_{n} \\end{bmatrix} $$ であり、まとめると$Y = X \\beta + \\varepsilon$である。\n計算自体は、最小二乗法を使用するが、幸い最小二乗法は次元$p$に特にこだわらない。しかし、単純回帰分析と異なり$p$次元に対して一般化されるため、$p \\ge 3$ではグラフで確認するのも難しい。\nただ見るだけでは分析が適切に行われたか分からないので、分析者は様々な診断を通じて結果を正当化する必要がある。そのような診断を通過したとしても、交互作用や多重共線性などの問題が残り、どの変数を選ぶかも重要な問題だ。\n関連項目 Rでの多重回帰分析結果 回帰係数のF検定 多重回帰係数ベクトルの推定量導出 Hadi. (2006). Regression Analysis by Example(4th Edition): p53.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":666,"permalink":"https://freshrimpsushi.github.io/jp/posts/666/","tags":null,"title":"多重回帰分析"},{"categories":"통계적검정","contents":"仮説検定 $$ \\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; x_{11} \u0026amp; \\cdots \u0026amp; x_{p1} \\\\ 1 \u0026amp; x_{12} \u0026amp; \\cdots \u0026amp; x_{p2} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 1 \u0026amp; x_{1n} \u0026amp; \\cdots \u0026amp; x_{pn} \\end{bmatrix} \\begin{bmatrix} \\beta_{0} \\\\ \\beta_{1} \\\\ \\vdots \\\\ \\beta_{p} \\end{bmatrix} + \\begin{bmatrix} \\varepsilon_{1} \\\\ \\varepsilon_{2} \\\\ \\vdots \\\\ \\varepsilon_{n} \\end{bmatrix} $$ 独立変数が$p$個、$n$個のデータが与えられた時、線形多重回帰モデルを設計行列で表すと上のようになり、簡単に$Y = X \\beta + \\varepsilon$としよう。モデル診断で残差が線形性や等分散性、独立性、正規性を満たすとする。多重回帰分析で各回帰係数に対する仮説検定は次の通り。\n$H_{0}$：$\\beta_{j} = 0$つまり、$j$番目の独立変数は従属変数との相関関係がない。 $H_{1}$：$\\beta_{j} \\ne 0$つまり、$j$番目の独立変数に関する回帰係数は有意である。 導出 1 回帰係数の正規性: $$ \\hat{\\beta} \\sim N_{1+p} \\left( \\beta , \\sigma^{2} \\left( X^{T} X \\right)^{-1} \\right) $$ 残差平方和の不偏推定量と回帰係数の標準誤差: $$ E \\widehat{\\sigma^{2}} = E \\left[ {{ 1 } \\over { n-p-1 }} \\sum_{i=1}^{n} \\left( y_{i} - \\hat{y}_{i} \\right)^{2} \\right] = \\sigma^{2} $$ $$ \\text{s.e.} \\left( \\hat{\\beta}_{k} \\right) = \\hat{\\sigma} \\sqrt{ \\left[ \\left( X^{T} X \\right)^{-1} \\right]_{kk} } $$ 回帰係数の推定値$\\hat{ \\beta_{j} }$と標準誤差$\\text{se} \\left( \\hat{ \\beta_{j} } \\right)$について$t_{j}$を次のように置こう。 $$ t_{j} := {{\\hat{ \\beta_{j} }} \\over {\\text{se} \\left( \\hat{ \\beta_{j} } \\right)}} $$\nカイ二乗分布に従う確率変数の和: 確率変数$X_{1} , \\cdots , X_{n}$が相互独立とする。$X_i \\sim \\chi^2 ( r_{i} )$ならば $$ \\sum_{i=1}^{n} X_{i} \\sim \\chi ^2 \\left( \\sum_{i=1}^{n} r_{i} \\right) $$ 残差平方和$\\sum_{i=1}^{n} \\left( y_{i} - \\hat{y}_{i} \\right)^{2} / \\sigma^{2}$はデータの数が$n$個、独立変数が$p$個と$1$個の定数項のサンプル平均を使用する―独立な確率変数は$(n-p-1)$個しか使用されていないため、自由度$(n-p-1)$のカイ二乗分布に従い、帰無仮説の下では―帰無仮説が真であると仮定すると$\\beta_{j} = 0$であることから$\\hat{\\beta}_{j} \\sim N \\left( 0 , \\sigma^{2} \\left( X^{T} X \\right)^{-1}_{jj} \\right)$が得られる。 $$ \\begin{align*} t_{j} =\u0026amp; {{\\hat{ \\beta_{j} }} \\over {\\text{se} \\left( \\hat{ \\beta_{j} } \\right)}} \\\\ =\u0026amp; {{\\hat{ \\beta_{j}} - 0 } \\over { \\hat{\\sigma} \\sqrt{ \\left[ \\left( X^{T} X \\right)^{-1} \\right]_{kk} } }} \\\\ =\u0026amp; {{\\hat{ \\beta_{j}} - 0 } \\over { \\sqrt{ {{ \\sum_{i=1}^{n} \\left( y_{i} - \\hat{y}_{i} \\right)^{2} } \\over { n-p-1 }} \\left[ \\left( X^{T} X \\right)^{-1} \\right]_{kk} } }} \\\\ =\u0026amp; {{ {{ \\hat{ \\beta_{j}} - 0 } \\over { \\sqrt{ \\left[ \\left( X^{T} X \\right)^{-1} \\right]_{kk} } }} } \\over { \\sqrt{ {{ \\sum_{i=1}^{n} \\left( y_{i} - \\hat{y}_{i} \\right)^{2} } \\over { n-p-1 }} } }} \\\\ =\u0026amp; {{ {{ \\hat{ \\beta_{j} } - 0 } \\over { \\sigma \\sqrt{ \\left[ \\left( X^{T} X \\right)^{-1} \\right]_{kk} } }} } \\over { \\sqrt{ {{ \\sum_{i=1}^{n} \\left( y_{i} - \\hat{y}_{i} \\right)^{2} } \\over { \\sigma^{2} }} / (n-p-1) } }} \\\\ \\sim \u0026amp; {{ N (0,1) } \\over { \\sqrt{\\chi^{2} (n-p-1) / n-p-1} }} \\end{align*} $$\nt分布の導出: ２つの確率変数$W,V$が独立であり$W \\sim N(0,1)$、$V \\sim \\chi^{2} (r)$とすると $$ T = { {W} \\over {\\sqrt{V/r} } } \\sim t(r) $$ 結論として、$t_{j}$は自由度$(n-p-1)$のt分布に従う。再び式で表すと $$ t_{j} = {{\\hat{ \\beta_{j} }} \\over {\\text{se} \\left( \\hat{ \\beta_{j} } \\right)}} \\sim t(n-p-1) $$ で、これを使って仮説検定を行う。もし $$ \\left| t_j \\right| \\ge t_{(n-p-1 , {{\\alpha} \\over {2}})} $$ であれば帰無仮説を棄却する。$\\left| t_j \\right|$がその程度大きいということは、帰無仮説が真であると信じるには$\\hat{ \\beta_{j} }$が大きすぎるという意味である。\n■\n説明 $j$番目の変数と有意水準$\\alpha$に対する信頼区間$\\hat{ \\beta_{j} } \\pm t_{(n-p-1 , {{\\alpha} \\over {2}})} \\text{se} ( \\hat{ \\beta_{j}} )$も計算できる。\n導出過程は落ち着いて書かれているが、実際に回帰分析を学ぶ学部生が理解するにはかなり難しいかもしれない。$t_{j}$がt分布に従うこと自体はそんなに難しくないが、その補助定理として回帰係数の分布を知る必要があり、モデル診断の概念をしっかりと把握している必要がある。\n一緒に見る 単純回帰分析 多重回帰分析 Rでの単純回帰分析結果 Hadi. (2006). Regression Analysis by Example(4th Edition): p0.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":654,"permalink":"https://freshrimpsushi.github.io/jp/posts/654/","tags":null,"title":"回帰係数のt検定"},{"categories":"열물리학","contents":"定義 次の式を満たす$S$をエントロピーentropyと定義する。\n$$ dS = {{ \\delta Q_{\\text{rev} } } \\over { T }} $$\n説明 エントロピーは「無秩序さ」を示す物理量で、数式だけ見てなぜ無秩序さを意味するのか理解するのは難しい。「部屋を散らかす」や「コップにインクを落とす」など、非専門家のための説明は「無秩序さ」を説明できても$dS = \\dfrac{\\delta Q_{\\text{rev}} }{ T }$を説明できない。\nこの概念を受け入れるためには、エントロピーは導出されたものではなく「定義」であると考える必要がある。「なぜこんなに定義したの？」と思ったら、簡単な視覚的説明が役立つだろう。以下の二つの状況を考えてみよう：\nケース1. システムの温度が低い場合\n冷えた空間に熱エネルギー$Q$を加えると想像してみよう。これは、低い温度$T_{1}$での熱エネルギーの変化$\\delta Q$を与えることである。このとき、エントロピーの変化を$d S_{1}$としよう。\nケース2. システムの温度が高い場合\nすでに熱い空間に**ケース1.**と同様に熱エネルギー$Q$をさらに加えると想像してみよう。これは、高い温度$T_{2}$での熱エネルギーの変化$\\delta Q$を与えることである。このとき、エントロピーの変化を$d S_{2}$としよう。\n投入されたエネルギーは熱力学の第二法則に従って、低い温度の場所へ拡散していくだろう。青かった空間**ケース1.は説明し難いような濁った色に変わり、空間ケース2.**は最初より少し赤くなっただけだ。この色の変化の差は、投入されたエネルギーによって空間がどれだけ変わったかの差を示しており、$d S_{1} \u0026gt; d S_{2}$を意味する。数式的に見れば$T_{1} \u0026lt; T_{2}$なので、次のようになる。\n$$ {{1} \\over {T_{1} }} \u0026gt; {{1} \\over {T_{2}}} $$\nそうすると、エントロピーの定義によって次の式が自然な結論として分かる。\n$$ d S_{1} = {{\\delta Q_{\\text{rev} }} \\over {T_{1} }} \u0026gt; {{\\delta Q_{\\text{rev} }} \\over {T_{2}}} = d S_{2} $$\n熱力学的に表せば、「**ケース1.のエントロピーの増加はケース2.**よりも大きい」となる。部屋を散らかすことで表せば、「同じいたずらをしても、散らかっている部屋より整っている部屋の方が散らかりやすい」となる。インクを落とすことで表せば、「同じ量を入れても、濁った水より清水の方が汚れやすい」となる。\n","id":651,"permalink":"https://freshrimpsushi.github.io/jp/posts/651/","tags":null,"title":"熱力学におけるエントロピーとは何か"},{"categories":"통계적분석","contents":"概要 回帰分析は、変数間の関係を見つける方法であって、特に線形関係を明らかにするのに便利だ。単純回帰分析Simple Linear Regressionは、その中でも一番簡単で、従属変数（反応変数）一つと独立変数（説明変数）一つに関する回帰分析を指す。\nモデル 1 独立変数 $x_{i}$ と従属変数 $y_{i}$ が線形関係にあるというのは、ある $a,b$ に対して $y_{i} = ax_{i} + b$ で表せるということだ。もちろん、実際のデータに関しては、誤差が生じるので、正確には誤差項を含めて $y_{i} = ax_{i} + b + \\varepsilon_{i}$ になる。これを回帰分析でよく使う形に変えてみると $$ y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\varepsilon_{i} $$ 設計行列で表すと $$ \\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; x_{1} \\\\ 1 \u0026amp; x_{2} \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; x_{n} \\end{bmatrix} \\begin{bmatrix} \\beta_{0} \\\\ \\beta_{1} \\end{bmatrix} + \\begin{bmatrix} \\varepsilon_{1} \\\\ \\varepsilon_{2} \\\\ \\vdots \\\\ \\varepsilon_{n} \\end{bmatrix} $$ 整理すると $Y = X \\beta + \\varepsilon$ を得る。\n最適化 これは 最小二乗法を通して $\\| \\varepsilon \\|_{2} = \\| Y - X \\beta \\|_{2}$ が最小になる $\\beta = \\begin{bmatrix} \\beta_{0} \\\\ \\beta_{1} \\end{bmatrix}$ を見つける問題になる。$\\beta$ は直線の切片と傾きを表しているので、$\\beta$ を見つけるのは、データを線形的に説明してくれる最も誤差が少ない直線を見つけることだ。もちろん、二変数が厳密にどのような関係を持っているかはわからないので、回帰係数の推定値 $\\hat{ \\beta_{0}}$ と$\\hat{ \\beta_{1} }$ を見つけなければならない。簡単に言えば、データに最も似た直線を引けばいい。\nこのような問題は普通、数理線形代数のツールを使って解くが、単純回帰分析は単純な微分積分学でも解くことができる。行列を再び分解して表すと $$ \\begin{align} \\varepsilon^2 = \\sum_{i=1}^{n} ( y_{i} - \\beta_{0} - \\beta_{1} x_{i} )^2 \\end{align} $$ 最小にする $\\beta_{0} = \\hat{ \\beta_{0} }$ と$\\beta_{1} = \\hat {\\beta_{1}}$ を見つけることだ。式 $(1)$ から $\\beta_{0}$ に関して偏微分を取ると $$ {{ \\partial \\varepsilon^2 } \\over { \\partial \\beta_{0}}} = -2 \\sum_{i=1}^{n} (y_{i} - \\beta_{0} - \\beta_{1} x_{i} ) $$ $\\varepsilon^2$ が最小になるためには $$ n \\beta_{0} = \\sum_{i=1}^{n} y_{i} - \\beta_{1} \\sum_{i=1}^{n} x_{i} $$ 従って$\\varepsilon^2$ は $\\beta_{0} = \\overline{y} - \\beta_{1} \\overline{x}$ の時、最小になる。式 $(1)$ から $\\beta_{1}$ に関して偏微分を取ると $$ {{ \\partial \\varepsilon^2 } \\over { \\partial \\beta_{1}}} = -2 \\sum_{i=1}^{n} x_{i} (y_{i} - \\beta_{0} - \\beta_{1} x_{i} ) $$ $\\varepsilon^2$ が $\\beta_{0} = \\overline{y} - \\beta_{1} \\overline{x}$ のため最小になるので $$ \\sum_{i=1}^{n} x_{i} (y_{i} - \\overline{y} + \\beta_{1} \\overline{x} - \\beta_{1} x_{i} ) = 0 $$ つまり、 $$ \\beta_{1} \\sum_{i=1}^{n} ( x_{i}^2 - \\overline{x} x_{i} ) = \\sum_{i=1}^{n} x_{i} y_{i} - \\sum_{i=1}^{n} x_{i} \\overline{y} $$ 整理すると $$ \\begin{align*} \\beta_{1} =\u0026amp; {{\\sum_{i=1}^{n} x_{i} y_{i} - \\sum_{i=1}^{n} x_{i} \\overline{y} } \\over {\\sum_{i=1}^{n} ( x_{i}^2 - \\overline{x} x_{i} ) }} \\\\ =\u0026amp; {{ \\sum_{i=1}^n ( x_{i} - \\overline{x} ) ( y_{i} - \\overline{y} ) } \\over { \\sum_{i=1}^{n} (x_{i}^2 - \\overline{x}^2 )}} \\\\ =\u0026amp; {{ \\text{Cov} (X,Y) } \\over { \\text{Var} ( X ) }} \\\\ =\u0026amp; \\text{Cor} (X,Y) {{s_{y}} \\over {s_{x}}} \\end{align*} $$ 実際の計算では、$\\hat{\\beta_{1}}$ よりも先に $\\hat{\\beta_{0}}$ を求めるべきだろう。\n参照 Rでの単純回帰分析の結果 回帰係数のt検定 多重回帰分析：単純回帰分析と違って、複数の独立変数でモデルを拡張する。 多重回帰係数ベクトルの推定量の導出 Hadi. (2006). Regression Analysis by Example(4th Edition): p24.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":648,"permalink":"https://freshrimpsushi.github.io/jp/posts/648/","tags":null,"title":"単純回帰分析"},{"categories":"통계적분석","contents":"定義 1 回帰分析で得られた回帰式を$Y \\gets X_{1} + X_{2} + \\cdots + X_{n}$とし、$y = \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} + \\cdots + \\beta_{n} x_{n}$で示そう。n番目のデータを$(y_{i} , x_{i1} , x_{i2} , \\cdots , x_{in})$と表す。\n平均Mean: $$ \\displaystyle \\overline{y} := {{1} \\over {n}} \\sum_{i=1}^{n} y_{i} $$ 適合値Fitted Value: n番目のデータ $y_{i}$ に対して $$ \\hat{y}_{i} := \\beta_{0} + \\beta_{1} x_{i1} + \\beta_{2} x_{i2} + \\cdots + \\beta_{n} x_{in} $$ 予測値Predicted Value: 新しいデータ $y_{0}$ に対して $$ \\hat{y}_{0} := \\beta_{0} + \\beta_{1} x_{01} + \\beta_{2} x_{02} + \\cdots + \\beta_{n} x_{0n} $$ 適合による偏差Deviation due to Fit: $$ \\hat{y}_{i} - \\overline{y} $$ 残差Residual: $$ y_{i} - \\hat{y}_{i} $$ TSS(総平方和)またはSST(全体の平方和): $$ \\text{TSS} =\\text{SST} := \\sum_{i=1}^{n} ( y_{i} - \\overline{y} )^2 $$ ESS(説明される平方和)またはSSR(回帰による平方和): $$ \\text{ESS} = \\text{SSR} := \\sum_{i=1}^{n} ( \\hat{y}_{i} - \\overline{y} )^2 $$ RSS(残差平方和)またはSSE(平方誤差和): $$ \\text{RSS} = \\text{SSE} := \\sum_{i=1}^{n} ( y_{i} - \\hat{y}_{i} )^2 $$ R二乗R-squaredまたは説明率: $$ R^2 := {{ \\text{ SSR } } \\over { \\text{ SST} }} $$ 説明 適合値と予測値は数学的には全く同じだが、回帰式に代入するデータが実データかどうかの違いがある。ここで$\\hat{y_{i}}$を求めるということは、与えられた情報を反映した値を計算することを意味する。この場合、5番の残差は、我々がどうしようもない―当然存在するべき、自然にあるべきエラーだ。回帰分析は、それらの平方和を最小化して回帰線を求め、その後残差を見て回帰分析の仮定が満たされているかを確認することをモデル診断と呼ぶ。 「説明される平方和ESS」は、「説明できない平方和RSS」と対照的な表現に過ぎない。困ったことに、EとRはそれぞれExplainedとRegression, ErrorとResidualで、似ているように書かれている。 $$ \\text{TSS} = \\text{SST} \\\\ \\text{ESS} = \\text{SSR} \\\\ \\text{RSS} = \\text{SSE} $$ 先に付くか後に付くかによって$E$と$R$が変わるような暗記はお勧めしない。ただ自分にとって快適な記号を一つ選んで、数式で覚えておき、知っていることと反対に書かれていたら、略語も反対にされ得るという事実だけを覚えておけば十分だ。 R二乗は説明率とも呼ばれ、分析がデータをどれ程よく説明しているかを示す尺度になる。一方で、$\\text{SST} = \\text{SSR} + \\text{SSE}$は容易に示され、これによると、$\\text{ESS}$が高まるにつれて、$\\text{RSS}$は小さくなり、$0 \\le R^{2} \\le 1$が真となる。それでは直感的に見た場合、 $$ R^2 = {{ \\text{ SSR } } \\over { \\text{ SST} }} = {{ \\text{ ESS } } \\over { \\text{ TSS } }} = {{\\text{설명할 수 있는 에러}} \\over {\\text{전체 에러}}} $$ となるので、分析でのデータの説明比率として理解することができる。 Hadi. (2006). 回帰分析の例（第4版）: p40~42.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":650,"permalink":"https://freshrimpsushi.github.io/jp/posts/650/","tags":null,"title":"適合値、予測値、残差、誤差"},{"categories":"위상수학","contents":"定義 1 位相空間 $X$ と $Y$ に対して次のように定義された積空間 $Y^{X}$を関数空間という。 $$ Y^{X} : = \\prod_{x \\in X} Y = \\left\\{ f \\ | \\ f : X \\to Y \\text{ is a function} \\right\\} $$\n関数空間の位相として次がある:\n$x \\in X$ と $Y$ の開集合 $U$ に対して $$ S (x , U) = \\left\\{ f \\in Y^{X} \\ | \\ f(x) \\in U \\right\\} $$ としよう。部分基底 $\\left\\{ S(x,U) \\ | \\ x \\in X , U \\subset Y \\right\\}$ によって生成される $Y^{X}$ の位相をポイント-オープン位相という。 コンパクト集合 $K \\subset X$ と $Y$ の開集合 $U$ に対して $$ S (K , U) = \\left\\{ f \\in Y^{X} \\ | \\ f(K) \\subset U \\right\\} $$ としよう。部分基底 $\\left\\{ S(K,U) \\ | \\ K \\subset X , U \\subset Y \\right\\}$ によって生成される $Y^{X}$ の位相をコンパクト-オープン位相という。 $(Y,d)$ を距離空間としよう。\nコンパクト集合 $K \\subset X$ と $\\varepsilon \u0026gt; 0$ に対して $$ B_{K} (f, \\epsilon) = \\left\\{ g \\in Y^{X} \\ \\left| \\ \\sup_{x \\in K} \\left\\{ d(f(x),g(x)) \\right\\} \u0026lt; \\varepsilon \\right. \\right\\} $$ としよう。基底 $\\left\\{ B_{K} (f, \\varepsilon ) \\ | \\ K \\subset X , \\varepsilon \u0026gt; 0 \\right\\}$ によって生成される $Y^{X}$ の位相をコンパクト収束位相という。 一様距離 $$ \\overline{ \\rho } (f,g) : = \\sup_{x \\in X} \\left\\{ \\min \\left\\{ d(f(x) , g(x) ) , 1 \\right\\} \\right\\} $$ によって生成される距離空間 $(Y^{X} , \\overline{ \\rho } )$ の位相を一様位相という。 定理 [5]: $X$ が離散空間ならば$Y^{X}$ のコンパクト収束位相はポイント-オープン位相と同じである。 [6]: $X$ がコンパクト空間ならば$Y^{X}$ のコンパクト収束位相はティコノフ位相と同じである。 $\\left\\{ f_{n} : X \\to Y \\right\\}$ を$Y^{X}$ での数列とし、定義域を$K \\subset X$ に限定した関数を$f_{n} |_{K} : K \\to Y$ と表すことにしよう。\n[7]: $\\left\\{ f_{n} \\right\\}$ が$Y^{X}$ のポイント-オープン位相に属する$f$ に収束する場合、全ての$x \\in X$ に対して$ f_{n} (x) $ は$f(x)$ に収束する。 [8]: $\\left\\{ f_{n} \\right\\}$ が$Y^{X}$ のコンパクト収束位相に属する$f$ に収束する場合、全てのコンパクト$K \\subset X$ に対して$f_{n} |_{K}$ は$f |_{K}$ に一様に収束する。 定義域が位相空間$X$ で値域が距離空間$Y$ の連続関数の集合を $$ C(X,Y) := \\left\\{ f \\in Y^{X} \\ | \\ f \\text{ is continuous} \\right\\} $$ とし、$C(X,Y)$ を$Y^{X}$ の部分空間としよう。\n[9]: $C(X,Y)$ のコンパクト-オープン位相とコンパクト収束位相は同じである。 [10]: $C(X,Y)$ のコンパクト収束位相は$Y$ の距離関数に依存しない。 [11]: $C(X,Y)$ の数列 $\\left\\{ f_{n} \\right\\}$ が$f \\in Y^{X}$ に収束すれば$f : X \\to Y$ は連続関数である。 説明 特に$C(X, \\mathbb{R})$ を$C(X)$ のように示し、特に$X$ が区間の時、すなわち$X=(a,b)$、$X=[a,b]$ の時はそれぞれ$C(a,b)$、$C[a,b]$ とも示される。\n1~4 要約すると、ポイント-オープン位相は小さく、一様位相は大きいと言える。\n[7], [8] 関数が一様連続であることを示すのに役立てられる。\n[10], [11] 一般位相を解析学の一般化と見た時、非常に重要な事実である。\nMunkres. (2000). 『トポロジー(第2版)』: p267.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":646,"permalink":"https://freshrimpsushi.github.io/jp/posts/646/","tags":null,"title":"位相数学における関数空間"},{"categories":"열물리학","contents":"法則 クラウジウス: 自分で冷たい方から熱い方へ熱を送る過程は存在しない。\nケルビン: 熱を完全に仕事に変える過程は存在しない。\n解説 ドイツの物理学者クラウジウスとイギリスの物理学者ケルビンの熱力学第二法則に対する声明は互いに等価だ。最も有名なのは、ギリシャの数学者カラテオドリの声明で、「閉じた系の無秩序度は減少しない」というものがある。この言葉は終末論を肯定し、「熱い」「仕事」などの言葉を使わないため、素人に特に人気がある。真剣な考察なしに擬似科学だけを選択的に受け入れた人々が好む概念だ、「シュレーディンガーの猫」のように。この法則に登場する単語をもう少し厳密に定義してみよう。\n寒いは相対的に温度が低いことを言う。\n熱いは相対的に温度が高いことを言う。\n熱を仕事に変えることを過程という。\n外部から熱を受け取り過程を行う閉じた系を熱機関という。\n熱機関が熱$Q_{h}$を受け取り仕事$W$をしたとする。その時$\\eta : = \\dfrac{W}{Q_{h}}$を効率という。\n蒸気機関や熱気球などが熱機関の例である。効率は「機関に提供されたエネルギーの中で、私が望む仕事に変えられた比率」を示すので、常識的な定義と言えるだろう。\n寒い時を青色と$l$、熱い時を赤色と$h$で示し、上のように示すことができる。高い温度でエネルギー$Q_{h}$を受け取り、その一部を仕事で使用した後のエネルギーを残すことである。\nクラウジウス: 自分で寒い方から熱い方へ熱を送る過程は存在しない\n上の図はクラウジウスの熱力学第二法則を図式化したものである。簡単に言えば、クラウジウスの声明は「熱は必ず高い所から低い所へ流れる」と言える。この法則が間違っていれば、水が下から上へ流れ、突然なかったエネルギーが現れて熱くなることもあり得ただろう。\nもちろん、上のように機関が外部から仕事を受け取る場合は話が異なる。「自分で」冷たい方から熱い方へ熱を送った過程ではないため、寒い方から熱い方へ熱が伝わった。これは熱力学第二法則に違反することではなく、「閉じた系」という前提が満たされていなかったために可能なことである。\nケルビン: 熱を完全に仕事に変える過程は存在しない\n上の図はケルビンの熱力学第二法則を図式化したものである。簡単に言えば、ケルビンの声明は「損失なし」はあり得ないということだ。この法則が間違っていれば、ガリレオの思考実験は実際の実験だったことになり、永久機関も存在することになるだろう。\n一緒に見る 熱力学第0法則 熱力学第1法則 ","id":643,"permalink":"https://freshrimpsushi.github.io/jp/posts/643/","tags":null,"title":"熱力学の第二法則"},{"categories":"추상대수","contents":"定理 1 $G,G'$ が群だとしよう。\n第一同型定理: 準同型写像 $\\phi : G \\to G'$ が存在するならば $$ G / \\ker ( \\phi ) \\simeq \\phi (G) $$ 第二同型定理: $H \\le G$ かつ $N \\triangleleft G$ の場合 $$ (HN) / N \\simeq H / (H \\cap N) $$ 第三同型定理: $H , K \\triangleleft G$ かつ $K \\leq H$ の場合 $$ G/H \\simeq (G/K) / (H/K) $$ 同型定理Isomorphism Theoremは、代数学者エミー・ネーターによって証明された三つの独立した定理を指す。\n$\\ker$ は核である。 $N \\triangleleft G$ は、$N$ が $G$ の正規部分群であることを意味する。 説明 第一同型定理は、上の図の赤色に相当する同型写像 $\\color{red} {\\mu}$ が存在することを意味している。これは、群において $\\phi$ に対して不必要な部分を捨て、核をある種の「単位」として扱うだけの構造を残すことができることを示している。\n証明 $K : = \\ker ( \\phi )$ とする。$\\mu : G / K \\to \\phi (G)$ を $\\mu (xK) = \\phi ( x)$ として定義しよう。$\\mu$ が同型写像であることを示せばよい。\n第1部. $\\mu$ は関数である。\n$x,y \\in G$ と $G'$ の単位元 $e'$ に対して\n$$ \\begin{align*} \u0026amp; xK = yK \\\\ \\iff \u0026amp; x^{-1} y \\in K \\\\ \\iff \u0026amp; \\phi ( x^{-1} y ) = e' \\\\ \\iff \u0026amp; \\phi ( x^{-1} ) \\phi ( y ) = e' \\\\ \\iff \u0026amp; \\phi ( x ) ^{-1} \\phi ( y ) = e' \\\\ \\iff \u0026amp; \\phi ( x ) = \\phi ( y ) \\end{align*} $$ 従って $xK = yK \\implies \\phi ( x ) = \\phi ( y )$ なので $\\mu$ は関数である。\n第2部. $\\mu$ は単射である。\n第1部のプロセスを逆にたどれば $\\phi ( x ) = \\phi ( y ) \\implies xK = yK$ となるので $\\mu$ は単射である。\n第3部. $\\mu$ は全射である。\n$\\mu ( G / K ) = \\left\\{ \\mu (xK) \\ | \\ x \\in G \\right\\} = \\left\\{ \\phi (x) \\ | \\ x \\in G \\right\\} = \\phi (G)$ だから、$\\mu$ は全射である。\n第4部. $\\mu$ は準同型写像である。\n$x,y \\in G$ に対して $$ \\mu (xKyK) = \\mu (xyK) = \\phi (xy) = \\phi (x) \\phi (y) = \\mu (xK) \\mu (yK) $$ 従って $\\mu$ は準同型写像である。\n■\n一般化 一方で、第一同型定理を環に対して拡張した定理が知られている。証明方法はほぼ同じで、群と異なり、加法と乗法の二つの操作を考える点が異なる。\n準同型写像の基本定理: 環 $R$, $r '$ に対して準同型写像 $\\phi : R \\to r '$ が存在するならば $R / \\ker ( \\phi ) \\simeq \\phi (R)$\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p307~309.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":637,"permalink":"https://freshrimpsushi.github.io/jp/posts/637/","tags":null,"title":"第一同型定理の証明"},{"categories":"추상대수","contents":"定義 1 単位元が$e$の群$G$と集合$X$に対し、以下の二つの条件を満たす二項演算$\\ast : G \\times X \\to X$を$X$上での$G$の作用Actionと言い、$X$を**$G$-集合**と呼ぶ。\n(i): すべての$x \\in X$に対して$ex = x$がある (ii): すべての$x \\in X$と$g_{1} , g_{2} \\in G$に対して$( g_{1} g_{2} ) (x) = g_{1} (g_{2} x)$がある 説明 群の作用とは、簡単に言えば「$x \\in X$に$g \\in G$を加える」ことである。直感的に理解するために、次のような図を考えてみよう：\n$$ X : = \\left\\{ C, 1,2,3,4 , p_{1}, p_{2} , p_{3} , p_{4} , s_{1}, s_{2} , s_{3} , s_{4} , d_{1}, d_{2} , m_{1} , m_{2} \\right\\} $$ 集合$X$に対し、正四面体群$D_{4}$を考える。正方形で考えられる線分と点の集合である$X$は、$D_{4}$によって裏返されたり回転させられたりして位置が変わる可能性があるため、$D_{4}$-集合である。$X$に変化を与える操作を作用と呼ぶことは非常に理にかなっており、妥当だと言えるだろう。\nちなみに$X$は特に群である必要はなく、$G$と関係がない場合もある。例えば、$\\mathbb{Z}$と $$ X:= \\left\\{ \\cdots , - {{3} \\over {2}} , - {{1} \\over {2}}, {{1} \\over {2}} , {{3} \\over {2}} , \\cdots \\right\\} $$ を考えた場合、$\\left\u0026lt; X , + \\right\u0026gt;$は群にならず$G = \\mathbb{Z}$とまだ関係がない。だが、$z \\in \\mathbb{Z}$と$x \\in X$に対して$\\ast : \\mathbb{Z} \\times X \\to X$が$ z * x = z + x$として定義されるならば\n(i): $0 + x = x$が成り立ち、 (ii): $(z_{1} + z_{2}) + x = z_{1} + (z_{2} + x)$が成り立つので、演算$\\ast$は$X$上での作用となり、$X$は$\\mathbb{Z}$-集合となる。 Fraleigh. (2003). A first course in abstract algebra(7th Edition): p154.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":630,"permalink":"https://freshrimpsushi.github.io/jp/posts/630/","tags":null,"title":"群の作用"},{"categories":"열물리학","contents":"法則 熱エネルギーが$Q$の系に対して働ける仕事が$W$の時、内部エネルギー$U$に対して次の式が成り立つ。\n$$ d U = \\delta Q + \\delta W $$\n$\\delta$は不完全微分inexact differentialを示している。\n説明 これらは綺麗な形で原始関数が存在しないため、線積分を通して計算しなければならない。内部エネルギーの変化だけでは具体的に熱エネルギーがどれだけ変わったのか、仕事がどれだけ変わったのかわからないことを意味する。例えば$10 = 2 + 8$でも$10 = -5 + 15$でも左辺は同じく$10$であることを考えると役立つだろう。\nしかし、これはただの熱力学第一法則の限界に過ぎないという話であり、言いたいことの本質ではない。逆に言えば、熱エネルギーと仕事がどうであれ、内部エネルギーの変化は綺麗に計算できるということだ。熱力学第一法則から導かれる式には次のようなものがある。\n式1 ピストンが押した距離$dx$と力$F$について$\\delta W = F dx$\n実は、この形は熱力学ではほとんど使用されない。ピストンの面積$A$に対して圧力$p$と力$F$は$F = pA$と表せるので、$A dx = - dV$である。\n式2 圧力$p$と体積$V$について$\\delta W = - p d V$\n上とは異なり、この形はかなり頻繁に使用されるので、特に符号に注意して覚えておくこと。\n併せて見る 熱力学の第零法則 熱力学の第二法則 ","id":629,"permalink":"https://freshrimpsushi.github.io/jp/posts/629/","tags":null,"title":"熱力学の第一法則"},{"categories":"추상대수","contents":"定義 1 $H \\subset G$のすべての剰余類の集合を$G / H$としよう。$(aH) \\ast\\ (bH) = (ab) H$としてよく定義された二項演算$\\ast$が存在する場合、$\\left\u0026lt; G / H , * \\right\u0026gt;$を商群Factor Groupと言う。\n定理 $H \\leqslant G$としよう。$H \\triangleleft G$であることと$G / H$が群であることは同値だ。\n説明 $H \\triangleleft G$ということは、$H$が$G$の正規部分群であるということだ。\n二項演算$\\ast$は剰余類の代表元だけで計算する二項演算で、集合$G / H$が商群を形成するようにする。$G / H$がなぜ群を形成するか直感的に理解できないなら、剰余類の概念が誤っている可能性が高い。\n証明 $( \\implies )$ $(aH) (bH) = (ab) H$であることを示せばよい。\n$H$は正規部分群なので、$h_{1} b \\in H b$の場合、$b h_{3} \\in bH$を満たす何らかの$h_{3} \\in H$が存在する。$ah_{1} \\in aH$と$bh_{2} \\in H$に対して $$ (ah_{1}) (b h_{2}) = a(h_{1} b)h_{2} = a b h_{3} h_{2} = ab (h_{3} h_{2}) \\in (ab) H $$ したがって$(aH) (bH) \\subset (ab) H$であり、このプロセスを逆にすると$(ab) H \\subset (aH) (bH)$であるから $$ (aH) (bH) = (ab) H $$\n$( \\impliedby )$ $gH = Hg$であることを示せばよい。\n$x \\in gH$そして$g^{-1} \\in g^{-1} H$の場合 $$ (xH) (g^{-1} H) = (x g^{-1}) H $$ なので、$h := x g^{-1} \\in H$でなければならない。一方で $x = hg$なので $$ x \\in Hg $$ したがって$gH \\subset Hg$であり、このプロセスを逆にすると$Hg \\subset gH$であるから $$ gH = Hg $$\n■\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p139.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":628,"permalink":"https://freshrimpsushi.github.io/jp/posts/628/","tags":null,"title":"抽象代数学における剰余群"},{"categories":"추상대수","contents":"定義 $G, G'$の単位元$e, e'$と準同型写像$\\phi : G \\to G'$に対して$\\left\\{ e' \\right\\}$の原像$ \\phi^{-1} [ \\left\\{ e' \\right\\} ]$を$\\phi$の核Kernelといい、$\\ker \\phi $と書く。\n定義 [1]: $g \\in G$に対して$g ( \\ker \\phi ) = ( \\ker \\phi ) g$ [2]: $\\ker \\phi \\triangleleft G$ [3]: $\\ker \\phi = \\left\\{ e \\right\\}$ $\\iff$ $\\phi$は単射だ。 [4]: $\\phi$が全射で$\\ker \\phi = \\left\\{ e \\right\\}$ならば、$\\phi$は同型写像である。 説明 定理[3]は必要十分条件だが、特に準同型写像が単射であることを示すのに便利に使われる。線形代数学では、零空間は与えられた方程式に対する解集合としてのアイデンティティが強かった。\n一方、抽象代数学では、少なくとも群論では、$G$が何であれ、正規部分群として「中心を持つこと」の性質が強い。面白いことに、定理[1]では$\\phi$が実際にどのように定義されたか、また$G'$がどのような群であるかさえ気にせず、$G'$は$G$から$\\phi$を受け取るだけで、それ以外は無意味だとされている。\n証明 [3] $( \\implies )$ $\\ker \\phi = \\left\\{ e \\right\\}$ならば、すべての$g \\in G$に対して$\\phi ( \\left\\{ g \\right\\} )$は正確に$\\left\\{ g \\right\\} = g \\left\\{ e \\right\\}$にのみ対応するので、$\\phi$は単射だ。\n$( \\impliedby )$ $\\phi$が単射であり、$\\phi (e) = e'$により、$\\ker \\phi = \\left\\{ e \\right\\}$でなければならない。\n■\n一緒に見る 線形代数学での零空間 ","id":622,"permalink":"https://freshrimpsushi.github.io/jp/posts/622/","tags":null,"title":"抽象代数学における核、カーネル"},{"categories":"정수론","contents":"アルゴリズム 自然数 $a,k,m$ に対して $b \\equiv a^{k} \\pmod{m}$ を次のように計算できる。\nステップ 1. $k$ の2進展開\n$u_{i} = 0$ 又は $u_{i} = 1$ に対して次のように表す。 $$ k = \\sum_{i=0}^{r} u_{i} 2^{i} = u_{0} + 2 u_{1} + \\cdots + 2^r u_{r} $$\nステップ 2.\n$a^{2^{r}} \\equiv ( a^{2^{r-1}} )^2 \\equiv A_{r-1}^2 \\equiv A_{r} \\pmod{m}$ を以下のように計算する。 $$ \\begin{align*} a \u0026amp; \u0026amp; \u0026amp; \\equiv A_{0} \\pmod{m} \\\\ a^{2} \u0026amp; \\equiv ( a^1 )^2 \u0026amp; \\equiv A_{0}^2 \u0026amp; \\equiv A_{1} \\pmod{m} \\\\ a^{4} \u0026amp; \\equiv ( a^2 )^2 \u0026amp; \\equiv A_{1}^2 \u0026amp; \\equiv A_{2} \\pmod{m} \\\\ a^{8} \u0026amp; \\equiv ( a^4 )^2 \u0026amp; \\equiv A_{2}^2 \u0026amp; \\equiv A_{3} \\pmod{m} \\\\ \u0026amp; \u0026amp; \u0026amp; \\vdots \\end{align*} $$\nステップ 3.\n$A_{0}^{u_{0}} \\cdot A_{1}^{u_{1}} \\cdot \\cdots A_{r}^{u_{r}}$ を計算すると次の結果が得られる。 $$ a^{k} \\equiv A_{0}^{u_{0}} \\cdot A_{1}^{u_{1}} \\cdot \\cdots A_{r}^{u_{r}} \\pmod{m} $$\n説明 コンピュータの発展によりべき乗の計算も速くできる時代が来たが、人間は満足を知らない。連続べき乗法はモジュラ演算において、非常に大きな数を直接計算することなくその答えを得る方法だ。一度べき乗するたびに$\\pmod{m}$ で割っていくために、数が$m$ 以下に落ちて、計算量が減るのだ。\n例 例えば、$7^{327} \\pmod{853}$ を計算するとしよう。$7^{327}$ に常用対数を取ると$327 \\log 7 \\approx 327 \\cdot 0.8450 = 276.315$ であり、桁数だけで $277$ の大きな数だ。正直に全部掛けて割り算するには大きすぎるから、上のアルゴリズムを使ってみよう。まず上のアルゴリズム通りに$327$ を2進展開すると以下のようになる。 $$ 327 = 256 + 64 + 4 +2 +1 = 2^{8} + 2^{6} + 2^2 + 2^1 + 2^0 $$ ステップ2通りに計算してみると $$ \\begin{align*} 7 \u0026amp; \u0026amp; \\equiv 7 \u0026amp; \\equiv 7 \\pmod{853} \\\\ 7^{2} \u0026amp; \\equiv ( 7^1 )^2 \u0026amp; \\equiv 7^2 \u0026amp; \\equiv 49 \\pmod{853} \\\\ 7^{4} \u0026amp; \\equiv ( 7^2 )^2 \u0026amp; \\equiv 49^2 \u0026amp; \\equiv 2401 \\equiv 695 \\pmod{853} \\\\ 7^{8} \u0026amp; \\equiv ( 7^4 )^2 \u0026amp; \\equiv 695^2 \u0026amp; \\equiv 227 \\pmod{853} \\\\ 7^{16} \u0026amp; \\equiv ( 7^8 )^2 \u0026amp; \\equiv 227^2 \u0026amp; \\equiv 349 \\pmod{853} \\\\ 7^{32} \u0026amp; \\equiv ( 7^{16} )^2 \u0026amp; \\equiv 349^2 \u0026amp; \\equiv 675 \\pmod{853} \\\\ 7^{64} \u0026amp; \\equiv ( 7^{32} )^2 \u0026amp; \\equiv 675^2 \u0026amp; \\equiv 123 \\pmod{853} \\\\ 7^{128} \u0026amp; \\equiv ( 7^{64} )^2 \u0026amp; \\equiv 123^2 \u0026amp; \\equiv 628 \\pmod{853} \\\\ 7^{256} \u0026amp; \\equiv ( 7^{128} )^2 \u0026amp; \\equiv 628^2 \u0026amp; \\equiv 298 \\pmod{853} \\end{align*} $$ したがって $$ 7^{327} = 7^{256} \\cdot 7^{64} \\cdot 7^{4} \\cdot 7^{2} \\cdot 7^{1} = 298 \\cdot 123 \\cdot 695 \\cdot 49 \\cdot 7 \\equiv 286 \\pmod{853} $$ この方法も大変と感じるかもしれないが、ただ$327$ 回を一つずつ乗算するよりは、とても計算量が少ない。\n証明 $$ \\begin{align*} a^{k} =\u0026amp; a^{u_{0} + 2u_{1} + \\cdots + 2^r u_{r} } \\\\ =\u0026amp; a^{u_{0}} a^{ 2u_{1} } \\cdots a^{ 2^r u_{r} } \\\\ \\equiv \u0026amp; A_{0}^{u_{0}} A_{1}^{u_{1}} \\cdots A_{r}^{u_{r}} \\pmod{m} \\end{align*} $$\n■\nコード 以下は、$7^{327} \\pmod{853}$ を連続べき乗法で計算する例のコードで、R言語で書かれている。注目すべき点は、power オプションは負の整数にも対応し、modが素数ならpower=-1を入力することで、与えられたbaseの乗算に対する逆元を求めることができる。\nFPM\u0026lt;-function(base,power,mod) #It is equal to (base^power)%%mod\r{\ri\u0026lt;-0\rif (power\u0026lt;0) {\rwhile((base*i)%%mod != 1) {i=i+1}\rbase\u0026lt;-i\rpower\u0026lt;-(-power)}\rif (power==0) {return(1)}\rif (power==1) {return(base%%mod)}\rn\u0026lt;-0\rwhile(power\u0026gt;=2^n) {n=n+1}\rA\u0026lt;-rep(1,n)\rA[1]=base\rfor(i in 1:(n-1)) {A[i+1]=(A[i]^2)%%mod}\rfor(i in n:1) {\rif(power\u0026gt;=2^(i-1)) {power=power-2^(i-1)}\relse {A[i]=1} }\rfor(i in 2:n) {A[1]=(A[1]*A[i])%%mod}\rreturn(A[1])\r}\rFPM(7,327,853)\rFPM(7,-1,11) ","id":621,"permalink":"https://freshrimpsushi.github.io/jp/posts/621/","tags":null,"title":"連続二乗法の証明"},{"categories":"위상수학","contents":"定義 1 インデックス集合 $\\mathscr{A}$ に対して、$\\left\\{ X_{\\alpha} \\ | \\ \\alpha \\in \\mathscr{A} \\right\\}$ を位相空間の集合とし、$O_{\\alpha}$ を $X_{\\alpha}$ での開集合とする。\nデカルト積 $\\displaystyle X := \\prod_{\\alpha \\in \\mathscr{A}} X_{ \\alpha}$ において $p_{\\alpha} : X \\to X_{\\alpha}$ を射影と言う。 部分基底 $\\mathscr{S} : = \\left\\{ p_{\\alpha}^{-1} ( O_{\\alpha} ) \\ | \\ O_{\\alpha} \\subset X_{\\alpha} , \\alpha \\in \\mathscr{A} \\right\\}$ によって生成される$X$ の位相を積位相と言う。 基底 $\\displaystyle \\mathscr{B} : = \\left\\{ \\prod_{\\alpha \\in \\mathscr{A}} O_{\\alpha} \\left. \\ \\right| \\ O_{\\alpha} \\subset X_{\\alpha} , \\alpha \\in \\mathscr{A} \\right\\}$ によって生成される$X$ の位相を箱位相と言う。 定理 [1]: $p_{\\alpha}$ は連続関数だ。 [2]: $\\left\\{ X_{\\alpha} \\ | \\ \\alpha \\in \\mathscr{A} \\right\\}$ がハウスドルフ空間の集合なら、$X$ はハウスドルフだ。 [3]: $\\left\\{ X_{\\alpha} \\ | \\ \\alpha \\in \\mathscr{A} \\right\\}$ が連結空間の集合なら、$X$ は連結だ。 [4]: $\\left\\{ X_{\\alpha} \\ | \\ \\alpha \\in \\mathscr{A} \\right\\}$ がコンパクト空間の集合なら、$X$ はコンパクトだ。 $\\mathscr{A} = \\mathbb{N}$ とする。\n[5]: $\\left\\{ X_{\\alpha} \\ | \\ \\alpha \\in \\mathscr{A} \\right\\}$ が可分空間の集合なら、$X$ は可分だ。 [6]: $\\left\\{ X_{\\alpha} \\ | \\ \\alpha \\in \\mathscr{A} \\right\\}$ が第一加算空間の集合なら、$X$ は第一加算だ。 [7]: $\\left\\{ X_{\\alpha} \\ | \\ \\alpha \\in \\mathscr{A} \\right\\}$ が第二加算空間の集合なら、$X$ は第二加算だ。 [8]: $\\mathscr{A}$ が有限集合なら、$X$ の積位相と箱位相は同じだ。 説明 定義で難解な部分基底が登場する理由は、主に交差を取るためであり、基底の定義によっては、合併以外は出てこないからだ。\n部分基底の定義に従って、部分基底$\\mathscr{S}$ によって生成される積位相の基底は $$ \\left\\{ \\left. \\bigcap_{i=1}^{n} p_{\\alpha_{i} }^{-1} ( O_{ \\alpha_{i} } ) \\ \\right| \\ p_{\\alpha_{i} }^{-1} ( O_{ \\alpha_{i} } ) \\in \\mathscr{S} \\right\\} $$ である。自然に、箱位相の基底$\\mathscr{B}$ について $$ \\left\\{ \\left. \\bigcap_{i=1}^{n} p_{\\alpha_{i} }^{-1} ( O_{ \\alpha_{i} } ) \\ \\right| \\ p_{\\alpha_{i} }^{-1} ( O_{ \\alpha_{i} } ) \\in \\mathscr{S} \\right\\} \\subset \\mathscr{B} $$ が成り立つ。箱位相の要素が積位相の部分基底から生成される基底を含むということは、箱位相の要素が積位相の要素よりも同じか多いという意味であり、このために積位相は小さい、粗い、弱いと表現される。\n定理[8]が成り立つことは、意外と稀なケースであるということだ。\u0026lsquo;箱の中に積が入っている\u0026rsquo;と覚えれば混乱しない。有限次元でも可算無限次元でもなく、任意の次元にまで触れるというのは、多少衝撃的だ。\n非専門家が見る位相数学 しかし、位相数学でこのようなデカルト積を考えることは、他のどの分野よりも興味深い。次元に対する一般化であれ、多変量解析であれ、何でもよいが、ようやく一般に知られた位相数学に近づいてきたと感じる。\n$I := [0,1]$ と $S^{1} = \\left\\{ (x,y) \\in \\mathbb{R}^2 \\ | \\ x^2 + y^2 =1 \\right\\}$ について以下の空間を考えよう。\n左から順に、正方形$I \\times I$、円筒$I \\times S^{1}$、トーラス$S^{1} \\times S^{1}$ だ。\n一点コンパクト化から始まり、今や空間がねじれたり、折り曲げられたりする数学になった。\n参照 集合のデカルト積 群のデカルト積 位相空間のデカルト積 Munkres. (2000). Topology(2nd Edition): p113~114.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":620,"permalink":"https://freshrimpsushi.github.io/jp/posts/620/","tags":null,"title":"位相空間のデカルト積"},{"categories":"추상대수","contents":"定義 1 2 群 $G_{1} , \\cdots , G_{n}$ の直積とその要素 $\\displaystyle (a_{1},\\cdots , a_{n}), (b_{1} , \\cdots , b_{n} ) \\in \\prod_{i=1}^{n} G_{i}$ について $$ (a_{1},\\cdots , a_{n}) (b_{1} , \\cdots , b_{n} ) = (a_{1} b_{1},\\cdots , a_{n} b_{n}) $$ これを$G_{1} , \\cdots , G_{n}$の直積Direct Productと言う。 特に $G_{1}, \\cdots , G_{n}$ が可換群の場合、$\\displaystyle \\bigoplus_{i=1}^{n} G_{i}$ と書き、直和Direct Sumとも呼ぶ。 $G_{1}$ が $G$ の部分群だとする時、次を満たす$G$の別の部分群$G_{2}$が存在すれば、$G_{1}$を直和因子Direct Summandと呼ぶ。 $$ G = G_{1} \\oplus G_{2} $$ 性質 $G = G_{1} \\oplus G_{2}$ とする。もし$H_{1}$ が $G_{1}$ の部分群で、$H_{2}$ が $G_{2}$ の部分群なら、$H_{1}$ と $H_{2}$ も直和として表せ、特に以下が成り立つ。 $$ {{ G } \\over { H_{1} \\oplus H_{2} }} \\simeq {{ G_{1} } \\over { H_{1} }} \\oplus {{ G_{2} } \\over { H_{2} }} $$\n[1]: $H_{1} \\simeq G_{1}$ で $H_{2} \\simeq \\left\\{ 0 \\right\\}$ とすると $$ G / G_{1} \\simeq G_{2} $$ [2]: $H_{1} \\simeq \\left\\{ 0 \\right\\}$ とすると $$ {{ G } \\over { H_{2} }} \\simeq G_{1} \\oplus {{ G_{2} } \\over { H_{2} }} $$ 説明 ベクトル空間は加法に関して群だが、群はベクトル空間ではないため、線形代数学の直和と完全に一致するわけではないが、比較が何らかの意味を持つためには少なくとも環くらいにはなっている必要がある。\n例えば、クラインの四元群は$V \\simeq \\mathbb{Z}_{2} \\times \\mathbb{Z}_{2}$を満たし、$\\gcd (m , n) = 1$ の場合、$\\mathbb{Z}_{m} \\times \\mathbb{Z}_{n} \\simeq \\mathbb{Z}_{mn}$が巡回群であるという定理が知られている。\n自由群 記法上は、自由アーベル群については、単に整数環 $\\mathbb{Z}$の直和と同型であると表現する方が便利だ。例えば $G$ がランク $3$ の自由アーベル群なら、$G$ は次のようにも表せる。 $$ G \\simeq \\mathbb{Z} \\oplus \\mathbb{Z} \\oplus \\mathbb{Z} $$\n参照 線形代数学における直和 集合の直積 群の直積 位相空間の直積 Fraleigh. (2003). 「抽象代数入門」(第7版): p104~105.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMunkres. (1984). 「代数的トポロジーの要素」: p23~24.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":619,"permalink":"https://freshrimpsushi.github.io/jp/posts/619/","tags":null,"title":"群のデカルト積"},{"categories":"르벡공간","contents":"証明 $p=1$の場合は、積分の性質によって自明に成立する。\n$$ \\int_{\\Omega} \\left| u(x) + v(x) \\right| dx \\le \\int_{\\Omega} \\left| u(x) \\right| dx + \\int_{\\Omega} \\left| v(x) \\right| dx $$\n$1 \\lt p \\lt \\infty$とする。$w$を$w \\ge 0$であり、$\\left\\| w \\right\\|_{p^{\\prime}} \\le 1$の関数とする。\nヘルダーの不等式\n$\\dfrac{1}{p} + \\dfrac{1}{p^{\\prime}} = 1$に関する$1 \\le p, p^{\\prime} \u0026lt; \\infty$で、$u \\in L^p(\\Omega)$と$v\\in L^{p^{\\prime}}(\\Omega)$ならば\n$$ \\int_{\\Omega} |u(x)v(x)| dx \\le \\| u \\|_{p} \\| v \\|_{p^{\\prime}} $$\nすると、ヘルダーの不等式により以下が成立する。\n$$ \\begin{align*} \\int_{\\Omega} \\left| u(x) + v(x) \\right| w(x) dx \\le \u0026amp; \\int_{\\Omega} \\left| u(x) \\right| w(x) dx + \\int_{\\Omega} \\left| v(x) \\right| w(x) dx \\\\ \\le \u0026amp; \\left\\| u \\right\\|_{p} \\left\\| w \\right\\|_{p^{\\prime}} + \\left\\| v \\right\\|_{p} \\left\\| w \\right\\|_{p^{\\prime}} \\\\ \\le \u0026amp; \\left\\| u \\right\\|_{p} + \\left\\| v \\right\\|_{p} \\end{align*} $$\n従って、以下の式が成立する。\n$$ \\sup \\left\\{ \\int_{\\Omega} \\left| u(x) + v(x) \\right| w(x) dx : v(x) \\ge 0 \\text{ on } \\Omega, \\left\\| w \\right\\|_{p^{\\prime}} \\le 1 \\right\\} \\lt \\left\\| u \\right\\|_{p} + \\left\\| v \\right\\|_{p} $$\nヘルダーの不等式の逆: $L^{p}$関数の十分条件\n$\\sup \\left\\{ \\int_{\\Omega} \\left| u(x) \\right| v(x) dx : v(x) \\ge 0 \\text{ on } \\Omega, \\left\\| v \\right\\|_{p^{\\prime}} \\le 1 \\right\\} \\lt \\infty$が成立すれば、 $$ u\\in L^{p}(\\Omega) \\quad \\text{and} \\quad \\left\\| u \\right\\|_{p} = \\sup \\left\\{ \\int_{\\Omega} \\left| u(x) \\right| v(x) dx : v(x) \\ge 0 \\text{ on } \\Omega, \\left\\| v \\right\\|_{p^{\\prime}} \\le 1 \\right\\} $$\n上記の定理により、\n$$ \\left\\| u + v \\right\\|_{p} \\lt \\left\\| u \\right\\|_{p} + \\left\\| v \\right\\|_{p} $$\n■\n参照 ユークリッド空間におけるミンコフスキーの不等式 ","id":614,"permalink":"https://freshrimpsushi.github.io/jp/posts/614/","tags":null,"title":"ルベーグ空間におけるミンコフスキーの不等式の証明"},{"categories":"열물리학","contents":"定義1 2 エネルギーが$E$の系があるとしよう。$E$に関する微視状態の数を$\\Omega (E) = \\Omega$とするとき、\n$$ \\dfrac{1}{k_{B} T} := \\dfrac{d \\ln ( \\Omega )}{d E } $$\n$T$を系の温度temperatureと定義する。（ただし、$k_{B}$はボルツマン定数）\n微視状態と巨視状態 統計力学で、ある系の巨視状態Macrostateと微視状態Microstateは、例えば次のような概念だ。箱の中にコイン四枚が入っているとする。この箱を強く振って開けると、表と裏がランダムに決まる。この時のコインの状態を表を白色、裏を濃灰色で表現すると、次のようになる。\n表の数だけを見るなら$0$個から$4$個までの合計$5$通りがあり、これを巨視状態の数 $S$という。一方、各コインが表か裏かまで数えると、知られているように$2^4=16$通りあり、これを微視状態の数 $\\Omega$という。\n当然ながら、微視状態の数$\\Omega$が大きければ、そのに対応する巨視状態が観測される確率が高い。上の状況で、コイン$n$個の中で$k$個が表である微視状態の数を$\\Omega (k, n-k)$とすると、微視状態の数が最も多いのは$\\Omega (2,2) = 6$で、したがって、表と裏がそれぞれ二枚ずつの場合が観測されやすい。\n導出 温度の定義は、相互作用する二つの系の巨視状態を探す過程で自然に導かれる。以下のような閉じた系$X$を考えよう。\n$X$は$A$と$B$に分かれている。$A$、$B$内部のエネルギーをそれぞれ$E_{A}$、$E_{B}$としよう。上のコインの例で言うと、$A$と$B$は特定のコインの集まりで、$E_{A}$と$E_{B}$はそれぞれ表のコインの数だ。\nすべての微視状態が起こりうる確率が同じで、$A$と$B$が十分に相互作用した（または時間が十分に流れた）と仮定して、二つの系が熱平衡状態にあるとする。全系のエネルギーは$E_{X} = E_{A} + E_{B}$と同じである。全系$X$の微視状態の数は、$A$が可能な微視状態の数$\\Omega (E_{A})$と$B$が可能な微視状態の数$\\Omega (E_{B})$の積で表される。\n$$ \\begin{equation} \\Omega_{X} (E_{X}) = \\Omega_{A} (E_{A}) \\Omega_{B} (E_{B}) \\end{equation} $$\nすると、熱平衡状態での巨視状態は、上の式の値が最も大きい時と自然に受け入れられる。実際、熱平衡時の巨視状態で可能な微視状態の数は、他の場合より圧倒的に多いと言われている。微視状態の数$\\Omega$を正規分布と考えれば、$(1)$を微分して$0$になる点が最大値であることを自然に受け入れられるだろう。\nしかし、実際には、粒子のエネルギーは連続した値ではなく量子化されている。したがって、系全体のエネルギー$E_{X}$も離散的な値を持つ。しかし、熱物理学の場合、扱う系の粒子の数が非常に多いので、可能な$E_{X}$の値も非常に多い。したがって、$E_{X}$、$E_{A}$、$E_{B}$を連続した値を持つ変数と考えよう。\n再び巨視状態を探すことに戻って、熱平衡時の巨視状態（エネルギー）を$\\overline{E} = \\overline{E}_{A} + \\overline{E}_{B}$としよう。すると、$(1)$を$E_{A}$で微分して$E_{A}=\\overline{E}_{A}$を代入すると、$0$になるということだ。\n$$ \\left. \\dfrac{d( \\Omega_{A} (E_{A} ) \\Omega_{B} (E_{B}) )}{dE_{A}} \\right|_{E_{A}=\\overline{E}_{A}} = 0 $$\n上の式を計算すると、積の微分法によって次のようになる。\n$$ \\Omega_{B} (E_{B}) \\left. \\dfrac{d \\Omega_{A} (E_{A} )}{d E_{A}} \\right|_{E_{A}=\\overline{E}_{A}} + \\Omega_{A} (E_{A}) \\left. \\dfrac{d \\Omega_{B} (E_{B} )}{d E_{B}} {{d E_{B} } \\over {d E_{A} }} \\right|_{E_{A}=\\overline{E}_{A}} = 0 $$\nここで、$A$と$B$の間でどのようにエネルギーが移動しても、全体のエネルギー$E_{X} = E_{A} + E_{B}$は変わらない定数であるため、次が成立する。\n$$ d E_{A} = - d E_{B} \\implies \\dfrac{d E_{B}}{d E_{A} } = -1 $$\nこれを上の式に代入すると、次の式を得る。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\Omega_{B} \\left. \\dfrac{ d \\Omega_{A} }{d E_{A}}\\right|_{E_{A}=\\overline{E}_{A}} - \\left. \\Omega_{A} \\dfrac{ d \\Omega_{B} }{d E_{B}}\\right|_{E_{B}=\\overline{E}_{B}} =\u0026amp; 0 \\\\ \\implies \u0026amp;\u0026amp; \\dfrac{1}{ \\Omega_{A} } \\left. \\dfrac{ d \\Omega_{A} }{d E_{A}}\\right|_{E_{A}=\\overline{E}_{A}} - \\dfrac{1}{\\Omega_{B} } \\left. \\dfrac{ d \\Omega_{B} }{d E_{B}} \\right|_{E_{B}=\\overline{E}_{B}} =\u0026amp; 0 \\\\ \\implies \u0026amp;\u0026amp; \\dfrac{1}{ \\Omega_{A} } \\left. \\dfrac{ d \\Omega_{A} }{d E_{A}} \\right|_{E_{A}=\\overline{E}_{A}} =\u0026amp; \\dfrac{1}{\\Omega_{B} } \\left. \\dfrac{ d \\Omega_{B} }{d E_{B}} \\right|_{E_{B}=\\overline{E}_{B}} \\\\ \\implies \u0026amp;\u0026amp; \\dfrac{ d \\ln \\Omega_{A} }{d E_{A}} \\left(\\overline{E}_{A}\\right) =\u0026amp; \\dfrac{ d \\ln \\Omega_{B} }{d E_{B}}\\left(\\overline{E}_{B}\\right) \\end{align*} $$\n最後の行は、対数関数の微分法と連鎖律により成立する。ここで、上の式は熱平衡の条件で、左辺は系$A$の変数だけで構成された値で、右辺は系$B$の変数だけで構成された値だ。熱平衡状態で両側がそれぞれの状態だけで同じ値を持つので、この値で温度を定義すれば妥当だろう。すると、$A$と$B$の温度をそれぞれ$T_{A}$と$T_{B}$と定義できる。\n$$ \\begin{align*} \\dfrac{1}{k_{B} T_{A} } \u0026amp;:= \\dfrac{ d \\ln \\Omega _{A} }{d E_{A}} \\left(\\overline{E}_{A}\\right) \\\\ \\dfrac{1}{k_{B} T_{B} } \u0026amp;:= \\dfrac{ d \\ln \\Omega _{B} }{d E_{B}} \\left(\\overline{E}_{B}\\right) \\end{align*} $$\n■\nStephen J. Blundell and Katherine M. Blundell, 熱物理学 (Concepts in Thermal Physics、이재우 による翻訳) (2版, 2014), p45-49\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. K. Pathria and Paul D. Beale, 統計力学 (3版, 2011), p1-5\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":613,"permalink":"https://freshrimpsushi.github.io/jp/posts/613/","tags":null,"title":"物理学における温度の定義"},{"categories":"르벡공간","contents":"要約1 $\\Omega \\subset \\mathbb{R}^{n}$を開集合としよう。次の式を満たす二つの定数$1 \\lt p \\lt \\infty, 1 \\lt p^{\\prime} \\lt \\infty$が与えられたとする。\n$$ \\dfrac{1}{p}+\\dfrac{1}{p^{\\prime}} = 1 \\left(\\text{or } p^{\\prime} = \\frac{p}{p-1} \\right) $$\nもし$u \\in L^p(\\Omega)$と$v\\in L^{p^{\\prime}}(\\Omega)$ならば$uv \\in L^1(\\Omega)$であり、下記の不等式が成り立つ。\n$$ \\| uv \\|_{1} = \\int_{\\Omega} |u(x)v(x)| dx \\le \\| u \\|_{p} \\| v \\|_{p^{\\prime}} $$\nこの不等式をヘルダーの不等式ヘルダーの不等式という。\n説明 $p^{\\prime}$は$p$のヘルダー共役ヘルダー共役または共役指数共役指数と呼ばれる。$q$と表記することが多い。\n$| u(x) |^{p}$と$| v(x) |^{p^{\\prime}}$が$\\Omega$においてほとんど至る所で比例関係にある場合、等式が成り立つ。\n本質的にユークリッド空間におけるヘルダーの不等式と同じで、$p=p^{\\prime}=2$の時にコーシー・シュワルツの不等式になるのも同様だ。証明自体はコーシー・シュワルツの不等式の証明と同じで、ヤングの不等式が追加されただけである。\n次のような形での一般化も可能である。\n$$ \\| uv \\|_{r} = \\left( \\int_{\\Omega} |u(x)v(x)|^{r} dx \\right)^{1/r} \\le \\| u \\|_{p} \\| v\\|_{p^{\\prime}} $$\n$$ \\| u \\|_{r} = \\left( \\int_{\\Omega} |u(x)|^{r} dx \\right)^{1/r} \\le \\prod_{j=1}^{N} \\| u_{j} \\|_{{p}_j} = \\| u_{1} \\|_{{p}_1} \\cdots \\| u_{N} \\|_{p_{N}} $$\n証明 ヤングの不等式\n$\\dfrac{1}{p} + \\dfrac{1}{p^{\\prime}} = 1$を満たし、1より大きい二つの定数$p, p^{\\prime}$と二つの正の数$a,b$に対して\n$$ ab \\le { {a^{p}} \\over {p} } + {{b^{p^{\\prime}}} \\over {p^{\\prime}}} $$\nケース1. $\\| u \\|_{p} = 0$または$\\| v \\|_{p^{\\prime}} = 0$\n$\\Omega$のほとんど至る所で$u(x) = 0$であるか、$\\Omega$のほとんど至る所で$v(x) = 0$であるため、$\\Omega$のほとんど至る所で$u(x)v(x) = 0$である。したがって\n$$ \\left| \\int_{\\Omega} u(x) v(x) dx \\right| = \\| uv \\|_{1} = 0 $$\nそして\n$$ \\| u \\|_{p} \\| v \\|_{p^{\\prime}} = 0 $$\nとなり、不等式が成り立つ。\nケース2. その他の場合\nヤングの不等式に$a = \\dfrac{\\left| u(x) \\right|}{\\| u \\|_{p}}$と$b = \\dfrac{\\left| v(x) \\right|}{\\| v \\|_{p^{\\prime}}}$を代入する。すると\n$$ \\dfrac{\\left| u(x) \\right|}{\\| u \\|_{p}} \\dfrac{\\left| v(x) \\right|}{\\| v \\|_{p^{\\prime}}} \\le \\dfrac{ \\left| u(x) \\right|^{p}}{ p \\| u \\|_{p}^{p}} + \\dfrac{\\left| v(x) \\right|^{p^{\\prime}}}{ p^{\\prime} \\| v \\|_{p^{\\prime}}^{p^{\\prime}}} $$\n両辺を積分すると以下のようになる。\n$$ \\begin{align*} \\dfrac{1}{\\| u \\|_{p} \\| v \\|_{p^{\\prime}}} \\int_{\\Omega}\\left| u(x)v(x) \\right| dx \\le \u0026amp; \\dfrac{1}{p \\| u \\|_{p}^{p}} \\int_{\\Omega} \\left| u(x) \\right|^{p} dx + \\dfrac{1}{ p^{\\prime} \\| v \\|_{p^{\\prime}}^{p^{\\prime}}} \\int_{\\Omega} \\left| v(x) \\right|^{p^{\\prime}} dx \\\\ \\le \u0026amp; \\dfrac{1}{p \\| u \\|_{p}^{p}} \\| u \\|_{p}^{p} + \\dfrac{1}{ p^{\\prime} \\| v \\|_{p^{\\prime}}^{p^{\\prime}}} \\| v \\|_{p^{\\prime}}^{p^{\\prime}} \\\\ \\le \u0026amp; \\dfrac{1}{p} + \\dfrac{1}{ p^{\\prime} } \\\\ =\u0026amp; 1 \\end{align*} $$\n左辺の定数を移行すると\n$$ \\| uv \\|_{1} = \\int_{\\Omega} |u(x)v(x)| dx \\le \\| u \\|_{p} \\| v \\|_{p^{\\prime}} $$\nよって$uv \\in L^{1}(\\Omega)$であり、不等式が成り立つ。\n■\n関連項目 ユークリッド空間におけるヘルダーの不等式 一般化されたヘルダーの不等式 Robert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003), p24-25\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":609,"permalink":"https://freshrimpsushi.github.io/jp/posts/609/","tags":null,"title":"ルベーグ空間におけるヘルダーの不等式の証明"},{"categories":"열물리학","contents":"数式 以下の方程式を スターリングの公式Stirling's formulaと呼ぶ。\n$$ \\lim_{n \\to \\infty} {{n!} \\over {e^{n \\ln n - n} \\sqrt{ 2 \\pi n} }} = 1 $$\n説明1 この近似は、大きな数に対する階乗の計算という観点で便利である。熱力学、統計力学のような分野では多くの数の分子を仮定するために必須であり、\n$$ \\begin{align*} n! \u0026amp;\\approx \\sqrt{2\\pi n}\\left( \\dfrac{n}{e} \\right)^{n} \\\\[0.6em] \\log_{2} n! \u0026amp;\\approx n \\log_{2} n - n \\log_{2}e \\\\[0.6em] \\ln n! \u0026amp;\\approx n \\ln n - n \\end{align*} $$\nというように、さらに簡略化された表現も使用される。以下の証明は解析的にやや厳密性が落ちるが、応用のために事実だけを重要視するならば十分である。\n参照 数理統計的証明 厳密な証明 導出 2 ガンマ関数 $\\displaystyle n! = \\int_{0}^{\\infty} x^{n} e^{-x} dx$ から始めよう。$f(x) = n \\ln x - x$ と置けば、次の式が成り立つ。\n$$ e^{f(x)} = x^{n} e^{-x} $$\n$\\displaystyle {{df(x)} \\over {dx}} = {{n } \\over {x }} - 1$ かつ $\\displaystyle {{d^2 f(x)} \\over {dx^2 }} = - {{n } \\over {x^2 }}$、そして $\\displaystyle {{d^3 f(x)} \\over {dx^3 }} = {{ 2n } \\over {x^3 }}$ であるため、$f$ のテイラー展開は次のようになる。\n$$ \\begin{align*} f(x) =\u0026amp; f(n) + f '(n) (x-n) + {{1} \\over {2!}} f ''(n) (x-n)^2 + {{1} \\over {3!}} f^{(3)} (n) (x-n)^3 + \\cdots \\\\ =\u0026amp; n \\ln n - n + 0 \\cdot (x-n) - {{1} \\over {2}} {{n} \\over{ n^2}} (x-n)^2 + {{1} \\over {6}} {{2n} \\over{ n^3}} (x-n)^3 + \\cdots \\\\ =\u0026amp; n \\ln n - n - {{ (x-n)^2 } \\over{ 2n }} + {{ (x-n)^3 } \\over{ 3 n^2 }} + \\cdots \\end{align*} $$\n$n$ が十分に大きい場合、$\\displaystyle {{ (x-n)^2 } \\over{ 2 n }}$ 以降の項は分母の増加速度が速すぎて無視できる。したがって、以下の結果を得る。\n$$ f(x) \\approx n \\ln n - n - {{ (x-n)^2 } \\over{ 2n }} $$\nさらに、$n$ が十分に大きければ、$0$ から $\\infty$ までのガウス曲線の積分と、$-\\infty$ から $\\infty$ までのものに大きな違いはない。結果として、十分に大きな $n$ に対して $$ n! = e^{n \\ln n - n} \\int_{0}^{\\infty} e^{-(x-n)^2 / 2n + \\cdots } dx \\approx e^{n \\ln n - n} \\int_{-\\infty}^{\\infty} e^{-(x-n)^2 / 2n } dx $$ ガウスの積分により $\\displaystyle \\int_{-\\infty}^{\\infty} e^{-(x-n)^2 / 2n } dx = \\sqrt{ 2 \\pi n}$ が成り立つため、次の式が成立する。\n$$ n! \\approx e^{n \\ln n - n} \\sqrt{ 2 \\pi n} $$\n両辺に対数を取ると、以下の式を得る。\n$$ \\ln n! \\approx n \\ln n - n + \\dfrac{1}{2}\\ln 2\\pi n $$\n$n$ が非常に大きければ、次の近似式が成り立つ。\n$$ \\ln n! \\approx n \\ln n - n $$\n■\nStephen J. Blundell and Katherine M. Blundell, 열 물리학(Concepts in Thermal Physics, 이재우 역) (2nd Edition, 2014), p12\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStephen J. Blundell and Katherine M. Blundell, 열 물리학(Concepts in Thermal Physics, 이재우 역) (2nd Edition, 2014), p591-593\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":608,"permalink":"https://freshrimpsushi.github.io/jp/posts/608/","tags":null,"title":"スターリングの公式の簡単な導出"},{"categories":"르벡공간","contents":"定義1 2 3 $\\Omega \\subset \\mathbb{R}^{n}$を開集合、$p$を正の実数としよう。\n$\\Omega$上で定義された全ての可測関数 $f$に対して、集合 $L^{p}(\\Omega)$を以下のように定義する。\n$$ L^{p}(\\Omega) := \\left\\{ f : \\int_{\\Omega} \\left| f(x) \\right|^{p} dx \u0026lt; \\infty \\right\\} $$\nこれをLp空間あるいはルベーグ空間と呼び、簡単に$L^{p}$などと表記することもある。通常、関数解析の教科書では上記のように記述され、測度論、実解析の教科書では次のように記述される。\n測度空間 $(X, \\mathcal{E}, \\mu)$が与えられたとする。$X$上で定義された可測関数 $f$に対して、集合 $L^{p}(X, \\mathcal{E},\\mu)$を次のように定義する。\n$$ L^{p}(X, \\mathcal{E}, \\mu) := \\left\\{ f : \\int \\left| f \\right|^{p} d \\mu \u0026lt; \\infty \\right\\} $$\nここで$\\mu$は測度である。簡単に$L^{p}(\\mu), L^{p}(X)$などと表記される。\n性質 $L^{p}$はベクトル空間である。 $1 \\le p \\le \\infty$に対して$L^{p}$はノルム空間である。 $L^{p}$は完備空間である。 $E\\subset X$に対して、$1 \\le p \\le q \\le \\infty$かつ$\\mu (E) \u0026lt; \\infty \\implies L^{q} (E) \\subset L^{p} (E)$である。 説明 2. $p \\lt 1$の場合、$\\left\\| \\cdot \\right\\|_{p}$は三角不等式を満たさず、ノルムにならない。しかし、$p = \\infty$の場合、$L^{p}$空間はノルム空間になる。\n完備なノルムベクトル空間を特にバナッハ空間と呼ぶ。したがって、$L^{p}$空間はバナッハ空間である。$L^{p}$は、ヘルダーの不等式およびミンコフスキーの不等式が成立する空間として特に重要である。\n内積が定義されたベクトル空間を内積空間と言う。完備な内積空間を特にヒルベルト空間と言う。$L^{2}$空間の場合は、次のように内積を定義することができる。\n$$ \\left( \\int |f(x)|^2 dx\\right)^{\\frac{1}{2}} = \\left( \\int f(x)\\overline{f(x)}dx \\right) ^{\\frac{1}{2}} = \\langle f,f \\rangle ^{\\frac{1}{2}} $$\nしたがって、$L^{2}$空間はヒルベルト空間である。\n4. $\\mu (E) \u0026lt; \\infty$という条件に注目しよう。もし積分範囲が有界でない場合、$L^{1} (E)$と$L^{2} (E)$はいかなる包含関係も持たなくなる。$1 \\le p \\lt q \\lt r$が特定の条件を満たす場合、${u \\in L^{p} \\cap L^{r} \\implies u \\in L^{q}}$が成り立つこともある。\n証明 2. $1\\le p \u0026lt;\\infty$に対して$\\| \\cdot \\|_{p}$を次のように定義する。\n$$ \\left\\| f \\right\\|_{p} := \\left( \\int_{\\Omega} \\left| f(x) \\right|^{p} dx \\right)^{1/p},\\quad f\\in L^{p}(\\Omega) $$\nすると、$\\| \\cdot \\|_{p}$は$L^{p}$空間のノルムになる（$0\u0026lt;p\u0026lt;1$の時はノルムにならない）。$\\| f \\|_{p} \\ge 0$であることは自明であり、$\\| f \\|_{p}=0 \\iff f=0$であることも自明だ。また、$c \\in \\mathbb{C}$に対して$\\| cf \\|_{p} = \\left| c \\right| \\left\\| f \\right\\|_{p}$が成立することも以下のように示すことができる。\n$$ \\begin{align*} \\left\\| cf \\right\\|_{p} =\u0026amp; \\left( \\int_{\\Omega} \\left| cf(x) \\right|^{p} dx \\right)^{1/p} \\\\ =\u0026amp; \\left( \\left| c \\right|^{p} \\int_{\\Omega} \\left| f(x) \\right|^{p} dx \\right)^{1/p} \\\\ =\u0026amp; \\left| c \\right| \\left( \\int_{\\Omega} \\left| f(x) \\right|^{p} dx \\right)^{1/p} \\\\ =\u0026amp; \\left| c \\right| \\left\\| f \\right\\|_{p} \\end{align*} $$\n$f,g \\in L^{p}$に対して、$\\left\\| f + g \\right\\|_{p} \\le \\| f \\|_{p} + \\| g \\|_{p}$も同様に成立し、これはミンコフスキーの不等式と呼ばれている。\n■\n3. 戦略：ほとんど全てがファトゥの補題によって解決される。\n与えられたコーシー数列 $f_{n}$ に対して、$\\left\\| f_{n} - f_{n_{k}} \\right\\|_{p} \u0026lt; \\dfrac{1}{2^{k}}$を満たす部分数列 $f_{n_{k}}$ を見つけることができる。全ての $k \\in \\mathbb{N}$ に対して\n$$ \\begin{align*} g_{k} :=\u0026amp; \\sum_{i=1}^{k} \\left| f_{n_{i+1}} - f_{n_{i}} \\right| \\\\ g :=\u0026amp; \\lim_{k \\to \\infty} g_{k} = \\sum_{i=1}^{\\infty} \\left| f_{n_{i+1}} - f_{n_{i}} \\right| \\end{align*} $$\nを定義すると、三角不等式によって\n$$ \\left\\| g_{k} \\right\\|_{p} \\le \\sum_{i}^{k} \\dfrac{1}{2^{i}} \u0026lt; 1 $$\nファトゥの補題\n関数値が非負の可測関数の数列 $\\left\\{ f_{n} \\right\\}$ に対して\n$$ \\int \\left( \\liminf_{n \\to \\infty} f_{n} \\right) d \\mu \\le \\liminf_{n \\to \\infty} \\int f_{n} d \\mu $$\nファトゥの補題により\n$$ \\left\\| g \\right\\|_{p}^{p} \\le \\int \\lim_{n \\to \\infty} g_{k}^{p} d \\mu \\le \\liminf_{k \\to \\infty} \\int g_{k}^{p} d \\mu \\le 1 $$\n$g$がほとんど至る所で有限であるため、\n$$ f_{n_{k}} = f_{n_{1}}(x) + \\sum_{i=1}^{ k } \\left[ f_{n_{i}} (x) - f_{n_{i-1}} (x) \\right] $$\nはほとんど至る所で収束する。$f := \\lim\\limits_{k \\to \\infty} f_{n_{k}}$と定義すると、ファトゥの補題により\n$$ \\left\\| f - f_{m} \\right\\|_{p} = \\int |f - f_{m}|^{p} d \\mu \\le \\liminf_{k \\to \\infty} \\int | f_{n_{k}} - f_{m}|^{p} d \\mu \\le \\varepsilon^{p} $$\nしたがって$f - f_{m} \\in L^{p}$であり、$f = f_{m} + (f - f_{m} ) \\in L^{p}$である。$L^{p}$の全てのコーシー数列が$L^{p}$の元に収束するため、$L^{p}$は完備空間である。\n■\n4. 戦略：$|f(x)|^{p} \\le 1 + |f(x)|^{q}$という不等式を示せば、残りはルベーグ積分の性質によって証明が終わる。\n$f \\in L^{q}$としよう。すると、次の式が成り立つ。\n$$ \\begin{align*} | f(x) | \\le 1 \\implies\u0026amp; |f(x) |^{p} \\le 1 \\\\ 1 \\le |f(x)| \\implies\u0026amp; |f(x)|^{p} \\le |f(x)|^{q} \\end{align*} $$\nしたがって、$| f(x) |$が$1$より大きいか小さいかにかかわらず、次が成り立つ。\n$$ |f(x)|^{p} \\le 1 + |f(x)|^{q} $$\nルベーグ積分 $\\displaystyle \\int_{E} d \\mu$ をとると、以下のようになる。\n$$ \\int_{E} |f|^{p} d \\mu \\le \\int_{E} 1 d \\mu + \\int_{E} |f|^{q} d \\mu = m(E) + \\int_{E} |f|^{q} d \\mu \u0026lt; \\infty $$\n$m(E) \u0026lt; \\infty$であり$\\displaystyle \\int_{E} |f|^{q} d \\mu \u0026lt; \\infty$であるため、次が成り立つ。\n$$ \\int_{E} |f|^{p} d \\mu \u0026lt; \\infty $$\n言い換えれば、$f \\in L^{q} \\implies f \\in L^{p}$であるため、\n$$ L^{q} (E) \\subset L^{p} (E) $$\n■\n参照 $L^{1}$空間 $L^{2}$空間 ヒルベルト空間 Capinski, Measure, Integral and Probability (1999), p140\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRobert A. Adams and John J. F. Foutnier, Sobolev Space (2nd Edition, 2003), p23\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGerald B. Folland, Real Analysis: Modern Techniques and Their Applications (2nd Edition, 1999), p181\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":605,"permalink":"https://freshrimpsushi.github.io/jp/posts/605/","tags":null,"title":"Lp空間、ルベーグ空間"},{"categories":"열물리학","contents":"式1 気体の分子数を$N$、体積を$V$、圧力を$p$、絶対温度を$T$とする。すると、次の式が成立し、これを理想気体方程式ideal gas equationと呼ぶ。\n$$ pV = N k_{B} T $$\nこの時、$k_{B} = 1.3807 \\times 10^{-23} J / K$をボルツマン定数Boltzmann constantと呼ぶ。\n説明 歴史的に見ると、実験法則から導かれ、後に気体運動論から数式的に導かれた。\u0026lsquo;理想気体\u0026rsquo;方程式と呼ぶ理由は、式を導く過程で以下のような仮定が使われたからである。\n各分子の間には何の力も働かない。つまり、互いに引き合わない。 各分子は大きさのない点粒子である。 現実には、分子は互いに相互作用をし、大きさがあるが、理論の単純さのために上記のように仮定される。地表で相対性理論を考慮せずにニュートン力学だけを使っても多くの現象をよく説明できるように、理想気体方程式も実際の気体をよく説明する。実際の気体は、系の分子量が少なく、温度が高く、圧力が低いほど理想気体に近づく。\n理想気体方程式がすべての気体現象を説明するわけではない。相対論的効果を考慮する必要がある時は、相対論的気体モデルを、量子効果を考慮する必要がある時は、量子気体モデルを使用しなければならない。\n理想気体方程式の定数を物質量$n$に対して表示すると$pV = nRT$の形で表すことができる。この時、$R$を気体定数と呼ぶが、熱力学ではほぼ半々で使われる。\n導出 $$ p \\propto \\dfrac{1}{V} $$\n一定の温度で、気体の圧力と体積に対して上記のような関係が成立し、これをボイルの法則Boyle\u0026rsquo;s lawと言う。後にボイルとは無関係にエドメ・マリオットEdme Mariotteも同じ事実を発見し、ボイル-マリオット法則とも言う。\n$$ V \\propto T $$\n一定の圧力で、気体の体積と温度に対して上記のような関係が成立し、これをシャルルの法則Charles\u0026rsquo; lawと言う。\n$$ p \\propto T $$\n気体の体積が一定の時、温度と圧力に対して上記のような関係が成立し、これをゲイ-リュサックの法則と言う。上の三つの比例式から、次の式を得る。\n$$ p^{2}V \\propto T^{2}/V \\implies p^{2}V^{2} \\propto T^{2} \\implies pV \\propto T $$\n比例定数を$Nk_{B}$とすると、以下の結果を得る。\n$$ pV = Nk_{B}T $$\nStephen J. Blundell and Katherine M. Blundell, 熱物理学（Concepts in Thermal Physics, イ・ジェウ訳）（第2版、2014年）、p8-10\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":602,"permalink":"https://freshrimpsushi.github.io/jp/posts/602/","tags":null,"title":"理想気体の方程式"},{"categories":"르벡공간","contents":"ビルドアップ 内積の一般的な定義は次のようである。\n$H$をベクトル空間とする。$x,y,z \\in H$と$\\alpha, \\beta \\in \\mathbb{C}$に対して、次の条件を満たす関数\n$$ \\langle \\cdot , \\cdot \\rangle \\ : \\ H \\times H \\to \\mathbb{C} $$\nを内積と定義し、$\\left( H, \\langle \\cdot ,\\cdot \\rangle \\right)$を内積空間という。\n線形性: $\\langle \\alpha x + \\beta y ,z \\rangle =\\alpha \\langle x,z\\rangle + \\beta \\langle y,z\\rangle$ 共役対称性: $\\langle x,y \\rangle = \\overline{ \\langle y,x \\rangle}$ 正定値性: $\\langle x,x \\rangle \\ge 0 \\quad \\text{and} \\quad \\langle x,x \\rangle = 0\\iff x=0$ 特に関数空間での内積は次のように定積分を使って定義される。\n$$ \\langle f, g \\rangle := \\int_{a}^{b} f(x) g(x) dx $$\nこれで$\\langle , \\rangle$が内積になることは簡単に示せるが、なぜこんなに定義するのかは理解し難い。それぞれの要素を掛け合わせて足すというユークリッド空間での内積の定義からあまりに遠く、全然実用的にも見えない。しかし、これらの定義は自然であり、関数解析を学ぶにつれて美しくはまっていく。\n例示 例で理解しよう：\n二つのベクトル$\\mathbf{f} = ( {\\color{blue} 1} , {\\color{orange} 5} , 0 , {\\color{purple} 4} , {\\color{red} 2} , {\\color{Green} 1} )$と$\\mathbf{g} = ( {\\color{blue} 9} , {\\color{orange} 6} , 0 , {\\color{purple} 1} , {\\color{red} 2} , {\\color{Green} 5} )$を考える。内積を計算すると\n$$ \\mathbf{f} \\cdot \\mathbf{g} = {\\color{blue} 1 \\cdot 9 } + {\\color{orange} 5 \\cdot 6} + 0 \\cdot 0+ {\\color{purple} 4 \\cdot 1 } + {\\color{red} 2 \\cdot 2 } + {\\color{Green} 1 \\cdot 5} = 52 $$\nである。ベクトルを成分ごとに分けた大きさを棒グラフで表示すると、次のようになる。\n$[-3,3]$から上の棒グラフの形になるように定義された二つの関数\n$$ f(x) := \\begin{cases} 1 \u0026amp; , -3 \\le x \\le -2 \\\\ 5 \u0026amp; , -2 \\le x \u0026lt; -1 \\\\ 0 \u0026amp; , -1 \\le x \\le 0 \\\\ 4 \u0026amp; , 0 \\le x \u0026lt; 1 \\\\ 2 \u0026amp; , 1 \\le x \u0026lt; 2 \\\\ 1 \u0026amp; , 2 \\le x \\le 3 \\end{cases} $$\n$$ g(x) := \\begin{cases} 9 \u0026amp; , -3 \\le x \\le -2 \\\\ 6 \u0026amp; , -2 \\le x \u0026lt; -1 \\\\ 0 \u0026amp; , -1 \\le x \\le 0 \\\\ 1 \u0026amp; , 0 \\le x \u0026lt; 1 \\\\ 2 \u0026amp; , 1 \\le x \u0026lt; 2 \\\\ 5 \u0026amp; , 2 \\le x \\le 3 \\end{cases} $$\nを考えると、\n$$ f(x) g(x) = \\begin{cases} 9 \u0026amp; , -3 \\le x \\le -2 \\\\ 30 \u0026amp; , -2 \\le x \u0026lt; -1 \\\\ 0 \u0026amp; , -1 \\le x \\le 0 \\\\ 4 \u0026amp; , 0 \\le x \u0026lt; 1 \\\\ 4 \u0026amp; , 1 \\le x \u0026lt; 2 \\\\ 5 \u0026amp; , 2 \\le x \\le 3 \\end{cases} $$\nであるから、$\\displaystyle \\int_{-3}^{3} f(x) g(x) dx = 52$であり、驚くべきことに$\\mathbf{f} \\cdot \\mathbf{g}$と一致する。\nもちろん、すべての関数がこんなに都合よくなるわけではないが、積分可能な関数であればリーマン和のアイデアを適用できる。そもそも定積分自体が分割して掛け合わせて足すことを含んでいるので、\u0026lsquo;内積\u0026rsquo;と呼ぶには不足がない。関数の内積は有限次元ベクトルの内積を無限次元に一般化したものと見ることができ、既存の内積の概念にしっかりとカバーされる。\n","id":599,"permalink":"https://freshrimpsushi.github.io/jp/posts/599/","tags":null,"title":"関数の内積を定積分で定義する理由"},{"categories":"르벡공간","contents":"定義 1 関数空間 $L^{2}$ を次のように定義する。\n$$ L^{2} (E) := \\left\\{ f : \\left( \\int_{E} | f |^2 dm \\right)^{{1} \\over {2}} \u0026lt; \\infty \\right\\} $$\n性質 $L^{2}$は距離空間である。距離は次のように定義される。 $$ d(f,g) := \\left( \\int \\left| f(x) - g(x) \\right|^{2}dx \\right)^{\\frac{1}{2}} = \\left\\| f-g \\right\\|_{2} = \\sqrt{\\braket{f-g, f-g}} $$ $L^{2}$はベクトル空間である。 $L^{2}$はノルム空間である。ノルムは次のように定義される。 $$ \\left\\| f \\right\\|_{2} := \\left( \\int \\left| f(x) \\right|^{2}dx \\right)^{\\frac{1}{2}} = \\sqrt{\\braket{f,f}} $$ $L^{2}$は完備空間である。 $L^{2}$は内積空間である。内積は次のように定義される。 $$ \\braket{f, g} := \\int \\overline{f(x)}g(x)dx $$ 説明 $L^{2}$空間は、$p=2$の時の$L^{p}$空間の特別な場合であり、$L^{p}$空間の中で唯一内積が定義される空間である。完備内積空間は特にヒルベルト空間Hilbert spaceと呼ばれる。したがって、$L^{2}$はヒルベルト空間である。ヒルベルト空間は、偏微分方程式や量子力学を含む様々な分野で登場する重要な空間である。\n$L^{p}$空間についての一般化された証明は、こちらを参照してください。\n証明 3. ノルムの定義\n$V$を$\\mathbb{F}$上のベクトル空間としよう。関数$\\left\\| \\cdot \\right\\| : V \\to \\mathbb{F}$が$\\mathbf{u}, \\mathbf{v} \\in V$と$k \\in \\mathbb{F}$に対して次の三つの条件を満たすならば、それを**$V$上のノルム**と定義する。\n正定性: $\\left\\| \\mathbf{u} \\right\\| \\ge 0$そして$\\mathbf{u} = \\mathbb{0} \\iff \\left\\| \\mathbf{u} \\right\\| = 0$ 斉次性: $\\left\\|k \\mathbf{u} \\right\\| = | k | \\left\\| \\mathbf{u} \\right\\| $ 三角不等式: $\\left\\| \\mathbf{u} + \\mathbf{v}\\right\\| \\le \\left\\|\\mathbf{v} \\right\\| + \\left\\| \\mathbf{u} \\right\\|$ $L ^{2}$のノルムを$\\displaystyle \\left\\| f \\right\\|_{2} := \\left( \\int_{E} | f |^2 dm \\right)^{{1} \\over {2}}$のように定義しよう。\nPart 1. 正定性\n$| f | \\ge 0$だから、$\\left\\| f \\right\\|_{2} \\ge 0$ほとんど至る所で$f = 0$ならば、$\\left\\| f \\right\\|_{2} = 0$である。反対に、$\\left\\| f \\right\\|_{2} = 0$ならば、ほとんど至る所で$f = 0$でなければならない。\nPart 2. 斉次性\n$$ \\left\\| c f \\right\\|_{2} = \\left( \\int_{E} | c f |^2 dm \\right)^{{1}\\over {2}} =\\left( |c|^2 \\int_{E} | f |^2 dm \\right)^{{1}\\over {2}} = |c| \\left( \\int_{E} | f |^2 dm \\right)^{{1}\\over {2}} = |c| \\left\\| f\\right\\|_{2} $$\nPart 3. 三角不等式\n$$ \\begin{align*} \\left\\| f + g \\right\\|_{2}^{2} =\u0026amp; \\int_{E} | f + g |^2 dm \\\\ =\u0026amp; \\int_{E} ( f + g ) \\overline{( f + g )} dm \\\\ =\u0026amp; \\int_{E} | f |^2 dm + \\int_{E} ( f \\overline{g} + \\overline{f} g ) dm +\\int_{E} | g |^2 dm \\end{align*} $$\nコーシー-シュワルツの不等式により、次を得る。\n$$ \\begin{align*} \\int_{E} ( f \\overline{g} + \\overline{f} g ) dm \\le \u0026amp; 2 \\int_{E} | fg | dm \\le 2 | f |_{2} | g |_{2} \\\\ =\u0026amp; | f + g | _{2}^{2} \\le | f | _{2} + 2 | f | _{2} | g | _{2} + | g | _{2} \\\\ =\u0026amp; \\left( | f |_{2} + | g |_{2} \\right)^{2} \\end{align*} $$\nまとめると\n$$ \\left\\| f + g \\right\\|_{2} \\le \\left\\| f \\right\\|_{2} + | g |_{2} $$\n■\n5. 内積の定義\n$H$をベクトル空間としよう。$x,y,z \\in H$と$\\alpha, \\beta \\in \\mathbb{C}$に対して、次の条件を満たす関数\n$$ \\langle \\cdot , \\cdot \\rangle : H \\times H \\to \\mathbb{C} $$\nを内積と定義し、$\\left( H, \\langle \\cdot ,\\cdot \\rangle \\right)$を内積空間と言う。\n線形性: $\\langle \\alpha x + \\beta y ,z \\rangle =\\alpha \\langle x,z\\rangle + \\beta \\langle y,z\\rangle$ 共役対称性: $\\langle x,y \\rangle = \\overline{ \\langle y,x \\rangle}$ 正定性: $\\langle x,x \\rangle \\ge 0 \\quad \\text{and} \\quad \\langle x,x \\rangle = 0\\iff x=0$ $L ^{2}$の内積を$\\displaystyle \\langle f , g \\rangle := \\int_{E} f \\overline{g} dm$のように定義しよう。\nPart 1. 線形性\n$$ \\langle f + g , h \\rangle = \\int_{E} ( f + g ) \\overline{g} dm = \\int_{E} f \\overline{g} dm + \\int_{E} g \\overline{g} dm = \\langle f , h \\rangle + \\langle g , h \\rangle $$\nそして\n$$ \\langle c f , g \\rangle = \\int_{E} c f \\overline{g} dm = c \\int_{E} f \\overline{g} dm = c \\langle f , g \\rangle $$\nPart 2. 共役対称性\n$$ \\langle f , g \\rangle = \\int_{E} f \\overline{g} dm = \\overline{ \\int_{E} \\overline{f} g dm} = \\overline{ \\int_{E} g \\overline{f} dm} = \\overline{ \\langle f , g \\rangle } $$\nPart 3. 正定性\n$$ \\langle f, f \\rangle = \\int_{E} f \\overline{f} dm = \\int_{E} | f |^2 dm = \\sqrt{ | f |_{2} } $$\n性質 3. のPart 1により証明終了だ。\n■\n参照 $L^{p}$空間 $L^{1}$空間 ヒルベルト空間 Capinski. (1999). Measure, Integral and Probability: p131.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":594,"permalink":"https://freshrimpsushi.github.io/jp/posts/594/","tags":null,"title":"L2空間"},{"categories":"르벡공간","contents":"定義1 次のように関数空間$L^{1}$を定義する。\n$$ L^{1} (E) := \\left\\{ f : E \\to \\mathbb{R} \\Big \\vert \\int_{E} | f | dm \\lt \\infty \\right\\} $$\n性質 $L^{1}$はベクトル空間だ。 $L^{1}$はノルム空間だ。ノルムは以下のように定義される。 $$ \\left\\| f \\right\\|_{1} := \\int \\left| f(x) \\right| dx $$ $L^{1}$は完備空間だ。 説明 $L^{1}$空間は$L^{p}$空間の$p=1$の時の特別なケースであり、ルベーグ可積分について話す時に、可積分な関数の集まりとして定義された。\n$L^{p}$空間に関する一般化された証明はここを参照。\n証明 2. ノルムの定義\n$V$を$\\mathbb{F}$上のベクトル空間としよう。関数$\\left\\| \\cdot \\right\\| : V \\to \\mathbb{F}$が$\\mathbf{u}, \\mathbf{v} \\in V$と$k \\in \\mathbb{F}$に対して以下の三つの条件を満たすなら、$\\left\\| \\cdot \\right\\|$を**$V$上のノルム**と定義する。\n正定値性: $\\left\\| \\mathbf{u} \\right\\| \\ge 0$かつ$\\mathbf{u} = \\mathbb{0} \\iff \\left\\| \\mathbf{u} \\right\\| = 0$ 斉次性: $\\left\\|k \\mathbf{u} \\right\\| = | k | \\left\\| \\mathbf{u} \\right\\| $ 三角不等式: $\\left\\| \\mathbf{u} + \\mathbf{v}\\right\\| \\le \\left\\|\\mathbf{v} \\right\\| + \\left\\| \\mathbf{u} \\right\\|$ $L ^{1}$のノルムを$\\displaystyle \\left\\| f \\right\\|_{1} := \\int_{E} |f| dm$として定義しよう。\nPart 1. 正定値性\n$| f | \\ge 0$より、ほとんど至る所で$f = 0$ならば$\\left\\| f \\right\\|_{1} = 0$である。逆に、$\\left\\| f \\right\\|_{1} = 0$ならば、ほとんど至る所で$f = 0$でなければならない。\nPart 2. 斉次性\n$$\\left\\| c f \\right\\| _{1} = \\int_{E} | c f | dm = |c| \\int_{E} | f | dm = |c| \\left\\| f \\right\\| _{1}$$\nPart 3. 三角不等式\n$$ \\left\\| f + g \\right\\|_{1} = \\int_{E} | f + g | dm \\le \\int_{E} | f | dm + \\int_{E} | g | dm = \\left\\| f\\right\\|_{1} + \\left\\| g\\right\\|_{1} $$\n■\n3. 完備性\nベクトル空間$X$に対して、ノルム$\\left\\| \\cdot \\right\\|_{X}$が定義されているとしよう。すべての$\\varepsilon \u0026gt; 0$に対して $$n, m \\ge N \\implies \\left\\| f_{n} - f_{m} \\right\\|_{X} \\lt \\varepsilon$$ $N \\in \\mathbb{N}$が存在するなら、数列$f_{n} \\in X$をコーシー数列と言う。もしすべてのコーシー数列が$X$の要素に収束するなら、$X$を完備と言う。\n$f_{n} \\in L^{1}$がコーシー数列なら\n$$ \\left\\| f_{n} - f_{N_{1}} \\right\\|_{1} \\lt {{1} \\over {2}} $$\n$N_{1}$が存在し、同様に\n$$ \\left\\| f_{n} - f_{N_{2}} \\right\\|_{1} \\lt {{1} \\over {2^2}} $$\n$N_{2} \u0026gt; N_{1}$が存在する。この方法で $$ \\left\\| f_{n} - f_{N_{n}} \\right\\|_{1} \\lt {{1} \\over {2^n}} $$ $N_{n} \u0026gt; N_{n-1}$が存在する。三角不等式により $$ \\left\\| f_{N_{n}} - f_{N_{n-1}} \\right\\|_{1} \\lt \\left\\| f_{N_{n}} - f_{n} \\right\\|_{1} + \\left\\| f_{n} - f_{N_{n-1}} \\right\\|_{1} \\lt {{1} \\over {2^n}} + {{1} \\over {2^{n-1}}} \\lt {{3} \\over {2^{n}}} $$\nレヴィの定理\n$\\displaystyle \\sum_{k=1}^{\\infty} \\int |f_{k}| dm \\lt \\infty$ならば$\\displaystyle \\sum_{k=1}^{\\infty} f_{k} (x)$はほとんど至る所では収束し、以下が成立する。\n$$ \\int \\sum_{k=1}^{\\infty} f_{k} dm = \\sum_{k=1}^{\\infty} \\int f_{k} dm $$\nレヴィの定理により、$\\displaystyle \\sum_{n=1}^{\\infty} | f_{N_{n}} - f_{N_{n-1}} |_{1}$は収束する。したがって、以下はほとんど至る所で収束する。\n$$ f_{N_{1}}(x) + \\sum_{n=2}^{ k } \\left[ f_{N_{n}} (x) - f_{N_{n-1}} (x) \\right] = f_{N_{k}} $$\nここで、右辺が$f(x)$に収束するとすれば、右辺の$f_{N_{k}} (x)$も$f(x)$に収束する。\nファトゥの補助定理\n関数値が非負の可測関数の数列$\\left\\{ f_{n} \\right\\}$に対して\n$$ \\displaystyle \\int_{E} \\left( \\liminf_{n \\to \\infty} f_{n} \\right) dm \\le \\liminf_{n \\to \\infty} \\int_{E} f_{n} dm $$\nファトゥの補助定理により、\n$$ \\begin{align*} \\left\\| f - f_{n} \\right\\|_{1} =\u0026amp; \\int |f - f_{n}| dm \\\\ \\le \u0026amp; \\liminf_{k \\to \\infty} \\int | f_{N_{k}} - f_{n}| dm \\\\ =\u0026amp; \\liminf_{k \\to \\infty} \\left\\| f_{N_{k}} - f_{n} \\right\\| \\\\ \\lt\u0026amp; \\varepsilon \\end{align*} $$\n$f_{n}$がコーシー数列であるため、任意の$\\varepsilon \u0026gt; 0$に対して上記の不等式が成立し、したがって$\\left\\| f_{n} - f \\right\\|_{1} \\to 0$である。要約すると、$f_{n}$がコーシーで、その部分数列が$f$に収束するので、$f_{n}$は$f$に収束する。ここで、$f - f_{n} \\in L^{1}$であり、$L^{1}$がベクトル空間なので、\n$$ ( f - f_{n} ) + f_{n} = f \\in L^{1} $$\n$L^{1}$のすべてのコーシー数列が$L^{1}$の要素に収束するので、$L^{1}$は完備空間である。\n■\n参照 $L^{p}$空間 $L^{2}$空間 Capinski. (1999). Measure, Integral and Probability: p127.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":592,"permalink":"https://freshrimpsushi.github.io/jp/posts/592/","tags":null,"title":"L1空間"},{"categories":"추상대수","contents":"定義 1 群 $G$ の部分集合 $H$ が群 $G$ の演算で群である時、$H$は群 $G$の部分群subgroupと呼ばれる。\n定理 部分群の判定法: 群 $G$ の空でない部分集合 $H$ において、$a,\\ b$ が $H$ の要素の時に $ab^{-1}$ も $H$ の要素であれば、$H$は $G$ の部分群である。つまり、$a,\\ b$ が $H$ の要素の時 $a-b$ も $H$ の要素であれば、$H$ は部分群である。\n証明 $a,\\ b$ が $H$ の要素の時に $ab^{-1}$ も $H$ の要素であると仮定しよう。すると、$H$ が群になるための3つの条件を満たすか確認すればいい。\n$H$ の演算は群 $G$ の演算と同じなので、結合法則は自明である。 $a=x,\\ b=x$ としよう。すると、$ab^{-1}=xx^{-1}=e$ であり、仮定により $H$ の要素になるので、$H$ は単位元を持つ。 $a=e,\\ b=x$ としよう。すると、$ex^{-1}=x^{-1}$ であり、仮定により $H$の要素になるので、$H$ の任意の要素 $b$ は逆元を持つ。 3により、どの要素も逆元を持つことが確認できたので、$a=x,\\ b=-y$ としよう。すると、$x(y^{-1})^{-1}=xy$ であり、仮定により $H$ の要素になるので、$H$ は演算に対して閉じている。 1~4により、$H$ は群 $G$ の演算に対して閉じており、結合法則が成立し、単位元と逆元を持つため、群である。したがって、部分集合 $H$ は群 $G$ の部分群である。\n■\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p50.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":589,"permalink":"https://freshrimpsushi.github.io/jp/posts/589/","tags":null,"title":"部分群の定義と部分群の判定法"},{"categories":"위상수학","contents":"定義 1 ある位相空間 $X$ の全ての無限部分集合の集積点が $X$ に属している場合、$X$ を ボルツァーノ-ワイエルシュトラス性質を持つというか、集積点コンパクトと呼ぶ。\n定理 [2]: $X$ が距離空間の場合、$X$ がコンパクトであることと集積点コンパクトであることは同値である。$X$ が距離空間で、コンパクトであることと集積点コンパクトであることは同値である。 説明 例えば、$[a,b]$ は集積点コンパクトだが、$(a,b)$ は集積点コンパクトではない。また、$$ P = \\left\\{ 3 , 3.1 , 3.14 , 3.141, 3.1415, \\cdots \\right\\} $$ のような無限部分集合を検討すると、$\\pi \\notin P$ があるため、集積点コンパクトではない。$\\mathbb{R}$ はこのような部分集合を持っているため、当然集積点コンパクトではない。\n興味深いことに、名前とは裏腹に、定義ではコンパクトについては全く言及されていない。名前だけ見たら、コンパクト空間の特別なケースと思われがちだが、実際はその逆の定理 1 のみが成立する。\n集積点コンパクトのもう一つの意義としては、定理 [2] のように、ある距離空間がコンパクトであることを証明するのに役立つことがある。距離空間がコンパクトであることが証明されれば、連続関数の一様連続性が保証されるので、これが有用であることは言うまでもない。\nMunkres. (2000). Topology(2nd Edition): p178.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":576,"permalink":"https://freshrimpsushi.github.io/jp/posts/576/","tags":null,"title":"ボルツァーノ-ワイエルシュトラスの性質と集積点のコンパクト性"},{"categories":"머신러닝","contents":"定義 分類の問題でポジティブPositive$P$とネガティブNegative$N$を区別するモデルがあると仮定しよう。ポジティブをポジティブと判断した数をトゥルーポジティブTrue Positive$TP$、ネガティブをネガティブと判断した数をトゥルーネガティブTrue Negative$TN$、ポジティブをネガティブと間違えて判断した数をフォルスネガティブFalse Negative$FN$、ネガティブをポジティブと間違えて判断した数をフォルスポジティブFalse Positive$FP$としよう。\nエラーマトリックス 分類の問題で、上記のような エラーマトリックスConfusion Matrixをモデルを評価する指標として参照できる。\n正解率 $$ \\text{Accuracy} = {{TP + TN} \\over { P + N }} $$ 上の表で、Pはポジティブ、Nはネガティブを表す。TPはポジティブと予測され実際にポジティブである場合、TNはネガティブと予測され実際にネガティブである場合だ。このTPとTNが比較的高いモデルを良いモデルと評価することは常識であり、妥当である。一方で、「正」があれば当然「誤」もある。誤分類率Error Rateは下記のように定義される。 $$ \\text{Error Rate} = 1 - \\text{Accuracy} = {{FP + FN} \\over {N + P}} $$\n精度 $$ \\text{Precision} = {{TP } \\over {TP + FP}} $$ 精度は真と予済まれるものの中で実際に真である物の割合だ。Accuracyと混同しないようにしよう。\n感度 $$ \\text{Sensitivity} = \\text{True Positive Rate} = \\text{Recall} = {{TP } \\over { P }} $$ 感度はポジティブなものの中で真と予測されたものの割合で、再現率Recallまたは真陽性率とも呼ばれる。\n特異度 $$ \\text{Specificity} = 1 - \\text{False Positive Rate} = 1- {{FP } \\over { N }} = {{TN } \\over {N }} $$ 特異度はネガティブなものの中で偽と予測されたものの割合だ。\nここで、偽陽性率と真陽性率を軸として描かれる図をROC曲線という。\n参照 エラーマトリックスと感度、特異度 $F_{1}$ スコア データ科学で正解率が過大評価される状況 精度Precision 再現率Recall ","id":571,"permalink":"https://freshrimpsushi.github.io/jp/posts/571/","tags":null,"title":"混同行列と感度、特異度"},{"categories":"위상수학","contents":"要約 1 コンパクト空間$X$に対し、関数$f : X \\to \\mathbb{R}$が連続であれば、全ての$x \\in X$に対して$f(c) \\le f(x) \\le f(d)$を満たす$c,d \\in X$が存在する。\n説明 $\\mathbb{R}$では、コンパクトとは閉区間$[a,b]$であることと同値であるため、結局これは高校や解析学で習う定理の一般化である。トポロジーの難しい理論を使うだけあって、証明はむしろ簡単である。\n証明 コンパクト空間に関する補助定理：$f : X \\to Y$として、$X$がコンパクトで、$f$が連続であるとする。\n[2]: $Y$がハウスドルフであれば、$f$は閉じた関数である。閉集合$C \\subset X$に対して、$f(C) \\subset Y$は閉集合である。 $f(X)$はコンパクトであるため、開被覆$\\mathcal{O} := \\left\\{ (-n,n) \\ | \\ n \\in \\mathbb{N} \\right\\}$に対して、 $$ f(X) \\subset \\bigcup_{n=1}^{m} (-n , n) \\subset \\mathcal{O} $$ を満たす$m$が存在する。補助定理1により、$f(X)$は有界であり、$\\mathbb{R}$はハウスドルフ空間であるため、補助定理[2]により全体集合$X \\subset X$のイメージ$f(X)$は$\\mathbb{R}$で閉集合である。$f(X)$が有界であるため、完備性公理により、 $$ u := \\sup f(X) \\\\ l : = \\inf f(X) $$ が存在する。$f(X)$が閉集合であるため、 $$ u, l \\in f(X) \\\\ f(c) = l \\\\ f(d) = u $$ を満たすある$c,d \\in X$が存在しなければならない。この$c,d$は全ての$x \\in X$に対して$f(c) \\le f(x) \\le f(d)$を満たす。\n■\n参照 距離空間における最大最小定理 Munkres. (2000). Topology(2nd Edition): p174.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":563,"permalink":"https://freshrimpsushi.github.io/jp/posts/563/","tags":null,"title":"位相空間における最大値最小値定理の証明"},{"categories":"위상수학","contents":"定理 $f : X \\to Y$について、$X$がコンパクトで、$f$が連続だとしよう。\n[2]: $Y$がハウスドルフならば、$f$は閉関数だ。閉集合$C \\subset X$に対して、$f(C) \\subset Y$も閉集合である。 説明 些細な性質ばかりのように感じるかもしれないが、最大最小値の定理を証明する時など、利用される場面は多い。\n1 コンパクトが連続関数を適用しても保持される性質を意味する。\n[2] 閉集合に関する話をしているが、ハウスドルフに関する話なので、広く使える。どんな距離空間も$T_{2}$空間であるから、通常は成り立つと見て良い。\n3 条件は多いが、位相同型写像の定義では逆関数も連続であることが含まれている点がポイントだ。逆関数の連続性を直接確認するよりも、定義域と値域の位相的性質を把握する方が簡単ならば、非常に有用である。\n4 一様連続は本来、距離空間でのみ論じる概念だが、$X$がコンパクト空間になると、$f$が連続性だけでなく、一様連続性まで保証されるため、有用である。コンパクトと連続は、非常に異なる概念からスタートしているが、このように多様な形で深く絡み合っており、切り離すことができない。\n","id":561,"permalink":"https://freshrimpsushi.github.io/jp/posts/561/","tags":null,"title":"コンパクト空間と連続関数の有用な性質들"},{"categories":"측도론","contents":"要約 1 測定可能集合 $E \\in \\mathcal{M}$ と $g \\in \\mathcal{L}^{1} (E)$ について、数列の測定可能関数 $\\left\\{ f_{n} \\right\\}$ が $E$ のほとんど至る所で $|f_{n}| \\le g$ を満たすとする。もし、$E$ のほとんど至る所で $\\displaystyle f = \\lim_{n \\to \\infty} f_{n}$ ならば、$f \\in \\mathcal{L}^{1}(E)$ が成り立つ。 $$ \\lim_{ n \\to \\infty} \\int_{E} f_{n} (x) dm = \\int_{E} f dm $$\n$f,g \\in \\mathcal{L}^{1} (E)$ は $f$ と $g$ がルベーグ可積分関数であることを意味する。 説明 単調収束定理と比べると、条件 $f_{n} \\nearrow f$ がなく、$f_{n} \\ge 0$ である必要もなくなった。\n興味深いことに、$\\left\\{ f_{n} \\right\\}$ を「支配」できる $g$ が必要だが、結果には $g$ が現れない。\n証明 パート 1.\n$f \\in \\mathcal{L}^{1}(E) $ であることを示す。\n$E$ で $|f_{n}| \\le g$ があるため、すべての $x \\in E$ に対して $-g(x) \\le f_{n} \\le g(x)$ が成り立つ。整理すると、 $$0 \\le f_{n} (x) + g(x) \\le 2 g(x)$$ で、$n \\to \\infty $ の時、 $$0 \\le f (x) + g(x) \\le 2 g(x)$$ したがって、 $$(f+g) \\in \\mathcal{L}^{1}(E)$$ 一方で、$f = (f + g ) + ( -g)$ であり、$\\mathcal{L}^{1}(E)$ はベクトル空間であるため、$f \\in \\mathcal{L}^{1}(E)$ が成り立つ。\nパート 2.\n$f_{n} \\ge 0$ と仮定する。\nファトゥの補題: 非負の測定可能関数の数列 $\\left\\{ f_{n} \\right\\}$ に対して、 $$\\displaystyle \\int_{E} f dm \\le \\liminf_{n \\to \\infty} \\int_{E} f_{n} dm $$\n仮定とファトゥの補題により、 $$\\displaystyle \\int_{E} f dm \\le \\liminf_{n \\to \\infty} \\int_{E} f_{n} dm $$ であり、$\\displaystyle \\limsup_{n \\to \\infty} \\int_{E} f_{n} dm \\le \\int_{E} f dm $ を示せばよい。\n$g-f_{n}$ にファトゥの補題を再適用すると、 $$\\displaystyle \\int_{E} \\lim_{n \\to \\infty} (g - f_{n}) dm \\le \\liminf_{n \\to \\infty} \\int_{E} (g - f_{n} ) dm $$ となる。ここで、$f, g \\ge 0$ であるため、左辺は $$\\displaystyle \\int_{E} \\lim_{n \\to \\infty} (g - f_{n}) dm =\\int_{E} g dm - \\int_{E} f dm$$ 右辺は $$ \\begin{align*} \u0026amp; \\liminf_{n \\to \\infty} \\int_{E} (g - f_{n} ) dm \\\\ =\u0026amp; \\liminf_{n \\to \\infty} \\left( \\int_{E} g dm - \\int_{E} f_{n} dm \\right) \\\\ =\u0026amp; \\int_{E} g dm - \\limsup_{n \\to \\infty} \\int_{E} f_{n} dm \\end{align*} $$ 整理すると、 $$ \\int_{E} g dm - \\int_{E} f dm \\le \\int_{E} g dm - \\limsup_{n \\to \\infty} \\int_{E} f_{n} dm $$ $g \\in \\mathcal{L}^{1} (E)$ であるため、$\\displaystyle \\int_{E} g dm \u0026lt; \\infty$ が成り立ち、両辺から削除が可能で、符号を整理すると以下を得る。 $$\\limsup_{n \\to \\infty} \\int_{E} f_{n} dm \\le \\int_{E} f dm$$\nパート 3.\n$f_{n} \\ge 0$ でない場合に一般化する。$h_{n} := f_{n} + g$ と定義すると、$h_{n} \\ge 0$ であるため、パート 2で行ったプロセスを繰り返すことができる。\n■\nCapinski. (1999). Measure, Integral and Probability: p92.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":557,"permalink":"https://freshrimpsushi.github.io/jp/posts/557/","tags":null,"title":"支配収束定理の証明"},{"categories":"통계적분석","contents":"ビルドアップ Rで内蔵データ faithful を読み込み、head()関数で確認してみよう。\nたった六つだけど、一見すると、eruptionsとwaitingは正の相関関係を持っているように見える。これらの関係を何らかの二つの定数$\\beta_{0}, \\beta_{1}$について $$\\text{(eruptions)} = \\beta_{0} + \\beta_{1} \\cdot \\text{(waiting) }$$ と表すことができればいい。上の式は二変数の線形関係を直線の方程式として表したもので、$\\beta_{0}$は定数項、$\\beta_{1}$は傾きを意味する。\nしかし、実際のデータでは、理論と異なり誤差が生じるため、何らかの誤差項$\\varepsilon$が必要だ。式を簡単にするために$y:=\\text{(eruptions)}$及び$x:=\\text{(waiting) }$とすると、次を得る。 $$y = \\beta_{0} + \\beta_{1} x + \\varepsilon$$\n上のスクリーンショットでは、合計$6$組の順序対が表示されているが、これらを連立方程式で表すと次のようになる。\n$$ \\begin{cases} 3.600 = \\beta_{0} + \\beta_{1} 79 + \\varepsilon_{1} \\\\ 1.800 = \\beta_{0} + \\beta_{1} 54 + \\varepsilon_{2} \\\\ 3.333 = \\beta_{0} + \\beta_{1} 74 + \\varepsilon_{3} \\\\ 2.283 = \\beta_{0} + \\beta_{1} 62 + \\varepsilon_{4} \\\\ 4.533 = \\beta_{0} + \\beta_{1} 85 + \\varepsilon_{5} \\\\ 2.883 = \\beta_{0} + \\beta_{1} 55 + \\varepsilon_{6} \\end{cases} $$\n実際にfaithfulは、272組の順序対を含んでいるので、このように全てを表現するのは非現実的だが、再び記号を通して表現してみよう。\n$$ \\begin{cases} y_{1} \u0026amp;= \\beta_{0} + \\beta_{1} x_{1} + \\varepsilon_{1} \\\\ y_{2} \u0026amp;= \\beta_{0} + \\beta_{1} x_{2} + \\varepsilon_{2} \\\\ \u0026amp;\\vdots\u0026amp; \\\\ y_{272} \u0026amp;= \\beta_{0} + \\beta_{1} x_{272} + \\varepsilon_{272} \\end{cases} $$\n一方、このような連立方程式は行列方程式としてよりシンプルに表現できる。 $$ \\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{272} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; x_{1} \\\\ 1 \u0026amp; x_{2} \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; x_{272} \\end{bmatrix} \\begin{bmatrix} \\beta_{0} \\\\ \\beta_{1} \\end{bmatrix} + \\begin{bmatrix} \\varepsilon_{1} \\\\ \\varepsilon_{2} \\\\ \\vdots \\\\ \\varepsilon_{272} \\end{bmatrix} $$ 通常通り、行列まで大文字で表記すると、ついに$Y = X \\beta + \\varepsilon$を得ることができる。\n定義 ここでは、$X$のように独立変数をまとめた行列を設計行列Design Matrixと呼ぶ。\n先修科目 このようにデータを行列で表現できることは、線形代数の様々なツールを統計学に適用できることを意味する。ここで$\\beta$を見つけることがまさに回帰分析であり、これを正確に理解するには、線形代数の知識が不可欠だ。\n統計学に接近する多くの学習者が線形代数の必要性を感じずに軽視してしまい、行列が出てくると苦労する。先輩たちの轍を踏まないためにも、解析学や線形代数のような低学年科目を徹底的に磨き上げることが重要だ。\n","id":550,"permalink":"https://freshrimpsushi.github.io/jp/posts/550/","tags":null,"title":"デザイン行列"},{"categories":"수리물리","contents":"まとめ1 3次元ベクトル関数$\\mathbf{F}$について、以下が成り立つ。\n$$ \\begin{equation} \\int_{\\mathcal{V}} \\nabla \\cdot \\mathbf{F} dV = \\oint_{\\mathcal{S}} \\mathbf{F} \\cdot d \\mathbf{S} \\label{1} \\end{equation} $$\nここで、$\\nabla \\cdot \\mathbf{F}$はダイバージェンス、$\\int_{\\mathcal{V}}$は体積積分、$\\oint_{\\mathcal{S}}$は閉曲面積分である。\n説明 これをガウスの定理Gauss\u0026rsquo;s theorem、グリーンの定理Green\u0026rsquo;s theorem、または発散定理divergence theoremと呼ぶ。特に電磁気学でよく使用される。\n数式的意味 数式的には、面積分を体積積分に、体積積分を面積分に変換できるという意味である。つまり、三重積分と二重積分を互いに変換できるということである。\n物理的意味 物理的には、各点（小さい体積）で入ってくる量と出て行く量の総和$\\big( \\eqref{1}$の左辺は、全体の体積の表面で入ってくる量と出て行く量の総和$\\big( \\eqref{1}$の右辺と同じという意味である。\n簡単な例として、ある部屋に人々がいると考えてみよう。人々はドアを通して部屋に入ったり、部屋を出たりする。部屋の中とドアを見ている2名の観察者がいるとする。合計で2名が部屋に入り、3名が部屋から出たとしよう。この場合、**部屋の中の観察者が見た人の変化2は$|2-3|=1$**であり、**ドア番が見た人の変化3は$|3-2|=1$**である。（$1$名がドアを開けて出た時、$+1$と数えるとする）この2つは常に同じである。\n証明 それぞれの面の体積積分を全て足した時に、発散の体積積分と同じになるか確認しよう。まずは、下の図のように各点の座標と各面の名前を設定しよう。\n全ての面について面積分を足すと、以下のようになる。\n$$ \\int _{S_{1}} \\mathbf{F} \\cdot d \\mathbf{S}_{1} + \\int_ {S_2} \\mathbf{F} \\cdot d \\mathbf{S}_2 + \\int_ {S_{3}} \\mathbf{F} \\cdot d \\mathbf{S}_ + \\int_ {S_{4}} \\mathbf{F} \\cdot d\\mathbf{S}_{4} + \\int _{S_{5}} \\mathbf{F} \\cdot d\\mathbf{S}_{5}+\\int _{S_{6}} \\mathbf{F} \\cdot d\\mathbf{S}_{6} $$\nまず、$x$軸に垂直な$S_{1}$と$S_2$の面について計算してみよう。$\\mathbf{F}= F_{x} \\hat{\\mathbf{x}} + F_{y} \\hat{\\mathbf{y}} + F_{z} \\hat{\\mathbf{z}}$であり、各面の方向は外向きである。$\\mathbf{F}$の方向が$S_{1}$の方向と同じだとしよう。すると、二つの面積分は以下のようになる。\n$$ \\begin{align*} \\int _{S_{1}} \\mathbf{F} \\cdot d \\mathbf{S}_{1} + \\int _{S_2} \\mathbf{F} \\cdot d \\mathbf{S}_2 \u0026amp;= \\int_ {S_{1}} F_{x} dS_{1} - \\int _{S_2} F_{x} dS_2 \\\\ \u0026amp;= \\int_{z}^{z+\\Delta z} \\int_{y}^{y+\\Delta y} F_{x} (x+\\Delta x,y,z) dydz - \\int_{z}^{z+\\Delta z} \\int_{y}^{y+\\Delta y} F_{x} (x,y,z) dydz \\\\ \u0026amp;= \\int_{z}^{z+\\Delta z} \\int_{y}^{y+\\Delta y} \\bigg[ F_{x} (x+\\Delta x,y,z) - F_{x} (x,y,z) \\bigg] dydz \\end{align*} $$\nこの時点で、微積分学の基本定理によれば$\\displaystyle \\int _{a} ^b \\dfrac{ dF(x)}{dx}dx=F(b) - F(a)$であるので、以下のようにまとめることができる。\n$$ \\begin{align*} \u0026amp; \\int_{z}^{z+\\Delta z} \\int_{y}^{y+\\Delta y} \\left[ F_{x} (x+\\Delta x,y,z) - F_{x} (x,y,z) \\right] dydz \\\\ =\u0026amp;\\ \\int_{z}^{z+\\Delta z} \\int_{y}^{y+\\Delta y} \\left[ \\int_{x} ^{x +\\Delta x} \\dfrac{ \\partial F_{x}(x,y,z) }{\\partial x} dx \\right] dydz \\\\ =\u0026amp;\\ \\int_{z}^{z+\\Delta z} \\int_{y}^{y+\\Delta y} \\int_{x} ^{x +\\Delta x} \\dfrac{ \\partial F_{x}(x,y,z) }{\\partial x} dx dydz \\\\ =\u0026amp;\\ \\iiint \\dfrac{ \\partial F_{x} }{\\partial x} dV \\end{align*} $$\nよって、以下の結果を得る。\n$$ \\int_ {S_{1}} \\mathbf{F} \\cdot d \\mathbf{S}_{1} + \\int_ {S_2} \\mathbf{F} \\cdot d \\mathbf{S}_2 =\\iiint \\dfrac{ \\partial F_{x} }{\\partial x} dV $$\n同様に、$S_{3}$と$S_{4}$に対する面積分と、$S_{5}$と$S_{6}$に対する面積分を計算すると、以下のようになる。\n$$ \\int _{S_{3}} \\mathbf{F} \\cdot d \\mathbf{S}_{1} + \\int _{S_{4}} \\mathbf{F} \\cdot d \\mathbf{S}_2 =\\iiint \\dfrac{ \\partial F_{y} }{\\partial y} dV $$\n$$ \\int_ {S_{5}} \\mathbf{F} \\cdot d \\mathbf{S}_{5} + \\int_ {S_{6}} \\mathbf{F} \\cdot d \\mathbf{S}_2 =\\iiint \\dfrac{ \\partial F_{z} }{\\partial z} dV $$\n最後に、6面すべてに対する面積分を全部足すと、以下のようになる。\n$$ \\begin{align*} \\oint _\\mathcal{S} \\mathbf{F} \\cdot d \\mathbf{S} \u0026amp;= \\iiint \\dfrac{ \\partial F_{x} }{\\partial x} dV + \\iiint \\dfrac{ \\partial F_{y} }{\\partial y} dV +\\iiint \\dfrac{ \\partial F_{z} }{\\partial z} dV \\\\ \u0026amp;= \\iiint \\left[ \\dfrac{ \\partial F_{x} }{\\partial x} + \\dfrac{ \\partial F_{y} }{\\partial y} + \\dfrac{ \\partial F_{z} }{\\partial z} \\right] dV \\\\ \u0026amp;= \\iiint \\nabla \\cdot \\mathbf{F} dV \\\\ \u0026amp;= \\int_\\mathcal{V} \\nabla \\cdot \\mathbf{F} dV \\end{align*} $$\n■\nDavid J. Griffiths, 基礎電磁気学(Introduction to Electrodynamics, 金 仁成 訳) (第4版, 2014), p35\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n中での変化、つまり、体積に関する変化を意味する。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n出入り口での変化、つまり、表面における変化を意味する。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":565,"permalink":"https://freshrimpsushi.github.io/jp/posts/565/","tags":null,"title":"ガウスの定理, 発散定理"},{"categories":"측도론","contents":"定義 1 基本性質 [2]: $f \\in \\mathcal{L}^{1} (E)$ ならば $\\displaystyle \\left| \\int_{E} f dm \\right| \\le \\int_{E} | f | dm$ [3]: $f \\in \\mathcal{L}^{1} (E) $ そして $c \\in \\mathbb{R}$ ならば $\\displaystyle \\int_{E} (c f) dm = c \\int_{E} f dm$ [4]: $f,g \\in \\mathcal{L}^{1} (E) $ ならば $\\displaystyle \\int_{E} ( f + g ) dm = \\int_{E} f dm + \\int_{E} g dm$ [5]: $f,g \\in \\mathcal{L}^{1} (E)$ そして $f \\le g$ ならば $\\displaystyle \\int_{E} f dm \\le \\int_{E} g dm$ [6]: 全ての $E \\in \\mathcal{M}$ に対して $\\displaystyle \\int_{E} f dm = \\int_{E} g dm$ ならば ほとんど至る所で $f= g$ だ。 説明 性質 1 が定義の直下にあるからといって簡単に見えるかもしれないが、ちょっと経つと混乱しやすいから、よく覚えておくこと。\n一方で性質 [3]～[5] からは、$\\mathcal{L}^{1}(E)$ が ベクトル空間であることがわかる。\nCapinski. (1999). Measure, Integral and Probability: p86. $E \\in \\mathcal{M}$ とするとき、可測関数 $f$ については $$f^{+} := \\max \\left\\{ f , 0 \\right\\} \\\\ f^{-} := \\max \\left\\{ -f , 0 \\right\\}$$ と表す。すると $$ f = f^{+} - f^{-} \\\\ | f | = f^{+} + f^{-} $$ と表せる。もし $\\displaystyle \\int_{E} | f | dm \u0026lt; \\infty$、すなわち $$ \\int_{E} f^{+} dm \u0026lt; \\infty \\\\ \\int_{E} f^{-} dm \u0026lt; \\infty $$ ならば $f$ を ルベーグ積分可能Lesbegue Integrableという。$E$ の積分可能な関数の集合を次のように表す。 $$ \\mathcal{L}^{1}(E) : = \\left\\{ f \\ \\left| \\ \\int_{E} | f | dm \u0026lt; \\infty \\right. \\right\\} $$\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":549,"permalink":"https://freshrimpsushi.github.io/jp/posts/549/","tags":null,"title":"ルベーグ積分可能"},{"categories":"통계적분석","contents":"説明 回帰分析は、ほぼすべての統計的手法の基礎となっているため、一般的すぎるか特殊すぎる説明が多い。回帰分析が何かを一言で説明するなら、変数間の関係を見つける方法と言えるだろう。\nこの便利で驚くべき分析方法は、優生学の父フランシス・ゴルトンFrancis Galtonのアイディアから生まれた。\nゴルトンは遺伝学を研究しているうちに、父とその息子の身長に関するデータに遭遇し、一般的に父が背が高ければ息子も高く、父が背が低ければ息子も低い傾向があることに気付いた。この関係自体は以前から皆が知っていたが、ゴルトンは世代が経つにつれて平均へ回帰Regressする現象に注目した。\n背の高い父の息子も背が高いが、父よりは低くなる傾向があり、背の低い父の息子も背が低いが、父よりは高くなる傾向があった。論理的に考えれば当然のことで、そうでなければ世代を重ねるごとに身長が無限に発散したり$0$に収束してしまうだろう。\n一方で、必ず平均へ回帰するわけではない。成長環境や突然変異のように避けられない誤差が生じるためだ。それにもかかわらず、明らかに現れる線形関係は、ゴルトンに「身長は遺伝する」という確信を与えたに違いない。\nでは、正確ではなくとも、ある程度の誤差はあるが、父の身長だけを見て息子の身長をだいたい当てることはできないだろうか？父の身長$x$と息子の身長$y$が$y = a + b x$のような関係にあるなら、$x$に父の身長を代入することで、息子の身長を推測することになる。もちろん、完全に一致するわけではないが、平均的にはだいたい合うだろう。\nこれが回帰分析の起こりだ。もちろん、今では回帰分析は非常に多岐にわたる分野に応用されており、世代が変わるといった話はもはや不要であるため、「回帰」という言葉はその意味を失った。語源を理解して、それでいい。\n","id":548,"permalink":"https://freshrimpsushi.github.io/jp/posts/548/","tags":null,"title":"回帰分析とは?"},{"categories":"측도론","contents":"要約 1 非負の可測関数の数列 $\\left\\{ f_{n} \\right\\}$ が $f_{n} \\nearrow f$ を満たすとする。そうすると、 $$ \\lim_{n \\to \\infty} \\int_{E} f_{n} dm = \\int_{E} f dm $$\n説明 $f_{n} \\nearrow f$ とは、すべての $x$ に対して、$f_{n}(x) \\le f_{n+1} (x)$ であり、かつ $\\displaystyle \\lim_{n \\to \\infty} f_{n} = f$ であることを意味する。公式はあまりにも単純で、この定理を知っているとは、「条件」を正確に理解しているということだ。便利さで言えば、極限が積分を自由に行き来できるということだから、言うまでもない。\n証明 $f_{n} \\le f$ であるから、 $$ \\limsup_{n \\to \\infty} \\int_{E} f_{n} dm \\le \\int_{E} f dm $$\nファトゥの補助定理: 非負の可測関数の数列 $\\left\\{ f_{n} \\right\\}$ に対して $$\\displaystyle \\int_{E} \\left( \\liminf_{n \\to \\infty} f_{n} \\right) dm \\le \\liminf_{n \\to \\infty} \\int_{E} f_{n} dm $$\nファトゥの補助定理と下限の性質により、$\\displaystyle \\int_{E} f dm \\le \\liminf_{n \\to \\infty} \\int_{E} f_{n} dm$ となり、まとめると、 $$\\displaystyle \\limsup_{n \\to \\infty} \\int_{E} f_{n} dm \\le \\int_{E} f dm \\le \\liminf_{n \\to \\infty} \\int_{E} f_{n} dm $$ しかし、当然 $\\displaystyle \\liminf_{n \\to \\infty} \\int_{E} f_{n} dm \\le \\limsup_{n \\to \\infty} \\int_{E} f_{n} dm$ であるから、 $$\\displaystyle \\limsup_{n \\to \\infty} \\int_{E} f_{n} dm = \\int_{E} f dm = \\liminf_{n \\to \\infty} \\int_{E} f_{n} dm$$ でなければならない。\n■\n結論 非負の可測関数の数列 $\\left\\{ f_{n} \\right\\}$ がほとんど至る所で $f_{n} \\nearrow f$ を満たすとする。そうすると、 $$\\lim_{n \\to \\infty} \\int_{E} f_{n} dm = \\int_{E} f dm$$ であり、特に $$ \\int \\sum_{n=1}^{\\infty} f_{n} dm = \\sum_{n=1}^{\\infty} \\int f_{n} dm$$\nCapinski. (1999). Measure, Integral and Probability: p84.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":535,"permalink":"https://freshrimpsushi.github.io/jp/posts/535/","tags":null,"title":"単調収束定理の証明"},{"categories":"측도론","contents":"定理1 測度が非負の可測関数の数列 $\\left\\{ f_{n} \\right\\}$ に対して、 $$ \\int_{E} \\left( \\liminf_{n \\to \\infty} f_{n} \\right) dm \\le \\liminf_{n \\to \\infty} \\int_{E} f_{n} dm $$\n$\\liminf$ はリミットインフィマムだ。 説明 実解析での単調収束定理と優収束定理を証明するために必要な補題である。可測関数でない数列に関するパトの補題のバージョンは以下の通り。\n数列のバージョン: 関数の数列 $\\left\\{ f_{k} : \\mathbb{N} \\to [0, \\infty) \\right\\}_{k \\in \\mathbb{N}}$ が非負の値を取る場合、 $$ \\sum_{j=1}^{\\infty} \\liminf_{k \\to \\infty} f_{k} (j) \\le \\liminf_{k \\to \\infty} \\sum_{j=1}^{\\infty} f_{k} (j) \\qquad , \\forall j \\in \\mathbb{N} $$\n証明 戦略: 単純関数に関する直感が必要である。全ての補題と同様に、役に立つが、証明はかなり長く複雑なので、頭がクリアで健康な状態の時に読むことを推奨する。数列の場合の証明も本質的には同じである。\nパート 1.\n$$ f : = \\liminf_{n \\to \\infty} f_{n} \\\\ \\displaystyle g_{n} : = \\inf_{k \\ge n } f_{k} $$ とすると $\\displaystyle f = \\lim_{n \\to \\infty} g_{n}$ である。 $$ \\int_{E} f dm \\le \\liminf_{n \\to \\infty} \\int_{E} f_{n} dm $$ これを示すには、全ての単純関数 $\\phi \\le f$ に対して $$ \\int_{E} \\phi dm \\le \\liminf_{n \\to \\infty} \\int_{E} f_{n} dm $$ を示せば十分である。\nこれで、非常に小さい正の数 $\\varepsilon \u0026gt; 0$ に対して、新しい単純関数を $$ \\phi_{\\varepsilon}(x) := \\begin{cases} \\phi (x) - \\varepsilon \u0026amp; , \\phi\u0026gt;0 \\\\ 0 \u0026amp; , \\phi = 0 \\end{cases} \\ge 0 $$ と定義してみよう。すると、十分に大きな自然数 $n$ に対して $\\phi_{\\varepsilon} \\le g_{n} \\le f$ が成り立つだろう。最後に $A_{k} : = \\left\\{ x \\ | \\ g_{k} \\ge \\phi_{\\varepsilon} \\right\\}$ を定義すると、 $$ A_{k} \\subset A_{k+1} \\\\ \\displaystyle \\bigcup_{k=1}^{\\infty} A_{k} = \\mathbb{R} $$\nパート 2.\n$A_{n}$ から $\\phi_{\\varepsilon} \\le g_{n}$ なので、 $$ \\int_{A_{n} \\cap E} \\phi_{\\varepsilon} dm \\le \\int_{A_{n} \\cap E} g_{n} dm $$ $\\displaystyle g_{n} = \\inf_{k \\ge n } f_{k}$ だったので、$k \\ge n$ から $$ \\int_{A_{n} \\cap E} g_{n} dm \\le \\int_{A_{n} \\cap E} f_{k} dm $$ である。一方、$A_{n} \\cap E \\subset E$ だから $$ \\int_{A_{n} \\cap E} f_{k} dm \\le \\int_{E} f_{k} dm $$ したがって、次を得る。 $$ \\int_{A_{n} \\cap E} \\phi_{\\varepsilon} dm \\le \\liminf_{n \\to \\infty} \\int_{E} f_{n} dm $$\nパート 3.\n$\\phi_{\\epsilon}$ は単純関数だったため、値域は有限集合 $\\left\\{ c_{1} , c_{2} , \\cdots , c_{r} \\right\\}$ として表せる。$B_{i} := \\phi_{\\epsilon}^{-1} ( \\left\\{ c_{i} \\right\\} )$ とすると、 $$ \\int_{A_{n} \\cap E} \\phi_{\\varepsilon} dm = \\sum_{i = 1}^{r} c_{i} m (A_{n} \\cap E \\cap B_{i}) $$\n[7]: $A_{n} \\in \\mathcal{M}$, $\\displaystyle A_{n} \\subset A_{n+1} \\implies m \\left( \\bigcup_{n=1}^{\\infty} A_{n} \\right) = \\lim_{n \\to \\infty} m (A_{n})$\nパート1では $A_{n} \\subset A_{n+1}$ であり、$\\displaystyle \\bigcup_{n=1}^{\\infty} A_{n} = \\mathbb{R}$ だったので、 $$ \\lim_{n \\to \\infty} \\sum_{i = 1}^{r} c_{i} m (A_{n} \\cap E \\cap B_{i}) = \\lim_{n \\to \\infty} \\int_{A_{n} \\cap E} \\phi_{\\varepsilon} dm = \\int_{E} \\phi_{\\varepsilon} dm $$ 今、パート2で得られた結果と合わせると、 $$ \\int_{E} \\phi_{\\varepsilon} dm \\le \\liminf_{k \\to \\infty} \\int_{E} f_{k} dm $$\nパート 4.\nケース 1. $m( \\left\\{ x \\ | \\ \\phi (x) \u0026gt;0 \\right\\} ) \u0026lt; \\infty$\n$$\\displaystyle \\int_{E} \\phi_{\\varepsilon} dm = \\int_{E} \\phi dm - \\varepsilon m( \\left\\{ x \\ | \\ \\phi (x) \u0026gt;0 \\right\\} ) $$ $\\varepsilon \\to 0$ で極限を取ると、以下を得る。 $$ \\displaystyle \\int_{E} \\phi_{\\varepsilon} dm = \\int_{E} \\phi dm $$\nケース 2. $m( \\left\\{ x \\ | \\ \\phi (x) \u0026gt;0 \\right\\} ) = \\infty$ $$\\displaystyle D_{n} : = \\left\\{ x \\ \\left| \\ g(x) \\ge {{1} \\over {2}} \\min \\left\\{ c_{i} \\right\\}_{i=1}^{n} \\right. \\right\\} $$ とすると、 $$ D_{n} \\subset D_{n+1} \\\\ \\displaystyle \\bigcup_{n=1}^{\\infty} D_{n} = \\mathbb{R} $$ であるから $\\displaystyle \\int_{D_{n} \\cap E} g_{n} dm \\to \\infty$ であり、$g_{n}$ の定義から以下を得る。 $$ \\displaystyle \\int_{D_{n} \\cap E} g_{n} dm \\le \\int_{D_{n} \\cap E} f_{k} dm \\le \\int_{E} f_{k} dm $$\nしたがって、どちらの場合でも、全ての単純関数 $\\phi \\le f$ に対して、以下が成立する。 $$ \\int_{E} \\phi dm \\le \\liminf_{n \\to \\infty} \\int_{E} f_{n} dm $$\n■\nCapinski. (1999). Measure, Integral and Probability: p82.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":534,"permalink":"https://freshrimpsushi.github.io/jp/posts/534/","tags":null,"title":"ファトゥの補題の証明"},{"categories":"편미분방정식","contents":"定義 以下の準線形偏微分方程式を ブルガース方程式Burgers\u0026rsquo; equationと呼ぶ。\n$$ \\begin{cases} u_{t} + u u_{x} = 0 \u0026amp; , t\u0026gt;0 \\\\ u(t,x) = f(x) \u0026amp; , t=0 \\end{cases} $$\nここで、$t$は時間を、$x$は位置を、$u(t,x)$は時間$t$での位置$x$の波形を表す。 $f$は初期条件で、特に$t=0$の時の波形を表す。\n説明 ブルガース方程式は、$\\displaystyle u_{t} + u u_{x} = \\nu u_{xx}$において拡散係数$\\nu$が$0$の場合を表す。\nもし$f ' (x)\u0026gt;0$ならば、$x$が大きくなるほど速度が速くなるという意味で、特性曲線は上のように希薄波rarefactionを形成する。$(t,x)$での勾配は波の速度を意味しており、希薄波を形成することは、速度が徐々に増加するか減少するだけであることを意味する。この場合、特性曲線は絶対に交わることはない。\n波形を描くと、上のように時間が経過すると速い点がより早く動き、遅い点がより遅く動くため、次第に差が広がる。\n一方で、上のように$x$が大きくなるほど速度が低下する場合、特性曲線は交差するようになる。\nこの場合、図に示すように、後ろにあった点が前にあった点を追い越すことが発生する。この図では$t=1$の時を基点として、$u$が与えられた$x$に対して複数の値を持ち始めている。自然現象の例としては、波ができる状況を想像すると役立つかもしれない。底の方で前進していた水は砂や小石との摩擦で遅くなっているが、上部では力を受けて進行し、結局は倒れる。これを破裂blow-upと言い、数学的には関数が存在しなくなり、物理的には同時に複数の状態が重なることを意味する。\n特性曲線が希薄波を形成する場合は解析が簡単すぎるため、主な関心事はこの場合だけではない。解を見つけること自体は難しくないが、それを明示関数の形にきれいに変換することが難しく、不可能な場合も多い。ブルガース方程式の解法には、破裂時間blow-up timeおよび破裂位置blow-up locationを求めることも重要な問題である。粘性係数のないブルガース方程式の解が存在する場合、解法は次の通りである。\n解法 ステップ1. $\\xi = x - tu$と置く。\nステップ2. $u = f(x - tu)$と置く。\nすると$\\xi = x - t f( x - tu) = x - t f (\\xi )$なので、特性直線は$x = f(\\xi) t + \\xi$となる。全ての$x \\in \\mathbb{R}$に対して$f ' (x) \u0026gt; 0$であれば、特性直線は希薄波を形成し、会うことはない。$f ' (x) \u0026lt; 0$となる$x$が存在するなら、特性直線はある点で会い、その点で破裂する。\nステップ3. 初期条件を$f(x-tu) = f(\\xi)$の形に変換する。\nもし$u = f(\\xi) = f(x - tu)$を明示関数の形に変換できるならば、変換する。\n■\n無粘性ブルガース方程式の解が有限時間内に破裂する場合、その時間および位置は次の通りである。\nステップ1. $u$を明示関数の形で求めたならば、$u$が発散する$t = t_{\\ast}$を見つける。\nその$t_{\\ast}$が破裂時間になる。\nステップ2. 明示関数の形を求められなかった場合、$f ' (x)$を求める。\n$$ t_{\\ast} : = \\inf \\left\\{ \\left. - {{1} \\over {f ' (x) }} \\ \\right| \\ f '(x) \u0026lt; 0 \\right\\} $$\nそれが破裂時間である。\nステップ3. 特性直線$x = f(\\xi) t + \\xi$と$x$軸が会う点$x_{0}$を探す。\n$x = f(\\xi) t + \\xi$に$\\xi=x_{0}$と$t = t_{\\ast}$を代入して得られる$x_{*} = x_{0} + f(x_{0}) t_{\\ast}$が破裂位置である。\n■\n例題 1 $\\displaystyle \\begin{cases} u_{t} + u u_{x} = 0 \u0026amp; , t\u0026gt;0 \\\\ u(t,x) = \\alpha (x - tu) + \\beta \u0026amp; , t=0 \\end{cases}$の破裂時間を求めよ。 これは$u$が明示関数の形できれいに表されるタイプである。\n$u = \\alpha ( x - tu ) + \\beta$を$u$について整理すると$\\displaystyle u(t,x) = {{\\alpha x + \\beta} \\over {1 + \\alpha t}}$であり、破裂時間は$\\displaystyle t_{\\ast} = - {{1} \\over {\\alpha}} $である。\n■\n2 $\\displaystyle \\begin{cases} u_{t} + u u_{x} = 0 \u0026amp; , t\u0026gt;0 \\\\ u(t,x) = {{1} \\over {2}} \\pi - \\tan^{-1} x \u0026amp; , t=0 \\end{cases}$の破裂時間を求めよ。 $u$を明示関数の形で表すのが煩わしいタイプである。\n$$ f ' (x) = \\left( {{1} \\over {2}} \\pi - \\tan^{-1} x \\right) = - {{1} \\over {1 + x^2}} $$\nであり、$x \\in \\mathbb{R}$に対して$f ' (x) \u0026lt; 0$が成り立つ。したがって、\n$$ t_{\\ast} = \\inf \\left\\{ \\left. - {{1} \\over {f ' (x) }} \\ \\right| \\ f '(x) \u0026lt; 0 \\right\\} = \\inf \\left\\{ \\left. (1+ x^2) \\ \\right| \\ f '(x) \u0026lt; 0 \\right\\} = 1 $$\n■\n","id":532,"permalink":"https://freshrimpsushi.github.io/jp/posts/532/","tags":null,"title":"非粘性バーガース方程式の解"},{"categories":"상미분방정식","contents":"まとめ1 $$ ay^{\\prime \\prime} + by^\\prime + cy=0 $$\n上で与えられた微分方程式の特性方程式 $ar^2+br+c=0$の解を$r_{1}$、$r_2$としよう。すると、\n$\\text{1.}$ $r_{1}$、$r_2$が異なる二つの実数の場合$(b^2-4ac\u0026gt;0)$、一般解は以下の通り。 $$ y(t)=c_{1}e^{r_{1}t}+c_2e^{r_2t} $$\n$\\text{2.}$ $r_{1}$、$r_2$が共役複素数$\\lambda \\pm i \\mu$の場合$(b^2-4ac\u0026lt;0)$、一般解は以下の通り。 $$ \\begin{align*} y(t) \u0026amp;= c_{1}e^{(\\lambda + i\\mu)t} + c_2e^{(\\lambda – i\\mu)t} \\\\ \u0026amp;= c_{3}e^{\\lambda t} \\cos \\mu t + c_{4} e^{\\lambda t} \\sin \\mu t \\end{align*} $$\n$\\text{3.}$ $r_{1}=r_2=r$場合$(b^2-4ac=0)$、一般解は以下の通り。 $$ y(t)=c_{1}e^{rt}+c_2te^{rt} $$\n解答 1. $r_{1} \\ne r_2$かつ$r_{1}, r_2\\in \\mathbb{R}$の場合 一般解は以下の通りです。\n$$ y(t)=c_{1}e^{r_{1}t}+c_2e^{r_2t} $$\nここで、$c_{1}, c_2$は定数で、二つの初期値$y(0)=y_{0}$と$y^\\prime (0) =y^\\prime_{0}$が分かれば、正確に求めることができる。\n■\n2. $r_{1} \\ne r_2$かつ$r_{1}, r_2 \\in \\mathbb{C}$の場合 特性方程式の判別式が$b^2-4ac\u0026lt;0$の場合である。$r_{1}$と$r_2$が共役複素数となるため、以下のように表現できる。\n$$ r_{1}=\\lambda + i\\mu,\\quad r_2=\\lambda – i\\mu $$\nすると、微分方程式の二つの解は以下の通り。\n$$ y_{1}=e^{r_{1}t}=e^{(\\lambda + i\\mu)t}, y_{2}=e^{(\\lambda – i\\mu)t} $$\n従って、一般解は以下の通り。\n$$ y(t)=c_{1}e^{(\\lambda + i\\mu)t} + c_2e^{(\\lambda – i\\mu)t} $$\nここまで、**1.**と特に変わりはない。一般解をオイラーの公式を使って三角関数で表すと、\n$$ \\begin{align*} \u0026amp;\\ c_{1}e^{(\\lambda + i\\mu)t} + c_2e^{(\\lambda – i\\mu)t} \\\\ =\u0026amp;\\ c_{1} e^{\\lambda t} (\\cos \\mu t + i\\sin \\mu t) + c_2 e^{\\lambda t} ( \\cos \\mu t – i \\sin \\mu t) \\\\ =\u0026amp;\\ c_{3}e^\\lambda \\cos \\mu t + c_{4} e^\\lambda \\sin \\mu t \\end{align*} $$\n従って、\n$$ y(t)=c_{3}e^{\\lambda t} \\cos \\mu t + c_{4} e^{\\lambda t} \\sin \\mu t $$\nこの時、$c_{4}$は$i$を含む複素数定数である。初期値が分かれば、$c_{3}$と$c_{4}$を正確に求めることができる。\n■\n3. $r_{1}=r_2=r=-\\dfrac{b}{2a}$の場合 特性方程式の判別式が$b^2-4ac=0$の場合である。$y_{1}$は$y_{1}=e^{\\frac{-b}{2a}t}$で求めることができるが、$y_{2}$を見つけることはできない。$y_{2}$を見つけるために、$y(t)=\\nu (t) y_{1}(t)$と仮定しよう。すると、\n$$ \\begin{align*} y^\\prime \u0026amp;= \\nu ^\\prime y_{1} + \\nu y_{1}^\\prime \\\\ y^{\\prime \\prime}\u0026amp;=\\nu^{\\prime \\prime}y_{1} +\\nu ^\\prime y_{1}^\\prime + \\nu^\\prime y_{1}^\\prime + \\nu y_{1}^{\\prime \\prime}=\\nu^{\\prime \\prime}y_{1}+2\\nu ^\\prime y_{1}^\\prime+ \\nu y_{1}^{\\prime \\prime} \\end{align*} $$\n$y^\\prime$と$y^{\\prime \\prime}$を与えられた微分方程式に代入すると、\n$$ a \\left( \\nu^{\\prime \\prime}y_{1}+2\\nu ^\\prime y_{1}^\\prime+ \\nu y_{1}^{\\prime \\prime} \\right) + b \\left( \\nu ^\\prime y_{1} + \\nu y_{1}^\\prime \\right) + c\\nu y_{1}=0 $$\n$\\nu$について整理すると、\n$$ \\nu \\left( ay_{1}^{\\prime \\prime} + b y_{1}^{\\prime} + cy_{1}\\right) + \\nu^\\prime \\left( 2ay_{1}^\\prime+by_{1} \\right) + ay_{1} \\nu ^{\\prime \\prime}=0 $$\nここで、$y_{1}$が与えられた微分方程式の解であるため、最初の括弧は$0$である。また、$y_{1}=e^{(-b/{2a})t}$と$y_{1}^\\prime = \\frac{-b}{2a}e^{({-b}/{2a})t}$のため、\n$$ \\begin{align*} \u0026amp;\u0026amp;\\nu^\\prime \\left( 2a \\dfrac{-b}{2a}e^{\\frac{-b}{2a}t} + b e^{\\frac{-b}{2a}t} \\right) + ae^{\\frac{-b}{2a}t} \\nu ^{\\prime \\prime}\u0026amp;=0 \\\\ \\implies\u0026amp;\u0026amp; \\nu^\\prime(-b+b)+a\\nu^{\\prime \\prime}\u0026amp;=0 \\\\ \\implies \u0026amp;\u0026amp; \\nu^{\\prime \\prime} \u0026amp;=0 \\end{align*} $$\n従って、$\\nu (t)=c_{1}+c_2t$である。最終的に、与えられた微分方程式の一般解は、\n$$ y(t)=\\nu (t) y_{1}(t)=c_{1}y_{1}(t)+c_2ty_{1}(t)=c_{1}e^{rt}+c_2te^{rt} $$\nつまり、$y_{2}=ty_{1}$であるというわけだ。\n■\n例題 1. $$ y^{\\prime \\prime}+ 5y^\\prime + 6y=0 \\\\ y(0)=2 \\\\ y^\\prime (0)=3 $$\n特性方程式は$r^2+5r+6=0$である。つまり$(r+2)(r+3)=0$で、従って$r_{1}=-2$、$r_2=-3$である。従って、一般解は、\n$$ y(t)=c_{1}e^{-2t}+c_2e^{-3t} $$\n一般解を微分すると、\n$$ y^\\prime(t)=-2c_{1}e^{-2t} -3c_2e^{-3t} $$\n初期値を代入すると、\n$$ \\begin{cases} c_{1}+c_2=2 \\\\ -2c_{1}-3c_2=3 \\end{cases} $$\n連立して解くと、\n$$ c_{1}=9,\\quad c_2=7 $$\n従って、与えられた初期値に対する解は、\n$$ y(t)=9e^{-2t}+7e^{-3t} $$\n■\n2. $$ y^{\\prime \\prime}+4y^\\prime + 8y=0 $$\n特性方程式は、\n$$ r^2+4r+8=0 $$\n特性方程式の解は、\n$$ r_{1,2}= \\dfrac{-4\\pm \\sqrt{16-32}}{2}=-2 \\pm 2i $$\nで、$\\lambda=-2$、$\\mu=2$である。したがって、与えられた微分方程式の一般解は、\n$$ y(t)=c_{1}e^{-2t}\\cos 2t + c_2 e^{-2t} \\sin 2t $$\n■\n3. $$ y^{\\prime \\prime} + 4y^\\prime + 4y=0 $$\n特性方程式は、\n$$ r^2+4r+4=(r+2)^2=0 $$\n特性方程式の解は、\n$$ r=-2 $$\nしたがって、$y_{1}(t)=e^{-2t}$で、一般解は、\n$$ y(t)=c_{1}e^{-2t} + c_2te^{-2t} $$\n■\nWilliam E. Boyce, Boyce\u0026rsquo;s Elementary Differential Equations and Boundary Value Problems (11th Edition, 2017), p120-133\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":544,"permalink":"https://freshrimpsushi.github.io/jp/posts/544/","tags":null,"title":"二次同次微分方程式の解"},{"categories":"측도론","contents":"ビルドアップ リーマン積分の一般化を考える前に、簡単な関数Simple Functionを定義する必要がある。\n関数値が非負の$\\phi : \\mathbb{R} \\to \\mathbb{R}$の値域が有限集合$\\left\\{ a_{1} , a_{2}, \\cdots , a_{n} \\right\\}$であるとする。$A_{i} = \\phi^{-1} \\left( \\left\\{ a_{i} \\right\\} \\right) \\in \\mathcal{M}$を満たすなら、$\\phi$を簡単な関数と呼ぶ。簡単な関数には以下の特性がある。\n(i): $i \\ne j$なら$A_{i } \\cap A_{j} = \\emptyset$ (ii): $\\displaystyle \\bigsqcup_{k=1}^{n} A_{k} = \\mathbb{R}$ (iii): $\\displaystyle \\phi (x) = \\sum_{k=1}^{n} a_{k} \\mathbb{1}_{A_{k}}(x)$は可測関数だ。 簡単な関数は定義から、取り扱いが非常に簡単な3つの要素で構成されている。まず第一に、関数値が非負であるため、符号を考える必要がなく、第二に有限であるために、加算と減算が自由であり、第三に可測だ。数学のさまざまな分野で簡単Simpleという言葉はさまざまな意味で使用されるが、少なくとも実解析では「複雑」の反対と考えてもよいだろう。このように扱いやすく便利な簡単な関数を定義した後、すぐにリーマン積分をカバーする新しい積分を考えることができる。\n簡単な関数のルベーグ積分 $\\phi$が簡単な関数で、$E \\in \\mathcal{M}$とするとき、$\\displaystyle \\int_{E} \\phi dm := \\sum_{k=1}^{n} a_{k} m (A_{k} \\cap E)$を簡単な関数$\\phi$のルベーグ積分と呼ぶ。ルベーグ積分には以下の特性がある。\n[1]: すべての$r\u0026gt;0$に対して$\\displaystyle \\int_{E} a \\phi dm = a \\int_{E} \\phi dm $ [2]: 二つの簡単な関数$\\phi , \\psi$に対して$\\phi \\le \\psi$ならば$\\displaystyle \\int_{E} \\phi dm \\le \\int_{E} \\psi dm$ [3]: $A, B \\in \\mathcal{M}$に対して$A \\cap B = \\emptyset$ならば$\\displaystyle \\int_{A \\cup B} \\phi dm = \\int_{A} \\phi dm + \\int_{B} \\phi dm$ $m$はルベーグ測度だ。 しかし、簡単な関数という条件はあまりにも強力で特殊であるため、多くの場所で使うことができない。分割求積法のアイデアのようなものを加えると、ある程度満足できる「ルベーグ積分」が完成する。\n定義 1 $\\phi$が簡単な関数であるとき、関数値が非負の可測関数$f$と$E \\in \\mathcal{M}$に対して $$\\displaystyle \\int_{E} f dm := \\sup \\left\\{ \\left. \\int_{E} \\phi dm \\ \\right| \\ 0 \\le \\phi \\le f \\right\\}$$ を可測関数$f$のルベーグ積分Lebesgue Integralと呼ぶ。\n基本性質 ルベーグ積分には以下の性質がある。\n[1]\u0026rsquo;: すべての$r \\ge 0$に対して$\\displaystyle \\int_{E} r f dm = r \\int_{E} f dm $ [2]\u0026rsquo;: 二つの簡単な関数$f, g$に対して$f \\le g$ならば$\\displaystyle \\int_{E} f dm \\le \\int_{E} g dm$ [3]\u0026rsquo;: $A, B \\in \\mathcal{M}$に対して$A \\cap B = \\emptyset$ならば$\\displaystyle \\int_{A \\cup B} f dm = \\int_{A} f dm + \\int_{B} f dm$ [4]\u0026rsquo;: $A, B \\in \\mathcal{M}$に対して$A \\subset B$ならば$\\displaystyle \\int_{A} f dm \\le \\int_{B} f dm$ [5]\u0026rsquo;: $N \\in \\mathcal{N}$ならば$\\displaystyle \\int_{N} f dm = 0$ [6]\u0026rsquo;: $\\displaystyle m(E) \\inf_{E} f \\le \\int_{E} f dm \\le m(E) \\sup_{E} f $ 説明 これらの基本的な性質に加えて、以下のような定理を考えることができる。この定理を使用すれば、$\\displaystyle \\int_{\\mathbb{R}} \\mathbb{1}_{\\mathbb{Q}} dm = 0$として新鮮な計算も一切れで終わらせることができる。見た目ほど証明は簡単ではないが、一度は見ておく価値があるだろう。\n定理 可測空間$( X , \\mathcal{E} )$の可測関数$f \\ge 0$とすべての可測集合$A \\in \\mathcal{E}$に対して $$ \\int_{A} f dm = 0 \\iff f = 0 \\text{ a.e.} $$\n$\\text{a.e.}$はほとんど至る所を意味する。 証明 $( \\implies )$\n$E := f^{-1} ( 0 , \\infty)$に対して$m(E) = 0$ならば、$f$はほとんど至る所$f=0$だ。$\\displaystyle E_{n} := f^{-1} \\left[ {{1} \\over {n}} , \\infty \\right)$と仮定して、$\\displaystyle E = \\bigcup_{n=1}^{\\infty} E_{n}$でありながら$\\displaystyle \\lim_{n \\to \\infty} E_{n} = E$が成り立つ場合を考える。簡単な関数$\\displaystyle \\phi_{n} := {{1}\\over {n}} \\mathbb{1}_{E_{n}} \\le f$を考えると $$ {{1}\\over {n}} m( E_{n} ) = \\int_{A} \\phi_{n} dm \\le \\int_{A} f dm = 0 $$ 従って $$ {{1} \\over {n}} m(E_{n}) \\le 0 $$ つまり、すべての$n \\in \\mathbb{N}$に対して$m(E_{n}) = 0$である。\n[7]: $E_{n} \\in \\mathcal{M}$, $\\displaystyle E_{n} \\subset E_{n+1} \\implies m \\left( \\bigcup_{n=1}^{\\infty} E_{n} \\right) = \\lim_{n \\to \\infty} m (E_{n})$\n一方で$E_{n} \\subset E_{n+1}$であるため、次のことが成り立つ。 $$ m \\left( \\bigcup_{n=1}^{\\infty} E_{n} \\right) = \\lim_{n \\to \\infty} m (E_{n}) = m(E) = 0 $$\n$( \\impliedby )$\n$f$がほとんど至る所$f=0$であり、簡単な関数$\\phi$が$0 \\le \\phi \\le f$を満たすため、$\\phi$もほとんど至る所$\\phi = 0$である。従って$\\displaystyle \\int_{A} f dm = 0$が真である。\n■\nCapinski. (1999). Measure, Integral and Probability: p77。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":527,"permalink":"https://freshrimpsushi.github.io/jp/posts/527/","tags":null,"title":"ルベーグ積分"},{"categories":"고전역학","contents":"単純調和運動1 バネに吊り下げられた物体の運動を考えてみよう。バネの復元力によって前後に振動する。このような運動を調和振動と呼ぶ。調和振動を表す関数である$\\sin$と$\\cos$が、昔は調和関数と呼ばれたからだ。調和振動の中でも、摩擦力や他の外力が全くなく、バネの復元力のみによって運動する場合を単純調和振動という。まず、バネによる復元力がどのように表されるかを見てみよう。$V(x)$を一次元単純調和振動子のポテンシャルエネルギーとしよう。そして、これが多項式の無限和、級数で表されると仮定しよう。すると、以下のように表せる。\n$$ V(x)=a_{0} + a_{1}x + a_2x^2+ a_{3}x^3 + \\cdots $$\nだけど、二つのポテンシャルの差だけが物理的意味を持つから、定数項は$0$としてもよい。平衡点を$0$とするのと同じだ。また、$-\\dfrac{dV}{dx}=F(x)$で復元力は$F(0)=0$だから、$V^\\prime(0)=0$となる。つまり、一次項の係数は$0$でなければならない。だから、復元力のポテンシャルは次のようになる。\n$$ V(x)=a_2x^2+a_{3}x^3+\\cdots $$\n$x$が十分に小さいとき、三次項以上の項は無視できる。最終的に復元力を求めると、以下のようになる。\n$$ F(x)=-\\dfrac{dV}{dx}=-2a_2x=-kx \\ \\ (k=2a_2) $$\nこの時、$k$を弾性係数あるいはバネ定数という。そして、$F(x)=-kx$をフックの法則Hooke’s law2という。今、復元力に関する運動方程式を解こう。$F=ma=m\\ddot{x}$で復元力は$F=-kx$なので、次の式が得られる。\n$$ \\begin{align*} \u0026amp;\u0026amp; m \\ddot{x} \u0026amp; =-kx \\\\ \\implies \u0026amp;\u0026amp; m \\ddot{x} + kx \u0026amp;= 0 \\\\ \\implies \u0026amp;\u0026amp; \\ddot{x} + \\dfrac{k}{m}x \u0026amp;= 0 \\end{align*} $$\nこの時、${\\omega_{0}}^2 \\equiv \\dfrac{k}{m}$と置き換えよう。二乗に置き換える理由は、最終式の形を単純にするためだ。$\\omega_{0}$を系の角振動数と呼ぶ。減衰振動システム、強制振動システムと区別するために固有角振動数あるいは固有振動数とも呼ぶ。今、運動方程式は以下のようになる。\n$$ \\ddot{x} + {\\omega_{0}}^2x=0 $$\n係数が負の2次微分方程式の解法\n以下のような2次微分方程式 $$ \\dfrac{d^{2}X}{dx^{2}} = -\\alpha^{2}X $$ の解は、次のようになる。\n$$ X(x) = Ae^{i\\alpha x} + B e^{-i \\alpha x} $$\nそして、以下のような解を得る。\n$$ \\begin{align*} x(t) \u0026amp;=A_{1}e^{i\\omega_{0} t}+A_2e^{-i\\omega_{0} t} \\\\ \u0026amp;=A_{3}\\cos \\omega_{0} t+ A_{4}\\sin \\omega_{0} t \\\\ \u0026amp;=A \\cos (\\omega_{0} t + \\phi) \\end{align*} $$\nここで、$A_{1}$、$A_{2}$、$A_{3}$、$A_{4}$、$A$は、それぞれ任意の複素数あるいは実数定数だ。普通、三番目の式のように、コサインやサイン関数の形で表されることが多い。\n一緒に見る 減衰振動 強制振動 多重バネ振動 結合振動 Grant R. Fowles and George L. Cassiday, Analytical Mechanics (7th Edition, 2005), p84-86\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nフックの法則とも呼ばれる。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":543,"permalink":"https://freshrimpsushi.github.io/jp/posts/543/","tags":null,"title":"復元力と一次元単純調和振動子"},{"categories":"상미분방정식","contents":"要約1 定数係数の2次線形同次微分方程式$a y^{\\prime \\prime} + by^\\prime +cy=0$の一般解は、以下の通りである。\n$$ y(x)=A e^{r_{1} x}+Be^{r_2 x} $$\nこの時、$r_{1,2}=\\dfrac{-b \\pm \\sqrt{b^2-4ac}} {2a}$\n系 $a y^{\\prime \\prime} + cy = 0$の解は、以下の通りである。\n$$ y(x) = A e^{i\\sqrt{\\frac{c}{a}} x}+Be^{-i\\sqrt{\\frac{c}{a}} x} = C\\cos{\\textstyle (\\sqrt{\\frac{c}{a}}x)} + D\\sin{\\textstyle (\\sqrt{\\frac{c}{a}}x)} $$\n解法 $$ \\begin{equation} a\\dfrac{d^2}{dx^2}y+b\\dfrac{d}{dx}y+cy = 0 \\label{eq1} \\end{equation} $$\nまず、微分演算子$D$を、以下のように定義しよう。\n$$ D:=\\dfrac{d}{dx} \\\\ Df = D(f) = \\dfrac{df}{dx} $$\nすると、$D$は$D(ay_{1}+y_{2}) = a\\dfrac{dy_{1}}{dx} + \\dfrac{dy_{2}}{dx} = aDy_{1}+Dy_{2}$を満たすため、線形演算子である。$D$を利用して式$\\eqref{eq1}$を表すと、以下のようになる。\n$$ \\begin{align} \u0026amp;\u0026amp;aD^2y+bDy+cy\u0026amp;=0 \\\\ \\implies\u0026amp;\u0026amp; (aD^2+bD+c)y\u0026amp;=0 \\end{align} $$\nこの時、$Dy=ry$を満たす定数$r$があるとすると、上の式から以下の式を得る。\n$$ (aD^2+bD+c) y = (ar^2+br+c) y = 0 $$\n私たちは$y \\ne 0$を満たす解を探しているため、以下の条件を得る。\n$$ aD^{2} + bD + c = ar^{2}+br+c = 0 $$\nこの2次方程式を特性方程式と呼ぶ。\n$$ \\begin{align*} r_{1} \u0026amp;= \\dfrac{-b + \\sqrt{b^2-4ac}} {2a} \\\\ r_2 \u0026amp;=\\dfrac{-b - \\sqrt{b^2-4ac}} {2a} \\end{align*} $$\n$r_{1}, r_{2}$が互いに異なる二つの実数だとしよう。すると、上の式から、以下の式を得る。\n$$ (aD^2 + bD+c)y=0 \\implies \\ a(D-r_{1})(D-r_2)y=0 $$\nケース1. $(D-r_{1})y=0$\n$\\dfrac{dy}{dx}=r_{1}y$であり、変数分離法を通じて$y$を求めると、\n$$ y_{1}(x)=Ae^{r_{1}t} $$\nケース2. $(D-r_2)y=0$\n同様に$y$を求めると、\n$$ y_{2}(x)=Be^{r_2t} $$\n$y_{1}$と$y_{2}$が与えられた微分方程式の解ならば、$y_{1}+y_{2}$も解であるため、与えられた微分方程式の一般解は、\n$$ y(x)=y_{1}+y_{2}=Ae^{r_{1}t} + Be^{r_2t} $$\n■\nWilliam E. Boyce, Boyce\u0026rsquo;s Elementary Differential Equations and Boundary Value Problems (11th Edition, 2017), p103-109\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":540,"permalink":"https://freshrimpsushi.github.io/jp/posts/540/","tags":null,"title":"定数係数の2階線形同次微分方程式と特性方程式"},{"categories":"측도론","contents":"定義 1 関数 function $f : E \\to \\overline{\\mathbb{R}}$ が、$E_{0} \\subset E$ の集合(ここで $m(E_{0}) = 0$)を除いて、ある性質 $P$ を持つ場合、$f$ は $E$ のほとんど至る所で $P$ の性質を持つと言われる。\n表記 確率を話す時に、ほとんど至る所ではほとんど確実にと表現され、短く書くために $$ f = g \\text{ a.e.} \\\\ P(E) = 0 \\text{ a.s.} $$ という略語を使うことがある。\n説明 簡単に言えば、零集合を除いた全ての点を\u0026rsquo;ほとんど至る所\u0026rsquo;と見ることである。この概念は正式に定義されただけで、高校で定積分を学んだ時に既に知っていたことだ。そのため、上限と下限が同じなら、その定積分は必ず $0$ であり、端点が含まれるかどうかを確率を計算する時には無視した。\n基本的な性質 [1]: $f : E \\to \\mathbb{R}$ が計測可能で、$E$ のほとんど至る所で $f = g$ ならば、$g$ は $E$ で計測可能である。 [2]: $f,g$ が $E$ で計測可能で、$E$ のほとんど至る所で $|f| , |g| \u0026lt; \\infty$ ならば、$\\alpha f + \\beta g$ は $E$ で計測可能である。 [3]: $f,g$ が $E$ で計測可能で、$E$ のほとんど至る所で $|f| , |g| \u0026lt; \\infty$ ならば、$f g$ は計測可能である。 証明 これらの性質は、一度は手で直接証明してみることが良いが、[3]を除いてはそれほど面白くなさそうだ。\n[1] $E_{0} = \\left\\{ x \\in E \\ | \\ f(x) \\ne g(x) \\right\\}$ とすると、$E_{0} \\subset E$ かつ $m(E_{0}) = 0$。任意の $c$ に対して $$ \\left\\{ x \\in E \\ | \\ g(x) \u0026gt; c \\right\\} = \\left\\{ x \\in E_{0} \\ | \\ g(x) \u0026gt; c \\right\\} \\cup \\left[ \\left\\{ x \\in E \\ | \\ f(x) \u0026gt; c \\right\\} \\cap ( E \\setminus E_{0} ) \\right] $$, 右辺の項を一つずつ見ると、$\\left\\{ x \\in E_{0} \\ | \\ g(x) \u0026gt; c \\right\\} \\subset E_{0}$ のため、 $$ \\left\\{ x \\in E_{0} \\ | \\ g(x) \u0026gt; c \\right\\} \\in \\mathcal{M} $$, $f$ は $E$ で計測可能なので、 $$ \\left\\{ x \\in E \\ | \\ f(x) \u0026gt; c \\right\\} \\in \\mathcal{M} $$, 最後に、 $$ E \\cap (\\mathbb{R} \\setminus E_{0}) = ( E \\setminus E_{0} ) \\in \\mathcal{M} $$, よって $\\left\\{ x \\in E \\ | \\ g(x) \u0026gt; c \\right\\} \\in \\mathcal{M}$ であり、$g$ は $E$ で計測可能である。\n■\n[2] $\\alpha = 0$ なら、$\\alpha f$ は計測可能で、$\\beta = 0$ なら、$\\beta g $ は計測可能である。\n$\\alpha \\ne 0$ なら、$f$ が計測可能なので、任意の $\\displaystyle {{c} \\over {\\alpha}}$ に対して $$ \\left\\{ x \\in E \\ \\left| \\ f(x) \u0026gt; {{c} \\over {\\alpha}} \\right. \\right\\} \\in \\mathcal{M} $$, ここで $\\alpha\u0026gt; 0$ なら、 $$ \\left\\{ x \\in E \\ | \\ \\alpha f(x) \u0026gt; c \\right\\} \\in \\mathcal{M} $$, そして $\\alpha \u0026lt;0$ なら、 $$ \\left\\{ x \\in E \\ | \\ \\alpha f(x) \u0026lt; c \\right\\} \\in \\mathcal{M} $$, 従って、$\\alpha f$ は計測可能であり、同じ方法で $\\beta \\ne 0$ の時に $\\beta g$ も計測可能であるとも示せる。\n今、$(f + g)$ が計測可能、つまり $\\left\\{ x \\in E \\ | \\ f(x) + g(x) \u0026lt; c \\right\\} \\in \\mathcal{M}$ を示せば、証明は終了する。両関数は有限の値を持つため、全ての $x \\in E$ に対して $f(x) + g(x) \u0026lt; c$ を満たす $c \\in \\mathbb{R}$ が存在するであろう。再び表示すると、$f(x) \u0026lt; c - g(x)$ で、有理数の密集性により、$f(x) \u0026lt; q \u0026lt; c - g(x)$ を満たす $q \\in \\mathbb{Q}$ が存在する。そうすると、 $$ \\bigcup_{q \\in \\mathbb{Q}} \\left\\{ x \\in E \\ | \\ g(x) \u0026lt; c - q \\right\\} \\cap \\left\\{ x \\in \\ | \\ E f(x) \u0026lt; q \\right\\} = \\left\\{ x \\in E \\ | \\ f(x) + g(x) \u0026lt; c \\right\\} \\in \\mathcal{M} $$\n■\nStrategy[3]**: $fg$ が計測可能であることを示すアイデアは、$\\displaystyle fg = {{1} \\over {2}} \\left[ (f+ g)^2 - f^2 - g^2\\right]$ の等式一つに要約される。\n[3] すでに [2]で、発散しない計測可能関数の和が計測可能であることを示したので、$f^2$ が計測可能であることを示せば十分である。$f$ が計測可能なので、全ての $c$ に対して $$ \\left\\{ x \\in E \\ | \\ f(x) \u0026gt; \\sqrt{c} \\right\\} \\in \\mathcal{M} \\\\ \\left\\{ x \\in E \\ | \\ f(x) \u0026lt; - \\sqrt{c} \\right\\} \\in \\mathcal{M} $$, 従って、 $$ \\left\\{ x \\in E \\ | \\ f(x) \u0026gt; \\sqrt{c} \\right\\} \\cup \\left\\{ x \\in E \\ | \\ f(x) \u0026lt; - \\sqrt{c} \\right\\} = \\left\\{ x \\in E \\ | \\ f^2 (x) \u0026gt; c \\right\\} \\in \\mathcal{M} $$\n■\n参照 ほとんど至る所での収束 $\\implies$ 測度による収束 ほとんど確実に収束 $\\implies$ 確率による収束 Capinski. (1999). Measure, Integral and Probability: p55.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":524,"permalink":"https://freshrimpsushi.github.io/jp/posts/524/","tags":null,"title":"測度論でのほとんど至る所とほとんど確実に"},{"categories":"측도론","contents":"定義 1 関数 $f: E \\in \\overline{ \\mathbb{R} }$ が全ての区間 $I \\subset \\overline{ \\mathbb{R} }$ に対して $$ f^{-1} (I) = \\left\\{ x \\in \\mathbb{R} \\ | \\ f(x) \\in I \\right\\} \\in \\mathcal{M} $$ であれば、$f$ を (ルベーグ) 可測(Lesbegue) Measurableと言う。\n$\\overline{ \\mathbb{R} } = \\mathbb{R} \\cup \\left\\{ - \\infty , + \\infty \\right\\}$ は、正負の無限大を含む $1$次元ユークリッド空間 の拡張実数空間を指す。 同値条件 以下の命題は互いに同値である。\n(1): $f$ はルベーグ可測関数である。 (2): すべての $r \\in \\mathbb{R}$ に対して $f^{-1} ( - \\infty , r ] \\in \\mathcal{M}$ (3): すべての $r \\in \\mathbb{R}$ に対して $f^{-1} (r, \\infty ) \\in \\mathcal{M}$ (4): すべての $r \\in \\mathbb{R}$ に対して $f^{-1} ( - \\infty , r ) \\in \\mathcal{M}$ (5): すべての $r \\in \\mathbb{R}$ に対して $f^{-1} [r, \\infty ) \\in \\mathcal{M}$ 定理 [1]: $f$ が可測であるための必要十分条件は、すべての開集合 $O$ に対して $f^{-1} ( O ) \\in \\mathcal{M}$ であることである。 [2]: $D \\subset E$、$D \\in \\mathcal{M}$ の時、$f |_{E}$ が可測であるための必要十分条件は、$f |_{D}$、$f |_{E \\setminus D}$ が可測であることである。 $f |_{X}$ は、定義域を $X$ に制限し、$f = f |_{X}$ を満たす縮小写像を意味する。 指示関数Indicator Functionとは、ある集合に属すれば $1$、そうでなければ $0$ を返す関数 $$\\displaystyle \\mathbb{1}_{E} (x) = \\chi _{E} (x) = \\begin{cases} 1 \u0026amp; , x \\in E \\\\ 0 \u0026amp; , x \\notin E \\end{cases}$$ である。この定義は $E \\in \\mathcal{M}$ という条件を省略しているため、注意が必要である。 説明 より容易な操作のために、原像の定義である $f^{-1} (-\\infty , r) = \\left\\{ x \\in E \\ | \\ f(x) \u0026lt; r \\right\\}$ をそのまま使用する方が便利である。\nルベーグ可測関数の条件の下で全ての区間 $I \\subset \\mathbb{R}$ が $f^{-1} (I) = \\left\\{ x \\in \\mathbb{R} \\ | \\ f(x) \\in I \\right\\} \\in \\mathcal{B}$ を満たす場合、それは ボレル可測Borel Measurableと呼ばれ、ボレル関数Borel Functionと呼ばれる。\n拡張実数 $\\overline{\\mathbb{R}} : = [ - \\infty, \\infty]$ は、実数全体と無限大も一点として含むものである。これまでの解析学で無限は非常に難しく恐ろしい概念であったが、今は単に征服すべき対象に過ぎない。あまり怖がらずに、高校時代の柔軟な思考を取り戻そう。\n一般的な可測空間を考えるとき、[1] は可測関数の定義にもなり得る。\n証明 [1] 閉区間の場合、開区間の両端に2点を加えるだけで十分であるため、開区間だけを考えれば十分である。\n$(\\Rightarrow)$\n開区間 $A_{k} := (a_{k}, \\infty)$、$B_{k} := (b_{k}, \\infty)$ を定義すると、$f$ が可測関数であるため、 $$f^{-1} (A_{k}), f^{-1} (B_{k}) \\in \\mathcal{M}$$ 任意の開集合 $O \\subset \\overline{ \\mathbb{R} }$ は $\\displaystyle O = \\bigcup_{k=1}^{\\infty} A_{k} \\cap B_{k}$ として表すことができるため、 $$\\displaystyle f^{-1} ( O ) = f^{-1} \\left[ \\bigcup_{k=1}^{\\infty} A_{k} \\cap B_{k} \\right] = \\bigcup_{k=1}^{\\infty} \\left[ f^{-1} (B_{k}) \\cap f^{-1} (B_{k}) \\right]$$ σ-フィールドの性質により $f^{-1} ( O ) \\in \\mathcal{M}$ である。\n$(\\Leftarrow)$ すべての開集合 $O \\subset \\overline{ \\mathbb{R} }$ に対して $f^{-1} ( O ) \\in \\mathcal{M}$ であるから、すべての開区間 $(a,b) \\subset \\overline{ \\mathbb{R} }$ に対しても $f^{-1} (a,b) \\in \\mathcal{M}$ である。\n可測関数の定義により、$f$ は可測関数である。\n■\nCapinski. (1999). Measure, Integral and Probability: p57.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":518,"permalink":"https://freshrimpsushi.github.io/jp/posts/518/","tags":null,"title":"ルベーグ可測関数"},{"categories":"확률론","contents":"定義 1 確率空間 $(\\Omega , \\mathcal{F} , P)$ が与えられたとしよう。\n$P(B)\u0026gt;0$ に対して、$\\displaystyle P (A | B) = {{P(A \\cap B)} \\over {P(B)}}$ を $B$ における $A$ の条件付き確率Conditional Probabilityと呼ぶ。 もし $P(A | B) = P(A)$、つまり$P( A \\cap B) = P(A) \\cdot P(B)$ ならば、$A, B$ は互いに独立Independentであるとされる。 測度論にまだ触れてなければ、確率空間という言葉は無視してもいい。 説明 確率空間がしっかりと定義されている限り、条件付き確率や事象の独立性などは高校レベルの定義をそのまま使うことができる。条件付き確率と独立性の定義自体が非常に直感的でよく定義されているため、それはある意味当然だ。これをわざわざ言及する理由は、「測度論を導入したからといって変わることはない」という点と、独立する変数、条件付き期待値との対比を強調するためだ。\n条件付き確率の変形として、次の二つの法則を得る。これらは直接ベイズの定理に応用することができる。\n定理 [1] 確率の乗法定理: $$ P(A \\cap B) = P(B) P(A | B) $$ [2] 全確率の法則: $$ P(C) = \\sum_{i=1}^{k} P(C_{i}) P (C|C_{i}) $$ Capinski. (1999). Measure, Integral and Probability: p47~49.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":521,"permalink":"https://freshrimpsushi.github.io/jp/posts/521/","tags":null,"title":"事象の独立と条件付き確率"},{"categories":"상미분방정식","contents":"定義 次のように与えられた微分方程式\n$\\psi=\\psi (x,y)$\nで\n$\\psi (x,y)$\nを満たす$\\psi=\\psi (x,y)$が存在する場合、完全exact 微分方程式と言います。\n説明 与えられた微分方程式が完全微分方程式であれば、その微分方程式は$\\psi (x,y)$に対する全微分として表現できる。\n$d\\psi (x,y)=\\dfrac{\\partial \\psi }{\\partial x}dx + \\dfrac{\\partial \\psi }{\\partial y}dy$\nこのとき、$d\\psi (x,y)=\\dfrac{\\partial \\psi }{\\partial x}dx + \\dfrac{\\partial \\psi }{\\partial y}dy$ であるから、$d\\psi (x,y)=0$ である。したがって、\n$d\\psi (x,y)=0$\nつまり、微分方程式の解が$y=y(x)$形の陽関数としてではなく、$\\psi (x,y)=C$形の陰関数として表される。一方、与えられた微分方程式が完全かどうかは、以下の定理に従って判別できる。\n定理 関数 $M,\\ N,\\ M_{y},\\ N_{x}$が連続であるとしよう。下付き添え字は該当変数に対する偏微分を意味する。すると、微分方程式\n$y=y(x)$\nが完全であることは、\n$\\psi (x,y)=C$\nであることと同値である。\n$M,\\ N,\\ M_{y},\\ N_{x}$\n証明 $(\\implies)$ $M(x,y)dx+N(x,y)dy=0$が完全であれば、定義により、次を満たす$\\psi$が存在する。\n$(\\implies)$\nそれぞれ$y, x$に対して偏微分すると、次のようになる。 $M(x,y)dx+N(x,y)dy=0$\n連続性の仮定により、次が成り立つ。\n$\\psi$\nしたがって、\n$y, x$\nすなわち、\n$(\\impliedby)$\n■\n$(\\impliedby)$ $M_{y}=N_{x}$と仮定しよう。そして、次を満たす$\\psi (x,y)$があるとしよう。\n$M_{y}=N_{x}$\nすると、$\\psi (x,y)$が$\\psi_{y}=N$を満たしていることを示せば、証明は完了する。$\\eqref{eq1}$の両辺を$x$に対して積分すると、\n$\\psi (x,y)$\n$\\psi$が$x,y$に対する二変数関数であるため、積分定数が$C$ではなく$y$に対する関数$h(y)$であることに注意しよう。$h(y)$を$x$に対して微分すると$0$である。これで、$\\eqref{eq2}$の両辺を再度$y$に対して微分すると、\n$\\psi (x,y)$\nこの式を$h^{\\prime}(y)$に対して整理すると、\n$\\psi_{y}=N$\nこの式をよく見ると、左辺は純粋に$y$に対する関数である。したがって、右辺もそうであるという意味で、これは右辺を$x$に対して微分すると$0$であるということと同じだ。右辺を$x$に対して微分すると、\n$\\eqref{eq1}$\n三番目の等号は、$M_{y}=N_{x}$という仮定によって成り立つ。$N=N(x,y)$であり、$N$に関わらず$0$でなければならないので、最後の行の括弧内は$0$と同じである。したがって、\n$x$\nしたがって、$M_{y}=N_{x}$であれば、$\\psi_{x}=M \\ \\mathrm{and}\\ \\psi_{y}=N$を満たす$\\psi (x,y)$が存在するので、与えられた微分方程式は完全である。\n■\n参照 完全微分方程式の解き方 ","id":516,"permalink":"https://freshrimpsushi.github.io/jp/posts/516/","tags":null,"title":"完全微分方程式の定義と判別法"},{"categories":"통계적검정","contents":"定義 1 帰無仮説が真だにも関わらず、検定で帰無仮説を棄却する誤りを第一種の過誤と言う。 対立仮説が真だにも関わらず、検定で帰無仮説を棄却できない誤りを第二種の過誤と言う。 第一種の過誤を犯す確率の最大値を有意水準Significance Levelと言う。 仮説検定を行うために使用する統計量を検定統計量Test Statisticと言う。 帰無仮説を棄却する検定統計量の観測値の領域を棄却域Rejection Regionと言う。 説明 いくらデータが山のように積もっていて、洗練された数学的手法を適用したとしても、使えなければ意味がない。ここで「使う」とは、何らかのデータに対して統計を取り、その統計を根拠に何らかの「主張をする」ことである。そのためには、当然その統計が信頼できる必要があるし、誰が何の基準で判断するのかという問いの答えが仮説検定である。\n例 上のデータを江北高校 理科3年生の1組から15組までの中間試験平均点としよう。一目で、15組の平均が格段に高いことがわかり、標準化をするとさらにはっきりする。しかし、順位をつけたり全体の平均より高かったり低かったりするのは簡単だが、どれだけよくなったり悪くなったりするかは判断しづらい。「平均のちょっとの差」で誰が上かを言っても、どんぐりの背比べに過ぎないというものだ。明らかにある程度以上から「クラスが違う」と言えるはずだが、その程度が曖昧である。\nここでZ-scoreが自由度$14$ のt-分布に従うと考えよう。\n$t_{14}$ の確率密度関数と平均の分布を一緒に表示すると、上の図のようになる。Z-scoreの平均は$0$で、Z-scoreが$0$に近いことは、それだけ元のデータが「平均から離れていない」という意味である。一方で、$0$から遠く離れたZ-scoreの元のデータは、高いか低いかにかかわらず、平均と似ているとは言い難いだろう。\n黄色で塗られた面積は両方を合わせて$\\color{red}{0.05}$で、これはデータがその区間に現れる確率が$\\color{red}{0.05}$であることを示している。ここに属するデータは理論的に$\\color{red}{5 \\%}$の確率の非常に珍しいケースであり、たまたま出てきたとは言い難いほどの大きな差がある。こうして平均と異なりつつも、点数が高い場合、たまたまではなく、実力自体が優れていると言えるのではないだろうか。\n再び例に戻ると、15組は単にたまたま試験でよくできたとは見られないほど平均が異常に高い。ここで帰無仮説$H_{0}$が「15組の平均は3年生全体の平均と大きな違いはない」とすれば、帰無仮説を棄却することができるだろう。このとき、黄色く塗られた領域が「棄却する領域」であるため、棄却域と呼ばれる。そして、その領域を定めるときの面積が「どの程度から意味があるかの度合い」であるため、有意確率と呼ばれる。一言で仮説検定は「ただの偶然とは言い難い」という言葉を統計学的に裏付けることであり、その判断は棄却域に入ったか入らなかったかであり、その基準は有意水準である。\n少なくとも棄却域と有意水準については、その正確な定義そのものよりも、その概念をしっかりと身につけることが重要である。実際に見ることもあまりなく、使わないからと軽視すると、本当に必要で、基本としてすぐに思い出さなければならないときに思い出せなくなる。\nR コード 以下は、本投稿に使用されたRのコードである。\nset.seed(150421);\ravg\u0026lt;-signif(6*rnorm(15)+60,3); names(avg)\u0026lt;-paste0(\u0026#39;(\u0026#39;,(1:15),\u0026#39;)\u0026#39;); avg\rZ = scale(avg)[,1]; Z\rwin.graph()\rplot(0,0,type=\u0026#39;n\u0026#39;,xlim=c(-4,4),ylim=c(-0.08,0.4),xlab=\u0026#39;Z-score\\\u0026#39;,ylab=\u0026#39;t\u0026#39;,main=\u0026#39;중간고사 결과\u0026#39;)\rabline(h=0)\rlines(seq(-5,5,0.01),dt(seq(-5,5,0.01),df=14))\rpoints(x=Z,y=rep(0,15),pch=16)\rtext(x=Z,-0.05,labels=paste0(\u0026#39;(\u0026#39;,(1:15),\u0026#39;)\u0026#39;))\rarrows(Z,-0.04,Z,-0.005,length=0.1)\rwin.graph()\rplot(0,0,type=\u0026#39;n\u0026#39;,xlim=c(-4,4),ylim=c(-0.08,0.4),xlab=\u0026#39;Z-score\\\u0026#39;,ylab=\u0026#39;t\u0026#39;,main=\u0026#39;중간고사 결과\u0026#39;)\rpolygon(c(seq(qt(0.975,14),5,0.01),qt(0.975,14)),\rc(dt(seq(qt(0.975,14),5,0.01),df=14),0),\rcol=\u0026#39;yellow\u0026#39;,lty=0)\rpolygon(c(seq(-5,qt(0.025,14),0.01),qt(0.025,14)),\rc(dt(seq(-5,qt(0.025,14),0.01),df=14),0),\rcol=\u0026#39;yellow\u0026#39;,lty=0)\rabline(h=0)\rlines(seq(-5,5,0.01),dt(seq(-5,5,0.01),df=14))\rwin.graph()\rplot(0,0,type=\u0026#39;n\u0026#39;,xlim=c(-4,4),ylim=c(-0.08,0.4),xlab=\u0026#39;Z-score\\\u0026#39;,ylab=\u0026#39;t\u0026#39;,main=\u0026#39;중간고사 결과\u0026#39;)\rpolygon(c(seq(qt(0.975,14),5,0.01),qt(0.975,14)),\rc(dt(seq(qt(0.975,14),5,0.01),df=14),0),\rcol=\u0026#39;yellow\u0026#39;,lty=0)\rpolygon(c(seq(-5,qt(0.025,14),0.01),qt(0.025,14)),\rc(dt(seq(-5,qt(0.025,14),0.01),df=14),0),\rcol=\u0026#39;yellow\u0026#39;,lty=0)\rabline(h=0)\rlines(seq(-5,5,0.01),dt(seq(-5,5,0.01),df=14))\rpoints(x=Z,y=rep(0,15),pch=16)\rtext(x=Z,-0.05,labels=paste0(\u0026#39;(\u0026#39;,(1:15),\u0026#39;)\u0026#39;))\rarrows(Z,-0.04,Z,-0.005,length=0.1) 一緒に見る 棄却域の複雑な定義 第一種の過誤と第二種の過誤の違い 慶北大学校 統計学科. (2008). エクセルを利用した統計学: p200~201.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":509,"permalink":"https://freshrimpsushi.github.io/jp/posts/509/","tags":null,"title":"棄却域と有意水準"},{"categories":"확률론","contents":"定義 1 $\\mathcal{F}$が集合$\\Omega$のシグマ場だとしよう。\n可測集合 $E \\in \\mathcal{F}$を事象と呼ぶ。 $\\mathcal{F}$上の測度 $P : \\mathcal{F} \\to \\mathbb{R}$が$P(\\Omega) = 1$を満たすなら、$P$を確率と呼ぶ。 $( \\Omega, \\mathcal{F} , P )$を確率空間と呼ぶ。 説明 測度論の力を借りれば、確率論のさまざまな概念に対する数学的基盤を提供し、あいまいさを取り除くことができる。\n高校の課程や確率論、または数理統計学で、事象とは任意の試行で起こり得るケースだった。数理統計学では確率が全ての事象を集めた集合を定義域とする関数として定義されていたのと異なり、今では$\\mathcal{F}$の元を事象とし、標本空間という言葉はもはや使わない。シグマ場$\\mathcal{F}$は、試行が正確に何であるかについて心配することなく、全体集合$\\Omega$とそれに関する形式的な代数体系としてのみ定義される。したがって、誰が、何を、どう話すかによって生じうるあいまいさは存在しない。 確率は標本空間が定義域であり、$[0,1]$が値域であり、確率の加法則を満たす関数であった。測度論で再定義された確率の概念は、「任意の試行」や「ケースの数」などの言葉さえ許容しない。測度の定義を考えれば、このような確率の定義は、既に馴染み深い確率の概念を完全にカバーしつつ、厳密に一般化したものである。 「確率空間」という新しい言葉をわざわざ定義したのは、今や空間$\\Omega$自体を$P$として捉えようという意図があるからだ。基礎的な数理統計学でのように、$\\Omega = \\mathbb{R}$ならば$\\mathcal{F}$はボレルシグマ場 $\\mathcal{B}$となり、$(\\Omega , \\mathcal{F})$を論じる意味はない。あまりにも簡単すぎるということであり、言い換えれば、応用できる範囲が限られているということだ。測度論の導入によって、確率の世界は、広大な一般化の段階に入る。しっかり勉強するつもりなら、この$\\Omega$がどれだけ驚異的に与えられるかに注意が必要だ。 参照 数理統計学で定義された確率 Capinski. (1999). Measure, Integral and Probability: p46.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":498,"permalink":"https://freshrimpsushi.github.io/jp/posts/498/","tags":null,"title":"測度論で定義される確率"},{"categories":"수리물리","contents":"公式 $\\mathbf{A} = A_{x}\\hat{\\mathbf{x}} + A_{y}\\hat{\\mathbf{y}} + A_{z}\\hat{\\mathbf{z}}, \\mathbf{B} = B_{x}\\hat{\\mathbf{x}} + B_{y}\\hat{\\mathbf{y}} + B_{z}\\hat{\\mathbf{z}}$を3次元直交座標系でのベクトルとしよう。$n$を任意のスカラーとする。すると、次の式が成立する。\n(a) $\\dfrac{ d \\left( n \\mathbf{A} \\right) }{dt} = \\dfrac{ dn }{dt} \\mathbf{A} + n\\dfrac{ d\\mathbf{A}}{dt}$\n(b) $\\dfrac{ d ( \\mathbf{A} \\cdot \\mathbf{B} )}{dt} = \\dfrac{ \\mathbf{A} }{dt} \\cdot \\mathbf{B} + \\mathbf{A} \\cdot \\dfrac{ d\\mathbf{B}}{dt}$\n(c) $\\dfrac{ d ( \\mathbf{A} \\times \\mathbf{B}) }{dt} = \\dfrac{ d \\mathbf{A} } {dt} \\times \\mathbf{B} + \\mathbf{A} \\times \\dfrac{ d \\mathbf{B} } {dt}$\n説明 高校生の頃から知っている積の微分法を思い出すと、結果を自然に受け入れることができるだろう。まず$\\mathbf{A}$の導関数を計算してみよう。\n$$ \\begin{align*} \\dfrac{ d \\mathbf{A} } { dt } =\u0026amp; \\dfrac{d}{dt} \\left(A_{x}\\hat{\\mathbf{x}} + A_{y}\\hat{\\mathbf{y}} + A_{z}\\hat{\\mathbf{z}} \\right) \\\\ =\u0026amp; \\dfrac{ d A_{x} }{dt} \\hat{\\mathbf{x}} + A_{x} \\dfrac{d \\hat{\\mathbf{x}}}{dt} + \\dfrac{ d A_{y} }{dt} \\hat{\\mathbf{y}} + A_{y} \\dfrac{d \\hat{\\mathbf{y}}}{dt} + \\dfrac{ d A_{z} }{dt} \\hat{\\mathbf{z}} + A_{z} \\dfrac{d \\hat{\\mathbf{z}}}{dt} \\\\ =\u0026amp; \\dfrac{ d A_{x} }{dt} \\hat{\\mathbf{x}} + \\dfrac{ d A_{y} }{dt} \\hat{\\mathbf{y}} + \\dfrac{ d A_{z} }{dt} \\hat{\\mathbf{z}} \\\\ =\u0026amp; \\left( \\dfrac{ d A_{x} }{dt}, \\dfrac{ d A_{y} }{dt}, \\dfrac{ d A_{z} }{dt} \\right) \\end{align*} $$\n各方向の単位ベクトルは時間によって変わらないため、微分した時に$\\mathbf{0}$である。この結果からベクトル関数の導関数もやはりベクトル関数であることがわかる。また、次の式も成立することがわかる。\n$$ \\left( \\dfrac{ d \\mathbf{A} } {dt} \\right)_{x}=\\dfrac{ d A_{x}}{ dt} $$\nしかし、この式は一般的に成立するわけではなく、直交座標系でのみ成立するので注意が必要である。単位ベクトルが時間によって変わる場合には成立しない。例えば極座標系での速度と加速度は次のようである。\n$$ \\begin{align*} \\mathbf{v}=\u0026amp;\\dot{r} \\hat{\\mathbf{r}} + r \\dot{\\theta} \\hat{\\boldsymbol{\\theta}} \\\\ \\mathbf{a}=\u0026amp; (\\ddot r -r\\dot{\\theta} ^2)\\hat{\\mathbf{r}} + (2\\dot{r} \\dot{\\theta} + r\\ddot{\\theta})\\hat{\\boldsymbol{\\theta}} \\end{align*} $$\nすると、以下の等式が成立しないことがわかる。\n$$ \\left( \\dfrac{d \\mathbf{v}}{dt} \\right)_{\\theta} = a_{\\theta} = 2\\dot{r} \\dot{\\theta} + r\\ddot{\\theta} \\ne \\dot{r}\\dot{\\theta} + r\\ddot{\\theta} = \\dfrac{d v_{\\theta}}{dt} $$\n証明 (a) $$ \\begin{align*} \\dfrac{ d (n\\mathbf{A}) }{ dt} =\u0026amp; \\dfrac{ d}{dt} (nA_{x}\\hat{\\mathbf{x}} + nA_{y}\\hat{\\mathbf{y}} + nA_{z} \\hat{\\mathbf{z}}) \\\\ =\u0026amp; \\left( \\dfrac{dn}{dt}A_{x} + n\\dfrac{dA_{x}}{dt} \\right) \\hat{\\mathbf{x}} +\\left( \\dfrac{dn}{dt}A_{y} + n \\dfrac{dA_{y}}{dt} \\right) \\hat{\\mathbf{y}} + \\left( \\dfrac{ dn }{ dt } A_{z} + n \\dfrac{ d A_{z} }{dt } \\right) \\hat{\\mathbf{z}} \\\\ =\u0026amp; \\dfrac{dn}{dt} \\left( A_{x} \\hat{\\mathbf{x}} + A_{y} \\hat{\\mathbf{y}} + A_{z} \\hat{\\mathbf{z}}\\right) + n \\left( \\dfrac{ dA_{x}}{dt}\\hat{\\mathbf{x}} + \\dfrac{dA_{y}}{dt}\\hat{\\mathbf{y}} + \\dfrac{dA_{z}}{dt} \\hat{\\mathbf{z}} \\right) \\\\ =\u0026amp; \\dfrac{ dn } {dt } \\mathbf{A} + n\\dfrac{ d \\mathbf{A} }{ dt } \\end{align*} $$\n■\n(b) $$ \\begin{align*} \\dfrac{ d (\\mathbf{A} \\cdot \\mathbf{B})} {dt} =\u0026amp; \\dfrac{d}{dt}\\left( A_{x}B_{x} + A_{y}B_{y} + A_{z}B_{z} \\right) \\\\ =\u0026amp; \\left( {\\color{blue}\\dfrac{ d A_{x}}{dt}B_{x}} + A_{x}\\dfrac{ d B_{x}}{dt} \\right) + \\left( {\\color{blue}\\dfrac{ dA_{y}}{dt}B_{y} } + A_{y}\\dfrac{d B_{y}}{dt} \\right) + \\left( {\\color{blue} \\dfrac{ d A_{z}}{dt}B_{z} }+ A_{z}\\dfrac{dB_{z}}{dt}\\right) \\\\ =\u0026amp; \\left( {\\color{blue}\\dfrac{ d A_{x}}{dt}B_{x} + \\dfrac{ dA_{y}}{dt}B_{y} + \\dfrac{ d A_{z}}{dt}B_{z} } \\right) + \\left( A_{x}\\dfrac{ d B_{x}}{dt} + A_{y}\\dfrac{d B_{y}}{dt} + A_{z}\\dfrac{dB_{z}}{dt} \\right) \\\\ =\u0026amp; \\left[ \\left( \\dfrac{ d \\mathbf{A}}{dt}\\right)_{x}B_{x} + \\left(\\dfrac{ d\\mathbf{A}}{dt}\\right)_{y}B_{y} + \\left( \\dfrac{ d \\mathbf{A}}{dt}\\right)_{z}B_{z} \\right] + \\left[ A_{x} \\left( \\dfrac{ d \\mathbf{B}}{dt} \\right)_{x} + A_{y} \\left( \\dfrac{d \\mathbf{B}}{dt}\\right)_{y} + A_{z} \\left( \\dfrac{d \\mathbf{B} }{dt} \\right)_{z}\\right] \\\\ =\u0026amp;\\dfrac{d\\mathbf{A}}{dt}\\cdot \\mathbf{B} + \\mathbf{A}\\cdot \\dfrac{d\\mathbf{B}}{dt} \\end{align*} $$\n■\n(c) $$ \\begin{align*} \\dfrac{d( \\mathbf{A} \\times \\mathbf{B}) }{ dt } =\u0026amp; \\dfrac{ d}{dt} \\left[ \\hat{\\mathbf{x}} (A_{y}B_{z}-A_{z}B_{y})+ \\hat{\\mathbf{y}} (A_{z}B_{x}-A_{x}B_{z}) + \\hat{\\mathbf{z}} (A_{x}B_{y}-A_{y}B_{x}) \\right] \\\\ =\u0026amp; \\hat{\\mathbf{x}} \\left( {\\color{blue} \\dfrac{ dA_{y}}{dt}B_{z} } +A_{y}\\dfrac{d B_{z}}{dt} {\\color{blue} -\\dfrac{d A_{z}}{dt}B_{y} } –A_{z}\\dfrac{B_{y}}{dt} \\right) + \\hat{\\mathbf{y}} \\left( {\\color{blue} \\dfrac{ dA_{z}}{dt}B_{x} } +A_{z}\\dfrac{d B_{x}}{dt} {\\color{blue} -\\dfrac{d A_{x}}{dt}B_{z} } –A_{x}\\dfrac{B_{z}}{dt} \\right) \\\\ \u0026amp;\\quad + \\hat{\\mathbf{z}} \\left( {\\color{blue} \\dfrac{ dA_{x}}{dt}B_{y} } +A_{x}\\dfrac{d B_{y}}{dt} {\\color{blue} -\\dfrac{d A_{y}}{dt}B_{x} } –A_{y}\\dfrac{B_{x}}{dt} \\right) \\\\ =\u0026amp; {\\color{blue} \\left[ {\\color{black} \\hat{\\mathbf{x}} \\left( \\dfrac{ dA_{y}}{dt}B_{z}-\\dfrac{d A_{z}}{dt}B_{y}\\right) + \\hat{\\mathbf{y}} \\left( \\dfrac{ dA_{z}}{dt}B_{x}-\\dfrac{d A_{x}}{dt}B_{z} \\right) + \\hat{\\mathbf{z}} \\left( \\dfrac{ dA_{x}}{dt}B_{y}-\\dfrac{d A_{y}}{dt}B_{x} \\right)} \\right] } \\\\ \u0026amp;\\quad + \\left[ \\hat{\\mathbf{x}} \\left( A_{y}\\dfrac{d B_{z}}{dt}-A_{z}\\dfrac{d B_{y}}{dt} \\right) + \\hat{\\mathbf{y}} \\left( A_{z}\\dfrac{d B_{x}}{dt}-A_{x}\\dfrac{d B_{z}}{dt} \\right) + \\hat{\\mathbf{z}} \\left( A_{x}\\dfrac{d B_{y}}{dt}-A_{y}\\dfrac{d B_{x}}{dt} \\right) \\right] \\\\ =\u0026amp; \\dfrac{d\\mathbf{A}}{dt} \\times \\mathbf{B} + \\mathbf{A} \\times \\dfrac{d \\mathbf{B}}{dt} \\end{align*} $$\n■\n","id":506,"permalink":"https://freshrimpsushi.github.io/jp/posts/506/","tags":null,"title":"デカルト座標系におけるベクトル、内積、外積の微分"},{"categories":"고전역학","contents":"運動エネルギー1 力が位置にのみ依存する時、つまり速度や時間に対して独立している時、粒子の直線運動の運動方程式（微分方程式）は以下のようになる。\n$$ \\begin{equation} F(x)=m\\ddot{x} \\label{force1} \\end{equation} $$\nこの場合、加速度$\\ddot{x}$を速度に対して以下のように表現できる。\n$$ \\begin{align*} \\ddot{x} \u0026amp;= \\dfrac{d \\dot{x}}{dt} \\\\ \u0026amp;=\\dfrac{dv}{dt} \\\\ \u0026amp;=\\dfrac{dv}{dx} \\dfrac{dx}{dt} \\\\ \u0026amp;=v\\dfrac{dv}{dx} \\\\ \u0026amp;= \\frac{1}{2}\\frac{ d (v^{2})}{ dx } \\end{align*} $$\nこれを$\\eqref{force1}$に代入すると、\n$$ F(x)=m\\ddot{x}= m\\frac{1}{2}\\frac{d(v^{2})}{dx}=\\frac{ d }{ dx }\\left( \\frac{1}{2}mv^{2} \\right) $$\n上記の式の括弧内の物理量を粒子の運動エネルギーと定義し、$T$と表記しよう。\n$$ T=\\dfrac{1}{2}mv^2 $$\nすると、運動方程式$\\eqref{force1}$は以下のように表される。\n$$ F(x)=\\dfrac{dT}{dx} $$\n運動エネルギーの記号は、kineticの最初の文字を取って$K$や$E_{K}$とされることがある。\nポテンシャルエネルギー ここで、関数$V(x)$を以下のように定義しよう。\n$$ -\\dfrac{dV(x)}{dx}=F(x) $$\n上記の式で定義される関数$V(x)$をポテンシャルエネルギーと呼ぶ2。すると、運動エネルギーと同じように以下の式で表せる。\n$$ -\\frac{ d V(x)}{ d x}=F(x)=\\frac{ d T}{ d x} $$\n上記の式を最初の位置$x_{0}$から後の位置$x_{1}$まで積分すると、以下のようになる。\n$$ -V(x_{1}) +V(x_{0}) =T_{1}-T_{0} $$\nこの式が意味するのは、物体が運動している間にポテンシャルエネルギーの変化量と運動エネルギーの変化量が大きさは同じで符号は逆であるということだ。つまり、一方が増加すると、もう一方が同じ大きさだけ減少する。これは、両者の合計が常に一定であるという意味だ。だから、両者の合計を粒子の総エネルギーや力学的エネルギーとし、$E$と表記しよう。\n$$ E=T_{0}+V(x_{0})=T_{1}+V_{x_{1}} $$\nこの式をエネルギー方程式と呼ぶ。上で見たように、力が位置に依存する関数、ポテンシャルエネルギー$V(x)$から得られる場合には、粒子の力学的エネルギーが保存されるため、その力を保存力と呼ぶ。位置に依存するポテンシャルエネルギーが存在しない場合、つまり保存力ではない場合は非保存力と呼ぶ。非保存力が物体に作用する場合、物体の力学的エネルギーは保存されない。\nGrant R. Fowles and George L. Cassiday, Analytical Mechanics (7th Edition, 2005), p63-64\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n라떼는 위치 에너지라고 불렀다. 정확하게 말하자면 \u0026lsquo;위치 에너지=중력의 퍼텐셜 에너지\u0026rsquo;이다.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":507,"permalink":"https://freshrimpsushi.github.io/jp/posts/507/","tags":null,"title":"物理学における運動エネルギーとポテンシャルエネルギーの定義"},{"categories":"선형대수","contents":"定義1 $S=\\left\\{ f_{1}, f_{2}, \\dots, f_{n} \\right\\}$が$n-1$回まで微分可能な関数の集合であるとする。このロンスキアンWronskian $W$を以下のような行列式で定義する。\n$$ W(x) = W(f_{1}, f_{2}, \\dots, f_{n}) := \\begin{vmatrix} f_{1} \u0026amp; f_{2} \u0026amp; \\cdots \u0026amp; f_{n} \\\\ f_{1}^{\\prime} \u0026amp; f_2^{\\prime} \u0026amp; \\cdots \u0026amp; f_{n}^{\\prime} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ f_{1}^{(n-1)} \u0026amp; f_{2}^{(n-1)} \u0026amp; \\cdots \u0026amp; f_{n}^{(n-1)} \\end{vmatrix} $$\n説明 微分可能な関数の集合もまた、ベクトル空間となる関数空間である。関数の集合の線形独立/従属を判断する一般的な方法はないが、微分可能な関数についてはロンスキアンを使用して線形独立性を知ることができる。\n以下で紹介する定理で最も重要な点は、命題が必要十分でないことである。逆が成立しないことをしっかり理解する必要がある。$W(x) \\ne 0$であれば線形独立であるとわかるが、$W(x)=0$のときは独立か従属か判断できない。\n定理 $S$を定義と同じ集合とする。$S$のロンスキアンが$0$ではない場合、$S$は線形独立である。\n証明 対偶法で証明する。つまり$S$が線形従属であればロンスキアン$W$は常に$0$であることを示す。\n$S=\\left\\{ f_{1},\\ f_{2},\\ \\cdots,\\ f_{n} \\right\\}$を線形従属と仮定する。すると定義により下の等式を満たす$0$ではない$k_{i}(i=1,2,\\dots,n)$が存在する。\n$$ \\begin{equation} k_{1} f_{1} + k_{2} f_{2} + \\cdots + k_{n} f_{n} = 0 \\label{eq1} \\end{equation} $$\n上の式を微分すると次のようになる。\n$$ \\begin{align*} k_{1} f_{1}^{\\prime} + k_{2} f_{2}^{\\prime} + \\cdots + k_{n} f_{n}\u0026rsquo;\u0026amp;=0 \\\\ k_{1} f_{1}^{(2)} + k_{2} f_{2}^{(2)} + \\cdots + k_{n} f_{n}^{(2)}\u0026amp;=0 \\\\ \\vdots\u0026amp; \\\\ k_{1} f_{1}^{(n-1)} + k_{2} f_{2}^{(n-1)} + \\cdots + k_{n} f_{n}^{(n-1)} \u0026amp;=0 \\end{align*} $$\nこの連立方程式を行列表現に変えると以下のようになる。\n$$ \\begin{align*} \\begin{pmatrix} f_{1} \u0026amp; f_{2} \u0026amp; \\cdots \u0026amp; f_{n} \\\\ f_{1}^{\\prime} \u0026amp; f_2^{\\prime} \u0026amp; \\cdots \u0026amp; f_{n}^{\\prime} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ f_{1}^{(n-1)} \u0026amp; f_{2}^{(n-1)} \u0026amp; \\cdots \u0026amp; f_{n}^{(n-1)} \\end{pmatrix} \\begin{pmatrix} k_{1} \\\\ k_{2} \\\\ \\vdots \\\\ k_{n} \\end{pmatrix} \u0026amp;= \\begin{pmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix} \\\\ \\mathbf{F} \\mathbf{k} \u0026amp;= \\mathbf{0} \\end{align*} $$\nこのとき上の式は非自明解 $\\mathbf{k} \\ne \\mathbf{0}$を持つ。それゆえに同値条件によって$\\mathbf{F}$は可逆行列ではなく、行列式は$0$である。$\\mathbf{F}$の行列式はロンスキアンなので\n$$ W(x) = 0,\\quad \\forall x \\in \\mathbb{R}. $$\nしたがって$S$が線形従属であればロンスキアン$W$は常に$0$である。\n■\nHoward Anton, Elementary Linear Algebra: Aplications Version (12th Edition, 2019), p234-235\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":501,"permalink":"https://freshrimpsushi.github.io/jp/posts/501/","tags":null,"title":"ヴロンスキアンの定義と線形独立の判断"},{"categories":"상미분방정식","contents":"定義1 1次微分方程式が下記の条件を満たす時、分離可能と言う。\n$$ f(x)+g(y)\\dfrac{dy}{dx}=0 \\quad \\text{or} \\quad f(x)dx = -g(y)dy $$\n説明 色々な形で表現できるが、大事な点は両辺に各変数が分離されなければならないということだ。このように二つの変数を分けて解を見つける方法を変数分離法と呼ぶ。\n分離可能とは非常に良い条件で、与えられた微分方程式が分離可能なら、解を簡単に見つけることができる。一方で変数分離がされない場合は、色々な方法を通じて分離可能な形にする。つまり、1次微分方程式を解く方法は色々あるが、その本質は結局変数分離であるということだ。\n解答 $$ \\begin{align*} \u0026amp;\u0026amp; g(y)\\dfrac{dy}{dx} + f(x)\u0026amp;=0 \\\\ \\implies \u0026amp;\u0026amp; g(y)\\dfrac{dy}{dx} \u0026amp;=-f(x) \\\\ \\implies \u0026amp;\u0026amp; g(y)dy \u0026amp;=-f(x)dx \\\\ \\implies \u0026amp;\u0026amp; \\int g(y)dy\u0026amp; =-\\int f(x)dx+C \\end{align*} $$\nこの時$C$は積分定数だ。積分後、左辺を$y$に対して整理すれば良い。\n■\n例 $\\dfrac{dy}{dx}+y=0$の一般解を求めよ。\n$$ \\begin{align*} \u0026amp;\u0026amp;\\dfrac{dy}{dx}\u0026amp; =-y \\\\ \\implies \u0026amp;\u0026amp; \\dfrac{1}{y}dy\u0026amp;=-dx \\\\ \\implies \u0026amp;\u0026amp; \\int \\dfrac{1}{y} dy \u0026amp;=-\\int dx \\\\ \\implies \u0026amp;\u0026amp;\\ln y \u0026amp;=-x+C \\\\ \\implies \u0026amp;\u0026amp; y\u0026amp;=e^{-x+C}=e^{-x}e^C=Ce^{-x} \\end{align*} $$ 初期値が$y(0)=y_{0}$ならば$y(0)=C=y_{0}$を得るので\n$$ y(x)=y_{0}e^{-x} $$\n■\n核の放射性崩壊 放射性核が単位時間当たりに崩壊する個数は核の個数$N$に比例する。\n$$ \\dfrac{dN}{dt}=-\\lambda N $$\nここで$\\lambda$は崩壊定数といる。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\dfrac{dN}{dt} \u0026amp;=-\\lambda N \\\\ \\implies \u0026amp;\u0026amp; \\dfrac{1}{N}dN\u0026amp;=-\\lambda dt \\\\ \\implies \u0026amp;\u0026amp; \\int \\dfrac{1}{N}dN \u0026amp;=-\\int \\lambda dt \\\\ \\implies \u0026amp;\u0026amp; \\ln N \u0026amp;=-\\lambda t+C \\\\ \\implies \u0026amp;\u0026amp; N\u0026amp;=Ce^{-\\lambda t} \\end{align*} $$\n初期値が$N(0)=N_{0}$ならば$N(0)=C=N_{0}$を得るので\n$$ N(t)=N_{0}e^{-\\lambda t} $$\n■\nWilliam E. Boyce, Boyce\u0026rsquo;s Elementary Differential Equations and Boundary Value Problems (11th Edition, 2017), p33-37\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":503,"permalink":"https://freshrimpsushi.github.io/jp/posts/503/","tags":null,"title":"分離可能な一階微分方程式"},{"categories":"측도론","contents":"定義 1 $E \\in \\mathcal{M}$に関して、関数$m : \\mathcal{M} \\to [0,\\infty]$を$m(E) := m^{ \\ast } (E)$のように定義しよう。$m$を**(ルベーグ)測度**という。\n$\\mathcal{M}$は$X = \\mathbb{R}$の可測集合の集合であるシグマ代数だ。 $m^{\\ast}$は外測度だ。 説明 外測度は$m^{ \\ast } : \\mathscr{P}( \\mathbb{R} ) \\to [0, \\infty]$によってきれいに定義されたが、長さの一般化としては物足りなかった。その代わり実数のシグマ-フィールドに定義域を制限することで、理想的な\u0026rsquo;長さの一般化\u0026rsquo;を完成させた。これはカラテオドリ条件を満たすために条件的に一歩下がると見ることができる。\nもちろん、一般的な測度と比較してみると、$X = \\mathbb{R}$での特別な例である。\n基本的な性質 $A, B, E \\in \\mathcal{M}$とすべての$n \\in \\mathbb{N}$に対して$A_{n}, B_{n}, \\in \\mathcal{M}$とする。測度は以下の性質を持つ。\n[1]: $$A \\subset B \\implies m(A) \\le m(B)$$ [2]: $A \\subset B$の場合、$$m(A) \u0026lt; \\infty \\implies m(B \\setminus A) = m(B) - m(A)$$ [3]: $$t \\in \\mathbb{R} \\implies m(E) = m(E + t)$$ [4]: $$m(A \\triangle B) = 0 \\implies B \\in \\mathcal{M} \\\\ m(A) = m(B)$$ [5]: すべての$\\varepsilon \u0026gt; 0, A \\subset \\mathbb{R}$に対して、以下を満たす開集合の$O$が存在する。 $$ A \\subset O \\\\ m(O) \\le m^{ \\ast }(A) + \\varepsilon $$ [6]: すべての$A \\subset \\mathbb{R}$に対して、以下を満たす開集合の数列$\\left\\{ O_{n} \\right\\}$が存在する。 $$ A \\subset \\bigcap_{n} O_{n} \\\\ m \\left( \\bigcap_{n} O_{n} \\right) = m^{ \\ast }(A) $$ [7]: $$\\displaystyle A_{n} \\subset A_{n+1} \\implies m \\left( \\bigcup_{n=1}^{\\infty} A_{n} \\right) = \\lim_{n \\to \\infty} m (A_{n})$$ [8]: $A_{n+1} \\subset A_{n}$の場合、$$\\displaystyle m(A_{1}) \u0026lt; \\infty \\implies m \\left( \\bigcap_{n=1}^{\\infty} A_{n} \\right) = \\lim_{n \\to \\infty} m (A_{n})$$ [9]: $$\\displaystyle m \\left( \\bigsqcup_{i=1}^{n} A_{i} \\right) = \\sum_{i = 1}^{n} m (A_{i})$$ [11]: $$B_{n} \\to \\emptyset \\implies m(B_{n}) \\to 0$$ $A \\triangle B = ( A \\setminus B ) \\cup ( B \\setminus A )$が真である。 証明 [1] $m = m^{ \\ast } |_{\\mathcal{M}}$より、外測度の性質から自然に導かれる。\n■\n[2] まず、$(B \\setminus A) \\in \\mathcal{M}$を証明する必要がある。$(B \\setminus A) = B \\cap (\\mathbb{R} \\setminus A) = B \\cap A^{c}$であり、$A \\in \\mathcal{M}$であるから$A^{c} \\in \\mathcal{M}$だ。したがって、$(B \\setminus A) \\in \\mathcal{M}$であり、$(B \\setminus A ) \\cap A = \\emptyset$かつ$(B \\setminus A ) \\cup A = B$であるから$m(B \\setminus A ) + m(A) = m(B)$となる。仮定から$m(A) \u0026lt; \\infty$であったので、両辺すると$m(B \\setminus A) = m(B) - m(A)$を得る。\n■\n[3] $m = m^{ \\ast } |_{\\mathcal{M}}$より、外測度の性質から自然に導かれる。\n■\n[4] $B = (A \\cap B) \\cup (B \\setminus A) = A \\setminus (A \\setminus B) \\cup (B \\setminus A)$より、$B \\in \\mathcal{M}$である。一方で$m(A \\triangle B) = 0$より、$m(A \\setminus B) = 0$かつ$m(B \\setminus A) = 0$だ。したがって、 $$ m(B) = m( B \\setminus A) + m(B \\cap A) = m( A \\setminus B) + m(A \\cap B) = m(A) $$\n■\n[7] $B_{n} :=A_{n} \\setminus A_{n-1}$とすると、$i \\ne j$に対して$B_{i} \\cap B_{j} = \\emptyset$かつ$\\displaystyle \\bigcup_{n=1}^{\\infty} A_{n} = \\bigsqcup_{n=1}^{\\infty} B_{n}$である。したがって、 $$ m \\left( \\bigcup_{n=1}^{\\infty} A_{n} \\right) = m \\left( \\bigsqcup_{n=1}^{\\infty} B_{n} \\right) = \\sum_{n=1}^{\\infty} m(B_{n}) = \\lim_{n \\to \\infty} \\sum_{k=1}^{n} m(B_{k}) = \\lim_{n \\to \\infty} m \\left( \\bigsqcup_{k=1}^{n} B_{k} \\right) = \\lim_{n \\to \\infty} m \\left( A_{n} \\right) $$\n■\n[8] $(A_{1} \\setminus A_{n} ) \\subset (A_{1} \\setminus A_{n+1} )$より、**[7]**によって $$ m \\left( \\bigcup_{n=1}^{\\infty} ( A_{1} \\setminus A_{n} ) \\right) = \\lim_{ n \\to \\infty} m (A_{1} \\setminus A_{n}) $$ $m(A_{n}) \u0026lt; \\infty$より、[3]によって $$ m (A_{1} \\setminus A_{n}) = m(A_{1}) - m(A_{n}) $$ 一方、$\\displaystyle \\bigcup_{n=1}^{\\infty} (A_{1} \\setminus A_{n}) = A_{1} \\setminus \\bigcap_{n=1}^{\\infty} A_{n}$より、 $$ m \\left( \\bigcup_{n=1}^{\\infty} (A_{1} \\setminus A_{n}) \\right) = m \\left( A_{1} \\right) - m \\left( \\bigcap_{n=1}^{\\infty} A_{n} \\right) $$ 整理すると、 $$ m \\left( \\bigcup_{n=1}^{\\infty} (A_{1} \\setminus A_{n}) \\right) = m(A_{1}) - \\lim_{n \\to \\infty} m(A_{n}) = m \\left( A_{1} \\right) - m \\left( \\bigcap_{n=1}^{\\infty} A_{n} \\right) $$ したがって$\\displaystyle \\lim_{n \\to \\infty} m(A_{n}) = m \\left( \\bigcap_{n=1}^{\\infty} A_{n} \\right)$\n■\n一般化 測度の一般化 Capinski. (1999)。Measure, Integral and Probability: p35.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":494,"permalink":"https://freshrimpsushi.github.io/jp/posts/494/","tags":null,"title":"ルベーグ測度"},{"categories":"측도론","contents":"定義 集合$X \\ne \\emptyset$に対して、以下の条件を満たす$\\mathcal{E} \\subset \\mathscr{P} (X)$を$X$上のシグマ代数またはシグマ場という。集合$X$とシグマ場$\\mathcal{E}$の順序対$(X , \\mathcal{E})$を可測空間と呼ぶ。\n(i): $\\emptyset \\in \\mathcal{E}$ (ii): $E \\in \\mathcal{E} \\implies E^{c} \\in \\mathcal{E}$ (iii): $\\displaystyle \\left\\{ E_{n} \\right\\}_{n \\in \\mathbb{N}} \\subset \\mathcal{E} \\implies \\bigcup_{n=1}^{\\infty} E_{n} \\in \\mathcal{E}$ (iv): $\\displaystyle \\left\\{ E_{n} \\right\\}_{n \\in \\mathbb{N}} \\subset \\mathcal{E} \\implies \\bigcap_{n=1}^{\\infty} E_{n} \\in \\mathcal{E}$ 説明 ある空間$X$に対してシグマ場$\\mathcal{E}$が与えられた場合、$(X , \\mathcal{E})$を可測空間と呼ぶ。測度$\\mu$が与えられた場合は測度空間と呼び、特に測度$\\mu$が確率である場合は確率空間と呼ぶ。\n同じ概念だが、数学ではシグマ代数、統計学ではシグマ場と呼ばれることに注意。\nカラテオドリの条件: $E \\subset \\mathbb{R}$が$A \\subset \\mathbb{R}$に対して$m^{ \\ast }(A) = m^{ \\ast } ( A \\cap E ) + m^{ \\ast } ( A \\cap E^{c} )$を満たす場合、$E$を可測集合と呼び、$E \\in \\mathcal{M}$と表記する。\n\u0026lsquo;可測集合\u0026rsquo;は、文字通り測ることができる集合を意味する。外測度の単調性から $$m^{ \\ast }(A) \\le m^{ \\ast } ( A \\cap E ) + m^{ \\ast } ( A \\cap E^{c} )$$ は自明であるので、ある集合が可測かどうかを確認することは、 $$m^{ \\ast }(A) \\ge m^{ \\ast } ( A \\cap E ) + m^{ \\ast } ( A \\cap E^{c} )$$ であるかを確認することと同じである。\n可測集合の集合のシグマ代数 上記の定義から、$X = \\mathbb{R}$の可測集合の集合である$\\mathcal{M}$は、以下の性質を持つシグマ代数となる。\n$\\mathcal{M}$は、以下の性質を持つシグマ代数である。\n[1]: $$ \\emptyset \\in \\mathcal{M} $$ [2]: $$ E \\in \\mathcal{M} \\implies E^{c} \\in \\mathcal{M} $$ [3]: $$ \\left\\{ E_{n} \\right\\}_{n \\in \\mathbb{N}} \\subset \\mathcal{M} \\implies \\bigcup_{n=1}^{\\infty} E_{n} \\in \\mathcal{M} $$ [4]: $$ \\left\\{ E_{n} \\right\\}_{n \\in \\mathbb{N}} \\subset \\mathcal{M} \\implies \\bigcap_{n=1}^{\\infty} E_{n} \\in \\mathcal{M} $$ [5]: $$ \\mathcal{N} \\subset \\mathcal{M} $$ [6]: $$ \\mathcal{I} \\subset \\mathcal{M} $$ [7]: $E_{i} , E_{j} \\in \\mathcal{M}$とすると、以下が成立する。 $$ E_{i} \\cap E_{j} = \\emptyset , \\forall i \\ne j \\implies m^{ \\ast } \\left( \\bigcup_{n=1}^{\\infty} E_{n} \\right) = \\sum_{n = 1} ^{\\infty} m^{ \\ast } ( E_{n}) $$ $\\mathcal{I}$はすべての区間の集合、$\\mathcal{N}$はすべての零集合の集合である。 特に[7]は、ルベーグが夢見た「長さの一般化」に絶対に必要な性質であることに注目してください。\n","id":490,"permalink":"https://freshrimpsushi.github.io/jp/posts/490/","tags":null,"title":"シグマ代数と可測空間"},{"categories":"위상수학","contents":"定義 1 位相空間 $\\left( X, \\mathscr{T} \\right)$ に対して $A \\subset X$ としよう。\n$X$ の開集合からなる集合 $\\mathscr{O} \\subset \\mathscr{T}$ が次を満たす場合、$\\mathscr{O}$ を $A$ の開被覆Open Coveringという。 $$ A \\subset \\bigcup_{O \\in \\mathscr{O}} O $$ $\\mathscr{O} ' \\subset \\mathscr{O}$ である $\\mathscr{O} ' $ を $\\mathscr{O}$ の部分被覆Subcoverという。特に $\\mathscr{O} ' $ の基数が自然数である場合は、有限部分被覆Finite Subcoverという。 $X$ の全ての開被覆が有限部分被覆を持つ場合、$X$ はコンパクトであるという。言い換えると、全ての開被覆 $\\mathscr{O}$ に対して、次を満たす有限集合 $\\mathscr{O} ' = \\left\\{ O_{1} , \\cdots , O_{n} \\right\\} \\subset \\mathscr{O}$ が存在する場合、$X$ はコンパクトである。 $$ X = \\bigcup_{i=1}^{n} O_{i} $$ $A$ が $X$ の部分空間としてコンパクトである場合、$A$ をコンパクトであるという。 位相空間 $X$ とする。部分集合 $K \\subset X$ の閉包 $\\overline{K}$ がコンパクトである場合、$K$ はプリコンパクトである、あるいは相対的にコンパクトrelatively compactであると言う。 説明 コンパクト 解析概論でコンパクトという条件がどれほど有用であったかを考えれば、その一般化を追求することは当然と言えるだろう。一般化されると、言葉は少し難しくなるが、本質的な部分は変わらない。\n実際に、コンパクトはさまざまな理論で非常に重要な応用を持つ。ある集合がコンパクトであるということは、有限の部分に分割して考えることができるということであり、厳密性が要求される証明では良い条件となる。逆に言えば、ある定理を証明する際に現れる集合 $A$ が本当にコンパクトであるかを示すことが鍵となる場合が多い。\nプリコンパクト プリコンパクトは、$K$ 自体はコンパクトではないが、$K$ に閉包を取るとコンパクトになるという点で、「まだコンパクトではないが、すぐにコンパクトになり得る」という概念をよく表している。距離空間では完全有界空間とも呼ばれ、別名相対的コンパクトは、閉じられた性質が相対的なものから来ていることを表す表現である。$K$ を $X$ の部分空間ではなく、それ自体で全体空間とした場合、$K$ は $K$ で閉じているため、$K = \\overline{K}$ であり、したがって$\\overline{K}$ がコンパクトであるということは、$K$ が（相対的に）コンパクトであるということになる。\n一方、数列によるプリコンパクトの定義も可能である。その定義は次のようである：\n$K \\subset X$ がプリコンパクトであるとは、$K$ で定義された全ての数列 $\\left\\{ x_{n} \\right\\} \\subset K$ に対して、$x \\in X$ に収束する部分数列 $\\left\\{ x_{n '} \\right\\} \\subset \\left\\{ x_{n} \\right\\}$ が存在することである。\n数式で再表現すると次のようになる：\n$$ K : \\text{precompact} \\iff \\forall \\left\\{ x_{n} \\right\\} \\subset K, \\exists \\left\\{ x_{n '} \\right\\} \\subset \\left\\{ x_{n} \\right\\} : x_{n '} \\to x \\in X \\text{ as } n \\to \\infty $$\n特に条件で $x \\in X$ ではなく $x \\in K$ の場合、$K$ を点列コンパクトSequentially Compactと呼ぶ。\n定理 [1]: $A$ がコンパクトであることは、$A$ の全ての開被覆が有限部分被覆を持つことと同値である。 [2]: コンパクト集合 $K$ の部分集合 $F$ が閉集合である場合、$F$ はコンパクト集合である。 [3]: $X$ がコンパクトであることは、$X$ の閉集合のみを含む全ての集合族が有限交差性を持ち、それを単に交差させても空集合にならないことと同値である。 証明 [1] $\\Gamma$ は指標集合である。\n$( \\implies )$\n$A \\subset X$ がコンパクトであり、$\\mathscr{U} := \\left\\{ U_{\\alpha} : \\alpha \\in \\Gamma \\right\\}$ が $A$ の開被覆であるとする。すると、$U_{\\alpha} \\cap A$ は $X$ の部分空間 $A$ で開集合であり、$\\mathscr{O} := \\left\\{ U_{\\alpha} \\cap A : U_{\\alpha} \\in \\mathscr{U} \\right\\}$ は $A$ の開被覆となる。$A$ はコンパクトであるため、$\\displaystyle A \\subset \\bigcup_{i=1}^{n} \\left( U_{\\alpha_{i}} \\cap A \\right)$ を満たす $\\alpha_{1} , \\cdots , \\alpha_{n} \\in \\Gamma$ が存在する。したがって、$\\left\\{ U_{\\alpha_{1}} , \\cdots , U_{\\alpha_{n}} \\right\\}$ が $\\mathscr{U}$ の有限部分被覆として存在することが確認できる。\n$( \\impliedby )$\n$A$ で開集合からなる開被覆 $\\mathscr{O} := \\left\\{ O_{\\alpha} : O_{\\alpha} \\text{ is open in } A, \\alpha \\in \\Gamma \\right\\}$ を考える。$O_{\\alpha}$ が $A$ で開集合であるため、各 $\\alpha \\in \\Gamma$ に対して $U_{\\alpha} \\cap A = O_{\\alpha}$ を満たす開集合 $U_{\\alpha}$ が存在する。これらの集合 $\\mathscr{U} := \\left\\{ U_{\\alpha} : \\alpha \\in \\Gamma \\right\\}$ は $A$ の開被覆である。全ての開被覆が有限部分被覆 $\\left\\{ U_{\\alpha_{1}} , \\cdots , U_{\\alpha_{n}} \\right\\}$ を持つという仮定から、$\\left\\{ O_{\\alpha_{1}} , \\cdots , O_{\\alpha_{n}} \\right\\}$ は $\\mathscr{O}$ の有限部分被覆となる。\n■\n[2] $$ F \\subset K \\subset X $$ $F$ が $X$ で閉集合であり、$K$ がコンパクトであるとする。$F$ は閉集合であるため、$F^{c}$ は $X$ で開集合であり、$K \\subset F^{c}$ であるため、$F^{c} \\cup \\left\\{ U_{\\alpha} \\right\\}$ は $K$ の開被覆の一つとなり、$K$ がコンパクトであるため、$F \\subset K \\subset \\Phi$ を満たす $F^{c}\\cup \\left\\{ U_{\\alpha} \\right\\}$ の有限部分被覆 $\\Phi$ が存在する。\nもし $F^{c}\\notin \\Phi$ であれば、$\\Phi$ は $\\left\\{ U_{\\alpha} \\right\\}$ の有限部分被覆となるため、$F$ はコンパクトである。 もし $F^{c}\\in \\Phi$ であれば、$\\Phi \\setminus \\left\\{ F^{c} \\right\\}$ が $\\left\\{ U_{\\alpha} \\right\\}$ の有限部分被覆となるため、$F$ はコンパクトである。 どちらの場合も、$F$ はコンパクトであるため、$F$ はコンパクトである。\n■\n[3] 戦略：言葉が非常に複雑なので、言葉を理解することが鍵である。$\\mathscr{C}$ が有限交差性を持つとしても、$\\displaystyle \\bigcap_{C \\in \\mathscr{C}} C \\ne \\emptyset$ が保証されるわけではなく、コンパクトという条件が必要である。一方で、コンパクトの定義では開集合の和集合を考慮しており、この定理では閉集合の交差を考慮している点に注意する必要がある。これらの考察から、有限交差性がどのようにコンパクトと関連しているのかを感じ取り、証明に入る必要がある。\n$\\Gamma$ は指標集合である。\n有限交差性: $X$ の部分集合からなる集合族 $\\mathscr{A} \\subset \\mathscr{P}(X)$ が有限交差性(f.i.p, finite intersection property)を持つとは、$\\mathscr{A}$ の全ての有限部分集合 $A \\subset \\mathscr{A}$ が交差を取ったときに空集合でないことである。数式で表すと、次のようになる。 $$ \\forall A \\subset \\mathscr{A}, \\bigcap_{a \\in A} a \\ne \\emptyset $$\n$( \\implies )$\n$X$ がコンパクトであり、$\\mathscr{C} := \\left\\{ C_{\\alpha} : C_{\\alpha} \\text{ is closed in } X, \\alpha \\in \\Gamma \\right\\}$ が有限交差性を持つとする。ここで、$\\displaystyle \\bigcap_{\\alpha \\in \\Gamma} C_{\\alpha} = \\emptyset$ と仮定し、$\\mathscr{O} := \\left\\{ X \\setminus C_{\\alpha} : C_{\\alpha} \\in \\mathscr{C} \\right\\}$ を選ぶ。すると、 $$ \\begin{align*} \\bigcup_{\\alpha \\in \\Gamma} ( X \\setminus C_{\\alpha}) =\u0026amp; X \\setminus \\bigcap_{\\alpha \\in \\Gamma} C_{\\alpha} \\\\ =\u0026amp; X \\setminus \\emptyset \\\\ =\u0026amp; X \\end{align*} $$ となるため、$\\mathscr{O}$ は $X$ の開被覆となる。$X$ がコンパクトであるため、$\\mathscr{O}$ は有限部分被覆 $\\displaystyle \\left\\{ (X \\setminus C_{\\alpha_{1}}) , \\cdots ,(X \\setminus C_{\\alpha_{n}}) \\right\\}$ を持つ。これは、つまり $$ X = \\bigcup_{i=1}^{n} ( X \\setminus C_{\\alpha_{i}}) = X \\setminus \\bigcap_{i=1}^{n} C_{\\alpha_{i}} $$ となり、$\\displaystyle \\bigcap_{i=1}^{n} C_{\\alpha_{i}} = \\emptyset$ であることを意味する。これは、$\\mathscr{C}$ が有限交差性を持つという仮定に矛盾する。したがって、$\\displaystyle \\bigcap_{C \\in \\mathscr{C}} C \\ne \\emptyset$ でなければならない。\n$( \\impliedby )$\n$X$ の開被覆 $\\mathscr{O} := \\left\\{ O_{\\alpha} : O_{\\alpha} \\text{ is open in } A, \\alpha \\in \\Gamma \\right\\}$ と $\\mathscr{C} := \\left\\{ X \\setminus O_{\\alpha} : O_{\\alpha} \\in \\mathscr{O} \\right\\}$ を考える。 $$ \\begin{align*} \\bigcap_{\\alpha \\in \\Gamma} C_{\\alpha} \u0026amp;= \\bigcap_{\\alpha \\in \\Gamma} ( X \\setminus O_{\\alpha}) \\\\ =\u0026amp; X \\setminus \\bigcup_{\\alpha \\in \\Gamma} O_{\\alpha} \\\\ =\u0026amp; X \\setminus X \\\\ =\u0026amp; \\emptyset \\end{align*} $$ となるため、対偶により、$\\mathscr{C}$ は有限交差性を持たない。これは、言い換えると、$\\displaystyle \\bigcap_{i=1}^{n} C_{\\alpha_{i}} = \\emptyset$ を満たす $C_{\\alpha_{1}} , \\cdots , C_{\\alpha_{n}} \\in \\mathscr{C}$ が存在することを意味する。すると、\n$$ \\begin{align*} X \\setminus \\bigcup_{i=1}^{n} O_{i} =\u0026amp; X \\setminus \\bigcup_{i=1}^{n} (X \\setminus C_{i}) \\\\ =\u0026amp; X \\setminus \\left( X \\setminus \\bigcap_{i=1}^{n} C_{i} \\right) \\\\ =\u0026amp; \\bigcap_{i=1}^{n} C_{i} \\\\ =\u0026amp; \\emptyset \\end{align*} $$ となるため、$\\displaystyle X = \\bigcup_{i=1}^{n} O_{i}$ である。つまり、開被覆 $\\mathscr{O}$ に対して有限部分被覆が存在するため、コンパクトである。\n■\n関連項目 距離空間におけるコンパクト Munkres. (2000). Topology(2nd Edition): p164.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":489,"permalink":"https://freshrimpsushi.github.io/jp/posts/489/","tags":null,"title":"位相空間におけるコンパクトとプレコンパクトとは？"},{"categories":"상미분방정식","contents":"説明 微分方程式を分類する基準はいくつかある。大きく常微分方程式か偏微分方程式かで分けられる。その次に係数と次数、線形/非線形でさらに細かく分類することができる。微分方程式を分類する理由は明らかに微分方程式を解くためである。微分方程式の分類によって解法も異なる。\n常微分方程式と偏微分方程式 常微分方程式は一つかそれ以上の従属変数を一つの独立変数で微分した導関数のみを含む微分方程式を指す。通常ODEOrdinary Differential Equationと略される。\n$$ \\begin{align*} \\dfrac{dy}{dx}\u0026amp;=2y-1 \\\\ \\dfrac{d^2y}{dx^2}+3\\dfrac{dy}{dx}-2y \u0026amp;=0 \\\\ \\dfrac{dy}{dt}+\\dfrac{dx}{dt}\u0026amp;=2c \\end{align*} $$\n偏微分方程式は一つかそれ以上の従属変数を二つ以上の独立変数で微分した導関数を含む微分方程式である。簡単に言うと偏導関数を含む微分方程式だ。PDEPartial Differential Equationと略され、$u=u(x,t)$とすると、\n$$ \\begin{align*} \\dfrac{\\partial u}{\\partial x}-\\dfrac{\\partial u }{\\partial t} =0 \\\\ \\dfrac{\\partial^2 u }{\\partial x^2}=\\dfrac{1}{c^2} \\dfrac{\\partial^2 u}{\\partial t^2} \\end{align*} $$\n係数と次数 係数orderと次数degreeを区別せずに次数と呼ぶことが多いが、明確に異なる。意味がまったく変わってくるので、用語を正しく使う必要がある。2階導関数を2次導関数と言わないことを覚えておこう。英語で言うと最も確実で、実際にオーダーという言葉をよく使う。\n微分方程式で係数は最大の微分回数を指す。赤色で表示された項が微分方程式の係数を決める。\n$$ \\begin{align} x^2 {\\color{red} \\dfrac{dy}{dx} }+y\u0026amp;=0 \\label{eq1} \\\\ {\\color{red}\\dfrac{d^2u}{dx^2}}+2 \\left( \\dfrac{dy}{dx} \\right) ^3\u0026amp;=5x \\label{eq2} \\end{align} $$\n$(1)$は1階微分方程式、$(2)$は2階微分方程式である。微分方程式で次数は最高係数項の累乗回数を指す。赤色で表示された項が微分方程式の次数を決める。\n$$ \\begin{align} x^2 \\left(\\dfrac{d^{\\color{blue}1}y}{dx^{\\color{blue}1}} \\right)^{\\color{red}1} + y \u0026amp; =0 \\label{eq3} \\\\ \\left( \\dfrac{d^{\\color{blue}3}y}{dx^{\\color{blue}3}} \\right)^{\\color{red}2} + x^2\\dfrac{dy}{dx}\u0026amp;=0 \\label{eq4} \\\\ \\left( \\dfrac{d^{\\color{blue}2}y}{dx^{\\color{blue}2}} \\right)^{\\color{red}3} + \\left( \\dfrac{dy}{dx} \\right)^5+x^2y\u0026amp;=0 \\label{eq5} \\end{align} $$\n$(3)$は$\\color{blue}1$階$\\color{red}1$次、$(4)$は$\\color{blue}3$階$\\color{red}2$次、$(5)$は$\\color{blue}2$階$\\color{red}3$次の微分式だ。\n線形と非線形 以下のような形の微分方程式があるとき、$\\mathrm{n}$階線形微分方程式と言う。\n$$ a_{n}(x)\\dfrac{d^ny}{dx^n}+a_{n-1}(x)\\dfrac{d^{n-1}y}{dx^{n-1}}+ \\cdots + a_{1}(x)\\dfrac{dy}{dx}+a_{0}(x)y=f(x) $$\n各項の係数が独立変数$x$にのみ依存していれば線形である。つまり、上のような微分方程式を$L(y)$という関数で表現でき、$L$が線形関数であれば、$L$で表される微分方程式を線形と言う。\n$$ x \\dfrac{dy}{dx} $$\n係数が従属変数$y$に依存する項が一つでもあれば非線形である。\n$$ L(y) = y\\dfrac{dy}{dx}\\\\ \\implies L(y+Y) = (y+Y)\\left( \\dfrac{dy}{dx} + \\dfrac{dY}{dx} \\right) \\ne y\\dfrac{dy}{dx} + Y\\dfrac{dY}{dx}=L(y) + L(Y) $$\n同次と非同次 同次（非同次）と言うこともあるが、同次（非同次）という言葉をより多く使う。次のような微分方程式が与えられたとする。 $$ a_{n}(x)\\dfrac{d^ny}{dx^n}+a_{n-1}(x)\\dfrac{d^{n-1}y}{dx^{n-1}}+ \\cdots + a_{1}(x)\\dfrac{dy}{dx}+a_{0}(x)y=f(x) $$\n$f(x)=0$であれば同次homogeneous、$f(x) \\ne 0$であれば非同次nonhomogenous, inhomogenousと言う。当然のことながら、同次微分方程式の方がずっと解きやすい。\n","id":483,"permalink":"https://freshrimpsushi.github.io/jp/posts/483/","tags":null,"title":"微分方程式の分類"},{"categories":"위상수학","contents":"定義 1 $X$ を位相空間と呼び、$C \\subset \\mathbb{R}^{n}$ だとしよう。\n連続関数 $p : [0,1] \\to X$ を 始点 $p(0)$ から 終点 $p(1)$ への 経路 とする。$\\overline{p}(t) = p(1-t)$ を $p$ の 逆経路 という。 すべての $a,b \\in X$ に対して、$p(0) = a$ と $p(1) = b$ を満たす経路 $p$ が存在する場合、$X$を 経路連結 空間という。 すべての $a,b \\in C$ と $t \\in [0,1]$ に対して、$(1-t) a + t b \\in C$ ならば 凸 であるという。 $p(0) = p(1)$ ならば 閉経路 という。 説明 簡単に言うと、ある空間の任意の二点を結ぶ経路が常に存在する場合、それを経路連結と呼ぶ。\n$X$ は非連結空間である $\\iff$ 連続関数 $f : X \\to \\left\\{ a, b \\right\\}$ が存在する。 $X$ は経路連結空間である $\\iff$ 連続関数 $p : [0,1] \\to X$ が存在する。 非連結空間と経路連結空間は、ある連続関数が特定の条件を満たしているかによって区別すると考えると理解しやすい。もちろん、上記の命題には多くの詳細が省略されているので、そのまま受け取らない方が良い。\n凸性 凸性の概念は、ユークリッド空間の部分集合でのみ定義される必要はなく、ベクトル空間の部分空間でも定義される。幾何学的に言えば、凸であるとは$C$ の任意の二点を結ぶ直線が常に$C$ 内に存在することである。\n例えば、上記の二つの図形を見ると、青の円は任意の二点を直線で結ぶことが可能なので凸である。オレンジの図形は内部に$a$ と $b$ を結ぶ直線が存在しないため、凸ではない。\n連結性 一方で、経路連結空間の定義をよく見ると、実質的に連結空間と変わらないように見える。実際に、次の定理はそれほど難しくなく証明でき、これら二つを区別することは無意味に思えるかもしれない。しかし、連結と経路連結は確かに異なる概念であり、定理の逆が成り立たない反例が存在するためだ。逆が成り立つケースには、$\\mathbb{R}$ の凸部分空間や開連結部分空間がある。\n定理: 経路連結空間であれば、連結空間である。 証明 経路連結空間 $X$ について $X = \\emptyset$ であれば、$X$ は連結空間である。$X \\ne \\emptyset$ であれば、ある点 $a \\in X$ を選べる。すると、任意の $x \\in X$ に対して、$p_{x} (0) = a$、$p_{x} (1) = x$ を満たす連続関数 $p_{x} : [0,1] \\to X$ が存在する。\n連結空間の連続像は連結空間 連結空間 $X$ に対して、$f : X \\to Y$ が連続関数であれば、$f(X)$ は連結空間である。\n$[0,1]$ が連結であるので、$p_{x} ( [0,1] )$ は連結であり、$\\displaystyle a \\in \\bigcap_{x \\in X} p_{x} ( [0, 1] )$ なので $\\displaystyle \\bigcap_{x \\in X} p_{x} ( [0, 1] ) \\ne \\emptyset$\n(3) $X$ の連結部分空間の集合 $\\left\\{ A_{\\alpha} \\ | \\ \\alpha \\in \\forall \\right\\}$ に対して、$\\displaystyle \\bigcap_{\\alpha \\in \\forall} A_{\\alpha} \\ne \\emptyset$ であれば、$\\displaystyle \\bigcup_{\\alpha \\in \\forall} A_{\\alpha}$ は連結空間である。\nしたがって、$\\displaystyle X = \\bigcup_{x \\in X} p_{x} ( [0, 1] )$ は連結空間である。\n■\n参照 ベクトル空間で一般的に定義された凸セット Munkres. (2000). Topology(2nd Edition): p155.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":478,"permalink":"https://freshrimpsushi.github.io/jp/posts/478/","tags":null,"title":"位相数学におけるパス連結성"},{"categories":"상미분방정식","contents":"定義 一つまたはそれ以上の従属変数を一つまたはそれ以上の独立変数に関して微分した導関数を含む等式を微分方程式differential equationという。\n$$ \\dfrac{dy}{dx}=y $$\n$$ \\dfrac{d^2y}{dx^2} = y $$\n説明 ほとんどの物理的な状況は、1次または2次微分方程式で表すことができる。\n落下する物体 $$ F=ma=mg $$\n$$ v=\\dfrac{dy}{dt} $$\n$$ a=\\dfrac{dv}{dt}=\\dfrac{d}{dt} \\left( \\dfrac{dy}{dt} \\right)=\\dfrac{d^2y}{dt^2} $$\n$$ \\dfrac{d^2y}{dt^2}=g $$\nスプリング質量系 $$ F=ma=-ky $$\n$$ a= -\\dfrac{k}{m}y $$\n$$ \\dfrac{d^2y}{dt^2}=-\\dfrac{k}{m}y $$\n$$ \\dfrac{d^2y}{dt^2}+\\dfrac{k}{m}y=0 $$\nこの時$ w^2=\\dfrac{k}{m}$とすると、\n$$ \\dfrac{d^2y}{dt^2}+w^2y=0 $$\nRLC回路 $$ L\\dfrac{d^2q}{dt^2}+R\\dfrac{dq}{dt}+\\dfrac{1}{c}q=V(t) $$\nシュレディンガー方程式 $$ i\\hbar \\dfrac{\\partial \\psi}{\\partial t}=-\\dfrac{\\hbar^2 }{2m} \\dfrac{\\partial^2 \\psi}{\\partial x^2} + u(x)\\psi $$\n","id":479,"permalink":"https://freshrimpsushi.github.io/jp/posts/479/","tags":null,"title":"微分方程式の定義と例"},{"categories":"위상수학","contents":"定義 1 $f : [a,b] \\to \\mathbb{R}$ が連続ならば、$f(a)$ と $f(b)$ の間にある $y_{0}$ に対して $y_{0} = f(c)$ を満たす $c \\in (a,b)$ が存在する。\n説明 対偶を利用すれば、$\\mathbb{R}^2$ において特定の条件を満たした二つの形を結ぶ曲線がないことを示すことができる。\n系 一方で、中間値の定理には次のように多くの有用な系がある。\n方程式 $f(x)=0$ の解の存在判定法：連続関数 $f:[a,b] \\to \\mathbb{R}$ において $f(a) f(b) \u0026lt; 0$ ならば $f(x) = 0$ は解 $x_{0} \\in [a,b]$ を持つ。\nこの事実は入試数学や数値解析の二分法などにも使われるなど非常に重要である。\n中間値の定理の不動点定理フォーム：$[a,b]$ と $f[a,b]$ が包含関係にある場合、連続関数 $f$ は $[a,b]$ で不動点を持つ。 中間値の定理を利用すれば、簡単に使える不動点定理を得ることができる。条件を数式で表すと $[a,b] \\subset f[a,b]$ または $f[a,b] \\subset [a,b]$ で、当然ながら $f[a,b] = [a,b]$ を同時に満たす場合も成立する。通常、不動点定理と言うときは $f[a,b] \\subset [a,b]$ が条件なので $[a,b] \\subset f[a,b]$ の時も成立するというのが新鮮に思えるかもしれないが、少なくとも中間値の定理の系としては過度に一般的な条件を考える必要がないので、当然の事実として受け入れても良い。\n証明 戦略：位相数学的な補助定理を動員する。非常に重要な定理だが、高校では証明なしで受け入れ、解析学の時はあまりにも難しく証明される。上の理論を使わない証明もそれなりに意味はあるが、中間値の定理の位相的証明はあまりにも簡単で、その誘惑を振り払うのは容易ではない。\n連結性は位相的性質であり、$f$ が連続なので$f[a,b]$ も連結空間である。\n$f(a) \\in V^{\\circ}$ を満たすすべての $V \\subset Y$ に対して、$f(c) = y_{0}$ を満たす $c \\in (a,b)$ が存在する\n連続関数の性質により、$f(c) = y_{0}$ を満たす $c \\in (a,b)$ が存在する。\n■\nMunkres. (2000). Topology(2nd Edition): p476.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":476,"permalink":"https://freshrimpsushi.github.io/jp/posts/476/","tags":null,"title":"中間値の定理の証明"},{"categories":"추상대수","contents":"定義 1 集合$G$とその部分群$H$で$aH = \\left\\{ ah \\ | \\ h \\in H \\right\\}$が左余剰類Left Coset、$Ha = \\left\\{ ha \\ | \\ h \\in H \\right\\}$が右余剰類Right Cosetと言われる。ここで$a \\in G$、$aH, Ha \\subset G$だ。 $H \\leqslant G$の左(右)余剰類の数を$(G : H)$と書き、$G$における$H$の指数Indexと言う。 $H$が$G$の部分群であり、全ての$g \\in G$に対して$gH = Hg$が成り立つなら、$H$を$G$の正規部分群Normal Subgroupと言い、$H \\triangleleft G$で書く。 $H = \\left\\{ e \\right\\}$や$H = G$以外の$H \\triangleleft G$を持たない$G$を単純Simpleと言う。つまり、$G$が単純だとは、$\\left\\{ e \\right\\}$と$G$自体だけを正規部分群として持つことを意味する。 説明 余剰類 余剰類のアイデアは、必然的に代数学をより高い次元に導く。\n例えば、$3$の倍数だけを集めた集合$3 \\mathbb{Z} = \\left\\{ \\cdots, -6, -3, 0 , 3, 6 , \\cdots\\right\\}$は群であり、特に$\\mathbb{Z}$が可換群であるため、$3 \\mathbb{Z} \\triangleleft \\mathbb{Z}$が成立する。\nここに整数を足すと考えてみると $$ \\begin{align*} 1 + 3 \\mathbb{Z} =\u0026amp; \\left\\{ \\cdots, -5, -2, 1 , 4, 7 , \\cdots\\right\\} \\\\ 2 + 3 \\mathbb{Z} =\u0026amp; \\left\\{ \\cdots, -4, -1, 2 , 5, 8 , \\cdots\\right\\} \\\\ 3 + 3 \\mathbb{Z} =\u0026amp; \\left\\{ \\cdots, -3, 0 , 3, 6 , 9 , \\cdots\\right\\} = 3 \\mathbb{Z} \\\\ 4 + 3 \\mathbb{Z} =\u0026amp; \\left\\{ \\cdots, -2, 1 , 4, 7 , 10 , \\cdots\\right\\} = 1 + 3 \\mathbb{Z} \\\\ 5 + 3 \\mathbb{Z} =\u0026amp; \\left\\{ \\cdots, -1, 2 , 5, 8 , 11 , \\cdots\\right\\} = 2 + 3 \\mathbb{Z} \\end{align*} $$ まるで$\\pmod{3}$で整数を足すのと似た形になる。\nつまり、$\\mathbb{Z}_{3} : = \\left\\{ 3 \\mathbb{Z} , 1 + 3 \\mathbb{Z} , 2 + 3 \\mathbb{Z}\\right\\}$のように集合を要素として持つ新しい群を考えることができるということだ。このような新たに作られる群を商群と言う。初めて勉強するときはかなり理解しづらい概念だが、通常は余剰類に対する誤解がその原因だ。取るに足らないように見えて使われないように見えるが、実際には手で書きながら余剰類をしっかり理解することが後の部分を楽にする。\n指数 左と書かれているのも右と書かれているのも別に区別する必要がないからだる。本来、指数は左余剰類の数として定義されるが、実際には右余剰類と一対一の対応が存在するので、右余剰類の数として定義しても良い。2\n正規性 $gH$と$Hg$が群になるか、$gH = Hg$が成り立つかを確認するのは、一見教科書で学んだ連続の定義を思い出させる。正規Normalという言葉がついているだけに、かなり強力な条件であり、多くの便利な性質が推測できるだろう。\n定義からすぐにわかる事実としては、$G$の単位元$e$に対して$\\left\\{ e \\right\\} \\triangleleft G$がある。少し考えればわかるのは、可換群$G$に対して$H \\leqslant G$ならば、$H \\triangleleft G$程度があることだ。\n単純性 例えば、素数$p$について、$\\mathbb{Z}_{p}$は自明群や自分自身以外に部分群を持たないため、単純群となる。\n参考文献 余剰類の性質 Fraleigh. (2003). 『現代の抽象代数学』(第7版): p97, 101, 132, 149.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFraleigh. (2003). 『現代の抽象代数学』(第7版): p103\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":469,"permalink":"https://freshrimpsushi.github.io/jp/posts/469/","tags":null,"title":"抽象代数学における剰余類と正規部分群"},{"categories":"추상대수","contents":"定義 1 $\\sim$の同値類を$\\sigma$の軌道Orbitという。 二つ以上の元を持つ軌道を高々一つだけ持つ順列を循環Cycleという。 循環が持つ軌道の中で最も基数が大きい軌道の基数を循環の長さLengthという。 長さが$2$の循環を転置Transpositionという。 循環に対応する軌道が元を共有しない場合、互いに素Disjointという。 説明 定義だけでは理解できないのが普通だから、実際の例を見てみよう。\n軌道 $S_{8}$から順列 $$ \\sigma = \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; 4 \u0026amp; 5 \u0026amp; 6 \u0026amp; 7 \u0026amp; 8 \\\\ 3 \u0026amp; 8 \u0026amp; 6 \u0026amp; 7 \u0026amp; 4 \u0026amp; 1 \u0026amp; 5 \u0026amp; 2 \\end{bmatrix} $$ を考えてみよう。この表現は $$ 1 \\to 3 \\to 6 \\to 1 \\\\ 2 \\to 8 \\to 2 \\\\ 4 \\to 7 \\to 5 \\to 4 $$ を表す。だから、同値関係$\\sim$は次の三つの同値類を決定する。 $$ \\left\\{ 1, 3, 6 \\right\\} \\\\ \\left\\{ 2, 8 \\right\\} \\\\ \\left\\{ 4 , 5 , 7 \\right\\} $$\n循環 $S_{5}$から順列 $$ \\mu_{1} = \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; 4 \u0026amp; 5 \\\\ 3 \u0026amp; 2 \u0026amp; 5 \u0026amp; 1 \u0026amp; 4 \\end{bmatrix} $$ を考えてみよう。この順列は$1 \\to 3 \\to 5 \\to 4 \\to 1$で表され、変わらない$2$を除いて$(1,3,5,4)$だけで表すことができる。この表現を使うとき、順序が重要で、$(1,3,5,4) = (3,5,4,1)$だけど$(1,3,5,4) \\ne (1,5,3,4)$ではないことを注目しなければならない。また、 $$ \\mu_{2} = \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{bmatrix} $$ を考えると、$(1,2)$は$3$が存在することさえ表されないから、$S_{3}$で$(1,2)$だと明確にする必要がある。\n長さ 循環 $$ \\mu_{1} = \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; 4 \u0026amp; 5 \\\\ 3 \u0026amp; 2 \u0026amp; 5 \u0026amp; 1 \u0026amp; 4 \\end{bmatrix} $$ の軌道は $$ \\left\\{ 1,3,4,5 \\right\\} \\\\ \\left\\{ 2 \\right\\} $$ 二つだけだ。この時、$ | \\left\\{ 1,3,4,5 \\right\\} | = 4$と$| \\left\\{ 2 \\right\\} | =1 $だから、$\\mu_{1}$の長さは$4$になる。\n転置 循環 $$ \\mu_{2} = \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{bmatrix} = (1,2) $$ は長さが$2$だから、転置だ。簡単に言うと、二つの元だけを交換する循環だ。一般的に、 $$ (1,2, \\cdots , n) = (1, n) (1, n-1 ) \\cdots (1,3) (1,2) $$ で表すことができる。もし$3$を基準にしたいなら、 $$ (1,2, \\cdots , n) = (3, 4, \\cdots , n , 1, 2 ) = (3 , 2) (3, 1) \\cdots (3,4) $$ に変えればいい。とても便利な性質だから、必ず覚えておこう。\n互いに素 $$ \\sigma = \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; 4 \u0026amp; 5 \u0026amp; 6 \u0026amp; 7 \u0026amp; 8 \\\\ 3 \u0026amp; 8 \u0026amp; 6 \u0026amp; 7 \u0026amp; 4 \u0026amp; 1 \u0026amp; 5 \u0026amp; 2 \\end{bmatrix} = (1,3,6) (2,8) (4,7,5) $$ を考えてみよう。三つの循環$(1,3,6)$、$(2,8)$、そして$(4,7,5)$は、対応する軌道が元を共有しないから、互いに素だ。この表現から分かることは、 $$ (1,3,6) (2,8) (4,7,5) = (4,7,5) (2,8) (1,3,6) $$ として表すのは全く問題ないということだ。順列は循環の積として表されるし、そういう積を同じものと見なせば、軌道は一意に決まる。\nまとめ Fraleigh. (2003). A first course in abstract algebra(7th Edition): p87~90. $\\sigma$ を群$G$の順列とするとき、$a, b \\in G$の同値関係$\\sim$は、$b=\\sigma^n (a)$を満たす整数$n \\in \\mathbb{Z}$が存在するとき、$a \\sim b$と定義される。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":460,"permalink":"https://freshrimpsushi.github.io/jp/posts/460/","tags":null,"title":"抽象代数学における軌道、巡回、置換"},{"categories":"위상수학","contents":"定義 1 位相空間$X$で、$A \\cap B = \\emptyset$と$A \\cup B = X$を満たす開集合$A \\ne \\emptyset$、$B \\ne \\emptyset$が存在する場合、$X$を非連結Disconnected空間という。非連結でない場合を連結Connected空間という。\n定理 説明 非連結であることの定義はかなり直感的で、その否定、つまり連結もまた容易に受け入れられるだろう。グラフ理論でも同様に連結を定義する。\n例えば、ユークリッド空間$( \\mathbb{R} , d )$を考えると、どのような開区間を考えても非連結の条件を満たさないため、連結空間である。一方、その部分空間$( \\mathbb{Q}, d )$を考えると、$( \\mathbb{Q} , d ) = ( \\mathbb{Q} , \\mathscr{P} ( \\mathbb{Q} ) )$が離散空間であるため、簡単に非連結空間であることが示せる。\n証明 1 位相同型写像$f : X \\to Y$が存在し、$X$が連結空間であるとする。$Y$が連結空間であることを示せば証明は完了である。\n$Y$が非連結空間であると仮定すると、 $$ A \\cap B = \\emptyset \\\\ A \\cup B = Y $$ を満たす開集合$A, B \\subset Y$が存在する。\n$f$が連続関数ならば、すべての開集合$V \\subset Y$に対して、$f^{-1} (V)$が$X$で開集合である。\n$Y$は連続関数なので、$f^{-1} (A)$と$f^{-1} (B)$は$X$で開集合である。しかし、 $$ f^{-1} (A) \\cap f^{-1} (B) = f^{-1} (A \\cap B) = f^{-1} ( \\emptyset ) = \\emptyset \\\\ f^{-1} (A) \\cup f^{-1} (B) = f^{-1} (A \\cup B) = f^{-1} ( Y ) = X $$ これは、$X$が非連結空間であるという前提と矛盾する。\n■\n2 自明空間$X$の位相$\\mathscr{T} = \\left\\{ \\emptyset , X \\right\\}$では、空でない2つの開集合が存在しないため、$X$は連結空間である。\n■\n3 $X$の元が1つだけの場合、離散空間より自明空間であるが、$X$が2つ以上の元を持つと仮定しなければならない。離散空間$X$で、空でないすべての開集合$U$について$V = X \\setminus U$が$X$で開集合であるため、非連結空間である。\n■\n4 $A , B \\subset \\left\\{ x \\right\\}$が$A \\cap B = \\emptyset$を満たすためには、$A$または$B$が必ず空でなければならず、非連結空間にはなり得ない。\n■\nMunkres. (2000). 『トポロジー(第2版)』: p148.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":457,"permalink":"https://freshrimpsushi.github.io/jp/posts/457/","tags":null,"title":"位相数学における連結性"},{"categories":"위상수학","contents":"定義 1 $X$ を位相空間とする。$a,b \\in X$ について、$a \\ne b$ 且つ $U, V \\subset X$ が $X$ で開集合であるとしよう。\n$T_{0}$：$a$ と $b$ のどちらか一方だけを含む $U$ が存在する場合、$X$ をコルモゴロフKolmogorov 空間という。 $T_{1}$：任意の $a,b$ に対して $$ a \\in U, b \\notin U \\\\ a \\notin V, b \\in V $$ を満たす $U,V$ が存在する場合、$X$ をフレシェFrechet 空間という。 $T_{2}$：任意の $a,b$ に対して $$ a \\in U, b \\in V \\\\ U \\cap V = \\emptyset $$ を満たす $U,V$ が存在する場合、$X$ をハウスドルフHausdorff 空間という。 $T_{3}$：$X$ が $T_{1}$-空間であり、$a$ を含まないすべての閉集合 $C \\subset X$ に対して $$ a \\in U , C \\subset V \\\\ U \\cap V = \\emptyset $$ を満たす $U,V$ が存在する場合 $X$ を正則Regular 空間という。 $T_{4}$：$X$ が $T_{1}$-空間であり、$A \\cap B = \\emptyset$ とする二つの閉集合 $A, B \\subset X$ に対して $$ A \\subset U , B \\subset V \\\\ U \\cap V = \\emptyset $$ を満たす $U,V$ が存在する場合 $X$ を正規Normal 空間という。 説明 これらの性質は分離公理Separation Axiom とも呼ばれ、文字通り空間を部分に分けることに焦点を当てている。$T_{i}$ と表される分類はコルモゴロフ分類Kolmogorov classificationと呼ばれる。定義を見るだけで $$ T_{4} \\implies T_{3} \\implies T_{2} \\implies T_{1} \\implies T_{0} $$ という感じがするだろうし、実際にそうで、見た目が良い分類法である。\n特に、$T_{2}$、ハウスドルフ空間は、その条件が多すぎず少なすぎずで、ちょうど使いやすいレベルであることがよく関心の対象になる。反例としてよく使われる様々な奇妙な空間は大抵$T_{2}$ を満たさない場合が多い。ハウスドルフ空間でない例としては、シェルピンスキー空間と離散でない空間がある。\n以下は、ハウスドルフ空間が持ついくつかの便利な性質である。すべての距離空間がハウスドルフ空間であるため、利用する余地は多いに違いない。\n定理 [2-1]: $T_{2}$ は位相的性質である。 [2-2]: $T_{2}$ は遺伝性の性質である。 [2-3]: [$T_{2}$-空間の数列 $\\left\\{ x_{n} \\right\\}$ は収束する場合、ただ一つの点に収束する。](../456) 証明 [2-1] 位相同型写像 $f : X \\to Y$ が存在し、$X$ がハウスドルフ空間だとしよう。$Y$ がハウスドルフ空間であることを示せば証明は完了する。\n$f$ は全単射であるから、異なる二つの $y_{1}, y_{2} \\in Y$ に対して、$$ a = f(x_{1}) \\\\ b = f(x_{2}) $$ を満たしつつ異なる二つの $x_{1}, x_{2} \\in X$ が存在する。前提から$X$ はハウスドルフ空間なので、 $$ x_{1} \\in U \\\\ x_{2} \\in V \\\\ U \\cap V = \\emptyset $$ を満たす開集合 $U, V \\subset X$ が存在する。$f$ は連続性によって開いた関数なので、$f(U)$ と $f(V)$ は $Y$ で開集合であり、 $$ a \\in f(U) \\\\ b \\in f(V) \\\\ f(U) \\cap f(V) = \\emptyset $$ したがって、$Y$ はハウスドルフ空間である。\n■\nMunkres. (2000). Topology(2nd Edition): p195.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":454,"permalink":"https://freshrimpsushi.github.io/jp/posts/454/","tags":null,"title":"位相数学における分離性質"},{"categories":"함수","contents":"要約：ベータ関数で表される二項係数 $0 \\le k\\le n$が二つの自然数 $k,n$に対して満たされるとき、下記の式が成立する。 $$ \\binom{n}{k}={}_{n}C_{k}=C(n,k)=\\frac{1}{(n+1)B(n-k+1,k+1)} $$ 二つの自然数 $m,n$に対して下記の式が成立する。 $$ B(m,n)=\\left[ \\frac{mn}{m+n} \\begin{pmatrix} m+n \\\\ n \\end{pmatrix}\\right]^{-1} $$\n説明 $B(p,q):=\\displaystyle \\int_{0}^{1}t^{p-1}(1-t)^{q-1}dt$で定義されるベータ関数は、このように二項係数の一般化と見ることもできる。証明は難しくないが、まず補助定理を証明しなければならない。\n証明 補助定理 1 $$ B(p,q)=B(p+1,q) +B(p,q+1) $$\n補助定理 1 の証明 $$ \\begin{align*} B(p+1,q) + B(p,q+1) =\u0026amp; \\int_{0}^{1} t^{p-1} (1-t)^{q-1}dt + \\int_{0}^{1}t^{p-1}(1-t)^{p-1}dt \\\\ =\u0026amp; \\int_{0}^{1}t^{p-1}(1-t)^{q-1}\\big[t+(1-t) \\big]dt \\\\ =\u0026amp; \\int_{0}^{1}t^{p-1}(1-t)^{q-1}dt \\\\ =\u0026amp; B(p,q) \\end{align*} $$\n■\n補助定理 2 (a): $B(p+1,q)=\\dfrac{p}{p+q}B(p,q)$ (b): $B(p,q+1)=\\dfrac{q}{p+q}B(p,q)$ 補助定理 2 の証明 $$ \\begin{align*} B(p+1,q) =\u0026amp; \\int_{0}^{1}t^{p}(1-t)^{q-1}dt \\\\ =\u0026amp; \\left[ -\\frac{1}{q}t^{p}(1-t)^{q}\\right]_{0}^{1} + \\int_{0}^{1} \\frac{p}{q}t^{p-1}(1-t)^{q}dt \\\\ =\u0026amp; 0 + \\frac{p}{q}\\int_{0}^{1} t^{p-1}(1-t)^{q}dt \\\\ =\u0026amp; \\frac{p}{q} B(p,q+1) \\end{align*} $$ 二番目の等式で部分積分を使用した。$(a)$の補助定理 1 により、$B(p,q+1)=B(p,q)-B(p+1,q)$ため、上記の式に代入すると $$ B(p+1,q)=\\frac{p}{q}B(p,q) -\\frac{p}{q}B(p+1,q) \\\\ \\Rightarrow \\frac{q+p}{q}B(p+1,q)=\\frac{p}{q}B(p,q) \\\\ \\Rightarrow B(p+1,q)=\\frac{p}{p+q}B(p,q) $$ $(b)$の補助定理 1 により、$B(p+1,q)=B(p,q)-B(p,q+1)$ため、上記の式に代入すると $$ B(p,q)-B(p,q+1)=\\frac{p}{q}B(p,q+1) \\\\ \\Rightarrow B(p,q)=\\frac{p+q}{q}B(p,q+1) \\\\ \\Rightarrow B(p,q+1)=\\frac{q}{p+q}B(p,q) $$\n■\n本証明 まず、$B(1,1)=1$であることを示そう。定義によりすぐにわかる。 $$ B(1,1)=\\int_{0}^{1}t^{0}(1-t)^{0}dt=1-0=1 $$ $m,n \\in \\mathbb{N}$とする。 補助定理 2の $(a)$を$B(m,n)$に繰り返し適用すると $$ \\begin{align*} B(m,n) =\u0026amp; \\frac{m-1}{m+n-1}B(m-1,n) \\\\ =\u0026amp; \\frac{m-1}{m+n-1}\\cdot \\frac{m-2}{m+n-2}B(m-2,n) \\\\ =\u0026amp; \\frac{m-1}{m+n-1} \\cdot \\frac{ m-2 }{m+n-2}\\cdot \\cdots \\frac{ 1 }{m+n-(m-1) }B(1,n) \\\\ =\u0026amp; \\frac{ (m-1)! }{ (m+n-1)(m+n-2)\\cdots(n+1) }B(1,n) \\end{align*} $$ 補助定理 2の $(b)$を繰り返し適用すると $$ \\begin{align*} B(m,n) =\u0026amp; \\frac{ (m-1)! }{ (m+n-1)(m+n-2)\\cdots(n+1) }B(1,n) \\\\ =\u0026amp; \\frac{ (m-1)! }{ (m+n-1)(m+n-2)\\cdots(n+1) } \\frac{ n-1 }{ n }B(1,n-1) \\\\ =\u0026amp; \\frac{ (m-1)! }{ (m+n-1)(m+n-2)\\cdots(n+1) } \\frac{ n-1 }{ n }\\cdot \\frac{ n-2 }{ n-1 }B(1,n-2) \\\\ =\u0026amp; \\frac{ (m-1)! }{ (m+n-1)(m+n-2)\\cdots(n+1) } \\frac{ n-1 }{ n }\\cdot \\frac{ n-2 }{ n-1 }\\cdots \\frac{ 1 }{ n+1-(n-1) }B(1,1) \\\\ =\u0026amp; \\frac{ (m-1)!(n-1)! }{ (m+n-1)(m+n-2)\\cdots(n+1)n(n-1)\\cdots2 } B(1,1) \\\\ =\u0026amp; \\frac{ (m-1)!(n-1)! }{ (m+n-1)! } \\\\ =\u0026amp; \\frac{ m+n }{ mn }\\frac{ m!n! }{ (m+n)! } \\\\ =\u0026amp; \\left[ \\frac{ mn }{ m+n } \\begin{pmatrix} m+n \\\\ n \\end{pmatrix} \\right]^{-1} \\end{align*} $$ 上記の式の下から三番目の等式に、$m=n-k+1$、$n=k+1$を代入すると $$ B(n-k+1,k+1)=\\frac{(n-k)!k!}{ (n+1)! }=\\frac{ (n-k)!k! }{(n+1)n! } $$ したがって $$ \\frac{ n! }{(n-k)!k! }=\\begin{pmatrix} n \\\\ k \\end{pmatrix}=\\frac{ 1 }{ (n+1)B(n-k+1,k+1) } $$\n■\n参照 オイラー積分 ","id":450,"permalink":"https://freshrimpsushi.github.io/jp/posts/450/","tags":null,"title":"二項係数の一般化：ベータ関数"},{"categories":"확률분포론","contents":"定義 1 連続型 $[a,b] \\subset \\mathbb{R}$に対して、次のような確率密度関数を持つ連続確率分布 $U(a,b)$を一様分布という。 $$ f(x) = {{ 1 } \\over { b - a }} \\qquad , x \\in [a,b] $$\n離散型 有限集合 $\\left\\{ x_{k} \\right\\}_{k=1}^{n}$に対して、次のような確率質量関数を持つ離散確率分布を一様分布という。 $$ p \\left( x_{k} \\right) = P \\left( X = x_{k} \\right) = {{ 1 } \\over { n }} \\qquad , k = 1, \\cdots , n $$\n説明 一般に一様分布は均一分布とも呼ばれる。離散一様分布の代表的な例として$x_{k} = k$のようなサイコロがあり、この場合、数理的な性質にはあまり関心を持つ必要がないことが多い。特に断りがない限り、一様分布は連続離散分布を指す。\n一様分布が重要な理由は、特別な理由があるというより、我々が考えうる最もシンプルな分布だからである。分布理論に慣れ親しんだ統計学の学生にはあまりにも素朴に見えるかもしれないが、まだ数学や人工知能などの分野で思ったよりも広く使われている。\n情報理論 情報理論の視点では非常に重要な分布で、離散型でも連続型でもシャノンエントロピーが最大化される分布だからである。考えてみれば、他の分布は確率関数で高低があっても、一様分布はサンプルがどうなるかのヒントすらないので、当然のことである。\n離散型でエントロピーが最大化されることは、ラグランジュ乗数法にとっても良い例である。\n基本性質 モーメント生成関数 [1]: $$m(t) = {{ e^{tb} - e^{ta} } \\over { t(b-a) }}$$ 平均と分散 [2]: $X \\sim U(a,b)$なら $$ E(X) = {{ a+b } \\over { 2 }} \\\\ \\text{Var}(X) = {{ (b-a)^{2} } \\over { 12 }} $$ 十分統計量と最尤推定量 [3]: ランダムサンプル $\\mathbf{X} := \\left( X_{1} , \\cdots , X_{n} \\right) \\sim U \\left( 0 , \\theta \\right)$が与えられたとする。 $\\theta$の十分統計量 $T$と最尤推定量 $\\hat{\\theta}$は次の通りである。 $$ \\begin{align*} T =\u0026amp; \\max_{k=1 , \\cdots , n} X_{k} \\\\ \\hat{\\theta} =\u0026amp; \\max_{k=1 , \\cdots , n} X_{k} \\end{align*} $$\n証明 [1] $$ \\begin{align*} m(t) = \\int_{a}^{b} e^{tx} {{ 1 } \\over { b-a }} dx =\u0026amp; {{ 1 } \\over { b-a }} \\left[ {{ 1 } \\over { t }} e^{tx} \\right]_{a}^{b} \\\\ =\u0026amp; {{ e^{tb} - e^{ta} } \\over { t(b-a) }} \\end{align*} $$\n■\n[2] 直接演繹する。\n■\n[3] 直接演繹する。\n■\nHogg et al. (2013). Introduction to Mathematical Statistics (7th Edition): p45.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":443,"permalink":"https://freshrimpsushi.github.io/jp/posts/443/","tags":null,"title":"二項分布"},{"categories":"정수론","contents":"同じく見る 解析整数論ではのトーティエント関数 定義 1 以下のように定義された$\\phi$ を オイラーのトーティエント関数という。\n$$ \\phi ( m ) := \\left| \\left\\{ a \\ | \\ 1 \\le a \\le m \\land \\gcd (a,m) = 1 \\right\\} \\right| = m \\prod_{p \\mid m} \\left( 1 - {{1} \\over {p}} \\right) $$\n説明 トーティエントTotientは全部を意味するTot-alのTot-と商を意味するQuo-tientから-tientが付いてできた言葉と理解しても構わない。数学、それも整数論以外では全く使われないし、その中でもファイPhi関数またはフィPhi関数と呼ばれることが多い。\n$1$から$10$までを直接計算して感覚を掴もう。 $$ \\begin{align*} \\phi ( 1 ) = \u0026amp; \\left| \\left\\{ 1 \\right\\} \\right| = 1 =1 \\\\ \\phi ( 2 ) = \u0026amp; \\left| \\left\\{ 1 \\right\\} \\right| = 2 {{1} \\over {2}} = 1 \\\\ \\phi ( 3 ) = \u0026amp; \\left| \\left\\{ 1 , 2 \\right\\} \\right| = 3 {{2} \\over {3}} = 2 \\\\ \\phi ( 4 ) = \u0026amp; \\left| \\left\\{ 1 , 3 \\right\\} \\right| = 4 {{1} \\over {2}} = 2 \\\\ \\phi ( 5 ) = \u0026amp; \\left| \\left\\{ 1 , 2 , 3 , 4 \\right\\} \\right| = 5 {{4} \\over {5}} = 4 \\\\ \\phi ( 6 ) = \u0026amp; \\left| \\left\\{ 1 , 5 \\right\\} \\right| = 6 {{1} \\over {2}} {{2} \\over {3}} = 2 \\\\ \\phi ( 7 ) = \u0026amp; \\left| \\left\\{ 1 , 2 , 3 , 4 , 5 , 6 \\right\\} \\right| = 7 {{6} \\over {7}} = 6 \\\\ \\phi ( 8 ) = \u0026amp; \\left| \\left\\{ 1 , 3 , 5 , 7 \\right\\} \\right| = 8 {{1} \\over {2}} = 4 \\\\ \\phi ( 9 ) = \u0026amp; \\left| \\left\\{ 1 , 2, 4 , 5 , 7, 8, \\right\\} \\right| = 9 {{2} \\over {3}} = 6 \\\\ \\phi ( 10 ) = \u0026amp; \\left| \\left\\{ 1 , 3 , 7 , 9 \\right\\} \\right| = 10 {{1} \\over {2}} {{4} \\over {5}} = 4 \\end{align*} $$\n一見なぜこんなに計算が難しく、規則性がないように見える関数が必要なのか疑問に思うかもしれない。実は美しい性質をたくさん持っているだけでなく、様々な理論の基盤になっていることがわかるだろう。\n定理 素数$p$に対して、$\\phi (p^{k}) = p^{k} - p^{k-1}$\n証明 戦略: 素数の累乗に対する関数値はカウンティングによって簡単に求めることができる。\n$p$が素数であるため、任意の$n \\in \\mathbb{N}$に対して$\\gcd (p, np) \\ne 1$すなわち $$ p , 2p , \\cdots (p^{k-1} - 2 ) p , (p^{k-1} - 1 ) p , p^{k} \\notin \\left\\{ a \\ | \\ 1 \\le a \\le p^{k} \\land \\gcd (a,p^{k}) = 1 \\right\\} $$ これらは正確に$p^{k-1}$個存在するので、$\\phi (p^{k}) = p^{k} - p^{k-1}$\n■\nSilverman. (2012). 数論への優しい導入 (第4版): p72.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":445,"permalink":"https://freshrimpsushi.github.io/jp/posts/445/","tags":null,"title":"ねじれ関数"},{"categories":"추상대수","contents":"定義 群 $\\left\u0026lt; G , \\ast\\ \\right\u0026gt; , \\left\u0026lt; G' , *' \\right\u0026gt;$ を $\\phi : G \\to G'$ としよう。\n$\\forall x ,y \\in G $、$\\phi (x \\ast\\ y) = \\phi (x ) *' \\phi ( y)$ のとき、$\\phi$ を準同型写像Homomorphismという。 準同型写像 $\\phi$ が単射の場合、$\\phi$ を単射写像Monomorphismといい、$G \\hookrightarrow G'$ と表す。 準同型写像 $\\phi$ が全射の場合、$\\phi$ を全射写像Epimorphismといい、$G \\twoheadrightarrow G'$ と表す。 準同型写像 $\\phi$ が全単射の場合、$\\phi$ を同型写像Isomorphismといい、$G \\simeq G'$ と表す。 準同型写像 $\\phi$ において $G = G'$ の場合、$\\phi$ を準自己同型写像Endomorphismという。 同型写像 $\\phi$ において $G = G'$ の場合、$\\phi$ を自己同型写像Automorphismという。 説明 急に出てくる定義に頭が痛くなるかもしれないけど、すぐに慣れるから難しく考えないで堂々と対面しよう。\n単射写像と全射写像は任意に翻訳されたもので、日本の数学界では単にモノ射かエピ射と使用されている。抽象代数学以外では、これらがそれぞれ単射と全射そのものとして使われるが、抽象代数学では通常、準同型写像が含まれる。\n同型写像はその性質が直ちに有益であるが、それが求められる条件が難点である。そのような条件を減らすことができれば、つまり単射写像や全射写像だけで十分であれば、より良いだろう。\n","id":439,"permalink":"https://freshrimpsushi.github.io/jp/posts/439/","tags":null,"title":"In Japanese: 抽象代数学における様々な写像"},{"categories":"위상수학","contents":"定義 1 二つの位相空間 $X,Y$について、全単射 $f : X \\to Y$が存在し、$f$とその逆関数 $f^{-1}$が共に連続関数ならば、$f$を位相同型写像Homeomorphismと呼び、二つの位相空間は位相同型Homeomorphicであるという。\n定理 以下の命題は互いに等価だ。\n(1): $f : X \\to Y$は位相同型写像だ。 (2): $f^{-1} : Y \\to X$は位相同型写像だ。 (3): $f : X \\to Y$は閉関数でありながら連続の全単射だ。 (4): $f : X \\to Y$は開関数でありながら連続の全単射だ。 説明 距離空間で定義されたものと同様に、位相同型の概念も簡単に拡張できる。連続関数を学ぶ理由そのものと見ても良い。\n特に(3)、そして特に(4)が良い理由は、逆関数に対するチェックが不要なためだ。開関数と閉関数の性質から簡単に推論されて、逆関数が連続でなければならない条件を代わりに満たしてくれる。\n特に、$f,f^{-1}$が微分可能ならば、微分同型写像Diffeomorphism, ディフィオモルフィズムと呼ぶ。\n参照 グラフ理論における位相同型 Munkres. (2000). Topology(2nd Edition): p105.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":438,"permalink":"https://freshrimpsushi.github.io/jp/posts/438/","tags":null,"title":"位相空間におけるホモトピー"},{"categories":"위상수학","contents":"定義 位相空間 $X,Y$ について $f : X \\to Y$ だとしよう。\n全ての開集合 $O \\subset X$ に対して、$f (O)$ が $Y$ で開集合ならば、$f$ を開関数という。 全ての閉集合 $C \\subset X$ に対して、$f (C)$ が $Y$ で閉集合ならば、$f$ を閉関数という。 定理 特に、連続関数は以下の性質を持つ。\n[1]: 連続関数 $f : \\mathbb{R} \\to \\mathbb{R}$ が全単射ならば、開関数であり閉関数でもある。 上の性質は以下の定理の非常に特別なケースを簡潔にまとめたものだ。\n$f : X \\to Y$ が全単射ならば、以下の命題は互いに同等だ。\n(1): $f^{-1} : Y \\to X$ が連続関数だ。 (2): $f : X \\to Y$ は開関数だ。 (3): $f : X \\to Y$ は閉関数だ。 説明 注意すべき点は、集合での定義と同様に、開かれたことと閉じられたことは互いに排他的ではないということだ。\n同等条件(1)では、$f^{-1}$ が連続関数であることが、位相同型についての議論で便利に使えることを示唆している。\n開かれている、閉じているという概念自体が、連続性とは必ずしも関係ないことを示す例として、床関数 $\\lfloor \\cdot \\rfloor : \\mathbb{R} \\to \\mathbb{R}$ は連続関数ではないが閉関数だ：\n$\\lfloor \\cdot \\rfloor$ が連続関数ではないことは自明だ。 任意の閉区間 $[a,b]$ に対して、$\\lfloor [a,b] \\rfloor \\subset \\mathbb{Z}$ であるため、$\\lfloor \\cdot \\rfloor$ は閉関数だ。 証明 [1] 任意の開区間 $(a,b)$ に対して、$f(a,b) = (c ,d)$ であるため、$f$ は開関数だ。\n任意の閉区間 $[a,b]$ に対して、$f[a,b] = [c,d]$ であるため、$f$ は閉関数だ。\n■\n","id":435,"permalink":"https://freshrimpsushi.github.io/jp/posts/435/","tags":null,"title":"開いた関数と閉じた関数"},{"categories":"정수론","contents":"要約 1 素数 $p$ とそれと互いに素な整数 $a$ について、$a^{p-1} \\equiv 1 \\pmod{p}$\n説明 フェルマーの小定理は、シンプルでありながら非常に多くの場所で利用される定理の一つである。オイラーによって一般化された定理もあるが、フェルマーの小定理だけで十分な場合も多い。特に有限体でのべき乗を多く扱う暗号理論などでは、不可欠な定理である。\n証明 戦略: 証明はシンプルだが、それほど簡単ではない。もし $$ a \\cdot 2a \\cdot \\cdots \\cdot (p-1)a \\equiv (p-1)! \\pmod{p} $$ が成立するなら、両辺から$(p-1)!$を消去してフェルマーの小定理を証明することができるだろう。つまり、二つの集合$\\left\\{ a,2a,\\cdots , (p-1)a \\right\\}$ と $\\left\\{1,2,\\cdots ,(p-1) \\right\\}$ が$\\pmod{p}$で同じであることを示せばよい。\n集合 $\\left\\{ a,2a,\\cdots , (p-1)a \\right\\}$ は、有限集合であるとして、正確に$(p-1)$個の要素を持つ。$a$が$p$と互いに素であるため、これらの要素を$p$で割った余りは$1$から$(p-1)$までの整数のいずれかになるだろう。そして、その余りに重複がなければ、 $$ \\left\\{ a,2a,\\cdots , (p-1)a \\right\\} = \\left\\{1,2,\\cdots ,(p-1) \\right\\} $$ が成立するだろう。$\\left\\{ a,2a,\\cdots , (p-1)a \\right\\}$から異なる任意の二つの要素$ia$と$ja$を取り出して考えてみる。これらを$p$で割った余りが同じと仮定すると、$ia \\equiv ja \\pmod{p}$が成立する。$a$が$p$と互いに素であるため、両辺から$a$を消去すると、$i \\equiv j \\pmod{p}$も成立する。しかし、上記の集合で$i$と$j$は$0$より大きく$p$より小さい整数だったため、$ia$と$ja$も同じである。これは仮定に反するので、異なる任意の二つの要素を$p$で割った余りは常に異なると言える。余りに重複がないため$\\pmod{p}$で、 $$ \\left\\{ a,2a,\\cdots , (p-1)a \\right\\} = \\left\\{1,2,\\cdots ,(p-1) \\right\\} $$ が成立する。一方で$p$は素数であるため、$(p-1)!$と互いに素である。 $$ (p-1)! a^{p-1} \\equiv (p-1)! \\pmod{p} $$ 両辺から$(p-1)!$を消去すると、合同式$a^{p-1} \\equiv 1 \\pmod{p}$を得る。\n■\n以下の系も覚えておくと良い。\n系 [1] 逆元: $\\pmod{p}$で、乗算に対する$a$の逆元は必ずこのように与えられる。 $$ a^{-1} \\equiv a^{p-2} \\pmod{p} $$ [2] フェルマーのテスト: $a^n \\equiv a \\pmod{n}$が成立しない場合、$n$は合成数である。 ある数が素数かどうかを判定することは難しいが、合成数であることは比較的判定しやすい。注意すべきは、フェルマーのテストの逆は成立しないということである。特に逆が成立しないことを示す反例としては、カーマイケル数がある。カーマイケル数の例として$561=3 \\cdot 11 \\cdot 17$は合成数だが、$a^{561} \\equiv a \\pmod{561}$は常に成立する。\nコード 以下は、Rでフェルマーテストを実装したもので、計算には連続べき乗法が使用された。\nFPM\u0026lt;-function(base,power,mod) #It is equal to (base^power)%%mod\r{\ri\u0026lt;-0\rif (power\u0026lt;0) {\rwhile((base*i)%%mod != 1) {i=i+1}\rbase\u0026lt;-i\rpower\u0026lt;-(-power)}\rif (power==0) {return(1)}\rif (power==1) {return(base%%mod)}\rn\u0026lt;-0\rwhile(power\u0026gt;=2^n) {n=n+1}\rA\u0026lt;-rep(1,n)\rA[1]=base\rfor(i in 1:(n-1)) {A[i+1]=(A[i]^2)%%mod}\rfor(i in n:1) {\rif(power\u0026gt;=2^(i-1)) {power=power-2^(i-1)}\relse {A[i]=1} }\rfor(i in 2:n) {A[1]=(A[1]*A[i])%%mod}\rreturn(A[1])\r}\rfermat.test\u0026lt;-function(n)\r{\rfor(i in 2:(n-1)) {if( i!=FPM(i,n,n) ) {return(paste(i,\u0026#34;is a Fermat witness!\u0026#34;))}}\rpaste(n,\u0026#34;passes the Fermat test!\u0026#34;)\r}\rfermat.test(121)\rfermat.test(341) #Almost composite yields fermat witness 2, but 341=11*31 doesn\u0026#39;t.\rfermat.test(561) #Carmicheal number 561 = 3*11*17\rfermat.test(1031) #1031 is a prime\rfermat.test(1105) #Carmicheal number 1105 = 5*13*17\rfermat.test(1729) #Carmicheal number 1729 = 7*13*19\rfermat.test(41041) #Carmicheal number 41041 = 7*11*13*41 以下はコードを実行した結果である。\nフェルマーテストは$121$や$341$のような合成数を確実に捕捉でき、$1031$のような素数を正しく通過させた。しかし、$561$、$1105$、$1729$、$41041$のようなカーマイケル数は捕捉できなかった。カーマイケル数を捕捉するには、ミラー-ラビンテストのような方法を使う必要がある。\nSilverman. (2012). A Friendly Introduction to Number Theory (4th Edition): p66.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":121,"permalink":"https://freshrimpsushi.github.io/jp/posts/121/","tags":null,"title":"フェルマーの小定理の証明"},{"categories":"위상수학","contents":"定義 日本語 位相空間 $(X, \\mathscr{T}_{X} )$ と $(Y, \\mathscr{T}_{Y} )$ に対して、$f: X \\to Y$ としよう。$f(a)$ を含む全ての $V \\in \\mathscr{T}_{Y}$ に対して、$f(U) \\subset V$ を満たしながら $a$ を含む $U \\in \\mathscr{T}_{X}$ が存在する場合、$f$ を $a$ で 連続Continuousという。$f$ が $X$ の全ての点で連続であれば 連続関数といい、$f \\in C(X,Y)$ で表せる。\n英語 $f$ is continuous at $a$ $\\iff$ For all neighborhood $V \\in \\mathscr{T}_{Y}$ of $f(a)$, there exists a neighborhood $ U \\in \\mathscr{T}_{X}$ of $a$ such that $a \\in U \\implies f(a) \\in f(U) \\subset V$\n説明 この定義を初めて見ると、何を意味しているのか理解するのが難しいかもしれないが、よく考えてみると、解析学での連続を定義する時に、$\\epsilon \u0026gt; 0$ が与えられるたびに $\\left| x - a \\right| \\lt \\delta \\implies \\left| f(x) - f(a) \\right| \\lt \\epsilon$ の $\\delta$ が存在することを話しているのと全く同じ感覚だと分かる。$\\left\\{ x : \\left| x - a \\right| \\lt \\delta \\right\\}$ と $\\left\\{ f(x) : \\left| f(x) - f(a) \\right| \\lt \\epsilon \\right\\}$ がオープンセットであることに注目すれば、$\\epsilon$ が与えられるたびに $\\delta$ を見つけられるということは、$Y$ でオープンセットが与えられるたびに、$X$ で条件を満たすオープンセットを見つけられるということと同じであると理解できるだろう。\nちなみに、$C(X,Y)$ は定義域が $X$ で値域が $Y$ の連続関数の集合である。位相数学を学ぶくらいなら、通常はイプシロン-デルタ論法を見飽きるほど見ているだろうから、文章よりも数式や記号の方が扱いやすいはずだ。\n連続性はユークリッド空間を超えて距離空間へ、そして今や距離空間を超えて位相空間へと一般化された。解析学で連続を議論する理由が微分のためであるならば、位相数学では位相同型を議論するために連続の概念が必要である。\n以下は連続点と連続関数に関するいくつかの有用な同値条件である。同値条件であるため、教科書によってはこれらの同値条件を定義として設定することもある。\n連続点の同値条件 $a \\in X$ とすると、以下の命題は互いに同値である。\n(1): $f : X \\to Y$ は $a$ で連続である。 (2): $f(a)$ を含む全ての $V \\in \\mathscr{T}_{Y}$ に対して、$a \\in U \\subset f^{-1} (V)$ を満たす $ U \\in \\mathscr{T}_{X}$ が存在する。 (3): 全ての $\\mathcal{N} ( f(a) )$ に対して、$f^{-1} ( \\mathcal{N} ( f(a) ) )$ は $a$ の近傍である。 (4): $f(a) \\in V^{\\circ}$ を満たす全ての $V \\subset Y$ に対して、$a \\in (f^{-1} (V))^{\\circ} $ ちなみに、$\\mathcal{N} (a)$ は $a$ を含む $X$ の開集合であり、$a$ の近傍Neighborhoodと呼ばれる。\n連続関数の同値条件 以下の命題は互いに同値である。\n[1]: $f : X \\to Y$ は連続関数である。 [2]: $f(a)$ を含む全ての $V \\in \\mathscr{T}_{Y}$ と全ての点 $a \\in f^{-1} (V)$ に対して、$a \\in U_{a} \\subset f^{-1} (V)$ を満たす $ U_{a} \\in \\mathscr{T}_{X}$ が存在する。 [3]: 全ての開集合 $V \\subset Y$ に対して、$f^{-1} (V)$ が $X$ で開集合である1。 [4]: 全ての閉集合 $C \\subset Y$ に対して、$f^{-1} (C)$ が $X$ で閉集合である。 [5]: 全ての $A \\subset X$ に対して、$f( \\overline{A} ) \\subset \\overline{ f(A) } $ [6]: 全ての $B \\in \\mathscr{B}$ に対して $f^{-1} (B) \\in \\mathscr{T}_{X}$ を満たす $\\mathscr{T}_{Y}$ の基底 $\\mathscr{B}$ が存在する。 [7]: 全ての $S \\in \\mathscr{S}$ に対して $f^{-1} (S) \\in \\mathscr{T}_{X}$ を満たす $\\mathscr{T}_{Y}$ の部分基底 $\\mathscr{S}$ が存在する。 [8] 連続関数の合成関数: $f : X \\to Y$ と $g : Y \\to Z$ が連続関数であれば、合成関数 $g \\circ f : X \\to Z$ も連続である。\n連続関数の別の定義 特に \u0026lsquo;定理 [3]: 全ての開集合 $V \\subset Y$ に対して、$f^{-1} (V)$ が $X$ で開集合である\u0026rsquo;は非常に頻繁に使用され、[3]をもって連続関数を定義する場合も多い。上で挙げた全ての条件を覚える必要はないが、[3]だけは必ず覚えて、いつでも取り出せるようにしておこう。\nMunkres. (2000). Topology(2nd Edition): p102.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":432,"permalink":"https://freshrimpsushi.github.io/jp/posts/432/","tags":null,"title":"位相数学における連続とは"},{"categories":"위상수학","contents":"定義 1 位相空間 $\\left( X , \\mathscr{T} \\right)$ に対して、$\\mathscr{S} \\subset \\mathscr{T}$ とする。\n$\\displaystyle \\mathscr{B} = \\left\\{ \\left. B = \\bigcap_{ i = 1}^{n} S_{i} \\ \\right| \\ S_{i} \\in \\mathscr{S} \\right\\}$ が $\\mathscr{T}$ の基底になる時、$\\mathscr{S}$ を $\\mathscr{T}$ の部分基底Subbasisという。\n説明 部分基底を受け入れにくい理由は、通常数学で「部分」と付ける時は、部分集合でありながら元の性質を保持するからだ。例えば、部分群は、部分集合が群の条件を満たす時、部分空間は、部分集合が空間の条件を満たす式である。この意味で、部分基底は概念よりもその用語が混乱させると言えるだろう。\nまず定義上、$\\mathscr{S}$ が部分基底になったら、基底$\\mathscr{B}$ に対する部分集合、すなわち$\\mathscr{S} \\subset \\mathscr{B}$ は自明である。しかし$\\mathscr{S}$ は、全ての有限交差の集合として$\\mathscr{B}$ を構成しなければならないので、基底としてはまだ未熟であると表現することもできる。\nむしろこれまでの数学を考えると、部分基底は基底の基底になると言うのがもっと自然なことだ。問題は、「部分」ということを納得したとしても、依然として部分基底の定義自体が複雑で奇妙だということである。こうした面については、ただ後で位相空間の積のために学ぶべきこととして受け入れるのが楽だ。それについてある程度勉強すれば、なぜ有限交差を考えるのかも理解できるようになるだろう。\nさて、例を見て、概念を少しでも掴もう。\n例 $\\mathscr{S} = \\left\\{ (- \\infty , b ), ( a , \\infty ) \\ | \\ a,b \\in \\mathbb{R} \\right\\}$ が距離空間 $\\mathbb{R}$ の部分基底であることを示せ。\n解答 二つの開区間 $( - \\infty , b )$ と $( a , \\infty )$ の交差として、全ての開区間 $(a,b)$ の集合を形成できるので、$\\mathscr{S}$ は部分基底となる。\n■\nMunkres. (2000). Topology(2nd Edition): p82.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":427,"permalink":"https://freshrimpsushi.github.io/jp/posts/427/","tags":null,"title":"位相数学における部分基底"},{"categories":"정수론","contents":"概要 1 natural number $n \u0026gt;2$ はユニークな素因数分解 $n = p_{1} p_{2} \\cdots p_{r}$ を持つ。この時、素数 $p_{1} , p_{2} , \\cdots , p_{r}$ の順序は無視される。\n説明 小学校から自然に使ってきた性質が証明が必要だと聞いて驚くかもしれないが、非常に重要だ。これほど簡単であることそのものが、基本定理という名前を持つに値する証拠になるだろう。\n証明 初等的証明 Part 1. 存在性\n$2=2$ であり、$3= 3$ そして $4 = 2^2$ の時、ユニークな素因数分解が存在する。すべての可能な $n$ に対して素因数分解を見つけてみて、その最後の数が $N$ だとしよう。ここでは、$N+1$ が素数か合成数のどちらかなので、両方のケースをチェックしてみよう。\nCase 1. $N+1$ が素数の場合\n$N+1$ が素数である場合、それは自身が素因数分解されたことを意味する。 Case 2. $N+1$ が合成数の場合\n$N+1$ が合成数であれば、二つの自然数の積 $N+1 = n_{1} n_{2}$ として表される。 $N$ までのすべての自然数に対して素因数分解を見つけることができたので、 $$ n_{1} = p_{1} p_{2} \\cdots p_{m} \\\\ n_{2} = q_{m+1} q_{m+2} \\cdots q_{r} $$ それを単に掛け合わせることで、$N+1 = p_{1} p_{2} \\cdots p_{r}$ となり、$N+1$ の素因数分解を求めることができる。したがって、$N+1$ に対して二つのケースすべてで素因数分解を見つけることができるので、$2$ より大きいすべての自然数は素因数分解を持つ。\nPart 2. 一意性\n次に、$n = p_{1} p_{2} \\cdots p_{r}$ がユニークであることを示す必要がある。\n$n$ の素因数分解がユニークではなく、$n = p_{1} p_{2} \\cdots p_{r} = q_{1} q_{2} \\cdots q_{s}$ が成り立つと仮定する。これらの順序は一意性に影響を与えないので、便宜上、$p_{1} \\le p_{2} \\le \\cdots \\le p_{r}$ としよう。\n素数分解の原理: 素数 $p$ が natural number $ n : = d_{1} d_{2} \\cdots d_{r}$ に対して $p \\mid n$ の場合、$p$ は $d_{1} , d_{2} , \\cdots , d_{r}$ の少なくとも一つを割らなければならない。\nある数を割る素数は、その約数の少なくとも一つを割らなければならないので、$p_{1}$ は $q_{1} , q_{2} , \\cdots , q_{s}$ の中の一つを割る必要がある。順序は自由なので、$p_{1}$ で割り切れる $q_{i}$ を $q_{1}$ とし、$q_{1}$ を $q_{i}$ として扱っても構わない。\nしかし、$q_{1}$ も $p_{1}$ と同じく素数であるので、$p_{1} = q_{1}$ が成り立ち $$ p_{1} p_{2} \\cdots p_{r} = q_{1} q_{2} \\cdots q_{s} $$ から両辺でそれを消去すると $$ p_{2} \\cdots p_{r} = q_{2} \\cdots q_{s} $$ を得る。同じ方法で、一方が $1$ になるまで消去を続けることができる。この時、もう一方の辺が $1$ ではない場合、素数の積は必ず $1$ より大きくなり、等式が成り立たない。したがって、 $$ p_{1} =q_{1} \\\\ p_{2} = q_{2} \\\\ \\vdots \\\\ p_{i} = q_{i} \\\\ \\vdots $$ が成り立ち、最終的に $p_{r} = q_{s}$ が成り立たなければならない。\n■\n代数的証明 算数の基本定理の代数的証明: 整数環 $\\mathbb{Z}$](../587) は PIDであるため、UFDである。\n■\nコード 以下はRで素因数分解を実装したコードだ。素数かどうかを判断するプロセスがないため速い。素数リストを使うからだ。アルゴリズムとしてはほぼ無価値だ。素因数分解をリスト形式で返すため、可読性が落ちるかもしれない。指数形式に変換したい場合は、単に返されたベクトルに組み込み関数の table() を使えば良い。\nprime = read.table(\u0026#34;../attachment\r/cfile8.uf@25411C3C5968BBE322F0D4.txt\u0026#34;); prime = prime[,1]\rfactorize\u0026lt;-function(p)\r{\rq=p\rfactors\u0026lt;-numeric(0)\ri=1; j=1\rwhile(q!=1)\r{\rif(q%%prime[i]) {i=i+1}\relse\r{\rq\u0026lt;-q/prime[i]\rfactors[j]\u0026lt;-prime[i]\ri=1\rj=j+1\r}\r}\rreturn(factors)\r}\rfactorize(54)\rfactorize(101)\rfactorize(256)\rfactorize(420)\rtable(factorize(420)) 以下はコードを実行した結果だ。\nSilverman. (2012). 「ナンバー・セオリー入門」(第4版): p49.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":419,"permalink":"https://freshrimpsushi.github.io/jp/posts/419/","tags":null,"title":"算術の基本定理の証明"},{"categories":"추상대수","contents":"定義 1 集合$A$に対する全単射$\\phi : A \\to A$を順列Permutationと言い、$S_{A}$は$A$の全ての順列を集めた集まりであり、関数の合成$\\circ$に関して群$\\left\u0026lt; S_{A} , \\circ \\right\u0026gt;$を成し、対称群Symmetric Groupと呼ぶ。\n説明 対称群が本当に群の条件を満たしているかは、順列が全単射として定義されている点から簡単に確認できる。主な関心事は$A$が有限集合である場合、つまり$|A| = n$の場合で、通常$S_{A} = S_{n}$と表される。\n三角形の対称性 順列は高校で学ぶものと本質的に異なるわけではないが、$A = \\left\\{ 1,2,3 \\right\\}$を考えてみよう。元の数が$3$個なので、対称群$S_{3}$の位数は$3! = 6$である。それほど多くないので、一つ一つ列挙してみよう。表現は行列に似ていて、$1$行にある元を$2$行にある元に対応させると考えればよい。 $$ \\rho_{0} = \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{bmatrix} \\qquad \\rho_{1} = \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{bmatrix} \\qquad \\rho_{2} = \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{bmatrix} \\\\ \\mu_{1} = \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{bmatrix} \\qquad \\mu_{2} = \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{bmatrix} \\qquad \\mu_{3} = \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{bmatrix} $$\n図で表すと次のようになる：\n$\\rho_{k}$は$|A| = n$に対して三角形を反時計周りに$\\displaystyle {{2 k \\pi} \\over {n}}$だけ回転させることである。ここで$\\rho_{0}$は図形に何の影響も与えない回転であり、対称群では単位元になる。$\\rho$を使う理由は回転Rotationから来ている。 $\\mu_{k}$は$k$を固定して残りの二点を交換することである。または、$1$と中心を横切る補助線に対して鏡に映したように反転させたとも表現できる。$\\mu$を使う理由は鏡像Mirror Imageから来ている。 可換群ではない $n \\ge 3$に関して、$S_{n}$は可換群ではない。\n対称群の興味深い性質の一つとして可換群ではないということがある。\n上の表は$S_{3}$の全ての操作を示しているが、例えば$\\rho_{1} \\circ \\mu_{1} = \\mu_{3}$で$\\mu_{1} \\circ \\rho_{1} = \\mu_{2}$である。つまり $$ \\rho_{1} \\circ \\mu_{1} \\ne \\mu_{1} \\circ \\rho_{1} $$ となり、したがって$S_{3}$は可換群ではない。実際の証明は、全ての$S_{n}$がこのような例外を持つことを示す数学的帰納法を通じて十分である。\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p76~79。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":421,"permalink":"https://freshrimpsushi.github.io/jp/posts/421/","tags":null,"title":"抽象代数学における対称群"},{"categories":"정수론","contents":"定理 1 二つの整数 $a,b$ に対して、$ax + by = \\gcd (a,b)$ は必ず整数解を持つ。\n説明 この定理は、$\\gcd (a,b)$ が $a$ と $b$ を含む線形式で表現できることを意味しているため、線形合同定理とも呼ばれる。\n見た目が少し複雑で存在性のみを論じるため、直接使用することは難しいかもしれないが、意外と広く使用される。具体的な解 $(x,y)$ を提供するわけではないが、$a$ と $b$ の関係式から出発できるというだけでも大きな収穫である。実際 $x$ や $y$ が何であるか知らないときには非常に便利に使用できる。\n証明 初等的な証明 戦略：名前の通り、拡張ユークリッド定理はユークリッドの互除法を繰り返す。そのため、拡張ユークリッドアルゴリズムとも呼ばれる。\nユークリッドの互除法：$ r_i\u0026lt;r_{i+1}$ に対して、漸化式 $r_{i-1} = q_{i+1} \\cdot r_{i} + r_{i+1}$ を満たす$a: = r_{-1}$ と$b:=r_{0}$ を定義しよう。$r_{n+1} = 0$ を満たす$n$ に対して、$r_{n} = \\gcd (a,b)$\nユークリッドの互除法によれば $r_{1}$ は $$ r_{1} = a - q_{1} b $$ すなわち、$a$ と $b$ の線形結合として表せる。一方で $$ r_{2} = b - q_{2} r_{1} $$ であり、直上の $r_{1} = a - q_{1} b$ を代入すると $$ \\begin{align*} r_{2} =\u0026amp; b - q_{2} r_{1} \\\\ =\u0026amp; b - q_{2} ( a - q_{1} b ) \\\\ =\u0026amp; -q_{2} a + ( 1 + q_{1} q_{2} ) b \\end{align*} $$ となり、$r_{2}$ も $a$ と $b$ の線形結合として表せる。これを$n$ 回繰り返すと、$r_{n}$ はある$x,y \\in \\mathbb{Z}$ に対して $$ r_{n} = x a + y b $$ と表されるだろう。ユークリッドの互除法では $r_{n} = \\gcd (a,b)$ だから、$(x,y)$ は $ax + by = \\gcd (a,b)$ の解として存在する。\n■\n代数的な証明 整数環 $\\mathbb{Z}$ は PIDであるため、ベズー領域である。\n■\n関連項目 一般化：ベズー領域 代数的証明で指摘したように、ある領域がベズー領域であれば、その場所がどこであっても拡張ユークリッド定理が成立する。\nSilverman. (2012). A Friendly Introduction to Number Theory (4th Edition): p42.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":417,"permalink":"https://freshrimpsushi.github.io/jp/posts/417/","tags":null,"title":"拡張ユークリッドの定理の証明"},{"categories":"위상수학","contents":"定義 位相空間 $\\left( X , \\mathscr{T} \\right)$ において $\\mathscr{B} , \\mathscr{B}_{x} \\subset \\mathscr{T}$ とする。\n$B_{\\lambda} \\in \\mathscr{B}$ とするとき、全ての $U \\in \\mathscr{T}$ に対して $$ U = \\bigcup_{\\lambda \\in \\Lambda} B_{ \\lambda } $$ を満たす近傍 $\\Lambda$ が存在すれば、$\\mathscr{B}$ を $\\mathscr{T}$ に対する基底Basisという。このとき位相 $\\mathscr{T}$ は $\\mathscr{B}$ によって生成されるGeneratedという。 $x \\in X$ とするとき、全ての $B \\in \\mathscr{B}_{x}$ に対して $x \\in B$ であり、$x$ を含む全ての $U \\in \\mathscr{T}$ に対して $$ x \\in B \\subset U $$ を満たす $B \\in \\mathscr{B}_{x}$ が存在すれば、$\\mathscr{B}_{x}$ を $x$ での局所基底Local Basisという。 説明 定義がかなり分かりづらく書かれているため、練習問題を解く前に概念的に受け入れる方がずっと楽だろう。線形代数学での基底と感じが似ているが、定義上で似ていることはほとんどないので、その関連性を探りすぎないようにしよう。\n一言で言えば、基底は与えられた位相を合集として作ることができる集合だ。交差点を考える必要はないので、位相で「小さい」開集合を集めて構成すればいい。\n例えば、距離空間を例に挙げると、全ての開球の集合は距離空間の基底になる。\n必要性 本を読んで学ぶ立場からすると、線形代数でそうだったように、位相 $\\mathscr{T}$ で基底 $\\mathscr{B}$ を探すと考えると難しくて理解しづらい概念だ。反対に、生成、つまり基底から位相を作る立場になれば、基底というものがどれほど便利かが分かる。\n例えば、自然数の数列で位相空間を作るとしたら、先頭を基準に位相を作りたい場合、$B_{1}$ は先頭が $1$ の数列の集合で、$B_{2}$ は先頭が $2$ の数列の集合、$B_{k}$ は先頭が $k$ の数列の集合\u0026hellip; といった方法でアプローチすることができる。問題は、$\\mathscr{T}$ に合集 $B_{1} \\cup B_{2}$ が存在しないことだ。なぜなら、先頭が $1$ または $2$ の数列が存在しないからだが、このとき$\\mathscr{B} = \\left\\{ B_{k} \\right\\}_{k \\in \\mathbb{N}}$ で可能な全ての合集があるとすれば、仕事はずっと簡単になる。これがまさに基底で生成された位相を上手く使ったことになる。\n判定法 1 基底の判定: 全集合 $X$ に対して、$\\mathscr{B} \\subset \\mathscr{P} (X)$ が以下の二つの条件を満たすとき、$X$ の基底だ。\n(i): $\\displaystyle X = \\bigcup_{B \\in \\mathscr{B}} B$ (ii): $x \\in B_{1} \\cap B_{2}$ である全ての $ B_{1} , B_{2} \\in \\mathscr{B}$ に対して、以下を満たす $B_{x} \\in \\mathscr{B}$ が存在する。 $$ x \\in B_{x} \\subset B_{1} \\cap B_{2} $$ この判定法は、実際の問題解決などで有効に使うことができる定理なので、必ず覚えておくべきだ。教科書によっては、この判定法が定義となることもある。\n局所基底は、基底とは違い、位相全体ではなく与えられた一点だけを扱う概念だ。言葉が長く難しいが、要約すると最終的には、$x$ を含む全ての開空間の中で最も「小さい」ものだけを集めても局所基底の条件を満たす。\n例えば、距離空間を例に挙げると、$x$ を中心とする全ての開球の集合は、$x$ での局所基底になる。\n基底と局所基底の関係 $X$ を位相空間としよう。\n$\\mathscr{B}$ が $X$ の基底であれば、$\\mathscr{B}_{x} := \\left\\{ B \\in \\mathscr{B} \\ | \\ x \\in B \\right\\}$ は $x \\in X$ の局所基底だ。逆に、全ての $x \\in X$ に対して $\\mathscr{B}_{x}$ が局所基底であれば、$\\displaystyle \\mathscr{B} := \\bigcup_{x \\in X} \\mathscr{B}_{x}$ は $X$ の基底だ。\n注意事項 必要十分条件ではないので、逆が成立するためには、全ての点での局所基底を考える必要がある点に注意しよう。\nMunkres. (2000). Topology(2nd Edition): p78.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":412,"permalink":"https://freshrimpsushi.github.io/jp/posts/412/","tags":null,"title":"位相数学における基底と局所基底"},{"categories":"위상수학","contents":"定義 1 位相空間 $X$ が与えられているとしよう。\nすべての点 $x \\in X$ に対して可算局所基底が存在するならば、第一可算空間という。 $X$ が可算基底を持っているならば、第二可算空間という。 説明 基底と局所基底という概念を通じて、可算の新しい枝分かれを作り出したと見ることができる。\n第一可算にならない例 離散空間 $\\left( \\mathbb{R} , \\mathscr{T}_{f} \\right)$ は第一可算にならず、言うまでもなく第二可算にもならない。\n直感的な理解 正確な説明ではないが、第一可算はすべての点で開集合が数えられるほど存在する感じで受け取ることができる。一方で第二可算は、可算集合が全体を包含する感覚で、可分の概念と似ている。第一可算と第二可算は、第一類＆第二類とは異なり、お互いに否定ではなく、包含関係にある。これは基底と局所基底の関係を考えれば、容易に確認できる。また、先に第二可算が可分の概念と似ていると言ったが、実際に真の可分であることも示すことができる。\n要約 証明 第二可算空間は第一可算である $X$ を第二可算空間とすると、$X$ は可算基底 $\\mathscr{B}$ を持つだろう。\n基底と局所基底の関係: $\\mathscr{B}$ が$X$ の基底ならば、$\\mathscr{B}_{x} := \\left\\{ B \\in \\mathscr{B} \\ | \\ x \\in B \\right\\}$ は $x \\in X$ の局所基底である。\n$\\mathscr{B}_{x} = \\left\\{ B \\in \\mathscr{B} \\ | \\ x \\in B \\right\\}$ はすべての $x \\in X$ に対して可算であるから、$X$ は第一可算である。\n■\n第二可算空間は可分である $X$ を第二可算空間としよう。$X$ は可算基底 $\\mathscr{B}$ を持つ。すべての空でない集合 $B \\in \\mathscr{B}$ に対して $x_{B} \\in B$ を選んで $D : = \\left\\{ x_{B} \\in B \\ | \\ \\emptyset \\ne B \\in \\mathscr{B} \\right\\}$ を定義しよう。$D$ は可算基底である $\\mathscr{B}$ から要素を一つずつ選んだ集合だから、$D$ もまた可算であり、$\\overline{D} = X$ を示せば証明は終わりだ。\n$U$ が $x \\in X \\setminus D$ を含む開集合だとしよう。\n$\\mathscr{B}$ が $X$ の基底であるから、$x \\in B \\subset U$ を満たす $B \\in \\mathscr{B}$ が存在するだろう。$x_{B} \\in B \\cap D$ かつ $x \\notin D$ だから、 $$ D \\cap (B \\setminus \\left\\{ x \\right\\} ) \\ne \\emptyset $$ になる。前にも述べたように、$B \\subset U$ だから $$ D \\cap (U \\setminus \\left\\{ x \\right\\} ) \\ne \\emptyset $$ でもなお成立する。集積点の定義によれば、$x$ は $D$ の集積点であり、$x \\in \\overline{D}$ だから、$X \\setminus D \\subset \\overline{D}$ が成り立つ。もちろん、$D \\subset \\overline{D}$ だから、これを同時に満たすためには、$X = \\overline{D}$ でなければならない。\n■\nMunkres. (2000). Topology(2nd Edition): p190.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":413,"permalink":"https://freshrimpsushi.github.io/jp/posts/413/","tags":null,"title":"第一加算と第二加算"},{"categories":"복소해석","contents":"定義 1 関数$f: A \\subset \\mathbb{C} \\to \\mathbb{C}$が$\\mathscr{R} \\subset A$で解析的であり、すべての$z \\in \\mathscr{R}$に対して$f ' (z) \\ne 0$を満たす場合、$f$とすると等角写像Conformal Mappingまたは等角変換Conformal Transformという。一方で、$f ' (\\alpha) = 0$を満たす点$\\alpha$が存在する場合、$\\alpha$を$f$の臨界点Critical Pointという。\n説明 等角等角という漢字そのまま、等角変換を取ると形状が作る角が保持される。\nその名の通り、等角写像同士の合成は等角写像であるという事実を覚えておこう。証明は、以下の対偶を確認することで十分だ。 $$ (f \\circ g) \u0026rsquo; = f '(g) g' = 0 \\iff g' = 0 \\lor f ' = 0 $$\nこのような等角変換は、単純閉路を多く扱う複素解析において非常に重要で、積分経路を扱うときに便利に使われる。幾何学的には、臨界点を考えると、つまり完全に停止するために方向を変えなければならない、つまり折れる点と言える。一方、解析的で単射である関数は、以下の二つの重要な性質を持つ。\n基本性質 1 [1]: もし関数$f$が$\\mathscr{R}$で解析的であり単射ならば、すべての$z \\in \\mathscr{R}$で$f ' (z) \\ne 0$が成り立つ。言い換えると、$f$は等角写像である。 [2]: もし関数$f$が$\\mathscr{R}$で解析的であり単射であり、単純閉路$\\mathscr{C}$を$\\mathscr{C} ' $に対応させる場合、$f$は$\\mathscr{C}$内部の点を$\\mathscr{C} ' $の内部か外部のみに対応させる。 [1]で必要十分条件ではないことに注意しよう。[2] は特に重要な性質であって、$\\mathscr{C}$内部の一点だけチェックすれば、他の点が$\\mathscr{C} ' $の内部に行くか外部に行くかがわかる。 Osborne (1999). Complex variables and their applications: p193, 196.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":409,"permalink":"https://freshrimpsushi.github.io/jp/posts/409/","tags":null,"title":"複素解析における等角写像とは？"},{"categories":"위상수학","contents":"定理 一般に、位相空間での数列の極限は一意ではない。\n説明 何のことかと思うかもしれないが、驚くべきことにこれは事実だ。これまで私たちは解析学などで数列が含まれる区間が徐々に狭まりながら一点に収束するイメージを持っていた。しかし、位相数学で定義される収束の概念によると、位相空間によっては一点に収束する理由が全くない。\n極限の一意性を保証するためには、主にハウスドルフ空間が仮定される。\n反証 極限が複数存在する反例を示せば十分である。\n余裕のある空間 $\\left( \\mathbb{R} , \\mathscr{T}_{f} \\right)$ で、異なる点からなる数列 $\\left\\{ x_{n} \\right\\}$ を考えてみよう。まず、$\\left\\{ x_{n} \\right\\}$ が収束する点を任意に $x \\in \\mathbb{R}$ としよう。ここで、$x$ を含む開集合 $U \\in \\mathscr{T}_{f}$ が存在し、$\\mathbb{R} \\setminus U$ は有限集合である。$\\left\\{ x_{n} \\right\\}$ は異なる点から成っているので、全ての $n \u0026gt; n_{0}$ に対して $x_{n} \\notin \\mathbb{R} \\setminus U$ を満たす $n_{0} \\in \\mathbb{N}$ が存在することはない。しかし、$\\left\\{ x_{n} \\right\\}$ が収束するので、全ての $n \u0026gt; n_{0}$ に対して $x_{n} \\in U$ を満たす $n_{0} \\in \\mathbb{N}$ が存在しなければならない。収束の定義により、$\\left\\{ x_{n} \\right\\}$ は $x$ に収束するが、ここでの $x$ は何であっても特に問題はない。\n■\n理解が難しい場合は、収束の定義がどのように変わったのか、余裕のある空間の開集合が何かを考えてみると良い。\n従来の距離空間で開集合と言えば、ある点を中心に与えられた距離以内の点の集合を指すものだった。したがって、「全ての開集合」と言っても実際はその点の近傍で条件を満たすことが重要だった。どんなに小さくしても条件を満たし続ければ、「全ての開集合」に対するチェックは終わったも同然だった。\nしかし、余裕のある空間で$U$ と別の開空間を考えると、$U \\setminus \\left\\{ a \\right\\}$ のようなものも開集合になる。全体空間が実数であれ何であれ、全ての点を一つずつ取り除くだけで開集合は開集合となり、「距離」を考えることは無意味である。したがって、全ての開集合に対してチェックすると言っても、距離空間のように徐々に小さくなる理由はなく、極限が特定されないのである。\n","id":407,"permalink":"https://freshrimpsushi.github.io/jp/posts/407/","tags":null,"title":"一般的な位相空間における数列の極限は唯一ではない。"},{"categories":"위상수학","contents":"定義 1 位相空間 $X$ について $A \\subset X$ とする。\n$x \\in O \\subset A$ を満たす開集合 $O$ が存在する時、$x$ を $A$ の内点Interior Pointという。 $A$ の内点の集合 $A^{\\circ}$ を $A$ の内部Interiorという。 $A$ とその値域の合集 $\\overline{A} : = A \\cup a '$ を $A$ の閉包Closureという。 $x \\in \\overline{A}$ であり、かつ $x \\in \\overline{X \\setminus A}$ の時、$x$ を $A$ の境界点Boundary Pointという。 $\\partial A : = \\overline{A} \\cap \\overline{X \\setminus A}$ を $A$ の境界Boundaryという。 $\\overline{A} = X$ の時、$A$ は $X$ で稠密であるDenseという。 $\\left( \\overline{A} \\right) ^{\\circ} = \\emptyset$ の時、$A$ は $X$ でどこにも稠密でないNowhere Denseという。 $X$ が稠密な可算部分集合を持つ時、$X$ は可分であるSeparableという。 $\\overline{A}$ と $A^{\\circ}$ はそれぞれ $A$ のクロージャー、インテリアだ。 説明 まず、距離空間で定義されていた様々な定義を持ち込んでも全く問題がないことを確認しよう。\n典型的な可分空間の例は、$\\overline{ \\mathbb{Q} } = \\mathbb{R}$ だ。\n可算部分集合という言葉が難しいなら、まず整数の集合 $\\mathbb{Z}$ で実数空間 $\\mathbb{R}$ を分割すると考えてみよう。このイメージは想像しやすいが、どんな集合でもその部分集合で分割するのは簡単なので、何の意味もない。逆に、不可算集合で分割すると、扱いづらく、結局意味がない。一方、定義されたように、稠密性と可算性を満たす概念があるなら、あまりにも簡単でも難しくもないとみなせるだろう。その場合、部分集合 $A$ とはいえ、実際には全体集合を支える大きな「骨格」のような感じでなければならない。\n骨格の比喩をもう一歩進めてみるなら、ある空間が可分空間であるということは、どんなに $x \\in X$ が与えられたとしても、$x$ に収束する数列 $\\left\\{ x_{n} \\right\\}_{n \\in \\mathbb{N} }$ の存在を保証することと同じだ。例えば、$x \\in \\mathbb{R}$ が与えられたとすると、$x$ が何であれ、$x$ に収束する有理数の数列 $\\left\\{ q_{n} \\right\\}_{n \\in \\mathbb{N}}$ を見つけることができるということだ。\n可分性が重要なのは、それによって、自分が望む元に収束（稠密）する数列（可算）を作ることができるからだ。実用的な側面から見れば、この性質の有用性がさらに際立つ。応用数学では、複雑な関数をよく知られた簡単な関数たちで近似することが非常に重要だ。\nたとえば、連続関数の空間 $C[a,b]$ は可分空間であり、それに応じて、どんなに $f$ が与えられても、$f$ に収束する連続関数の数列 $\\left\\{ f_{n} \\right\\}_{n \\in \\mathbb{N}}$ の存在が保証されるということがわかる。この $\\left\\{ f_{n} \\right\\}_{n \\in \\mathbb{N}}$ が具体的に何であるかを明らかにするのは、数値解析などの応用数学の役割だが、存在を明らかにすることは純粋数学の役割だ。\n要約 稠密性の判定法 稠密性を判定する方法として、以下の有用な同値条件を必ず覚えておこう。\n$A$ が $X$ で稠密であることと、$X$ のすべての開部分集合 $U$ に対して、$U \\cap A \\ne \\emptyset$ が交差することは同値だ。\n基本性質: 部分空間の境界 [1]: $\\partial A \\subset A \\iff A = \\overline{A}$ [2]: $\\partial A \\subset X \\setminus A \\iff A = A^{\\circ}$ [3]: $\\partial A = \\emptyset \\iff A = A^{\\circ}= \\overline{A}$ 可算性とは関係ないが、これらの概念を新たに定義しただけに、これらの性質を知っておくことが重要だ。\n境界を利用して、空間が開いているか閉じているかを判断できる便利な性質だ。位相数学に慣れるにつれ、空間という空間がますます抽象的になるので、単語だけでも定義を推測し、考えることができることに感謝しよう。\nMunkres. (2000). 『位相空間』(第2版): p95, 97.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":405,"permalink":"https://freshrimpsushi.github.io/jp/posts/405/","tags":null,"title":"位相空間における可分性と閉包"},{"categories":"추상대수","contents":"定義 1 二つの二項演算構造 $\\left\u0026lt; S , * \\right\u0026gt;$ と $\\left\u0026lt; S' , *' \\right\u0026gt;$ に対し、全ての $x , y \\in S$ について $$ \\phi (x \\ast\\ y) = \\phi ( x ) *' \\phi ( y ) $$ を満たす全単射関数 $\\phi : S \\to S'$ が存在する場合、$\\phi$ を同型写像と呼び、$S$ と $S'$ は同型Isomorphicであると言い、$S \\simeq S'$ と書く。\n説明 定義を要約すると、演算を保持する全単射が存在する場合、実質的に同じとみなすということです。抽象代数に限らず、同型Isomorphismとして知られるこの写像は、数学全般でとても重要です。\nもし $\\phi$ が演算を保持するが全単射ではない場合、これを準同型写像Homomorphismという。このように、同型写像ではないが多くの重要な写像が存在し、その研究も無数にあります。\n次は、同型写像によって、恒等元の同一性も保持されるという意味の定理です。\n定理 同型写像 $\\phi$ により $S \\simeq S'$ であり、$e$ が $S$ の恒等元であれば、$\\phi (e)$ は $S'$ の恒等元です。\n証明 $e$ が $S$ の恒等元であるため、$s \\in S$ に対して $$ e \\ast\\ s = s \\ast\\ e = s $$\n$$ \\phi ( e \\ast\\ s ) = \\phi ( s \\ast\\ e ) = \\phi ( s ) $$ $\\phi$ が同型写像であるため、$s' : = \\phi (s) \\in S'$ に対して $$ \\phi ( e ) *' \\phi ( s ) = \\phi ( s ) *' \\phi ( e ) = \\phi ( s ) $$ したがって、$\\phi (e)$ は $S'$ の恒等元です。\n■\n参照 グラフ理論における同型 Fraleigh. (2003). A first course in abstract algebra(7th Edition): p29.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":403,"permalink":"https://freshrimpsushi.github.io/jp/posts/403/","tags":null,"title":"抽象代数学における同型"},{"categories":"위상수학","contents":"定義 1 位相空間 $\\left( X , \\mathscr{T} \\right)$ が与えられているとする。\n$A \\subset X$ に対して、$x$ を含む任意の開集合 $O$ が $O \\cap ( A \\setminus \\left\\{ x \\right\\} ) \\ne \\emptyset$ を満たすとき、$x$ を $A$ の集積点、$A$ の全ての集積点の集合 $a '$ を $A$ の導出集合という。 $X$ の数列 $\\left\\{ x_{n} \\right\\}$ が $x$ に収束するとは、$x$ を含む任意の開集合 $O$ に対して、次を満たす $n_{0} \\in \\mathbb{N}$ が存在することを意味する。 $$ n \\ge n_{0} \\implies x_{n} \\in O $$ 説明 収束しないからといって、特に発散することを別に定義していないことに注意。\n位相空間になっても、集積点を定義することができ、言葉だけではほとんど違いがないようにみえる。距離空間の定義から変わっていないが、概念的には、距離空間では「全ての開集合」とは言ったが、実際にはその区間を「狭めていく」感じだったのに対して、位相空間では文字通りあらゆる種類の開集合を想定しなければならない。\n$A$ が $X$ で閉集合であることは、$ A ' \\subset A$ と同値で、閉集合と集積点の定義から容易に証明できる。もっとスッキリと、$\\overline{A} = A \\cup a '$ のため、$A = \\overline{A}$ で示すこともできる。特に距離空間では、次のような定理で表される。\n定理 距離空間 $(X,d)$ で、$K \\subset X$ とする。\n[1]: $K$ の集積点 $x \\in X$ に収束する$K$ の異なる点の数列が存在する。 [2]: $K$ が$X$ で閉集合ならば、$K$ の全ての収束する数列は$K$ の点に収束する。 Munkres. (2000). Topology(2nd Edition): p97.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":400,"permalink":"https://freshrimpsushi.github.io/jp/posts/400/","tags":null,"title":"位相空間における集積点と収束、値域"},{"categories":"위상수학","contents":"定義 位相空間 1 集合 $X$ が与えられた時、$\\mathscr{T} \\subset \\mathscr{P} (X)$ が $T \\in \\mathscr{T}$ に対して次の三つの条件を満たすなら、$\\mathscr{T}$ を $X$ の位相と呼び、$\\left( X , \\mathscr{T} \\right)$ を位相空間と呼ぶ。\n(i): $$\\emptyset , X \\in \\mathscr{T}$$ (ii): $$\\displaystyle \\bigcup_{ \\alpha \\in \\forall } T_{\\alpha} \\in \\mathscr{T}$$ (iii): $$\\displaystyle \\bigcap_{ i= 1}^{n} T_{i} \\in \\mathscr{T}$$ 条件 (i)~(iii) を言葉で説明すると以下の通り:\n(i): $\\mathscr{T}$ は空集合 $\\emptyset$ と全集合 $X$ を含む。 (ii): $\\mathscr{T}$ の元の和集合は $\\mathscr{T}$ に属する。 (iii): $\\mathscr{T}$ の元の有限交わりは $\\mathscr{T}$ に属する。 開集合と閉集合 2 $O \\in \\mathscr{T}$ を開集合と定義する。 $C \\subset X$ に対して $ X \\setminus C \\in \\mathscr{T}$ ならば $C$ を閉集合と定義する。 開集合であり、かつ、閉集合であるならば開かつ閉な集合と言う。 説明 位相空間 定義上、$\\mathscr{T}$ は $\\cup$ と $\\cap$ に対して閉じていて、代数的な考えが思い浮かぶかもしれないが、この定義だけでは代数的な性質を見つけるのが難しい。\n高校で習ったように「力と方向を持つ量」ではなく、条件を満たせばベクトルになるように、位相空間の位相も単に条件を満たす部分集合の集合として一般化されたものである。\n開集合と閉集合 位相を定義すると同時に、開けることと閉じることも新たに定義される。従来の距離空間では、開区間と閉区間の概念から続くものとして直感的に定義されていたが、一般的な位相では集合を使うため、抽象的で奇妙な空間を生み出すことができる。\n定義を見ると、開けることは完全に位相の概念を借りて新たに定義されているが、閉じることは距離空間でほとんど同じであることがわかる。\n位相と開けること、閉じることの定義に従って、以下の性質を簡単に確認できる。\n定理 [1-1]: $\\displaystyle \\bigcup_{ \\alpha \\in \\forall } O_{\\alpha} \\in \\mathscr{T}$ は開集合である。 [1-2]: $\\displaystyle \\bigcap_{ i= 1}^{n} O_{i} \\in \\mathscr{T}$ は開集合である。 [2-1]: $\\displaystyle \\bigcap_{ \\alpha \\in \\forall } C_{\\alpha} \\in \\mathscr{T}$ は閉集合である。 [2-2]: $\\displaystyle \\bigcup_{ i= 1}^{n} C_{i} \\in \\mathscr{T}$ は閉集合である。 [3]: $\\emptyset$ と $X$ は開集合であり、かつ、閉集合である。 例 以下の例を通して位相についての感覚を掴んでみよう。\n$X:=\\left\\{ a,b,c,d \\right\\}$ に対して $\\mathscr{T} : = \\left\\{ \\emptyset , \\left\\{ b \\right\\} , \\left\\{ a, b \\right\\} , \\left\\{ b,c \\right\\} , \\left\\{ a,b,c \\right\\} , \\left\\{ a,b,c,d \\right\\} \\right\\}$ が $X$ の位相であることを示せ。\n(i): $\\emptyset \\in \\mathscr{T}$ であり、$\\left\\{ a,b,c,d \\right\\} =X \\in \\mathscr{T}$ である。 (ii): 全集合 $X$ を除いては、$d$ は使われず、$\\left\\{ a,b,c \\right\\} \\in \\mathscr{T}$ である。 (iii): 空集合 $\\emptyset$ を除いては、すべて $b$ を共有し、$\\left\\{ b \\right\\} \\in \\mathscr{T}$ である。 ■\nMunkres. (2000). Topology(2nd Edition): p76.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMunkres. (2000). Topology(2nd Edition): p93.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":398,"permalink":"https://freshrimpsushi.github.io/jp/posts/398/","tags":null,"title":"位相空間とは？"},{"categories":"거리공간","contents":"定義 1 距離空間 $\\left( X , d \\right)$について、$A \\subset X$としよう。\n$X$ の 数列 $\\left\\{ x_{n} \\right\\}$ の全てが、$\\varepsilon \u0026gt; 0$ に対して $n,m \u0026gt; n_{0}$ の時いつでも $d(x_{n} , x_{m}) \u0026lt; \\varepsilon$ を満たす 自然数 $n_{0}$ が存在すれば、コーシー数列Cauchy sequenceという。 $\\left( X , d \\right)$ 上のコーシー数列が収束する点が $X$ に属していれば、$\\left( X , d \\right)$を 完備Completeといい、そうでなければ 不完備Incompleteという。 $\\overline{A} = X$ の時、$A$が $X$ で 稠密Denseであるという。 $\\left( \\overline{A} \\right) ^{\\circ} = \\emptyset$ の時、$A$が $X$ で 至る所で稠密でないNowhere Denseであるという。 $X$ が $X$ の 加算Countablyな数の至る所で稠密でない 部分集合たちの 和集合で表せるなら、$\\left( X , d \\right)$ を 第1カテゴリーfirst category、そうではないなら 第2カテゴリーsecond categoryと呼ぶ。 $\\overline{A}$ と $A^{\\circ}$ は、それぞれ$A$ の クロージャー、インテリア である。 説明 完備性 完備性とは概念は閉じていることと非常に似ており、位相数学に触れる前には、むしろそれを閉集合と理解されがちだ。明確な違いは、完備では全体空間の概念が不要であり、閉じていることは全体空間が与えられて初めて知ることができるということである。\n例えば、$[a,b)$ の全体空間を $[a,b)$ とすると、$[a,b)$は全体空間なので閉集合になる。しかし、$[a,b)$上には$b$に収束するコーシー数列が存在するので、全体空間が何であれ$[a,b)$ は不完備である。\nしかし、完備性はこの直感的な理解よりも、より重要な概念を内包している。つまり、我々が関心を持っている空間の我々が関心を持っている数列が我々が関心を持っている要素に収束することを保証しているのである。空間が完備性を持たないということは、解答が実数でなければならない方程式の解が虚数で出てくることに例えられるかもしれない。その答えが何であれ、我々が望む形でなければ無用であり、完備性とはそれを保証しているのである。\n解析学での完備性公理に初めて触れた時、なぜそれがCompletenessと呼ばれるのか理解するのが難しかった。日常生活で、英単語のCompleteは完全に備えるという表現としてはあまり使われず、\u0026lsquo;完成\u0026rsquo;や\u0026rsquo;完結\u0026rsquo;など、何かが続いているもののその終わりと一緒に使われることが多いからである。完備性の一般化された定義を見ると、数列のその終わり、つまり収束点が（その空間の中に）存在することを保証している点で、completeという表現が適切であることがわかる。\n稠密性 稠密性は、新しく学ぶというよりは、直感的に受け入れてきた概念を位相数学的に考え直すことで十分である。そのような表現はクリーンだが、実際にはより詳しく説明する方が良い場合もある。別の表現では、すべての開集合 $O \\subset X$ に対して、$A \\cap O \\ne \\emptyset$ であれば、$A$ が $X$ で稠密であるという。後に、稠密性という概念は分離可能空間という概念に発展し、どのような数列を選べるかどうかという重要な問題に直面する。\n距離空間の研究は、数学的な重要性だけでなく、概念的にも我々人類にとって最も直感的なものである。当然、距離空間に関する研究も多く、完備性に関する議論も上述のように多くの、非常に多くの性質がある。\nカテゴリー バイアのカテゴリー定理: すべての完備距離空間は、それ自体を全体集合とみなした場合、第2カテゴリーである。\nカテゴリーについては、実際に理論が展開されるのを見るまでは、定義すらもなぜ存在するのか理解するのが難しい。とりあえずは、学びながら理解するしかない。バイアのカテゴリー定理の文脈では、それが自分自身を全体集合として考えるのか部分集合として考えるかよりも、「完備性」に焦点を当てるようにしよう。\nCroom. (1989). Principles of Topology: p87~89.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":396,"permalink":"https://freshrimpsushi.github.io/jp/posts/396/","tags":null,"title":"距離空間における完備性と密度性"},{"categories":"추상대수","contents":"定義 1 群 $G$ において、ある元 $a$ と任意の $x \\in G$ に対して $x = a^{n}$ を満たす整数 $n \\in \\mathbb{Z}$ が存在する場合、 $G$ を巡回群Cyclic Groupと言い、 $a$ を生成元Generatorという。\n説明 簡単に言えば、群のすべての元を生成元のべき乗で表現できる場合、それは巡回群です。「巡回」という表現が非常に適切であることがわかります。これは、すべての元を生成元のべき乗で表現する形式を続けるからです。\n定義からすぐにはわかりませんが、すべての巡回群はアーベル群であり、生成元が必ずしも一意ではない。定理[1]がその例である。\nさらに、定義によると巡回群が必ずしも有限群である必要はない。注意すべき点は、$n$ が存在するが、自然数ではなく整数であることであり、これは生成元の逆元を加えても構わないという意味です。定理[2]がその例である。\n定理 [1]: $\\mathbb{Z}_{4} = \\left\\{ 0,1,2,3 \\right\\}$ の生成元は一意ではない。 [2]: $\\mathbb{Z}$ は巡回群である。 証明 [1] $1$ だけでもすべての元を表現できるが、 $3 \\equiv -1 \\pmod{4}$ ゆえに、$3$ でもすべての元を表現できる。\nしたがって、$\\left\u0026lt; 1 \\right\u0026gt; = \\left\u0026lt; 3 \\right\u0026gt; = \\mathbb{Z}_{4}$ であり、生成元が一意である必要はないことがわかる。\n■\n[2] $\\left\u0026lt; \\mathbb{Z} , + \\right\u0026gt;$ のすべての元は$1 \\cdot n = (-1) \\cdot (-n) = n$ として表現できるから、$\\mathbb{Z} = \\left\u0026lt; 1 \\right\u0026gt; = \\left\u0026lt; -1 \\right\u0026gt;$\n■\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p59.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":392,"permalink":"https://freshrimpsushi.github.io/jp/posts/392/","tags":null,"title":"抽象代数学における巡回群"},{"categories":"거리공간","contents":"定義 距離空間 $\\left( X, d \\right)$ とするとき、$a \\in X$ であり、$r \u0026gt; 0$ とする。\n中心が $a$ で、半径が $r$ の開いた球Open Ballを $B_{d} (a,r) = \\left\\{ x \\in X \\ | \\ d(a,x) \u0026lt; r \\right\\}$ という。 中心が $a$ で、半径が $r$ の閉じた球Closed Ballを $B_{d} [a,r] = \\left\\{ x \\in X \\ | \\ d(a,x) \\le r \\right\\}$ という。 $O \\subset X$ が開いた球の和集合なら、$O$ を $X$ での開集合Open Setという。 $C \\subset X$ に対し、$X \\setminus C$ が開集合なら、$C$ を $X$ での閉集合Closed Setという。 説明 開集合と閉集合は別に定義できるが、本質的には同じだ。\n球とは区間、開区間、閉区間を一般化した概念であり、区間も $1$次元の球だと考えると、これは自然な話だ。もちろん、ユークリッド空間 $\\mathbb{R}$ の次元に対する一般化に止まらず、距離がきちんと与えられればどこでもしっかり定義される。\n開集合と閉集合は一般的に以下の性質を満たす。\n性質 全空間 $X$ 上の開集合を $O_{\\alpha}$ 、閉集合を $C_{\\alpha}$ とする。\n[1]: $X$ と $\\emptyset$ は開いていると同時に閉じている。 [2]: 開集合の和集合 $\\displaystyle \\bigcup_{\\alpha \\in \\forall} O_{\\alpha}$ は $X$ で開集合である。 [3]: 開集合の有限交差 $\\displaystyle \\bigcap_{i = 1}^{n} O_{i} $ は $X$ で開集合である。 [4]: 閉集合の交差 $\\displaystyle \\bigcap_{\\alpha \\in \\forall} C_{\\alpha}$ は $X$ で閉集合である。 [5]: 閉集合の有限和集合 $\\displaystyle \\bigcup_{i = 1}^{n} C_{i}$ は $X$ で閉集合である。 [3]で有限という条件がなければ、$\\displaystyle \\bigcap_{n = 1}^{ \\infty } \\left( -{{1} \\over {n}} , {{1} \\over {n}} \\right) = \\left\\{ 0 \\right\\}$ という反例が出せる。**[5]**で有限という条件がなければ、$\\displaystyle \\bigcup_{n = 1}^{ \\infty } \\left[ 0 , 1-{{1} \\over {n}} \\right] = [ 0 , 1 )$ という反例が出せる。\n証明 [1] このポストで紹介されている。\n[2]~[5] このポストで紹介されている。\n","id":382,"permalink":"https://freshrimpsushi.github.io/jp/posts/382/","tags":null,"title":"距離空間での球と開集合閉集合"},{"categories":"거리공간","contents":"定義 集合 set $X$ に対して、関数 $d : X \\times X \\to [0, \\infty)$が $x,y,z \\in X$ について以下の条件を満たす場合、$d$ を距離metricと呼び、$\\left( X, d\\right)$を距離空間metric spaceという。距離が自明の場合、簡単に $X$ と記されることもある。\n$d(x,y)=0 \\iff x = y$\n$d(x,y) = d(y,x)$\n$d(x,y) + d(y,z) \\ge d(x,z)$\n説明 線型代数学でノルムの概念を理解したなら、大きさや距離が直感的にのみ定義される必要はないことがわかるだろう。以下の3つの例は特に $\\mathbb{R}^{n}$上で定義され、前述のように、線型代数学で見たノルムと大きく変わらない。これは、ノルム $\\left\\| \\cdot \\right\\|$がどのように定義されても常に距離 $d ( \\mathbf{x} , \\mathbf{y} ) := \\left\\| \\mathbf{x} - \\mathbf{y} \\right\\|$ を定義できるためで、ある種のノルムがあればそれに対応する距離も存在する。\n例 $\\mathbf{x} = (x_{1} , x_{2} , \\cdots , x_{n} )$ そして $\\mathbf{y} = (y_{1} , y_{2} , \\cdots , y_{n} ) $ とする。\nユークリッド距離: $d(\\mathbf{x} , \\mathbf{y}) = \\sqrt{ \\sum \\limits_{i = 1}^{n} (x_{i} - y_{i} )^2 }$\nタクシーキャブ距離: $d^{\\prime}(\\mathbf{x} , \\mathbf{y}) = \\sum \\limits_{i = 1}^{n} | x_{i} - y_{i} |$\nマックス距離: $d^{\\prime \\prime}(\\mathbf{x} , \\mathbf{y}) = \\max \\left\\{ | x_{i} - y_{i} | \\right\\}_{i=1}^{n}$\n基本的な解析学では、主に $\\mathbb{R}^{1}$ を扱い、ユークリッド距離だけが使われると考えても良い。解析学に限って言えば、距離空間について詳細に学ぶ必要はなく、実数集合 $\\mathbb{R}$ を距離空間 $\\left( \\mathbb{R} , d \\right)$として受け入れるだけで十分だ。以下の二つの例は、ユークリッド空間を超えた距離の概念だ。\n離散距離:\n$$ d_{0} (x,y) = \\delta_{xy} = \\begin{cases}1, \u0026amp; \\ x \\ne y \\\\ 0, \u0026amp; \\ x = y \\end{cases} $$\n離散距離はクロネッカーのデルタを使用し、二つの要素が同じかどうかだけで判断する。三角不等式を満たしているかは、場合分けをして簡単に証明できる。\n積分距離:\n$$ \\rho (f,g) = \\int_{a}^{b} | f(x) - g(x) | dx $$\n積分距離は連続関数の集合 $C[a,b]$ で定義できる距離だ。二つの関数のグラフが完全に同じであれば、その値は $0$ になる。\n図で示すと、実線で囲まれた部分がちょうど $\\rho (f,g)$ になる。\nこれらの定義から、metric は従来の意味での‘距離’よりも、二つの間の‘距離感’として理解する方が適していることがわかる。完全に同じものは必ず $0$ であるため、‘無限大にどれだけ近いか’ではなく、‘$0$ からどれだけ遠いか’が重要だ。より抽象的な思考のために、距離が大きくなるほど‘場所’が遠くなるという直感的な考えから離れよう。\n","id":381,"permalink":"https://freshrimpsushi.github.io/jp/posts/381/","tags":null,"title":"距離空間の定義"},{"categories":"행렬대수","contents":"定義1 行列 $A \\in \\mathbb{C}^{m \\times n}$ とベクトル $\\mathbf{b} \\in \\mathbb{C}^{m}$ に関する線形システム $A\\mathbf{x} = \\mathbf{b}$ が過剰決定 あるいは 過少決定 であるとしよう。この場合、システムは解を持たないまたは無数に持つ。ここで、\n$$ \\left\\| A \\mathbf{x} - \\mathbf{b} \\right\\|_{2} $$\nの値を最小化する問題について考えてみよう。これを**最小二乗問題(LSP, Least Square Problem)と呼ぶ。この問題の解$\\mathbf{x}_{\\ast}$を最小二乗解(least square solution)**と呼ぶ。\n$$ \\mathbf{x}_{\\ast} = \\argmin \\left\\| A \\mathbf{x} - \\mathbf{b} \\right\\|_{2} $$\n$A \\mathbf{x} - \\mathbf{b}$を最小二乗誤差ベクトル(least square error vector), $\\left\\| A \\mathbf{x} - \\mathbf{b} \\right\\|$ を**最小二乗誤差(least square error)**と呼ぶ。\n説明 方程式の解が存在しないのは残念だが、だからといって解を求める努力を諦めるわけにはいかない。現実には、解決が難しい、学術の最前線で数学者たちの解法を待っている方程式が多く、そういう問題を、できるだけ近似的に解く方法を研究することは、間違いなく価値がある。その中でも、最小二乗法は、最も代表的な方法の一つだ。実用科学を問わず、活発に使用されており、特に、統計学においては、回帰分析を支える理論の根幹だ。 $\\left\\| \\mathbf{b} - A \\mathbf{x} \\right\\|_{2}$ の大きさが最小になるということは、$A \\mathbf{x}$ と$\\mathbf{b}$ の間の距離、つまり誤差が小さくなるということだ。直交影 $P : \\mathbb{C}^{m} \\to \\mathcal{C} (A)$ について\n$$ \\mathbf{b} = P \\mathbf{b} + (I -P) \\mathbf{b} $$\n$$ P \\mathbf{b} \\in \\mathcal{C} (A) $$\nであり、あるベクトル$\\mathbf{x}_{\\ast}$に対して$A \\mathbf{x}_{\\ast} = P \\mathbf{b}$ である。これについて\n$$ \\left\\| A \\mathbf{x} - \\mathbf{b} \\right\\|_{2} = \\left\\| A \\mathbf{x} - P \\mathbf{b} + P \\mathbf{b} - \\mathbf{b} \\right\\|_{2} $$\nと表示してみると、$( A \\mathbf{x} - P \\mathbf{b} ) \\in \\mathcal{C} (A)$ と $(I -P )\\mathbf{b} \\in \\mathcal{N}(A)$ が直交していることが分かる。ピタゴラスの定理によって\n$$ \\left\\| \\mathbf{b} - A \\mathbf{x} \\right\\|_{2}^{2} = \\left\\| A \\mathbf{x} - P \\mathbf{b} \\right\\|_{2}^{2} + \\left\\| (I -P )\\mathbf{b} \\right\\|_{2}^{2} $$\nであり、$\\left\\| \\mathbf{b} - A \\mathbf{x} \\right\\|_{2}$ が最も小さくなるのは、$\\mathbf{x} = \\mathbf{x}_{\\ast}$ の時だ。\nまた、影の性質から $A \\in \\mathcal{C} (A)$ であり、$(I - P) \\mathbf{b} \\in \\mathcal{C} (A)^{\\perp}$ ので、\n$$ A^{\\ast} (I - P) \\mathbf{b} = A^{\\ast} ( \\mathbf{b} - A \\mathbf{x}_{\\ast} ) = 0 $$\n結論として$A^{\\ast} A \\mathbf{x}_{\\ast} = A^{\\ast} \\mathbf{b}$なので、最小二乗法とは、基本的に標準方程式 $A^{\\ast} A \\mathbf{x}_{\\ast} = A^{\\ast} \\mathbf{b}$ を満たす解 $\\mathbf{x}_{\\ast}$ を見つけることである。\n数式ではなく、図を通して直感的に理解するためには、次の例をみると役に立つだろう。\n上のように平面上に置かれた点をすべて通る直線を引く問題を考えてみよう。当然ながら、この問題の解となる直線(解)は存在せず、できる限り近く通る直線(近似解)を探さなければならない。\n緑と赤の線を比較すると、左の方が右よりも正確であることが一目でわかるだろう。青で描かれた線の長さは、各点が直線に射影されたときに離れた距離を示している。この問題での最小二乗解は、これらの距離の二乗の和が最小となるある直線である。\nHoward Anton, Elementary Linear Algebra: Aplications Version (12番目版, 2019), p417-418\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":356,"permalink":"https://freshrimpsushi.github.io/jp/posts/356/","tags":null,"title":"最小二乗法"},{"categories":"행렬대수","contents":"定義 正方行列 $P \\in \\mathbb{C}^{m \\times m}$ が $P^2 = P$ であれば、射影作用素Projectorという。\n説明 代数学の用語では、冪等元Idempotentと表現し、同様に $a^2 = a$ のような元を指す。\n$P$ が射影であるなら、$(I-P)^2 = I - 2P + P^2 = I - 2P + P = (I-P)$ であり、従って $(I-P)$ も射影であることがわかる。\nこのような射影作用素 $(I - P)$ を $P$ の補射影作用素Complementary Projectorと呼ぶ。\n射影を幾何学的に考えると、空間図形に光を当ててその影を得ることである。例えば、$f(x,y,z) := (x,y,0)$ のような関数は $z$ 軸の方向に光を投げて、$xy$ 平面にできる影を表す。その影を再度射影しても結果は同じで、その意味で $P^2 = P$ が射影作用素である定義は理にかなっていると言える。\n性質 射影 $P \\in \\mathbb{C}^{m \\times m}$ とその補射影 $I-P$ は次の性質を満たす。\n(a) $\\mathcal{C} (I-P) = \\mathcal{N} (P)$\n(b) $\\mathcal{N} (1-P) = \\mathcal{C} (P)$\n(c) $\\mathcal{N} (1-P) \\cap \\mathcal{N} (P) = \\left\\{ 0 \\right\\}$\n(d) $\\mathcal{C} (P) \\cap \\mathcal{N} (P) = \\left\\{ 0 \\right\\}$\n(e) $\\mathcal{C} (P) \\oplus \\mathcal{N} (P) = \\mathbb{C}^{m}$\n証明 (a)(b) $\\mathcal{C} ( I - P)$ と $\\mathcal{N}(P)$ が互いに含まれることを示せばよい。任意のベクトル $\\mathbb{v} \\in \\mathbb{C}^{m}$ に対して、\n$$ (I - P) \\mathbb{v} = \\mathbb{v} - P \\mathbb{v} $$\nもし $\\mathbb{v} \\in \\mathcal{N} (P)$ ならば、$P \\mathbb{v} = \\mathbb{0}$ となり、従って $(I - P) \\mathbb{v} = \\mathbb{v}$、つまり $\\mathbb{v} \\in \\mathcal{C} (I - P)$ であり、\n$$ \\mathcal{N} (P) \\subset \\mathcal{C} (I - P) $$\n$\\mathbb{w} \\in \\mathcal{C} (I-P)$ とすると、$\\mathbb{w} = (I - P) \\mathbb{v}$ を満たす $\\mathbb{v}$ も $\\mathcal{C} (I-P)$ に存在する。$\\mathbb{w} = (I - P) \\mathbb{v}$ に射影 $P$ を適用すると、\n$$ P \\mathbb{w} = P \\mathbb{v} - P^2 \\mathbb{v} = P \\mathbb{v} - P \\mathbb{v} = \\mathbb{0} $$\nつまり $\\mathbb{w} \\in \\mathcal{N} (P)$ であり、従って、\n$$ \\mathcal{C} (I - P) \\subset \\mathcal{N} (P) $$\nこれにより (1) が証明され、$P = I - (I- P)$ であるから、射影 $P$ の補射影 $(I - P)$ について考えると、すぐに (2) が証明される。\n■\n(c)(d) $\\mathbb{v} \\in \\mathcal{N} (I - P) \\cap \\mathcal{N} (P)$ が零ベクトルではないと仮定する。しかし、$\\mathbb{v} \\in \\mathcal{N} (I - P)$ であるから、$(I-P) \\mathbb{v} = \\mathbb{0}$ であり、$\\mathbb{v} \\in \\mathcal{N} (P)$ であるから、\n$$ P \\mathbb{v} = \\mathbb{0} $$\n両辺を足せば $(I-P) \\mathbb{v} + P \\mathbb{v} = \\mathbb{v} = \\mathbb{0}$ となり、これは仮定に矛盾する。\nこれにより (3) が証明され、1 と 2 によってすぐに 4 も証明される。\n■\n(e) 直和の定義に従い、存在性、排他性、一意性を示せばよい。\n(i) 存在性\n(a) により $\\mathcal{N} (P) = \\mathcal{C} (I - P)$ であり、任意のベクトル $\\mathbb{s} \\in \\mathbb{C}^{m}$ に対して、$P \\mathbb{s} \\in \\mathcal{C}(P)$ そして $(I - P)\\mathbb{s} \\in \\mathcal{C} (I - P)$ である。\n一方、$P \\mathbb{s} + (I - P) \\mathbb{s} = P \\mathbb{s} + \\mathbb{s} - P \\mathbb{s} = s \\in \\mathbb{C}^{m}$ であるから、$\\mathbb{s}$ は常に $\\mathcal{C}(P)$ と $\\mathcal{C} (I - P)$ の和として表すことができる。\n(ii) 排他性\n既に (d) で証明されている。\n(iii) 一意性\n上記の (ii) により、$s \\in \\mathbb{C}^{m}$ に対して、\n$$ \\mathbb{s} = \\mathbb{c}_{1} + \\mathbb{n}_{1} = \\mathbb{c}_{2} + \\mathbb{n}_{2} $$\nを満たす $\\mathbb{c}_{1} , \\mathbb{c}_{2} \\in \\mathcal{C}(P)$ と $\\mathbb{n}_{1}, \\mathbb{n}_{2} \\in \\mathcal{N}(P)$ が存在する。\nここで $\\mathbb{c}_{1} \\ne \\mathbb{c}_{2} $ と $\\mathbb{n}_{1} \\ne \\mathbb{n}_{2}$ と仮定する。\n$\\mathbb{c}_{1} + \\mathbb{n}_{1} = \\mathbb{c}_{2} + \\mathbb{n}_{2}$ の両辺に $P$ を乗じると、\n$$ P\\mathbb{c}_{1} + P\\mathbb{n}_{1} = P\\mathbb{c}_{2} + P\\mathbb{n}_{2} $$\n一方、$\\mathbb{n}_{1}, \\mathbb{n}_{2} \\in \\mathcal{N} (P)$ であるから、\n$$ P\\mathbb{c}_{1} = P\\mathbb{c}_{2} $$\nつまり $P( \\mathbb{c}_{1} - \\mathbb{c}_{2} ) = \\mathbb{0}$。零空間の定義から、$( \\mathbb{c}_{1} - \\mathbb{c}_{2}) \\in \\mathcal{N}(P)$ であり、ベクトル空間の性質から、$( \\mathbb{c}_{1} - \\mathbb{c}_{2}) \\in \\mathcal{C}(P)$ だが、(排他性) により、$\\mathbb{c}_{1} - \\mathbb{c}_{2} = \\mathbb{0}$ でなければならない。\nこれは $\\mathbb{c}_{1} \\ne \\mathbb{c}_{2}$ という仮定と矛盾するし、同様に $\\mathbb{n}_{1} = \\mathbb{n}_{2}$ も証明できる。\n■\n参考 抽象代数学での冪等元 ","id":352,"permalink":"https://freshrimpsushi.github.io/jp/posts/352/","tags":null,"title":"線形代数における射影"},{"categories":"선형대수","contents":"定義 ベクトル空間 $V$ の二つの部分空間 $W_{1}$と$W_{2}$に対して、次のことを満たせば、$V$を$W_{1}$と$W_{2}$の直和direct sumと呼び、$V = W_{1} \\oplus W_{2}$と表記する。\n(i) 存在性: 任意の$\\mathbf{v} \\in V$に対して、$\\mathbf{v} = \\mathbf{v}_{1} + \\mathbf{v}_{2}$を満たす$\\mathbf{v}_{1} \\in W_{1}$と$\\mathbf{v}_{2} \\in W_{2}$が存在する。\n(ii) 排他性: $W_{1} \\cap W_{2} = \\left\\{ \\mathbf{0} \\right\\}$\n(iii) 一意性: 与えられた$\\mathbf{v}$に対して、$\\mathbf{v} = \\mathbf{v}_{1} + \\mathbf{v}_{2}$を満たす$\\mathbf{v}_{1} \\in W_{1}$と$\\mathbf{v}_{2} \\in W_{2}$は一意である。\n一般化1 $W_{1}, W_{2}, \\dots, W_{k}$をベクトル空間 $V$の部分空間としよう。これらの部分空間が次の条件を満たすとき、$V$を$W_{1}, \\dots, W_{k}$たちの直和と呼び、$V = W_{1} \\oplus \\cdots \\oplus W_{k}$と表記する。\n$\\displaystyle V = \\sum\\limits_{i=1}^{k}W_{i}$\n$\\displaystyle W_{j} \\bigcap \\sum\\limits_{i \\ne j}W_{i} = \\left\\{ \\mathbf{0} \\right\\} \\text{ for each } j(1\\le j \\le k)$\nこのとき、$\\sum\\limits_{i=1}^{k}W_{i}$は$W_{i}$たちの和である。\n解説 (i) 存在性: この条件は$V = W_{1} + W_{2}$、つまり「$V$は$W_{1}$と$W_{2}$の和である」と書き換えることができる。\n(iii) 一意性: 実際、この条件は必要ない。条件**(ii)**により$\\mathbf{v}_{1} \\in W_{1}$であれば、$\\pm \\mathbf{v}_{1} \\notin W_{2}$であり、$W$のゼロベクトルに対して次の表現だけが存在する。\n$$ \\mathbf{0} = \\mathbf{0} + \\mathbf{0},\\quad \\mathbf{0}\\in W_{1}, W_{2} $$\nしたがって、$\\mathbf{v}$に対して、二つの表現$\\mathbf{v}_{1} + \\mathbf{v}_{2}$と$\\mathbf{v}_{1}^{\\prime} + \\mathbf{v}_{2}^{\\prime}$が存在するならば、\n$$ \\mathbf{0} = \\mathbf{v} - \\mathbf{v} = (\\mathbf{v}_{1} - \\mathbf{v}_{1}^{\\prime}) + (\\mathbf{v}_{2} - \\mathbf{v}_{2}^{\\prime}) = \\mathbf{0} + \\mathbf{0} \\implies \\mathbf{v}_{1}=\\mathbf{v}_{1}^{\\prime},\\ \\mathbf{v}_{2}=\\mathbf{v}_{2}^{\\prime} $$\nさらに、(i), (ii) $\\iff$ (iii) が成り立つ。\n定義を見ただけでは把握しづらいが、ユークリッド空間での例を見れば、これが非常に理にかなった便利な概念であることがわかる。例えば、$\\mathbb{R}^{3} = \\mathbb{R} \\times \\mathbb{R} \\times \\mathbb{R}$を考えると、$\\mathbb{R}^{3}$の要素は$3$次元ベクトル$(x,y,z)$であるが、これを$(x,y)$と$(z)$に分けてみよう。\n一方で、分けたものを再結合する過程を考えれば、$(x,y) \\in \\mathbb{R}^2$となり、それによって$(z) \\in \\mathbb{R}$となるため、これらの単純な和集合$\\mathbb{R}^2 \\cup \\mathbb{R}$は、スカラーと$2$次元ベクトルを要素に含むことになる。これらの記号だけでは、実際に私たちが望む空間の拡張と分離を表現するのが難しいことがわかる。だから、直和という概念を導入すると、部分空間がベクトル空間をきれいに分割するときに、多くの面で説明しやすくなるだろう。\n参照 抽象代数学での直和 ベクトル空間の和 $+$ Stephen H. Friedberg, Linear Algebra (第4版, 2002), p275\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":353,"permalink":"https://freshrimpsushi.github.io/jp/posts/353/","tags":null,"title":"ベクトル空間における直和"},{"categories":"행렬대수","contents":"概要 行列を常に固有値対角化を通じて分解できたらいいと思うが、残念ながら提供される行列が正方行列でなければならないという制限がある。非正方行列である行列の分解を拡張しようとする。\nビルドアップ 二つの自然数 $m \u0026gt; n$について、行列 $A \\in \\mathbb{C}^{ m \\times n}$の係数が$\\text{rank} A = n$で与えられるとする。それならば、$\\dim C(A) = \\dim C(A^{T}) = n $の正規直交ベクトル$\\mathbb{v}_{1} , \\cdots , \\mathbb{v}_{n} \\in C(A)$と$\\mathbb{u}_{1} , \\cdots , \\mathbb{u}_{n} \\in C(A^{T})$を考えることができる。\n固有値の幾何学的意味を考えるときと同様に $$ A \\mathbb{v}_{i} = \\sigma_{i} \\mathbb{u}_{i} $$ を満たす$\\sigma_{i}\u0026gt;0$が存在すると仮定する。すると$A$は$\\mathbb{v}_{i}$の方向を$\\mathbb{u}_{i}$と一致させる線形変換であり、$\\sigma_{i}\u0026gt;0$はその大きさを合わせるものと見ることができる。このような$\\sigma_{i}$は固有値ではなく、$A$の特異値と定義されるが、残念ながらその意味は特異値分解とは全く関係がない。ここで固有値の議論と異なるのは$A \\in \\mathbb{C}^{ m \\times n}$、つまり$\\mathbb{v}_{i} \\in \\mathbb{C}^{n}$であり、$\\mathbb{u}_{i} \\in \\mathbb{C}^{m}$であるため、$A$がベクトルの次元も変えるということである。行列$A$について整理して、$A = \\sigma_{i} \\mathbb{u}_{i} \\mathbb{v}_{i}^{\\ast}$であり、両辺の次元が$m \\times n = ( m \\times 1 ) \\times (1 \\times n )$になることを確認してみよう。一方、固有ベクトルとは異なり、左右が区別されるため、$\\mathbb{u}_{i}$は左特異ベクトル、$\\mathbb{v}_{i}$は右特異ベクトルと定義されることに注意しよう。今$A \\mathbb{v}_{i} = \\sigma_{i} \\mathbb{u}_{i}$を$1 \\le i \\le n$に対して展開して書くと $$ A \\begin{bmatrix} \\mathbb{v}_{1} \u0026amp; \\mathbb{v}_{2} \u0026amp; \\cdots \u0026amp; \\mathbb{v}_{n} \\end{bmatrix} = \\begin{bmatrix} \\mathbb{u}_{1} \u0026amp; \\mathbb{u}_{2} \u0026amp; \\cdots \u0026amp; \\mathbb{u}_{n} \\end{bmatrix} \\begin{bmatrix} \\sigma_{1} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\sigma_2 \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; 0 \\\\ 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\sigma_{m} \\end{bmatrix} $$ 簡潔に表現するため、以下のように表すことにしよう。 $$ V := \\begin{bmatrix} \\mathbb{v}_{1} \u0026amp; \\mathbb{v}_{2} \u0026amp; \\cdots \u0026amp; \\mathbb{v}_{n} \\end{bmatrix} \\in \\mathbb{C}^{n \\times n} \\\\ \\widehat{U} := \\begin{bmatrix} \\mathbb{u}_{1} \u0026amp; \\mathbb{u}_{2} \u0026amp; \\cdots \u0026amp; \\mathbb{u}_{n} \\end{bmatrix} \\in \\mathbb{C}^{m \\times n} \\\\ \\widehat{\\Sigma} := \\text{diag} ( \\sigma_1 , \\sigma_2 , \\cdots , \\sigma_n) $$ 便宜上、$\\sigma_{i} \\ge \\sigma_{j} \u0026gt; 0$としよう。整理すると $$ AV = \\widehat{U} \\widehat{\\Sigma} $$ であり、$V V^{\\ast} = V^{\\ast} V = I$であるため $$ A = \\widehat{U} \\widehat{\\Sigma} V^{\\ast} $$ もちろん$\\widehat{U} \\in \\ \\mathbb{C}^{m \\times n}$であり、$\\widehat{U}$がユニタリ行列ではないが、正規直交性を持つため、$\\widehat{U}^{\\ast} \\widehat{U} = I_{n}$であり、 $$ \\begin{align*} A^{\\ast}A =\u0026amp; (\\widehat{U} \\widehat{\\Sigma} V^{\\ast})^{\\ast} (\\widehat{U} \\widehat{\\Sigma} V^{\\ast}) \\\\ =\u0026amp; V \\widehat{\\Sigma} \\widehat{U}^{\\ast} \\widehat{U} \\widehat{\\Sigma} V^{\\ast} \\\\ =\u0026amp; V \\widehat{\\Sigma}^{2} V^{\\ast} \\\\ =\u0026amp; V \\widehat{\\Sigma}^{2} V^{-1} \\end{align*} $$\n$$ A = S \\begin{bmatrix} \\lambda_{1} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\lambda_2 \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; 0 \\\\ 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\lambda_{m} \\end{bmatrix} S^{-1} $$\nこれは正方行列 $A^{\\ast} A \\in \\mathbb{C}^{m \\times m}$に対して固有値対角化を完了した結果と見ることができる。$\\sigma^{2}_{i}$は$A^{\\ast}A$の固有値であり、$\\mathbb{v}_{i}$が$\\sigma^{2}_{i}$に対応する固有ベクトルになる。これで数値計算のために、このプロセスを逆にたどってみよう。\nアルゴリズム 行列 $A \\in \\mathbb{C}^{m \\times n} (m \\ge n)$について、$\\text{rank} A = n$であるとする。\nステップ 1.\n$A^{\\ast} A$の固有値$\\sigma_{1} \\ge \\sigma_{2} \\ge \\cdots \\ge \\sigma_{n}\u0026gt;0$を見つける。任意の$\\mathbb{x} \\ne \\mathbb{0}$に対して $$ \\mathbb{x}^{\\ast} A^{\\ast} A \\mathbb{x} = || A \\mathbb{x} ||^2 \u0026gt; 0 $$ つまり$A^{\\ast} A$が正定値であるため、$\\sigma_{i} \u0026gt;0$が保証される。\nステップ 2.\n見つかった固有値に対応する固有ベクトルとして正規直交ベクトル$V = \\begin{bmatrix} v_{1} \u0026amp; v_{2} \u0026amp; \\cdots \u0026amp; v_{n} \\end{bmatrix}$を見つける。\nステップ 3.\n$\\displaystyle \\mathbb{u}_{i} = {{1} \\over { \\sigma_{i} }} A \\mathbb{v}_{i}$を通じて$\\widehat{U} = \\begin{bmatrix} u_{1} \u0026amp; u_{2} \u0026amp; \\cdots \u0026amp; u_{n} \\end{bmatrix}$を見つける。\n縮小特異値分解と全特異値分解 $A = \\widehat{U} \\widehat{\\Sigma} V^{\\ast}$を満たす3つの行列に$A$を分割することを縮小reduced特異値分解(rSVD)と呼ぶ。\nこれを全体full特異値分解(fSVD)と区別するために、$\\widehat{U}$や$\\widehat{\\Sigma}$のような表記が使われた。全特異値分解のアイデアは、単に縮小特異値分解を少し拡張したものに過ぎない。行列 $$ U := \\begin{bmatrix} \\widehat{U} \u0026amp; \\mathbb{u}_{n+1} \u0026amp; \\cdots \u0026amp; \\mathbb{u}_{m} \\end{bmatrix} \\in \\mathbb{C}^{m \\times m} $$ がユニタリ行列になるような$\\mathbb{u}_{n+1} , \\cdots , \\mathbb{u}_{m}$を見つけて、$\\widehat{\\Sigma}$の下部に$0$を追加する形で行列$\\Sigma := \\begin{bmatrix} \\widehat{\\Sigma} \\\\ O \\end{bmatrix}$ を構成し、$A = U \\Sigma V^{\\ast}$の形で分解することである。\nfSVDは理論的により堅牢であり、rSVDは実際の計算により有利であるだろう。fSVDであれrSVDであれ、SVDを行うということは、$U$または$\\widehat{U}$、$\\Sigma$または$\\widehat{\\Sigma}$、$V$を求めるということだ。\n参照 ユークリッド空間における楕円の拡張、楕円体 ","id":340,"permalink":"https://freshrimpsushi.github.io/jp/posts/340/","tags":null,"title":"行列の特異値分解"},{"categories":"행렬대수","contents":"証明 $A$は可逆行列であるため、$A$の線形独立な固有ベクトル$\\mathbb{x}_{1}, \\mathbb{x}_{2}, \\cdots , \\mathbb{x}_{m}$が存在する。 $$ \\begin{align*} AS =\u0026amp; A \\begin{bmatrix} \\mathbb{x}_{1}, \\mathbb{x}_{2}, \\cdots , \\mathbb{x}_{m} \\end{bmatrix} \\\\ =\u0026amp; \\begin{bmatrix} A \\mathbb{x}_{1}, A \\mathbb{x}_{2}, \\cdots , A \\mathbb{x}_{m} \\end{bmatrix} \\\\ =\u0026amp; \\begin{bmatrix} \\lambda_{1} \\mathbb{x}_{1}, \\lambda_{2} \\mathbb{x}_{2}, \\cdots , \\lambda \\mathbb{x}_{m} \\end{bmatrix} \\\\ =\u0026amp; \\begin{bmatrix} \\mathbb{x}_{1}, \\mathbb{x}_{2}, \\cdots , \\mathbb{x}_{m} \\end{bmatrix} \\begin{bmatrix} \\lambda_{1} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\lambda_2 \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; 0 \\\\ 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\lambda_{m} \\end{bmatrix} \\\\ =\u0026amp; S \\begin{bmatrix} \\lambda_{1} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\lambda_2 \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; 0 \\\\ 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\lambda_{m} \\end{bmatrix} \\end{align*} $$\n$S$は線形独立な固有ベクトルで構成された行列であるため、$S^{-1}$が存在し、左側に移すと $$ S^{-1} A S = \\begin{bmatrix} \\lambda_{1} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\lambda_2 \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\ddots \u0026amp; 0 \\\\ 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\lambda_{m} \\end{bmatrix} $$\n■\n","id":339,"permalink":"https://freshrimpsushi.github.io/jp/posts/339/","tags":null,"title":"可逆行列の固有値対角化"},{"categories":"양자역학","contents":"定義 昇降作用素raising operator: $L_+ \\equiv L_{x} + iL_{y}$ 降下作用素lowering operator: $L_- \\equiv L_{x} - iL_{y}$、すなわち、$(L_-)^{\\ast}=L_+$ 説明 なぜこれらの作用素が「昇る」、「降りる」と呼ばれるのか？それは、角運動量の大きさの二乗の同時固有関数と角運動量成分に適用した時に、固有関数の状態が昇ったり降りたりするからだ。これは角運動量の二乗と角運動量成分の同時固有関数、固有値を求めるで確認できる。\nこの記事では、計算のための様々な関係式と交換子を取りあげる。また、他の作用素の固有ベクトル（固有関数）の固有値を変える作用素を梯子作用素$\\mathrm{Ladder\\ Operator}$という。固有値を増加させる作用素を昇降作用素、減少させる作用素を降下作用素という。※注意 : 作用素の操作順序はとても重要で、普段数値を扱うように簡単に扱ってはいけない。任意の二つの作用素は一般に交換則が成り立たないからである。例えば、$(A+B)^2$を展開する時に、$A^2+2AB+B^2$は間違っている。$(A+B)^2=(A+B)(A+B)=A^2+AB+BA+B^2$として展開しなければならない。\n$$ \\begin{align*} 1)\\ L_+L_-= (L_{x} + iL_{y})(L_{x}-iL_{y}) =\u0026amp;\\ {L_{x}}^2 + iL_{y}L_{x} - iL_{x}L_{y} + {L_{y}}^2 \\\\ =\u0026amp;\\ {L_{x}}^2+{L_{y}}^2 -i[L_{x},L_{y}] \\\\ =\u0026amp;\\ {\\vec L}^2 -{L_{z}}^2 +\\hbar L_{z} \\end{align*} $$\n$$ \\begin{align*} 2)\\ L_-L_+= (L_{x} - iL_{y})(L_{x} + iL_{y}) =\u0026amp;\\ {L_{x}}^2 - iL_{y}L_{x} + iL_{x}L_{y} + {L_{y}}^2 \\\\ =\u0026amp;\\ {L_{x}}^2+{L_{y}}^2 +i[L_{x},L_{y}] \\\\ =\u0026amp;\\ {\\vec L}^2 -{L_{z}}^2 -\\hbar L_{z} \\end{align*} $$ 1)$과 $2)$의 결과를 종합하면\n$$ \\begin{align*} 3)\\ {\\vec L}^2 =\u0026amp;\\ L_+L_- + {L_{z}}^2 -\\hbar L_{z} \\\\ =\u0026amp;\\ L_-L_+ + {L_{z}}^2 +\\hbar L_{z} \\\\ =\u0026amp;\\ L_\\pm L_\\mp + {L_{z}}^2 \\mp \\hbar L_{z} \\end{align*} $$\n$$ \\begin{align*} 4)\\ [L_{z},L_\\pm] =\u0026amp;\\ [L_{z},L_{x} \\pm iL_{y}] \\\\ =\u0026amp;\\ [L_{z},L_{x}] \\pm i [L_{z},L_{y}] \\\\ =\u0026amp;\\ i\\hbar L_{y} \\pm i (-i\\hbar L_{x}) \\\\ =\u0026amp;\\ \\pm \\hbar(L_{x} \\pm iL_{y}) \\\\ =\u0026amp;\\ \\pm \\hbar L_\\pm \\end{align*} $$\n$$ \\begin{align*} 5)\\ [{\\vec L}^2, L_\\pm] =\u0026amp;\\ [\\vec L^2, L_{x} \\pm iL_{y}] \\\\ =\u0026amp;\\ [{\\vec L}^2,L_{x}] \\pm i [{\\vec L}^2 , L_{y}] \\\\ =\u0026amp;\\ 0 \\end{align*} $$ ( \\because [${\\vec L}^2, L_{i}]=0$ 참고 $)\n$L_+$と$L_-$を連立すると：\n$$ \\begin{align*} 6)\\ L_{x}=\u0026amp;\\ \\dfrac{1}{2} (L_+ + L_-) \\\\ L_{y} =\u0026amp;\\ \\dfrac{-i}{2}(L_{x} - L_-) \\end{align*} $$\n","id":344,"permalink":"https://freshrimpsushi.github.io/jp/posts/344/","tags":null,"title":"角運動量のためのはしご演算子：上昇演算子と下降演算子"},{"categories":"R","contents":"概要 Rは代表的な統計プログラミング言語で、便利なメソッドだけでなく、例示に適したデータセットも提供しています。このようなデータセットがなければ、講義のたびに新しいデータをダウンロードして読み込む手間をかけなければならないでしょう。\nガイド データセットを読み込む方法は非常に簡単です。読み込みたいデータセットの名前を使用する変数に割り当てるだけです。統計学を勉強するうちに何度も見ることになるアイリス（花）データを見てみましょう。\n各列は順に、がくの長さ、幅、花びらの長さ、幅、種類を意味します。列ごとに名前は書かれていますが、これだけでデータを把握するのが難しい場合は、?iris を入力してヘルプを読んでみましょう。\nもちろん、アイリスだけがデータセットにあるわけではありません。コンソール窓に library(help=datasets) を入力すると、以下のように読み込むことができるデータセットのリストと簡単な説明を見ることができます。\n大まかな分類 回帰分析 attitude LifeCycleSavings Loblolly attenu faithful iris quakes wiss trees 時系列 AirPassengers BJsales EuStockMarkets WorldPhones JohnsonJohnson LakeHuron Nile UKDriverDeaths UKgas USAccDeaths USPersonalExpenditure WWWusage airmiles airquality austres co2 discoveries freeny lh longley lynx nhtemp nottem presidents sunspot.month sunspot.year sunspots treering uspop 多変量 Harman23.cor Harman74.cor USJudgeRatings カテゴリー HairEyeColor Titanic UCBAdmissions ability.cov 実験 CO2 ChickWeight DNase Indometh InsectSprays Orange OrchardSprays PlantGrowth Puromycin Theoph cars chickwts morley mtcars npk pressure warpbreaks 小標本 BOD Formaldehyde VADeaths anscombe euro sleep stackloss women その他 crimtab esoph eurodist islands occupationalStatus precip randu rivers rock volcano ","id":331,"permalink":"https://freshrimpsushi.github.io/jp/posts/331/","tags":null,"title":"Rで組み込みデータセットを読み込む方法"},{"categories":"추상대수","contents":"定義 1 群 $\\left\u0026lt; G, \\ast\\ \\right\u0026gt;$の二つの元$a, b$に対して$a \\ast\\ b = b \\ast\\ a$が成り立つ場合、$\\left\u0026lt; G, \\ast\\ \\right\u0026gt;$を可換群Abelian Groupと定義する。\n説明 可換とは「交換法則が成り立つ」という意味で捉えてもいい。英語ではCommutativeの代わりにAbelianという言葉が使われており、これは天才数学者アーベルにちなんで名付けられた。もちろん、韓国語でアーベル群と呼んでも、意味の伝達上全く問題はない。\n可換群となると、もうかなり多くの条件を満たしたため、想像しにくい構造ではない。群でありながら可換群になれない例を見てみよう。\n逆行列が存在する正方行列の集合$\\text{GL}_{n} (\\mathbb{R}) = \\left\\{ A \\in \\mathbb{R}^{n \\times n} \\ | \\ \\det A \\ne 0 \\right\\}$において、群$\\left\u0026lt; \\text{GL}_{n} (\\mathbb{R}) , \\cdot \\right\u0026gt;$は可換群ではない。\n行列の乗算は交換法則が成り立たない。 行列の操作を扱う上で乗算では交換法則が成り立たないことが重要だと認識されることが多い。それだけ交換法則は我々が日常で扱う数において当然の性質であるため、注意すべきことを意味している。逆に言えば、交換法則を満たす例はかなり多く、それらの例は通常、我々にとって親しいものであることを意味している。\n群$\\left\u0026lt; \\mathbb{R} , + \\right\u0026gt;$は可換群である。\n実数の加算は交換法則が成り立つ。 我々にとって最も身近な実数だけ考えてもそうだし、複素数や有理数、整数も同様である。通常、群でありながら可換群でない例を見つける方がずっと難しい。\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p39.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":309,"permalink":"https://freshrimpsushi.github.io/jp/posts/309/","tags":null,"title":"抽象代数学における可換群"},{"categories":"복소해석","contents":"要約 1 解析的関数 $f: A \\subset \\mathbb{C} \\to \\mathbb{C}$ が単純閉曲線 $\\mathscr{C}$ 内部に有限個の特異点 $z_{1} , z_{2} , \\cdots , z_{m}$ を持つとする。すると、 $$ \\int_{\\mathscr{C}} f(z) dz = 2 \\pi i \\sum_{k=1}^{m} \\text{Res}_{z_{k}} f(z) $$\n説明 初めて読むと、この定理はとても不思議に感じるかもしれない。積分値を求めなければならないが、微積分的な計算はなく、特異点や留数の話が出てくるので、困惑することもあるだろう。定理を見る限り、留数を足すだけで積分値を見つけることができるようだが、本当にそうなのか？意外なことに、答えは「はい」で、それが留数定理の役割である。\nこのアプローチは、積分計算を他の計算に置き換えるだけで、解けなかった多くの積分が可能になる。実数では扱えなかったいくつかの積分も留数定理を使えば比較的簡単に解決される。複素解析には重要な定理がたくさんあるが、留数定理は特に有用な結果をたくさん与えるため、必ず理解しておくべきだ。\n証明 まず、$\\mathscr{C}$ を $m$ 個に分割して考えよう。\n分割に対する一般化された収縮補助定理: 単純閉曲線 $\\mathscr{C}$ を含む単純連結領域内で、$\\mathscr{C}$ 内部の有限個の点 $z_{1} , z_{2}, \\cdots z_{m}$ を除く全ての点で解析的であるとする $f: A \\subseteq \\mathbb{C} \\to \\mathbb{C}$。その場合、$\\mathscr{C}$ 内部で$z_{k}$ を中心とする円 $\\mathscr{C_k}$ に対して、 $$ \\int_{\\mathscr{C}} f(z) dz = \\sum_{k=1}^{m} \\int_{\\mathscr{C}_{k}} f(z) dz $$\n各 $\\mathscr{C}_{k}$ をローラン展開すると、 $$ \\int_{\\mathscr{C}_{k}} f(z) dz = \\int_{\\mathscr{C}_{k}} \\sum_{n = 0 }^{\\infty} a_{nk} (z-z_{k}) ^{n} dz + \\int_{\\mathscr{C}_{k}} \\sum_{n = 1 }^{\\infty} { {b_{nk} } \\over{ (z-z_{k}) ^{n} } } dz $$ コーシーの定理により、 $$ \\int_{\\mathscr{C}_{k}} f(z) dz = \\int_{\\mathscr{C}_{k}} \\sum_{n = 1 }^{\\infty} { {b_{nk} } \\over{ (z-z_{k}) ^{n} } } dz $$ 一方で、$\\int_{\\mathscr{C}_{k}} {{1} \\over {(z - z_{k})^n}} dz = \\begin{cases} 2 \\pi i \u0026amp; n = 1 \\\\ 0 \u0026amp; n \\ge 2 \\end{cases}$ なのでコーシーの積分公式によると、 $$ \\int_{\\mathscr{C}_{k}} f(z) dz = 2 \\pi i b_{1k} = 2 \\pi i \\text{Res}_{z_{k}} f(z) $$ だから、 $$ \\int_{\\mathscr{C}} f(z) dz = 2 \\pi i \\sum_{k=1}^{m} \\text{Res}_{z_{k}} f(z) $$\n■\n注意点 証明で特に留意すべき点は、$n=1$ のとき、$\\displaystyle {{1} \\over {z - z_{k}}}$ 係数、つまり留数 $b_1k$ を除き、その他すべてが $0$ になって消えるということである。留数定理があまりにも有用であるため、その応用しか勉強していないと、なぜそのような結果が出るのかさえも忘れがちになる。コーシーの積分公式とローラン展開の形を思い出せるなら、十分に密度の高い勉強をしたと言えるだろう。\nOsborne (1999). Complex variables and their applications: p153.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":308,"permalink":"https://freshrimpsushi.github.io/jp/posts/308/","tags":null,"title":"留数定理の証明"},{"categories":"양자역학","contents":"説明 $A^{\\dagger}$は[アイ ダガー]と読む。ここでのdaggerは短剣のことだ。\nフランスの数学者Hermiteの名前にちなんで名付けられた。英語風にはハーミシャン演算子と読む。\n量子力学に出てくるすべての演算子はエルミート演算子だ。\n数学では、複素共役の表記法は$\\overline{a+ib}$で、共役転置の表記法は$A^{\\ast}$である。物理学では、$\\ast$の意味を複素共役に限定して説明することが多く、$A^{\\dagger} = (A^{T})^{\\ast}$と表現することもある。しかし、ディラック表記法を考えると、物理学で使用する$\\ast$にも「共役 + 転置」の両方の意味が含まれていることがわかる。つまり、物理学では複素共役の表記法と共役転置の表記法を重複して$\\ast$と書いている。$\\ast$がついた対象がスカラーならば複素共役を、行列やベクターならば共役転置を意味する。$\\ast$を共役転置ではなく複素共役としてだけ考えると、行ベクター、列ベクターが混乱して行列積をする際に間違える可能性があるので、次のように覚えておこう。\n$\\ast =$ 共役 $+$ 転置 $=\\dagger$\nProperties The expectation value (eigenvalue) of a Hermitian operator is always real.\nThe different eigenfunctions (eigenvectors) of a Hermitian operator are orthogonal to each other.\nFor a Hermitian operator $A$, the following equation holds.\n$$ \\left\\langle A\\psi|\\phi \\right\\rangle = \\left\\langle \\psi|A\\phi \\right\\rangle $$\nThe condition under which the product $AB$ of two Hermitian operators $A,B$ is a Hermitian operator is $[A,B]=0$.\nFor any operator $A$, the equation below is always a Hermitian operator.\n$$ A+A^\\dagger \\\\ i(A-A^\\dagger) \\\\ AA^\\dagger $$\nProof 3 For two matrices $A, B$, since $(AB)^{\\dagger} = B^{\\dagger}A^{\\dagger}$,\n$$ \\begin{align*} \\langle A\\psi|\\phi \\rangle =\u0026amp;\\ \\int(A\\psi)^{\\dagger}\\phi dx \\\\ =\u0026amp;\\ \\int \\psi^{\\dagger} A^{\\dagger} \\phi dx \\\\ =\u0026amp;\\ \\langle \\psi|A^{\\dagger}\\phi \\rangle \\\\ =\u0026amp;\\ \\langle \\psi|A\\phi \\rangle \\end{align*} $$\n■\n","id":304,"permalink":"https://freshrimpsushi.github.io/jp/posts/304/","tags":null,"title":"エルミート演算子"},{"categories":"양자역학","contents":"定義 量子力学では、波動関数はベクトルであり、基本的に列ベクトルとして扱われます。列ベクトルは右の矢印かっこで示され、これをケットベクトルket vectorと言います。\n$$ \\psi = \\ket{\\psi} = \\begin{pmatrix} \\psi_{1} \\\\ \\psi_{2} \\\\ \\vdots \\\\ \\psi_{n} \\end{pmatrix} $$\n$\\ket{\\psi}$の共役転置行列を左の矢印かっこで示し、これをブラベクトルbra vectorと言います。\n$$ \\psi^{\\ast} = \\ket{\\psi}^{\\ast} = \\bra{\\psi} = \\begin{pmatrix} \\psi_{1}^{\\ast} \u0026amp; \\psi_{2}^{\\ast} \u0026amp; \\cdots \u0026amp; \\psi_{n}^{\\ast} \\end{pmatrix} $$\n$^{\\ast}$は共役転置を意味します。\n説明 共役転置 物理学では、$^{\\ast}$は通常、複素共役と説明されますが、数学では、$^{\\ast}$は複素共役と転置行列の両方の意味を持っています。しかし、上の定義を見ると、実は物理学でも転置transposeの意味を持つ表記であることがわかります。つまり、物理学ではスカラーに$\\ast$が付いているときは複素共役を意味し、ベクトルや行列に付いているときは共役転置を意味するために、表記法を重複して使用しています。$^{\\ast}$を単に複素共役の意味だけで考えると、$\\psi$が列ベクトルのとき、$\\psi^{\\ast}$がなぜ行ベクトルなのかわかりませんので、注意しましょう。\n名前の由来 名前の由来は、括弧を意味する英単語ブラケットbracketです。括弧を半分ずつ使うので、単語も半分ずつ分けて名前にしています。簡単に日本語で例えると、ブラ-ベクトル(bra-vector)とケット-ベクトル(ket-vector)と言えます。\n内積との関連性 量子力学では、演算子と波動関数（固有関数）を便利に操作するために使用される一つの表記法です。矢印かっこを使う理由は、それが内積と関連があるためです。数学では、一般化された内積の表記は$\\left\\langle \\quad \\right\\rangle$と書きます。従って、行ベクトルと列ベクトルを上記のように表記すると、二つのベクトルの乗算自体が内積になるため、意味が合っています。\n$$ \\braket{ \\psi \\vert \\phi } = \\begin{pmatrix} \\psi_{1}^{\\ast} \u0026amp; \\psi_{2}^{\\ast} \u0026amp; \\cdots \u0026amp; \\psi_{n}^{\\ast} \\end{pmatrix}\\begin{pmatrix} \\phi_{1} \\\\ \\phi_{2} \\\\ \\vdots \\\\ \\phi_{n} \\end{pmatrix} = \\psi_{1}^{\\ast}\\phi_{1} + \\psi_{2}^{\\ast}\\phi_{2} + \\cdots + \\psi_{n}^{\\ast}\\phi_{n} $$\n任意の演算子$Q$の期待値は次のようになります。\n$$ \\braket{Q}= \\int \\psi^{\\ast} Q \\psi dx $$\nこれは、$\\psi$と$Q\\psi$の内積と見ることができ、従って次のように表記することができます。\n$$ \\braket{\\psi \\vert Q\\psi} $$\nあるいは、$Q^{\\ast}\\psi$と$\\psi$の内積として考えることもできます。\n$$ \\int \\psi^{\\ast} Q \\psi dx = \\int (Q^{\\ast}\\psi)^{\\ast} \\psi dx = \\braket{Q^{\\ast}\\psi \\vert \\psi} $$\n下記の表記法はすべて同じ式を意味します。\n$$ \\braket{Q} = \\braket{\\psi \\vert Q \\vert \\psi} = \\braket{\\psi \\vert Q \\psi} = \\braket{Q^{\\ast}\\psi \\vert \\psi} = \\int \\psi^{\\ast} Q \\psi dx $$\n","id":303,"permalink":"https://freshrimpsushi.github.io/jp/posts/303/","tags":null,"title":"ディラックの記法とは?"},{"categories":"선형대수","contents":"定義1 ベクトル空間 $V$ の 基底 の要素（ベクトルの数）を $V$ の 次元dimension と定義し、以下のように表記する。\n$$ \\dim (V) $$\n説明 このような次元の一般化は、単にベクトル空間に対する探求を超えて、この社会を支える様々な技術に応用されている。世界が $3$ 次元で、描けもしない $4$ 次元が何の役に立つのかと思うかもしれないが、ユークリッド空間だけがベクトル空間ではないからである。例えば、統計学で使われる データセットを考えると、それをベクトルとして見ることができる。例えば、「アダム」という人が身長が175、体重が62、年齢が22、IQが103、視力が1.2であるとすると、「アダム=(175,62,22,103,1.2)」と表せるのである。こんな単純なデータでさえ、既に $5$ 次元を使用しており、些細な制限があると役に立たなくなる。\n一方で、ベクトル空間の基底が一意ではないことを考慮すると、上記の定義が妥当な定義であるためには、すべての基底が同じ数の要素を持つ必要があるという条件が必要である。以下の二つの定理から、有限次元ベクトル空間のすべての基底が同じ数のベクトルを持たなければならないことがわかる。\n定理 $S = \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots \\mathbf{v}_{n} \\right\\}$ をベクトル空間 $V$ の任意の基底とする。\n(a) 基底よりもベクトルの数が多い $V$ の部分集合は 線形従属である。\n(b) 基底よりもベクトルの数が少ない $V$ の部分集合は、$V$ を 生成できない。\n証明2 (a) $W=\\left\\{ \\mathbf{w}_{1},\\ \\mathbf{w}_{2},\\ \\cdots ,\\ \\mathbf{w}_{m} \\right\\} \\subset V$ とすると、$m \u0026gt; n$ である。$S$ が $V$ の基底であるので、$W$ の要素は $S$ のベクトルの 線形結合 で表すことができる。\n$$ \\begin{equation} \\begin{aligned} \\mathbf{w}_{1} \u0026amp;= a_{11}\\mathbf{v}_{1}+a_{21}\\mathbf{v}_{2} + \\cdots + a_{n1}\\mathbf{v}_{n}=\\sum \\limits _{i}^{n} a_{i1}\\mathbf{v}_{i} \\\\ \\mathbf{w}_{2} \u0026amp;= a_{12}\\mathbf{v}_{1}+a_{22}\\mathbf{v}_{2} + \\cdots + a_{n2}\\mathbf{v}_{n}=\\sum \\limits _{i}^{n} a_{i2}\\mathbf{v}_{i} \\\\ \u0026amp; \\vdots \\\\ \\mathbf{w}_{m} \u0026amp;= a_{1m}\\mathbf{v}_{1}+a_{2m}\\mathbf{v}_{2} + \\cdots + a_{nm}\\mathbf{v}_{n}=\\sum \\limits _{i}^{n} a_{im}\\mathbf{v}_{i} \\end{aligned} \\label{wlincom1} \\end{equation} $$\n$W$ が線形従属であることを示すために、\n$$ \\begin{equation} k_{1}\\mathbf{w}_{1} + k_2\\mathbf{w}_{2} + \\cdots + k_{m}\\mathbf{w}_{m}= \\mathbf{0} \\label{wlincom2} \\end{equation} $$\n$(k_{1},k_{2},\\dots,k_{m}) \\ne (0,0,\\dots,0)$ が存在することを示せばよい。$(1)$ を $(2)$ に代入すると、次のようになる。\n$$ \\begin{align*} \u0026amp;k_{1}(a_{11}\\mathbf{v}_{1} + a_{21}\\mathbf{v}_{2} + \\cdots + a_{n1}\\mathbf{v}_{n}) \\\\ + \u0026amp;k_2(a_{12}\\mathbf{v}_{1} + a_{22}\\mathbf{v}_{2} + \\cdots + a_{n2}\\mathbf{v}_{n}) \\\\ + \u0026amp;\\cdots \\\\ + \u0026amp;k_{m}(a_{1m}\\mathbf{v}_{1} + a_{2m}\\mathbf{v}_{2} + \\cdots + a_{nm}\\mathbf{v}_{n}) = \\mathbf{0} \\end{align*} $$\nこれを $\\mathbf{v}_{i}$ に対して整理すると、次のようになる。\n$$ \\left( \\sum \\limits _{j} ^{m} k_{j}a_{1j} \\right)\\mathbf{v}_{1} + \\left( \\sum \\limits _{j} ^{m} k_{j}a_{2j} \\right)\\mathbf{v}_{2} + \\cdots + \\left( \\sum \\limits _{j} ^{m} k_{j}a_{nj} \\right)\\mathbf{v}_{n} = \\mathbf{0} $$\nこの時、$S$ が $V$ の基底であり、線形独立であるため、上記の方程式を満たす解は、係数がすべて $0$ の場合のみである。したがって、次の方程式が成り立つ。\n$$ \\begin{align*} a_{11}k_{1} + a_{12}k_{2} + \\cdots + a_{1m}k_{m} = 0 \\\\ a_{21}k_{1} + a_{22}k_{2} + \\cdots + a_{2m}k_{m} = 0 \\\\ \\vdots \\\\ a_{n1}k_{1} + a_{n2}k_{2} + \\cdots + a_{nm}k_{m} = 0 \\end{align*} $$\n連立方程式を見ると、方程式の数は $n$ 個、未知数 $k$ の数は $m$ 個である。方程式の数よりも未知数の数が多いため、この連立方程式は無数の非自明な解を持つ。したがって、$(2)$ を満たすすべてが $0$ ではない $k_{1},\\dots,k_{m}$ が存在する。したがって、$W$ は線形従属である。また、この証明は基底よりも要素の数が多い任意の集合にも適用される。\n■\n(b) 背理法で証明する。\n$W=\\left\\{ \\mathbf{w}_{1},\\ \\mathbf{w}_{2},\\ \\cdots ,\\ \\mathbf{w}_{m} \\right\\} \\subset V$ とすると、$m \u0026lt; n$ である。そして、$W$ が $V$ を生成すると仮定してみる。すると、$V$ のすべてのベクトルを $W$ の線形結合で表すことができる。\n$$ \\begin{equation} \\begin{aligned} \\mathbf{v}_{1} \u0026amp;= a_{11}\\mathbf{w}_{1}+a_{21}\\mathbf{w}_{2} + \\cdots + a_{m1}\\mathbf{w}_{m} \\\\ \\mathbf{v}_{2} \u0026amp;= a_{12}\\mathbf{w}_{1}+a_{22}\\mathbf{w}_{2} + \\cdots + a_{m2}\\mathbf{w}_{m} \\\\ \u0026amp; \\vdots \\\\ \\mathbf{v}_{n} \u0026amp;= a_{1n}\\mathbf{w}_{1}+a_{2n}\\mathbf{w}_{2} + \\cdots + a_{mn}\\mathbf{w}_{m} \\end{aligned} \\label{vlincom1} \\end{equation} $$\nすると、$\\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots \\mathbf{v}_{n} \\right\\}$ が線形従属であるという矛盾が生じる。次の同次方程式を見てみよう。\n$$ k_{1}\\mathbf{v}_{1} + k_2\\mathbf{v}_{2} + \\cdots + k_{n}\\mathbf{v}_{n}= \\mathbf{0} $$\nここに $(1)$ を代入すると、次のようになる。\n$$ \\begin{align*} \u0026amp;k_{1}(a_{11}\\mathbf{w}_{1} + a_{21}\\mathbf{w}_{2} + \\cdots + a_{m1}\\mathbf{w}_{m}) \\\\ + \u0026amp;k_2(a_{12}\\mathbf{w}_{1} + a_{22}\\mathbf{w}_{2} + \\cdots + a_{m2}\\mathbf{w}_{m}) \\\\ + \u0026amp;\\cdots \\\\ + \u0026amp;k_{n}(a_{1n}\\mathbf{w}_{1} + a_{2n}\\mathbf{w}_{2} + \\cdots + a_{mn}\\mathbf{w}_{m}) = \\mathbf{0} \\end{align*} $$\nこれを $\\mathbf{w}_{i}$ に対して整理すると、次のようになる。\n$$ \\left( \\sum \\limits _{j} ^{n} k_{j}a_{1j} \\right)\\mathbf{w}_{1} + \\left( \\sum \\limits _{j} ^{n} k_{j}a_{2j} \\right)\\mathbf{w}_{2} + \\cdots + \\left( \\sum \\limits _{j} ^{n} k_{j}a_{mj} \\right)\\mathbf{w}_{m} = \\mathbf{0} $$\nすると、未知数 $k$ に関する次の同次線形システムが得られる。\n$$ \\begin{align*} a_{11}k_{1} + a_{12}k_{2} + \\cdots + a_{1n}k_{n} = 0 \\\\ a_{21}k_{1} + a_{22}k_{2} + \\cdots + a_{2n}k_{n} = 0 \\\\ \\vdots \\\\ a_{m1}k_{1} + a_{m2}k_{2} + \\cdots + a_{mn}k_{n} = 0 \\end{align*} $$\n未知数の数が $n$ で、方程式の数が $m$ であり、$m \u0026lt; n$ であるので、この線形システムは無数の非自明な解を持つ。したがって、$S = \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots \\mathbf{v}_{n} \\right\\}$ が線形従属であるという結果が得られ、これは $S$ が線形独立であるという事実と矛盾する。したがって、仮定は間違っていることがわかる。よって、$W$ は $V$ を生成することができない。\n■\nHoward Anton, Elementary Linear Algebra: Applications Version (第12版, 2019), p248\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHoward Anton, Elementary Linear Algebra: Applications Version (第12版, 2019), p252-253\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3018,"permalink":"https://freshrimpsushi.github.io/jp/posts/3018/","tags":null,"title":"ベクトル空間の次元"},{"categories":"양자역학","contents":"ビルドアップ 数学では、関数functionとは、ある集合$X$の各要素に別の集合$Y$の要素を一対一で対応させる関係のことを言い、functionの頭文字を取ってよく$f$と表記される。$X$の集合の$x$という要素を$Y$の集合の$y$という要素に対応させる関数が$f$である場合、次のように表記される。\n$$ y = f(x) $$\nもし、その関係が$x$に対して具体的に与えられているなら、次の例のように表される。\n$$ y = 3x^{3} -2 x^{2} + x + 1 \\quad \\text{and} \\quad y = e^{3x} \\quad \\text{and} \\quad y = 5 \\cos x $$\n多くの理工学生は、おそらく関数というものを、ある数$x$と別の数$y$の関係として考えているだろう。実際に接するほとんどの関数がそうであるからだ。しかし、上の定義を見れば分かるように、関数が必ずしも数と数を結びつけなければならないわけではない。つまり、集合$X$と$Y$が数で構成された集合でなくても構わないということだ。例えば、ある関数と数を結びつける関数を考えることができ、これは古典力学でハミルトンの原理を学ぶ時に作用actionという概念で出会える。\n汎関数の定義\nある関数に別の関数を代入してそれによりある数が出る時、その関数を汎関数functionalという。例えば、下に定義された関数$F$は汎関数である。\n$$ {\\color{blue}F\\big( {\\color{orange}f(x)} \\big)} := {\\color{red} \\int_{1}^{2} f(x) dx} $$\nつまり関数$F$はある関数を$1$から$2$まで定積分した値を関数値として持つ。実際に計算してみると\n$$ {\\color{blue}F( {\\color{orange} e^{x} })} = \\int_{1}^2 e^x dx = {\\color{red}e^2-e},\\quad {\\color{blue}F({\\color{orange}x^2})}=\\int_{1}^2 x^{2} dx = {\\color{red}\\frac{7}{3} } $$\nそれでは、もう少し進んで集合$X$と$Y$を関数の集合としてみよう。この場合にも、$X$を$Y$に対応させる関数を考えることができる。このような関数の具体的な例として微分がある。$f$の関数を微分としよう。すると$f$は二次多項式$x^{2} + 3x + 1$を一次多項式$2x + 3$に対応させる関数になる。\n$$ f\\left( x^{2} + 3x + 1 \\right) = 2x + 3 $$\n次に、$g$の関数を不定積分としよう。すると$g$は$\\cos x$を$\\sin x$に対応させる関数になる。（積分定数は省略しよう）\n$$ g(\\cos x) = \\sin x $$\nけれども、関数を関数に対応させる関数という言い回しには、「関数」という言葉が繰り返し登場して混乱しやすく、良い表現ではない。「だから」量子力学では、このような関数に特別な名前を付ける。\n定義 量子力学では、（波）関数を（波）関数に対応させる関数を特に演算子operatorと呼ぶ。\n説明 演算子は、関数解析学という数学分野で登場し、「演算子」ではこの範囲で翻訳される。上の定義は演算子の厳密な定義ではないが、専門の数学を学んでいない物理学部の学生には、これで充分だ。\nデル演算子を演算子と呼ぶ理由も、上の定義と関連している。例えば、グラディエント$\\nabla$はスカラー関数$f$をベクトル関数$\\left( \\dfrac{\\partial f}{\\partial x}, \\dfrac{\\partial f}{\\partial y}, \\dfrac{\\partial f}{\\partial z} \\right)$に対応させる演算子である。「えび寿司」でデル演算子をベクトルと思わないようにと繰り返し言っている理由がこれだ。\n量子力学で扱う演算子には、運動量演算子$p_{\\text{op}}$、エネルギー演算子ハミルトン$H$などがある。\n例 量子力学における波動関数とは、時刻と位置によってどのような粒子の運動状態を説明する関数である。ある粒子の波動関数が$\\psi = \\psi (x,t) = A e ^{i(kx+\\omega t) }$だとしよう。運動量$p$との混同を避けるために、運動量演算子を$p_{\\text{op}}$のように記される。この演算子は、波動関数$\\psi$が与えられた時、その波動関数とこの粒子の運動量$p$を掛け合わせた関数に対応させる演算子である。数式で表現すると次のようになる。\n$$ p_{\\text{op}} (\\psi) = p_{\\text{op}} \\psi = p \\psi $$\nこの場合、括弧を省略して書くのが一般的である。これは、演算子$p_{\\text{op}}$に波動関数$\\psi$を代入することを、二つの行列の乗算と同様に扱えるためである。上の式を行列の乗算とみなすと、この式は固有値方程式になり、波動関数$\\psi$と運動量$p$は運動量演算子$p_{\\text{op}}$の固有関数と固有値になる。運動量演算子は具体的には次のように与えられる。\n$$ p_{\\text{op}}=\\displaystyle \\frac{\\hbar}{i}\\frac{\\partial}{\\partial x} $$\n上の演算子に波動関数$\\psi$を代入すると、運動量$p=\\hbar k$を得ることが次のように確認できる。\n$$ p_{\\text{op}}(\\psi) = p_{\\text{op}}\\psi = \\frac{\\hbar}{i}\\frac{\\partial }{\\partial x} \\left( \\psi \\right) = \\frac{\\hbar}{i}\\frac{\\partial \\psi}{\\partial x} = (ik)\\frac{\\hbar}{i}\\psi = p \\psi $$\n","id":301,"permalink":"https://freshrimpsushi.github.io/jp/posts/301/","tags":null,"title":"物理学（量子力学）における演算子란"},{"categories":"복소해석","contents":"ビルドアップ テイラーの定理は、微分の回数に関して平均値の定理を一般化したものだ。もともと$1$回微分されたものを扱っていたが、それを$n \\in \\mathbb{N}$回に拡張したんだ。でも、自然数に一般化できたら、整数全体にもできないかな？もちろん、$-n$回微分することはできないけど、微分と逆操作の関係にある積分を考えたらどうだろう？以下のローランの定理を証明なしで紹介する。\n$f: A \\subset \\mathbb{C} \\to \\mathbb{C}$の特異点$\\alpha$を中心とする二つの同心円$\\mathscr{C}_{1}: |z-\\alpha| = r_{1}$と$\\mathscr{C}_{2}: |z-\\alpha| = r_{2}$$(r_{2} \u0026lt; r_{1})$上で$f$が解析的だとしよう。すると、二つの同心円の間にあるすべての点に対して、$f$は$\\displaystyle f(z) = \\sum_{n = 0 }^{\\infty} a_{n} (z-\\alpha) ^{n} + \\sum_{n = 1 }^{\\infty} { {b_{n} } \\over{ (z-\\alpha) ^{n} } }$で表すことができる。\n$\\displaystyle a_{n} = {{1} \\over {2 \\pi i}} \\int_{\\mathscr{C}_{1}} {{f(z)} \\over {(z - \\alpha)^{ 1 + n} }} dz \\qquad , n = 0,1,2, \\cdots$ $\\displaystyle b_{n} = {{1} \\over {2 \\pi i}} \\int_{\\mathscr{C}_{2}} {{f(z)} \\over {(z - \\alpha)^{ 1 - n} }} dz \\qquad , n=1,2,3,\\cdots$ 定義 以下の級数をローラン級数と呼ぶ。 $$ f(z) = \\sum_{n = 0 }^{\\infty} a_{n} (z-\\alpha) ^{n} + \\sum_{n = 1 }^{\\infty} { {b_{n} } \\over{ (z-\\alpha) ^{n} } } $$\n説明 微分に対するコーシーの積分公式の一般化：関数$f: A \\subseteq \\mathbb{C} \\to \\mathbb{C}$が単連結領域$\\mathscr{R}$で解析的だとする。\n$\\mathscr{R}$内部の単純閉路$\\mathscr{C}$がある点$\\alpha$を囲んでいるなら、自然数$n$に対して\n$$ {{f^{(n)} (\\alpha) } \\over {n!}} = {{1} \\over {2 \\pi i }} \\int_{\\mathscr{C}} {{f(z)} \\over { (z - \\alpha)^{1+n} }} dz $$\nコーシーの積分公式を使うと、テイラーの定理の一般化という側面がよりはっきりとするだろう。\n$$ f(z) = \\sum_{n = 0 }^{\\infty} {{f^{(n)} (\\alpha) } \\over {n!}} (z-\\alpha) ^{n} + \\sum_{n = 1 }^{\\infty} { {b_{n} } \\over{ (z-\\alpha) ^{n} } } $$ このような級数形式で、$\\displaystyle \\sum_{n = 1 }^{\\infty} { {b_{n} } \\over{ (z-\\alpha) ^{n} } }$を主要部と呼ぶ。特に$\\displaystyle {{1} \\over {z-\\alpha}}$の係数、すなわち$b_{1}$は、$\\alpha$での$f$の留数と定義され、$b_{1} = \\text{Res}_{\\alpha} f(z)$のように表される1。\nOsborne (1999). Complex variables and their applications: p144.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":290,"permalink":"https://freshrimpsushi.github.io/jp/posts/290/","tags":null,"title":"ローラン級数とは?"},{"categories":"수리물리","contents":"解説 物理学で、デル演算子$\\nabla$を含む4つの演算、勾配、発散、回転、ラプラシアンは非常に重要である。そのため、3つの座標系における上記の演算を必ず知っておく必要がある。もちろん、これが暗記しなければならないという意味ではない。物理学の勉強は公式を暗記するのではなく、勉強しているうちに自然と覚えることになるので、わざわざ暗記しようとせず、公式を印刷して持ち歩くか、このページをブックマークして必要な時にすぐに取り出せるようにしよう。\n公式 $f$をスカラー関数、ベクトル関数$\\mathbf A$を$\\mathbf A= A_{1}\\mathbf{\\hat e_{1}}+A_2\\mathbf{\\hat e_2}+A_{3}\\mathbf{\\hat e_{3}}$とする。\n勾配:\n$$ \\begin{align*} \\nabla f \u0026amp;= \\mathbf{\\hat e_{1}}\\frac{1}{h_{1}}\\frac{\\partial f}{\\partial e_{1}}+ \\mathbf{\\hat e_2}\\frac{1}{h_2}\\frac{\\partial f}{\\partial e_2}+\\mathbf{\\hat e_{3}}\\frac{1}{h_{3}}\\frac{\\partial f}{\\partial e_{3}} \\\\ \u0026amp;= \\sum \\limits_{i=1}^3 \\mathbf{\\hat e_{i}}\\frac{1}{h_{i}}\\frac{\\partial f}{\\partial e_{i}} \\end{align*} $$\n発散:\n$$ \\nabla \\cdot \\mathbf A=\\frac{1}{h_{1}h_2h_{3}} \\left[ \\frac{\\partial}{\\partial e_{1}} (h_2h_{3}A_{1}) + \\frac{\\partial}{\\partial e_2} (h_{1}h_{3}A_2) + \\frac{\\partial}{\\partial e_{3}} (h_{1}h_2A_{3}) \\right] $$\n回転:\n$$ \\nabla \\times \\mathbf A =\\frac{1}{h_{1}h_2h_{3}} \\begin{vmatrix} h_{1} \\mathbf{\\hat e_{1}} \u0026amp; h_2 \\mathbf{\\hat e_2} \u0026amp; h_{3} \\mathbf{\\hat e_{3}} \\\\[0.5em] \\dfrac{\\partial}{\\partial e_{1}} \u0026amp; \\dfrac{\\partial }{\\partial e_2} \u0026amp; \\dfrac{\\partial}{\\partial e_{3}} \\\\[1em] h_{1}A_{1} \u0026amp; h_2A_2 \u0026amp; h_{3}A_{3} \\end{vmatrix} $$\nラプラシアン:\n$$ \\begin{align*} \u0026amp; \\nabla \\cdot (\\nabla f) \\\\ =\u0026amp;\\ \\nabla ^2 f \\\\ =\u0026amp;\\ \\frac{1}{h_{1}h_2h_{3}} \\left[ \\frac{\\partial }{\\partial e_{1}} \\left( \\frac{h_2h_{3}}{h_{1}} \\frac{\\partial f}{\\partial e_{1}} \\right) +\\frac{\\partial }{\\partial e_2} \\left( \\frac{h_{1}h_{3}}{h_2} \\frac{\\partial f}{\\partial e_2} \\right) + \\frac{\\partial }{\\partial e_{3}} \\left( \\frac{h_{1}h_2}{h_{3}} \\frac{\\partial f}{\\partial e_{3}} \\right) \\right] \\end{align*} $$\nこのとき、各座標系ごとの単位ベクトル、スケールファクターは以下の通りである。\n直交座標系:\n$$ \\mathbf{\\hat{e_{1}}}=\\mathbf{\\hat{\\mathbf{x}}},\\quad\\mathbf{\\hat{e_{2}}}=\\mathbf{\\hat{\\mathbf{y}}},\\quad\\mathbf{\\hat{e_{3}}}=\\mathbf{\\hat{\\mathbf{z}}},\\quad h_{1}=1,\\quad h_{2}=1,\\quad h_{3}=1 $$\n円筒座標系:\n$$ \\mathbf{\\hat{e_{1}}}=\\boldsymbol{\\hat \\rho},\\quad\\mathbf{\\hat{e_{2}}}=\\boldsymbol{\\hat \\phi},\\quad\\mathbf{\\hat{e_{3}}}=\\mathbf{\\hat{\\mathbf{z}}},\\quad h_{1}=1,\\quad h_{2}=\\rho,\\quad h_{3}=1 $$\n球座標系\n$$ \\mathbf{\\hat{e_{1}}}=\\mathbf{\\hat r},\\quad\\mathbf{\\hat{e_{2}}}=\\boldsymbol{\\hat \\theta},\\quad\\mathbf{\\hat{e_{3}}}=\\boldsymbol{\\hat \\phi},\\quad h_{1}=1,\\quad h_{2}=r,\\quad h_{3}=r\\sin\\theta $$\n直交座標系 勾配 $$ \\nabla f = \\frac{\\partial f}{\\partial x}\\mathbf{\\hat{\\mathbf{x}} }+ \\frac{\\partial f}{\\partial y}\\mathbf{\\hat{\\mathbf{y}}} + \\frac{\\partial f}{\\partial z}\\mathbf{\\hat{\\mathbf{z}}} $$\n発散 $$ \\nabla \\cdot \\mathbf A=\\frac{\\partial A_{x}}{\\partial x}+\\frac{\\partial A_{y}}{\\partial y}+\\frac{\\partial A_{z}}{\\partial z} $$\n回転 $$ \\begin{align*} \\nabla \\times \\mathbf A\u0026amp;=\\left(\\frac{\\partial A_{z}}{\\partial y}-\\frac{\\partial A_{y}}{\\partial z} \\right) \\mathbf{\\hat{\\mathbf{x}}}+\\left(\\frac{\\partial A_{x}}{\\partial z}-\\frac{\\partial A_{z}}{\\partial x} \\right) \\mathbf{\\hat{\\mathbf{y}}}+\\left(\\frac{\\partial A_{y}}{\\partial x}-\\frac{\\partial A_{x}}{\\partial y} \\right) \\mathbf{\\hat{\\mathbf{z}}} \\\\ \u0026amp;= \\begin{vmatrix} \\mathbf{\\hat{\\mathbf{x}}} \u0026amp; \\mathbf{\\hat{\\mathbf{y}}} \u0026amp; \\mathbf{\\hat{\\mathbf{z}}} \\\\ \\dfrac{\\partial}{\\partial x} \u0026amp; \\dfrac{\\partial }{\\partial y} \u0026amp; \\dfrac{\\partial}{\\partial z} \\\\ A_{x} \u0026amp; A_{y} \u0026amp; A_{z} \\end{vmatrix} \\end{align*} $$\nラプラシアン $$ \\begin{align*} \\nabla \\cdot (\\nabla f) = \\nabla ^2 f \u0026amp;= \\left( \\frac{\\partial}{\\partial x}\\mathbf{\\hat{\\mathbf{x}}}+\\frac{\\partial}{\\partial y}\\mathbf{\\hat{\\mathbf{y}}}+\\frac{\\partial}{\\partial z}\\mathbf{\\hat{\\mathbf{z}}} \\right) \\cdot \\left( \\frac{\\partial f}{\\partial x}\\mathbf{\\hat{\\mathbf{x}}}+\\frac{\\partial f}{\\partial y}\\mathbf{\\hat{\\mathbf{y}}}+\\frac{\\partial f}{\\partial z}\\mathbf{\\hat{\\mathbf{z}}} \\right) \\\\ \u0026amp;= \\frac{\\partial^2 f}{\\partial x^2}+\\frac{\\partial^2 f}{\\partial y^2}+\\frac{\\partial^2 f}{\\partial z^2} \\end{align*} $$\n円筒座標系 勾配 $$ \\nabla f = \\frac{\\partial f}{\\partial \\rho}\\boldsymbol{\\hat \\rho} + \\frac{1}{\\rho}\\frac{\\partial f}{\\partial \\phi}\\boldsymbol{\\hat \\phi} + \\frac{\\partial f}{\\partial z}\\mathbf{\\hat{\\mathbf{z}}} $$\n発散 $$ \\nabla \\cdot \\mathbf A=\\frac{1}{\\rho}\\frac{\\partial (\\rho A_\\rho)}{\\partial \\rho}+\\frac{1}{\\rho}\\frac{\\partial A_\\phi}{\\partial \\phi}+\\frac{\\partial A_{z}}{\\partial z} $$\n回転 $$ \\begin{align*} \\nabla \\times \\mathbf A\u0026amp;=\\left[\\frac{1}{\\rho}\\frac{\\partial A_{z}}{\\partial \\phi}-\\frac{\\partial A_\\phi}{\\partial z} \\right] \\boldsymbol{\\hat \\rho}+\\left[\\frac{\\partial A_\\rho}{\\partial z}-\\frac{\\partial A_{z}}{\\partial \\rho} \\right] \\boldsymbol{\\hat \\phi}+\\frac{1}{\\rho}\\left[\\frac{\\partial (\\rho A_\\phi)}{\\partial \\rho}-\\frac{\\partial A_\\rho}{\\partial \\phi} \\right] \\mathrm{\\hat{\\mathbf{z}}} \\\\ \u0026amp;= \\frac{1}{\\rho}\\begin{vmatrix} \\boldsymbol{\\hat \\rho} \u0026amp; \\rho\\boldsymbol{ \\hat \\phi} \u0026amp; \\mathbf{\\hat{\\mathbf{z}}} \\\\ \\dfrac{\\partial}{\\partial \\rho} \u0026amp; \\dfrac{\\partial }{\\partial \\phi} \u0026amp; \\dfrac{\\partial}{\\partial z} \\\\ A_\\rho \u0026amp; \\rho A_\\phi \u0026amp; A_{z} \\end{vmatrix} \\end{align*} $$\nラプラシアン $$ \\nabla \\cdot (\\nabla f) = \\nabla ^2 f = \\frac{1}{\\rho}\\frac{\\partial}{\\partial \\rho}\\left( \\rho \\frac{\\partial f}{\\partial \\rho} \\right) + \\frac{1}{\\rho^2}\\frac{\\partial^2 f}{\\partial \\phi^2} + \\frac{\\partial^2 f}{\\partial z^2} $$\n球座標系 勾配 $$ \\nabla f = \\frac{\\partial f}{\\partial r} \\mathbf{\\hat{\\mathbf{r}}} + \\frac{1}{r}\\frac{\\partial f}{\\partial \\theta} \\boldsymbol{\\hat{\\boldsymbol{\\theta}}} + \\frac{1}{r\\sin\\theta}\\frac{\\partial f}{\\partial \\phi}\\boldsymbol{\\hat \\phi} $$\n発散 $$ \\nabla \\cdot \\mathbf A=\\frac{1}{r^2}\\frac{\\partial (r^2 A_{r})}{\\partial r}+\\frac{1}{r\\sin\\theta}\\frac{\\partial (\\sin\\theta A_\\theta)}{\\partial \\theta}+\\frac{1}{r\\sin\\theta}\\frac{\\partial A_\\phi}{\\partial \\phi} $$\n回転 $$ \\begin{align*} \\nabla \\times \\mathbf A \u0026amp;=\\frac{1}{r\\sin\\theta} \\left[\\frac{\\partial (\\sin\\theta A_\\phi)}{\\partial \\theta}-\\frac{\\partial A_\\theta}{\\partial \\phi} \\right]\\mathbf{\\hat{\\mathbf{r}}}+\\frac{1}{r}\\left[\\frac{1}{\\sin\\theta} \\frac{\\partial A_{r}}{\\partial \\phi}-\\frac{\\partial (rA_\\phi)}{\\partial r} \\right] \\boldsymbol{\\hat{\\boldsymbol{\\theta}}} \\\\ \u0026amp; \\quad+ \\frac{1}{r} \\left[\\frac{\\partial (rA_\\theta)}{\\partial r}-\\frac{\\partial A_{r}}{\\partial \\theta} \\right]\\boldsymbol{\\hat \\phi} \\\\ \u0026amp;= \\frac{1}{r^2\\sin\\theta}\\begin{vmatrix} \\mathbf{\\hat{\\mathbf{r}}} \u0026amp; r\\boldsymbol{\\hat{\\boldsymbol{\\theta}}} \u0026amp; r\\sin\\theta\\boldsymbol{\\hat \\phi} \\\\ \\dfrac{\\partial}{\\partial r} \u0026amp; \\dfrac{\\partial }{\\partial \\theta} \u0026amp; \\dfrac{\\partial}{\\partial \\phi} \\\\ A_{r} \u0026amp; r A_\\theta \u0026amp; r\\sin\\theta A_\\phi \\end{vmatrix} \\end{align*} $$\nラプラシアン $$ \\nabla \\cdot (\\nabla f) = \\nabla ^2 f = \\frac{1}{r^2}\\frac{\\partial}{\\partial r} \\left( r^2\\frac{\\partial f}{\\partial r} \\right) + \\frac{1}{r^2\\sin\\theta}\\frac{\\partial}{\\partial\\theta}\\left( \\sin\\theta \\frac{\\partial f}{\\partial \\theta} \\right) + \\frac{1}{r^2\\sin^2\\theta}\\frac{\\partial^2 f}{\\partial^2 \\phi} $$\n","id":299,"permalink":"https://freshrimpsushi.github.io/jp/posts/299/","tags":null,"title":"曲線座標系における勾配、発散、回転、ラプラシアン"},{"categories":"복소해석","contents":"定義 特異点 1 関数$f$が$\\alpha$で$\\mathcal{N}(\\alpha)$の全ての点で微分可能なら、$\\alpha$で解析的Analyticだという。 関数$f$が$\\alpha \\in \\mathbb{C}$では解析的ではないが、$\\mathcal{N}(\\alpha)$のいくつかの点で解析的な時、$\\alpha$を$f$の特異点Singular Pointと呼ぶ。 特異点$\\alpha$が$\\alpha$を除く全ての点で解析的な$\\mathcal{N}(\\alpha)$が存在するなら、$\\alpha$は孤立Isolatedしているという。 $\\mathcal{N}$は近傍を意味し、$\\alpha$を含む開集合を指す。 種類 $\\alpha \\in \\mathbb{C}$が$f$の特異点だとしよう。\n$\\displaystyle \\exists \\lim_{z \\to \\alpha} f(z) \\iff$$\\alpha$は取り除けるRemovable特異点だ。 $\\displaystyle \\lim_{z \\to \\alpha} (z - \\alpha)^n f(z) = k \\ne 0 \\iff$$\\alpha$は**$n$次の極**Pole of Order $n$だ。 $\\alpha$が極ではない、または分岐に関連している。$\\iff$$\\alpha$は本質的特異点essential singular pointだ。 説明 特に、極が$n=1$の時、単純極Simple Poleと言う。\n実際、非常に変態的なケースでなければ、普通は$f$が定義されていない点がそのまま特異点になる。\n例えば、$\\displaystyle f(z) = {{z - i} \\over {(z^2+1)(z+i)}}$という場合、特異点は$z= \\pm i$になるだろう。$\\csc z$のケースでは、特に有限である必要はなく、$z = n \\pi ( n \\in \\mathbb{Z} )$全てが特異点だ。一方で$\\text{Log} z$は$z= 0$で特異点を持っており、上で挙げた例とは少し違う感じがするだろう。\n$\\displaystyle f(z) = {{z - i} \\over {(z^2+1)(z+i)}}$で、$z = i$は取り除け、$z = -i$は$2$次の極だ。\n$\\displaystyle \\lim_{z \\to n \\pi} {{ z - n \\pi } \\over {\\sin z }} = 1$なので、$\\csc z$の特異点は全て$1$次の極、即ち単純極だ。\n最後に、$\\text{Log} z$では、$z = 0$は**分岐点**であるため、本質的特異点だ。\nこのような特異点の分類は一見何の意味もない定義の遊びのように思えるが、後に続く積分に関する議論では非常に重要な概念になる。\nOsborne (1999). Complex variables and their applications: p63.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":281,"permalink":"https://freshrimpsushi.github.io/jp/posts/281/","tags":null,"title":"複素解析での特異点の種類"},{"categories":"선형대수","contents":"定義1 $S = \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r} \\right\\}$をベクター空間$V$の空集合ではない部分集合としよう。定数$k_{1}, k_{2}, \\dots, k_{r}$に対して、次の方程式\n$$ k_{1} \\mathbf{v}_{1} + k_{2} \\mathbf{v}_{2} + \\dots + k_{r} \\mathbf{v}_{r} = \\mathbf{0} $$\nは少なくとも一つの解\n$$ k_{1} = 0,\\ k_{2} = 0,\\ \\dots,\\ k_{r} = 0 $$\nを持つ。これを自明解という。自明解だけが唯一の解である場合、ベクター$\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r}$は線形独立と呼ばれる。自明解ではない解が少なくとも一つ存在する場合は、線形従属と言う。\n説明 自明解とは、一見して分かる解で、そのためあまり価値がないとされる。なぜなら上記の定義の内容と同様に、$0$の場合が多いからだ。\nこの定義から次の定理がすぐに導き出される。\n$S = \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r} \\right\\}$をベクター空間$V$の空集合ではない部分集合としよう。$S$のどのベクターも他のベクターの線形組み合わせで表すことができない場合、線形独立だとされる。逆に、他のベクターの線形組み合わせで表せるベクターが少なくとも一つ存在する場合は、線形従属だとされる。\nこの定理の内容を考えると、「独立」と「従属」の命名がピンとくるだろう。教科書によっては、定義と定理が反対になっているものもある。\n興味深いことに、脚注の参考文献「Elementary Linear Algebra」は、翻訳版がこの文章と同じように定義されていて、原著は反対に定義されている。個人的には、この文章のように定義する方がクリーンだと思う。それは、反対に定義する場合、要素が一つの集合に対して独立/従属を別途定義する必要があるからだ。定理の証明は下で紹介する。\nもう少しかんたんに説明すると、異なる二つのベクターがある時、一つのベクターを増やしたり減らしたりしても、もう一つのベクターと同じにすることができない場合、それは独立だとされる。例えば、$(1,0)$と$(0,1)$は、どんな定数を乗じても、つまり増やしたり減らしたりしても、互いに同じにすることができない。定義に合わせて書き直すと、\n$$ k_{1} (1,0) + k_{2} (0,1) = \\mathbf{0} $$\n二番目の項を移項すると、\n$$ k_{1}(1,0) = - k_{2}(0,1) $$\n再整理すると、\n$$ (k_{1}, 0) = ( 0 , - k_{2}) $$\nとなるため、上記の式を満たす解は$k_{1} = k_{2} = 0$だけであるため、$(1,0)$、$(0,1)$は線形独立である。これは定理として証明することができる内容である。\n定理 (a) 零ベクターを含む有限集合は線形従属である。\n(b) 一つのベクター$\\mathbf{v}$が線形独立であるための必要十分条件は$\\mathbf{v} \\ne \\mathbf{0}$である。\n(c) 異なる二つのベクターが線形独立であるための必要十分条件は、一つのベクターが他のベクターの定数倍で表すことができないことである。\n(d) $S=\\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r} \\right\\}$を二つ以上のベクターを持つ集合としよう。$S$が線形独立であるための必要十分条件は、$S$のどのベクターも他のベクターの線形組み合わせで表すことができないことである。\n(e) $T \\subset S$としよう。$S$が線形独立であれば、$T$も線形独立である。\n(e') $T \\subset S$としよう。$T$が線形従属であれば、$S$も線形従属である。\n証明 (a) $S=\\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r}, \\mathbf{0} \\right\\}$としよう。すると次の式が成り立つ。\n$$ 0 \\mathbf{v}_{1} + 0 \\mathbf{v}_{2} + \\dots + 0 \\mathbf{v}_{r} + 1 \\mathbf{0} = \\mathbf{0} $$\nしたがって、定義により$S$は線形従属である。\n■\n(b) **(a)**を要素が一つの集合に適用すると成立する。\n■\n(c) $(\\Longrightarrow)$\n$\\mathbf{v}_{1}, \\mathbf{v}_{2}$が線形独立と仮定しよう。すると、\n$$ k_{1} \\mathbf{v}_{1} + k_{2} \\mathbf{v}_{2} = \\mathbf{0} $$\nこの式を満たす解は$k_{1}=k_{2}=0$だけであるため、$\\mathbf{v}_{1} = -\\frac{k_{2}}{k_{1}}\\mathbf{v}_{2} = -k\\mathbf{v}_{2}$を満たす定数$k$は存在しない。\n$(\\Longleftarrow)$\n$\\mathbf{v}_{1}$が$\\mathbf{v}_{2}$の定数倍で表されないと仮定しよう。つまり、次の方程式\n$$ \\mathbf{v}_{1} = k_{2}\\mathbf{v} $$\nを満たす$k_{2}$が存在しないとしよう。すると、\n$$ k_{1} \\mathbf{v}_{1} + k_{2} \\mathbf{v}_{2} = \\mathbf{0} $$\nこの式を満たす解は自明解だけであるため、$\\mathbf{v}_{1}, \\mathbf{v}_{2}$は線形独立である。\n■\n(d) $(\\Longrightarrow)$\n$S$が線形独立と仮定しよう。\n$$ k_{1} \\mathbf{v}_{1} + k_{2} \\mathbf{v}_{2} + \\dots + k_{r} \\mathbf{v}_{r} = \\mathbf{0} $$\nこの式を満たす解は$k_{1}=k_{2}=\\cdots=k_{r}=0$だけであり、\n$$ \\mathbf{v}_{1} = -\\frac{k_{2}}{k_{1}}\\mathbf{v}_{2} - \\cdots - \\frac{k_{r}}{k_{1}}\\mathbf{v}_{r} $$\nこの式を満たす定数$\\frac{k_{2}}{k_{1}}, \\dots, \\frac{k_{r}}{k_{1}}$は存在しない。これはすべての$\\mathbf{v}_{i}$に当てはまるため、どのベクターも他のベクターの線形組み合わせで表すことができない。\n$(\\Longleftarrow)$\nどのベクターも他のベクターの線形組み合わせで表すことができないと仮定しよう。つまり、次の方程式\n$$ \\mathbf{v}_{1} = k_{2}\\mathbf{v}_{2} + \\cdots + k_{r}\\mathbf{v}_{r} $$\nを満たす$k_{2}, \\dots, k_{r}$が存在しないとしよう。すると、\n$$ k_{1}\\mathbf{v}_{1} + k_{2}\\mathbf{v}_{2} + \\cdots + k_{r}\\mathbf{v}_{r} = \\mathbf{0} $$\nこの式を満たす解は自明解だけであるため、$S$は線形独立である。\n■\n(e) 二つの集合$T$、$S$が次のようであるとしよう。\n$$ T = \\left\\{ \\mathbf{v}_{1},\\ \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r} \\right\\},\\quad S = \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r}, \\mathbf{v}_{r+1}, \\dots, \\mathbf{v}_{n} \\right\\} $$\n$T$は$S$の部分集合である。現在、$S$が線形独立と仮定しよう。すると、\n$$ c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + \\cdots +c_{r} \\mathbf{v}_{r} + c_{r+1} \\mathbf{v}_{r+1} + \\cdots + c_{n} \\mathbf{v}_{n} = \\mathbf{0} $$\nこの式を満たす解は自明解$c_{1}=c_{2} = \\cdots = c_{r} = c_{r+1} = \\cdots = c_{n} = 0$だけである。したがって、$c_{r+1} = \\cdots = c_{n} = 0$であるため、次の式が成り立つ。\n$$ \\begin{align*} \u0026amp;\u0026amp; c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + \\cdots +c_{r} \\mathbf{v}_{r} + c_{r+1} \\mathbf{v}_{r+1} + \\cdots + c_{n} \\mathbf{v}_{n} =\u0026amp;\\ \\mathbf{0} \\\\ \\implies \u0026amp;\u0026amp; c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + \\cdots +c_{r} \\mathbf{v}_{r} + \\left( 0\\mathbf{v}_{r+1} + \\cdots + 0 \\mathbf{v}_{n} \\right) =\u0026amp;\\ \\mathbf{0} \\\\ \\implies \u0026amp;\u0026amp; c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + \\cdots +c_{r} \\mathbf{v}_{r} + \\mathbf{0} =\u0026amp;\\ \\mathbf{0} \\\\ \\implies \u0026amp;\u0026amp; c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + \\cdots +c_{r} \\mathbf{v}_{r} =\u0026amp;\\ \\mathbf{0} \\end{align*} $$\nしかし、この式は$c_{1} = c_{2} = \\cdots = c_{r} = 0$の時にのみ成り立つため、$T$は線形独立である。\n■\n(e') **(e)**の対偶として成立する。\n■\nHoward Anton, Chris Rorres, Anton Kaul, Elementary Linear Algebra: Applications Version(12th Edition). 2019, p228-229\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":253,"permalink":"https://freshrimpsushi.github.io/jp/posts/253/","tags":null,"title":"線形独立と線形従属"},{"categories":"추상대수","contents":"定義 1 モノイド $\\left\u0026lt; G, \\ast\\ \\right\u0026gt;$ の元 $a$ と単位元 $e$ に対し、$a \\ast\\ a\u0026rsquo; = a\u0026rsquo; \\ast\\ a = e$ を満たす $a '$ が存在すれば、$\\left\u0026lt; G, \\ast\\ \\right\u0026gt;$を群Groupと定義する。すなわち、群は以下の性質を満たす二項演算構造だ。\n(i): 演算に対して結合法則が成立する。 (ii): すべての元に対して単位元が存在する。 (iii): すべての元に対して逆元が存在する。 説明 マグマから始まり半群、モノイドを経て、ついに群に到達した。大したことないように見えるかもしれないが、マグマと比較すると、条件はかなり増えている。演算で閉じており、結合法則が成立し、単位元と逆元の存在が必要なので、何でもかんでも群とは言えなくなっている。\n群を研究する理由は、他の代数的構造よりもずっとシンプルで簡単だからだ。群より条件が少なければ有用な性質も少なくなり、群より条件が多ければ使い道が減る。\n代数学で興味を持つ代数的構造のほとんどは基本的に群に基づいており、代数学は純粋数学の数論から、暗号理論のような日常生活に溶け込んだ応用数学にまで応用されている。数学以外では、驚くべきことに物理学でも群論が使われている。\nモノイドになりながら、群にならない例を見てみよう。\n正方行列の集合 $\\mathbb{R}^{n \\times n}$ に対して、モノイド $\\left\u0026lt; \\mathbb{R}^{n \\times n} , \\cdot \\right\u0026gt;$ は群ではない。\n行列 $A \\in \\mathbb{R}^{n \\times n}$ に対して、$\\det A = 0$ ならば、$A^{-1}$ が存在しない。 もちろん、集合に制限を加えれば、これも群になりえる。\n逆行列が存在する正方行列の集合 $\\text{GL}_{n} (\\mathbb{R}) = \\left\\{ A \\in \\mathbb{R}^{n \\times n} \\ | \\ \\det A \\ne 0 \\right\\}$ に対して、モノイド $\\left\u0026lt; \\text{GL}_{n} (\\mathbb{R}) , \\cdot \\right\u0026gt;$ は群である。\n$\\left\u0026lt; \\mathbb{R}^{n \\times n} , \\cdot \\right\u0026gt;$ の部分モノイド $\\left\u0026lt; \\text{GL}_{n} (\\mathbb{R}) , \\cdot \\right\u0026gt;$ は、乗算の逆元を持つので、定義 $\\text{GL}_{n} (\\mathbb{R})$ により群である。 対称性？ 群の概念を掴むとき、よく対称性についての話から始まったり、完全に数学的な定義だけで築き上げられたりする。\n対称性の例としては、ルービックキューブを回したりそのままにしたり(単位元)、元に戻したり(逆元)することがよく言及される。しかし、このような説明は構造が対称性を持つものが群の構造を持つことを理解しやすいが、群の構造が対称性を持つことを把握するのは難しい。群の形で数学的な定義に従うと、対称とは逆元の概念を思い浮かべるのが良い。\n$a$ が存在するなら、群の定義により、それに対応する $a '$ が必ず存在する。\n一方で、$a '$ もそれに対応する $a\u0026rsquo;\u0026rsquo;=a$ が存在するので、このような関係から対称を考えるのは自然だと言える。モノイドと群の違いは逆元だけなので、概念と定義がより妥当に一致していることが確認される。\n対称の話が出たからには、対称にピッタリの群の例を見てみよう。\nモノイド $\\left\u0026lt; \\mathbb{Z} , + \\right\u0026gt;$ は群である。\n整数 $a$ に対して、$-a$ は常に $a + (-a) = 0$ を満たす逆元になる。 $1$ が存在すれば、単位元 $0$ を中心に対称の $-1$ が存在し、$-2$ に対しては $2$ が存在し\u0026hellip;$n$ に対しては $-n$ が存在する。対称という意味で考えれば、かなり自然な例だ。\n一般的に、群だけを扱う場合、群 $\\left\u0026lt; G, \\ast\\ \\right\u0026gt;$ はただ $G$ と表し、操作は特に言及がなければ $\\cdot$ と書く。ただし$\\left\u0026lt; \\mathbb{Z} , + \\right\u0026gt;$ のように加算が明確なコンテキストでは、$+$ のように場合に応じて適切な操作を使う。\nFraleigh. (2003). \u0026ldquo;A first course in abstract algebra(7th Edition)\u0026rdquo;: p37.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":278,"permalink":"https://freshrimpsushi.github.io/jp/posts/278/","tags":null,"title":"抽象代数学における群"},{"categories":"추상대수","contents":"定義 1 半群$\\left\u0026lt; M , \\ast\\ \\right\u0026gt;$において、全ての元$a$に対して、$a \\ast\\ e = e \\ast\\ a = a$を満たす$e$が存在するなら、$\\left\u0026lt; M , \\ast\\ \\right\u0026gt;$をモノイドMonoidと定義する。\n説明 モノイドは恒等元が存在する半群だ。恒等元という概念を導入することで、話せることが格段に増える。半群でありながらモノイドにならない典型的な例を見てみよう。\n半群$\\left\u0026lt; \\mathbb{N} , +\\right\u0026gt;$はモノイドではない。\n任意の自然数$a$に対して恒等元$ e$が存在して$a + e = a$を満たすと仮定する。 $e$は$1$以上の自然数だから、$a + e \\ge a + 1$が成り立つ。一方で$a + 1 \u0026gt; a$であるから、$a + e \u0026gt; a$となり、これは仮定に矛盾する。\nこのように自然に反証できる例があるということは、恒等元の存在が必ずしも自明ではないとも言えるだろう。\n定方行列の集合$\\mathbb{R}^{n \\times n}$について、$\\left\u0026lt; \\mathbb{R}^{n \\times n} , \\cdot \\right\u0026gt;$はモノイドだ。\n行列の積の定義に従って、$\\left\u0026lt; \\mathbb{R}^{n \\times n} , + \\right\u0026gt;$が半群になることは容易に示せる。一方、単位行列$I_{n}$と任意の行列$( a_{ij} )$を考えると、$a_{ij} \\cdot 1 = a_{ij}$および$a_{ij} \\cdot 0 = 0$だから$(a_{ij}) I = I (a_{ij}) = (a_{ij})$となる。したがって、$I$は$\\left\u0026lt; \\mathbb{R}^{n \\times n} , \\cdot \\right\u0026gt;$の恒等元になる。 ■\nFraleigh. (2003). A first course in abstract algebra(7th Edition): p42.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":277,"permalink":"https://freshrimpsushi.github.io/jp/posts/277/","tags":null,"title":"抽象代数学におけるモノイド"},{"categories":"선형대수","contents":"定義: 線形組み合わせ1 $\\mathbf{w}$をベクトル空間$V$のベクトルとする。もし$\\mathbf{w}$が$V$のベクトル$\\mathbf{v}_{1},\\mathbf{v}_{2},\\cdots ,\\mathbf{v}_{r}$と任意の定数$k_{1}, k_{2}, \\cdots, k_{r}$によって以下のように表されるなら、$\\mathbf{w}$は$\\mathbf{v}_{1},\\mathbf{v}_{2},\\cdots ,\\mathbf{v}_{r}$の線形組み合わせlinear combination、または一次組み合わせという。\n$$ \\mathbf{w} = k_{1}\\mathbf{v}_{1} + k_{2}\\mathbf{v}_{2} + \\cdots + k_{r}\\mathbf{v}_{r} $$\nこの場合、定数$k_{1}, k_{2}, \\cdots, k_{r}$は線形組み合わせ$\\mathbf{w}$の係数coefficientsと呼ばれる。\n解説 式で示されると見慣れないかもしれないが、難しい概念ではない。2次元直交座標系でのベクトル表示は、まさに二つの単位ベクトル$\\hat{\\mathbf{x}} = (0,1)$と$\\hat{\\mathbf{y}} = (0,1)$の線形組み合わせである。\n$$ \\mathbf{v} = (v_{1}, v_{2}) = (v_{1},0)+(0,v_{2}) = v_{1}(1,0) + v_{2}(0,1) = v_{1}\\hat{\\mathbf{x}} + v_{2} \\hat{\\mathbf{y}} $$\n定理 $S = \\left\\{ \\mathbf{w}_{1}, \\mathbf{w}_{2}, \\dots, \\mathbf{w}_{r} \\right\\}$をベクトル空間$V$の空でない部分集合とする。すると、以下が成り立つ。\n(a) $S$の要素の全ての可能な線形組み合わせの集合を$W$としよう。$W$は$V$の部分空間である。\n(b) **(a)**の$W$は$S$を含む$V$の部分空間の中で最小の部分空間である。つまり、$W^{\\prime}$を$S$を含む$V$の部分空間とすると、次の式が成立する。\n$$ S \\subset W \\le W^{\\prime} $$\n証明 (a) $W$が加算とスカラー倍に対して閉じているかを確認するためには、部分空間判定法を適用する。\n$$ \\mathbf{u} = c_{1} \\mathbf{w}_{1} + c_{2} \\mathbf{w}_{2} + \\cdots + c_{r} \\mathbf{w}_{r}, \\quad \\mathbf{v} = k_{1} \\mathbf{w}_{1} + k_{2} \\mathbf{w}_{2} + \\cdots + k_{r} \\mathbf{w}_{r} $$\n(A1)\n$\\mathbf{u}+\\mathbf{v}$は以下のようになる。\n$$ \\mathbf{u} +\\mathbf{v} = ( c_{1} + k_{1} ) \\mathbf{w}_{1} + ( c_{2} + k_{2} ) \\mathbf{w}_{2} + \\cdots + ( c_{r} + k_{r} ) \\mathbf{w}_{r} $$\nこれは$\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\dots, \\mathbf{w}_{r}$の線形組み合わせなので、$\\mathbf{u} + \\mathbf{v} \\in W$が成り立つ。\n(M1)\n任意の定数$k$に対して、$k\\mathbf{u}$は以下のようになる。\n$$ k\\mathbf{u} = ( k c_{1} ) \\mathbf{w}_{1} + ( k c_{2} ) \\mathbf{w}_{2} + \\cdots + ( k c_{r} ) \\mathbf{w}_{r} $$\nこれは$\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\dots, \\mathbf{w}_{r}$の線形組み合わせなので、$k\\mathbf{u} \\in W$が成り立つ。\n結論\n$W$が加算とスカラー倍に対して閉じているので、部分空間判定法により、$W$は$V$の部分空間である。\n$$ W \\le V $$\n■\n(b) $W^{\\prime}$を$S$を含む$V$の部分空間とする。すると、$W^{\\prime}$は加算とスカラー倍に対して閉じているので、$S$の要素の全ての線形組み合わせは$W^{\\prime}$の要素である。従って、\n$$ W \\le W^{\\prime} $$\n■\n定義: 生成 定理の$W$は$S$によって生成されたspanned$V$の部分空間という。また、ベクトル$\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\dots, \\mathbf{w}_{r}$が$W$を生成するspanと言い、以下のように表記される。\n$$ W = \\text{span}\\left\\{ \\mathbf{w}_{1}, \\mathbf{w}_{2}, \\dots, \\mathbf{w}_{r} \\right\\} \\quad \\text{or} \\quad W = \\text{span}(S) $$\n解説 生成という概念が必要な理由は、ある要素を含む最小の集合を考えるためである。実際、上の定理でこの点を確認することができる。さらに、$S$自体から重複する要素をすべて除くと、これはベクトル空間の基底になる。\n定理 $S = \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r} \\right\\}$と$S^{\\prime} = \\left\\{ \\mathbf{w}_{1}, \\mathbf{w}_{2}, \\dots, \\mathbf{w}_{r} \\right\\}$をベクトル空間$V$の空でない部分集合とする。すると、\n$$ \\text{span} \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{r} \\right\\} = \\text{span} \\left\\{ \\mathbf{w}_{1}, \\mathbf{w}_{2}, \\dots, \\mathbf{w}_{r} \\right\\} $$\n必要十分条件は、$S$の全てのベクトルが$S^{\\prime}$のベクトルの線形組み合わせとして表され、$S^{\\prime}$の全てのベクトルが$S$のベクトルの線形組み合わせとして表されることである。\nHoward Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), p220-222\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":512,"permalink":"https://freshrimpsushi.github.io/jp/posts/512/","tags":null,"title":"線形結合、生成"},{"categories":"추상대수","contents":"ビルドアップ 数学を大きく三つに分けるなら、幾何学、解析学、代数学と言えるだろう。その中で、代数学は教育課程で学ぶ二項、約分などを扱う数学の一分野だった。代数学とは基本的に「数」の代わりに文字を使ってどんな方程式でも解くことを目標とする学問だ。特定の数に限らず、一般的で強力な解法を探求するため、当時は最先端技術と言えた。しかし、このような数学的テクニックは教育が発達した現代では誰にでも当たり前の常識のようになった。\n一方、数学界はこれらの概念をさらに発展させ、「数」を超えて抽象的な「構造」に関心を持ち始めた。我々がもともと「数」と「計算」と呼んでいたものを「元素」と「演算」に抽象化したのだ。だから、現代代数学は代数的テクニックを使える条件やあるいは構造自体を研究する学問になった。上の説明から想像できるように、現代代数学は特に抽象的な面が強く、一般に「抽象代数学」と呼ばれる。\n抽象代数学で関心を持つのは主にある集合とその集合で定義される演算の性質だ。集合$S$と演算$\\ast$が与えられているなら、$S$は$\\ast$に対して閉じているか、単位元は存在するか、逆元は存在するかなどを研究する。その中でも抽象代数学で関心を持つ演算は、$a \\ast\\ b = c$のように二つの元が一つの元に対応する二項演算二項演算だ。\n定義1 二項演算は$\\ast : S \\times S \\to S$で定義される関数と見なせ、そのような二項演算が定義される集合を二項演算構造と言う。 集合$M \\ne \\emptyset$の元素$a,b$と二項演算$\\ast$に対して、$a * b \\in M$ならば$\\left\u0026lt; M , \\ast\\ \\right\u0026gt;$をマグママグマと定義する。 説明 マグマは抽象代数学が関心を持つ二項演算構造の中でも最も単純な概念だ。単に閉じている閉じているだけで良い。\nマグマになれない例 奇数の集合を$O$、無理数の集合を$I$としたら、$\\left\u0026lt; O , + \\right\u0026gt;$と$\\left\u0026lt; I , \\cdot \\right\u0026gt;$はマグマではない。\nマグマになれない例としては、奇数の集合や無理数の集合などがある。これらは乗算や加算などに対して閉じていないため、二項演算構造であってもマグマになれない。\n奇数の集合を$O$とした時、加算を考えると二つの奇数の和は必ず偶数なので$O$は閉じておらず、マグマになれない。 無理数の集合を$I$とした時、$I$で乗算を考えると$\\sqrt{2} , 2\\sqrt{2} \\in I$であり$\\sqrt{2} \\cdot 2 \\sqrt{2 } = 4 $だが、$ 4 \\notin I$なので$I$はマグマではない。 マグマになる例 任意の集合$S$のべき集合$\\mathscr{P}(S)$と差集合$\\setminus$について$\\left\u0026lt; \\mathscr{P}(S) , \\setminus \\right\u0026gt;$はマグマだ。\n$S$の部分集合$A$と$B$について、$( A \\setminus B ) \\subset S$なので、$( A \\setminus B ) \\in \\mathscr{P}(S)$であり$\\left\u0026lt; \\mathscr{P}(S) , \\setminus \\right\u0026gt;$はマグマだ。 演算も重要だ 大切なことは、代数的構造を探求する時は集合そのものだけでなく、演算も一緒に考えなければならない点だ。マグマにならない例をもう一度見よう。\n奇数の集合を$O$、無理数の集合を$I$としたら、$\\left\u0026lt; O , \\cdot \\right\u0026gt;$はマグマだが、$\\left\u0026lt; I , + \\right\u0026gt;$はマグマではない。\n奇数の集合を$O$とした時、乗算を考えると二つの奇数の乗算は必ず奇数なので、$O$は閉じておりマグマになる。 無理数の集合を$I$とした時、$I$で加算を考えると$\\sqrt{2} , -\\sqrt{2} \\in I$であり$\\sqrt{2} + ( - \\sqrt{2 } ) = 0$だが、$ 0 \\notin I$なので$I$はマグマではない。 奇数の集合は演算を変えることでマグマになったが、無理数の集合は依然としてマグマになれなかった。つまり、今は意味がないと思われる集合も、与えられた演算によっては意味のある代数的構造を持つ可能性があるということだ。\n一方で、マグマの定義は非常にシンプルで一般的なため、マグマ自体が有用な性質を提供するわけではない。マグマという名前自体が我々が知る「溶岩」に同じルーツを持ち、フランス語で「ごちゃごちゃしたもの」という意味を持つ。それだけ多くの代数的構造がマグマから始まるが、その概念自体はそれほど重要ではないと言えるだろう。\nFraleigh. (2003). アブストラクトアルジェブラ序論(第7版): p20, 29.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":275,"permalink":"https://freshrimpsushi.github.io/jp/posts/275/","tags":null,"title":"抽象代数における二項演算"},{"categories":"선형대수","contents":"定義1 $W$がベクトル空間$V$の空集合でない部分集合とする。$W$が$V$で定義された加算とスカラー乗算に対してベクトル空間の定義を満たす時、$W$をベクトル空間$V$の部分空間subspaceと呼び、以下のように表記する。\n$$ W \\le V $$\n説明 ベクトル空間$V$の部分集合$W$が$V$の部分空間かどうかを判断するためには、ベクトル空間になるための10の規則を全て満たさなければならない。ベクトル空間の部分集合を取るたびに10の規則を全て確認するのはかなり面倒で困難なことになるだろう。しかし幸いなことに、あるベクトル空間の部分集合である理由だけで、いくつかの規則は自明に成立する。\n例えば$\\mathbf{u},\\mathbf{v}$が$W$の要素であれば、同時に$V$の要素であるため、(A2), (A3), (M2)-(M5) は自然に成立する。したがって、加算に対する閉包性**(A1)、ゼロベクトルの存在(A4)、逆の存在(A5)、スカラー乗算に対する閉包性(M1)だけを確認すれば、$W$は部分空間であることがわかる。しかし実際にはもっと単純である。条件(A1)、(M1)**を満たすことが部分空間であるための同値条件となる。\n例 ベクトル空間$V$の部分空間の例には、以下のものがある。\n自分自身$V$ 剰余類$v + W$ 線形変換$T : V \\to W$に対して、\n$T$の零空間$N(T) \\le V$ $T$の値域$R(T) \\le W$ 線形変換$T : V \\to V$に対して、\n固有空間$E_{\\lambda}$ $T$-不変空間 $T$-循環空間 定理: 部分空間判定法 $W$をベクトル空間$V$の空集合でない部分集合とする。$W$が$V$の部分空間であることと$W$が以下の二つの条件を満たすことは必要十分条件である。\n(A1) 部分集合$W$が$V$で定義された加算に対して閉じている。\n(M1) 部分集合$W$が$V$で定義されたスカラー乗算に対して閉じている。\n証明 $(\\implies)$\n$W$が$V$の部分空間であると仮定する。$W$が部分空間であれば、ベクトル空間の定義により$W$が**(A1)、(M1)**を満たすことは自明である。\n$(\\impliedby)$\n$W$が$(A1)$、$(M1)$を満たすと仮定する。そして$\\mathbf{u} \\in W$とする。そうすると$W$はスカラー乗算に対して閉じていて、$0\\mathbf{u}=\\mathbf{0}$であるため、以下が成立する。\n$$ 0 \\mathbf{u} = \\mathbf{0} \\in W $$\n同じ理由で$(-1)\\mathbf{u}=-\\mathbf{u}$によって以下が成立する。\n$$ (-1)\\mathbf{u} = -\\mathbf{u} \\in W $$\nしたがって$W$は**(A1)-(M5)**を全て満たすので$V$の部分空間である。\n■\n定理: 部分空間の交差も部分空間である2 $W_{1}, W_2$をベクトル空間$V$の部分空間とする。すると$W_{1} \\cap W_2$も$V$の部分空間である。\n証明 部分空間判定法により、$W_{1} \\cap W_2$が**(A1)、(M1)**を満たしているか確認すればよい。$W= W_{1} \\cap W_2$とする。\n(A1)\n$W = W_{1} \\cap W_2$であるため、$W$内の任意の二つのベクトル$\\mathbf u,\\mathbf v$はそれぞれ$W_{1}$、$W_2$にも含まれている。$W_{1}, W_2$は部分空間であるため、加算に対して閉じている。したがって\n、以下が成立する。\n$$ \\mathbf u + \\mathbf v \\in W_{1}, \\quad \\mathbf u + \\mathbf v \\in W_2 $$\nしたがって、交差の定義により以下が成立する。\n$$ \\mathbf u + \\mathbf v \\in W $$\n$W$内の任意の二つのベクトル$\\mathbf u,\\ \\mathbf v$に対して、$\\mathbf u + \\mathbf v$も$W$の要素であるため、$W$は加算に対して閉じており、**(A1)**を満たす。\n(M1)\n上記の場合と同様に証明する。\n$W = W_{1} \\cap W_2$であるため、$W$内の任意のベクトル$\\mathbf u$は$W_{1}$、$W_2$にも含まれている。$W_{1},\\ W_2$は部分空間であるため、スカラー乗算に対して閉じている。したがって、あるスカラー$k$に対して以下が成立する。\n$$ k\\mathbf{u} \\in W_{1} \\quad k \\mathbf{u} \\in W_2 $$\nしたがって、交差の定義により以下が成立する。\n$$ k\\mathbf u \\in W $$\n$W$内の任意のベクトル$\\mathbf u$に対して、$k\\mathbf u$も$W$の要素であるため、$W$はスカラー乗算に対して閉じており、**(M1)**を満たす。\n結論\n$W_{1}, W_{2}$が部分空間の時、$W = W_{1} \\cap W_2$が**(A1)、(M1)**を満たすので、$W$も部分空間である。\n■\n従来の定理 $W_{1}, W_{2}, \\dots W_{n}$がベクトル空間$V$の部分空間とする。すると$W = W_{1} \\cap \\cdots \\cap \\dots W_{n}$も$V$の部分空間である。\nHoward Anton, 基礎線形代数: アプリケーションバージョン (第12版, 2019), p211-212\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHoward Anton, 基礎線形代数: アプリケーションバージョン (第12版, 2019), p216\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":285,"permalink":"https://freshrimpsushi.github.io/jp/posts/285/","tags":null,"title":"ベクトル空間の部分空間"},{"categories":"선형대수","contents":"定義1 空集合ではない集合 $V$ の要素が二つの演算 加算additionと スカラー乗算scalar multiplicationに対して下記の10個の規則を満たす時、$V$を体2 $\\mathbb{F}$に対するベクトル空間vector spaceまたは$\\mathbb{F}$-ベクトル空間と呼び、$V$の要素をベクトルvectorという。\n$\\mathbf{u}, \\mathbf{v}, \\mathbf{w} \\in V$と$k, l \\in \\mathbb{F}$に対して、\n(A1) $\\mathbf{u}, \\mathbf{v}$が$V$の要素であれば$\\mathbf{u}+\\mathbf{v}$も$V$の要素である。\n(A2) $\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}$\n(A3) $(\\mathbf{u}+\\mathbf{v})+\\mathbf{w}=\\mathbf{u}+(\\mathbf{v}+\\mathbf{w})$\n(A4) $V$内の全ての$\\mathbf{u}$に対して、$\\mathbf{u} + \\mathbf{0} = \\mathbf{0} + \\mathbf{u} = \\mathbf{u}$を満たす$\\mathbf{0}$が$V$内に存在する。この時$\\mathbf{0}$を零ベクトルzero vectorと呼ぶ。\n(A5) $V$内の全ての$\\mathbf{u}$に対して$\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u} = \\mathbf{0}$を満たす$\\mathbf{v}$が$V$内に存在する。この時$\\mathbf{v}$を**$\\mathbf{u}$の負**negative of $\\mathbf{u}$と呼び、$\\mathbf{v} = -\\mathbf{u}$と表記する。\n(M1) $\\mathbf{u}$が$V$の要素であれば$k \\mathbf{u}$も$V$の要素である。\n(M2) $k(\\mathbf{u} + \\mathbf{v})=k\\mathbf{u} + k\\mathbf{v}$\n(M3) $(k+l)\\mathbf{u}=k\\mathbf{u}+ l\\mathbf{u}$\n(M4) $k(l\\mathbf{u})=(kl)(\\mathbf{u})$\n(M5) $1\\in \\mathbb{F}$に対して、$1\\mathbf{u} = \\mathbf{u}$\n説明 線形空間linear spaceという言葉も使われる。 当然ながらスカラー(体)が実数である必要はない。特に$\\mathbb{F} = \\mathbb{R}$の場合を実ベクトル空間real vector spaceと呼び、$\\mathbb{F} = \\mathbb{C}$の場合を複素ベクトル空間complex vector spaceと呼ぶ。\n数学部の線形代数学では主に$\\mathbb{R}^{n}$や$\\mathbb{C}^{n}$を扱う。$\\mathbb{R}^{n}$は実数$n$個の順序対を要素とするベクトル空間を意味し、即ち$n$次元ユークリッド空間を意味し、具体的に$\\mathbb{R}^{3}$は高校数学、微分積分学でよく見た3次元空間を意味する。\nベクトル空間となる集合は様々ある。関数の集合もベクトル空間となり得て、これを関数空間と呼ぶ。\n物理学では大きさと方向があるものをベクトルと呼ぶ。その概念を一般化したものが線形代数学のベクトルである。例えば大きさが$m\\times n$の実数行列を集めた集合$M_{m\\times n}(\\mathbb{R})$を考えると、$M_{m\\times n}(\\mathbb{R})$は上記の10個の規則を全て満たすことがわかる。したがって同じ大きさの行列を集めた集合はベクトル空間となり、各々の行列はその中でのベクトルとなる。このような抽象的なベクトル空間に初めて接したならば、行列もベクトルだという事実に驚くかもしれないが、これまでに座標空間のベクトルをどのように表記していたかを考えれば驚くこともない。\nある集合がベクトル空間であるかどうかを判断するには、上記の定義を満たしているか一つ一つ確かめればよい。一見ベクトル空間に思えるけれどもそうでない場合もあり、また一見ベクトル空間でなさそうに思えるけれども実はベクトル空間である場合もある。直感とは全く異なる場合があるので、問題を解く時は一つ一つしっかりと確認することが良い。また零ベクトル$\\mathbf{0}$とスカラー$0$は全く異なるものであるので、しっかり区別するようにしよう。通常、教科書ではベクトルは太字で表される。\n定理1 $V$をベクトル空間、$\\mathbf{u}$を$V$の要素とする。\n(1a) $V$の零ベクトルは唯一である。\n(1b) $\\mathbf{u}$の負は唯一である。\n証明 ベクトル空間の定義を利用した証\n明である。\n(1a) $\\mathbf{0},\\mathbf{0}^{\\prime}$が$V$の零ベクトルであるとする。するとベクトル空間の定義により次が成立する。\n$$ \\begin{align*} \\mathbf{0} \u0026amp;= \\mathbf{0} + \\mathbf{0}^{\\prime} \u0026amp;\u0026amp; \\text{by (A4)} \\\\ \u0026amp;= \\mathbf{0}^{\\prime} + \\mathbf{0} \u0026amp;\u0026amp; \\text{by (A2)} \\\\ \u0026amp;= \\mathbf{0}^{\\prime} \u0026amp;\u0026amp; \\text{by (A4)} \\end{align*} $$\nしたがって、二つの零ベクトルは互いに等しい。\n■\n(1b) $\\mathbf{v}, \\mathbf{v}^{\\prime}$が$\\mathbf{u}$の負であるとする。するとベクトル空間の定義により次が成立する。\n$$ \\begin{align*} \\mathbf{v} \u0026amp;= \\mathbf{v} + \\mathbf{0} \u0026amp;\u0026amp; \\text{by (A4)} \\\\ \u0026amp;= \\mathbf{v} + \\left( \\mathbf{u} + \\mathbf{v}^{\\prime} \\right) \u0026amp;\u0026amp; \\text{by (A5)} \\\\ \u0026amp;= \\left( \\mathbf{v} + \\mathbf{u} \\right) + \\mathbf{v}^{\\prime} \u0026amp;\u0026amp; \\text{by (A3)} \\\\ \u0026amp;= \\mathbf{0} + \\mathbf{v}^{\\prime} \u0026amp;\u0026amp; \\text{by (A5)} \\\\ \u0026amp;= \\mathbf{v}^{\\prime} \u0026amp;\u0026amp; \\text{by (A4)} \\end{align*} $$\nしたがって、$\\mathbf{u}$の二つの負は互いに等しい。\n■\n定理2 $V$をベクトル空間、$\\mathbf{u}$を$V$の要素、$k$をスカラーとする。\n(2a) $0 \\mathbf{u} = \\mathbf{0}$\n(2b) $k \\mathbf{0} = \\mathbf{0}$\n(2c) $(-1) \\mathbf{u} = -\\mathbf{u}$\n(2d) もし$k \\mathbf{u} = \\mathbf{0}$であれば、$k = 0$か$\\mathbf{u} = \\mathbf{0}$である。\n証明 ベクトル空間の定義を利用した証明である。\n(2a) $$ \\begin{align*} \u0026amp;\u0026amp; 0\\mathbf{u} \u0026amp;= (0 + 0)\\mathbf{u} \\\\ \u0026amp;\u0026amp; \u0026amp;= 0\\mathbf{u} + 0\\mathbf{u} \u0026amp;\u0026amp;\\text{by (M3)} \\\\ \u0026amp; \u0026amp; \\\\ \\implies \u0026amp;\u0026amp; 0\\mathbf{u}+(-0\\mathbf{u}) \u0026amp;= 0\\mathbf{u} + 0\\mathbf{u} +(-0\\mathbf{u}) \\\\ \\implies \u0026amp;\u0026amp; \\mathbf{0} \u0026amp;= 0\\mathbf{u} \u0026amp;\u0026amp;\\text{by (A5)} \\end{align*} $$\n■\n(2b) $$ \\begin{align*} \u0026amp;\u0026amp; k\\mathbf{0} \u0026amp;= k(\\mathbf{0} + \\mathbf{0}) \u0026amp;\u0026amp;\\text{by (A4)} \\\\ \u0026amp;\u0026amp; \u0026amp;= k\\mathbf{0} + k\\mathbf{0} \u0026amp;\u0026amp;\\text{by (M2)} \\\\ \u0026amp; \u0026amp; \\\\ \\implies \u0026amp;\u0026amp; k\\mathbf{0}+(-k\\mathbf{0}) \u0026amp;= k\\mathbf{0} + k\\mathbf{0} +(-k\\mathbf{0}) \\\\ \\implies \u0026amp;\u0026amp; \\mathbf{0} \u0026amp;= k\\mathbf{0} \u0026amp;\u0026amp;\\text{by (A5)} \\end{align*} $$\n■\n(2c) $$ \\begin{align*} \\mathbf{u} + (-1)\\mathbf{u} \u0026amp;= 1 \\mathbf{u} + (-1)\\mathbf{u} \u0026amp;\u0026amp;\\text{by (M5)} \\\\ \u0026amp;= \\big( 1 + (-1) \\big) \\mathbf{u} \u0026amp;\u0026amp;\\text{by (M3)} \\\\ \u0026amp;= 0 \\mathbf{u} \\\\ \u0026amp;= \\mathbf{0} \u0026amp;\u0026amp;\\text{by (a2)} \\end{align*} $$\nすると**（A5）により$(-1)\\mathbf{u}$は$\\mathbf{u}$の負であり、（1b）**により$\\mathbf{u}$の負は唯一であるため、\n$$ (-1)\\mathbf{u} = -\\mathbf{u} $$\n■\n(2d) $k$は必ず$0$か$0$のどちらか一方の場合にのみ該当するので、二つの場合に分けて考える。\n$k=0$の場合\n結論を満たす。\n$k\\ne 0$の場合\n$k$が$0$でないため、$k$で割ることができる。したがって\n$$ \\begin{align*} \u0026amp;\u0026amp; k \\mathbf{u} \u0026amp;= \\mathbf{0} \\\\ \\implies \u0026amp;\u0026amp; \\mathbf{u} \u0026amp;= \\frac{1}{k}\\mathbf{0} \\\\ \u0026amp;\u0026amp; \u0026amp;= \\mathbf{0} \u0026amp;\u0026amp; \\text{by (2b)} \\end{align*} $$\n■\n一緒に見る ベクトルの簡単な定義 抽象代数 線形代数学でのベクトル空間 抽象代数学でのベクトル空間 下記の文書で述べられている$F$-ベクトル空間は、実際に上記の文書のベクトル空間と何の差異もない。ただ視点が少し異なるだけで、線形代数学でのベクトル空間が直感的なユークリッド空間の抽象化であり、抽象代数学でのベクトル空間はそれを真の意味での\u0026rsquo;代数\u0026rsquo;として扱うことである。\n逆に$R$-モジュールは$F$-ベクトル空間のスカラー体$F$をスカラー環$R$に一般化することに意義があり、したがって$F$-ベクトルフィールドの歴史と意味に関心がないネーミングでそのアイデンティティを示している。群$G$の立場から見れば、環$R$と新しい演算$\\mu$が加えられたことであるため、その逆も加群加群である。\n抽象代数学でのR-モジュール 抽象代数学での$F$-ベクトル空間 Howard Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), p202-203\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n体をよくわからなければ、簡単に$\\mathbb{F}=\\mathbb{R}$または$\\mathbb{F}=\\mathbb{C}$と考えればよい。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":282,"permalink":"https://freshrimpsushi.github.io/jp/posts/282/","tags":null,"title":"ベクトル空間の定義"},{"categories":"함수","contents":"定義 区間 $I \\subset \\mathbb{R}$ の二つの要素 $x_{1} , x_{2}$ と関数 $f : I \\to \\mathbb{R}$ および $0 \\le t \\le 1$ について、\n$f( t x_{1} + (1-t) x_{2}) \\le t f(x_{1}) + (1-t) f(x_{2})$ のとき、$f$ は $I$での凸関数と定義される。 $f( t x_{1} + (1-t) x_{2}) \\ge t f(x_{1}) + (1-t) f(x_{2})$ のとき、$f$ は $I$での凹関数と定義される。 説明 凸や凹には、上向きの凸や下向きの凹など、混乱しやすい表現が多いため、グラフの形状に対応させて**凸(convex)と凹(concave)**を英語のまま使用して記憶することを強く推奨する。式を見ただけでは一見馴染みのない定義に思えるが、内分の概念を考えれば、非常に直感的な定義として受け入れられるだろう。直感的に難しくない概念なので、式的な展開や説明が必要ない場合は、わざわざ定義を覚える必要もない。通常、中学校の二次関数から始まり、二階導関数の符号などを延々と見てきたため、その性質も親しみやすいはずだ。\n正直に言って 正直に言って、凹はあまり使われず、凸だけで考えればいいと思う。\n二階導関数 凸関数の二階導関数: $f$ が $I$ で二回微分可能とする。$f$ が $I$ で凸であることと $f '' (x) \\ge 0$ は必要十分条件である。\nここで、二回微分可能という条件が加わっていることに注目しよう。通常、例として $y = x^2$ や $y = \\ln {x}$ のような曲線が使用されるが、見逃しやすい点だが、私たちが再定義した凸関数では「連続」であることは言及されていないことに気づくだろう。\n一緒に見る 一般的な凸ベクトル関数 集合の凸 凸関数の様々な性質 ","id":262,"permalink":"https://freshrimpsushi.github.io/jp/posts/262/","tags":null,"title":"凸関数、凹関数"},{"categories":"다변수벡터해석","contents":"定義 ベクトル空間 $V = \\mathbb{R}^n$ について、$\\mathbb{x}, \\mathbb{y}, \\mathbb{z} \\in V$ そして $k \\in \\mathbb{R}$ とする。\n$\\left\u0026lt; \\cdot , \\cdot \\right\u0026gt; : V^2 \\to \\mathbb{R}$ が下の四つの条件を満たすとき、$\\left\u0026lt; \\cdot , \\cdot \\right\u0026gt;$ を**$V$ 上の内積**として定義する。\n(1) 対称性: $\\left\u0026lt; \\mathbb{x} , \\mathbb{y} \\right\u0026gt; = \\left\u0026lt; \\mathbb{y}, \\mathbb{x} \\right\u0026gt;$\n(2) 加算性: $\\left\u0026lt; \\mathbb{x} + \\mathbb{y} , \\mathbb{z} \\right\u0026gt; = \\left\u0026lt; \\mathbb{x}, \\mathbb{z} \\right\u0026gt; + \\left\u0026lt; \\mathbb{y}, \\mathbb{z} \\right\u0026gt;$\n(3) 斉次性: $\\left\u0026lt; k \\mathbb{x} , \\mathbb{y} \\right\u0026gt; = k \\left\u0026lt; \\mathbb{x}, \\mathbb{y} \\right\u0026gt;$\n(4) 正定値性: $\\left\u0026lt; \\mathbb{x} , \\mathbb{x} \\right\u0026gt; \\ge 0$ そして $\\left\u0026lt; \\mathbb{x} , \\mathbb{x} \\right\u0026gt; =0 \\iff \\mathbb{x}=\\mathbb{0}$\n特に $\\mathbb{x} = (x_{1}, x_{2}, \\cdots , x_{n})$ かつ $\\mathbb{y} = (y_{1}, y_{2}, \\cdots , y_{n})$ のとき、\n$$ \\left\u0026lt; \\mathbb{x} , \\mathbb{y}\\right\u0026gt; = \\mathbb{x} \\cdot \\mathbb{y} = x_{1} y_{1} + x_{2} y_{2} + \\cdots + x_{n} y_{n} = \\mathbb{x}^T \\mathbb{y} $$\nを点積またはユークリッド内積と定義する。\n説明 もともとベクトル空間自体はどんな体 $\\mathbb{F}$ についても一般化され得る概念である。当然、内積もまた一般化可能だが、基礎レベルの線形代数では通常ユークリッド空間だけを扱うことが普通である。\nしかし大学で内積を学ぶ際に混乱する理由は、高等学校レベルの内積よりも十分に一般化されているからである。内積自体を考えた場合、条件を満たす写像が存在すれば、わざわざ成分同士を掛け合わせる必要がない。まず「既に知っていた内積」が「大学で学んだ内積の一つ」である点積になるところから違いが生じる。さらに$n$次元について一般化され、幾何学的な性質を失い、大きさやベクトル間の内角の定義が大きな混乱を招くことになる。\n[1]: $\\left\\| \\mathbb{x} \\right\\| = \\sqrt{ \\mathbb{x} \\cdot \\mathbb{x} }$ を**$\\mathbb{x}$ の大きさまたは長さ**として定義する。 [2] $d(\\mathbb{x}, \\mathbb{y} ) = \\left\\| \\mathbb{x} - \\mathbb{y} \\right\\|$ を**$\\mathbb{x}$ と $\\mathbb{y}$ の距離**として定義する。\n[3] $\\theta \\in [0 , \\pi]$ について、$\\displaystyle \\cos \\theta = {{ \\mathbb{x} \\cdot \\mathbb{y} } \\over { \\left\\| \\mathbb{x} \\right\\| \\left\\| \\mathbb{y} \\right\\|}}$ を**$\\mathbb{x}$ と $\\mathbb{y}$ 間の内角**として定義する。\n[4] $\\mathbb{x} \\cdot \\mathbb{y} = 0$ の時、$\\mathbb{x}$ と $\\mathbb{y}$ は垂直であると定義する。\n3次元までは直接計算し、描きながらこれらの定義が直感と一致していることを確認できるが、4次元からはそれが不可能である。しかし、このように超越的な一般化がちょうど数学の魅力であり、強みであり、これらの定義だけでもいくつかの定理を容易に一般化することができる。以下の二つの例を見よ。\n一般化された コーシー-シュワルツの不等式 $$ \\left\\| \\mathbf{x} \\right\\| \\left\\| \\mathbf{y} \\right\\| \\ge \\left\u0026lt; \\mathbf{x} , \\mathbf{y} \\right\u0026gt; $$\nコーシー-シュワルツの不等式はもともと四つの未知数に対して成り立っていた不等式だが、内積を用いることで簡単に一般化できる。\n証明 任意のスカラー $t \\in \\mathbb{R}$ に対して、\n$$ ( t \\mathbb{x} + \\mathbb{y} ) ^2 = t^2 (\\mathbb{x} \\cdot \\mathbb{x}) + 2t (\\mathbb{x} \\cdot \\mathbb{y} )+ ( \\mathbb{y} \\cdot \\mathbb{y} ) $$\n$t$ は実数であるため、二次方程式の解の公式により $(\\mathbb{x} \\cdot \\mathbb{y})^2 - ( \\mathbb{x} \\cdot \\mathbb{x} )( \\mathbb{y} \\cdot \\mathbb{y} ) \\le 0$ でなければならない。\n■\n一般化された ピタゴラスの定理 二つのベクトル $\\mathbb{x}$ と $\\mathbb{y}$ が互いに垂直であるとすると、$\\left\\| \\mathbb{x} \\right\\|^2 + \\left\\| \\mathbb{y} \\right\\|^2 = \\left\\| \\mathbb{x} +\\mathbb{y} \\right\\|^2$\nピタゴラスの定理ももとは平面上の三角形に対して成立する定理であった。一般化するためには、次元を上げるごとに低い次元のピタゴラスの定理を続けて使う必要があったが、内積を使えばはるかに簡単で簡潔である。\n証明 $$ \\begin{align*} \\left\\| \\mathbb{x} +\\mathbb{y} \\right\\|^2 =\u0026amp; ( \\mathbb{x} + \\mathbb{y} ) ^2 \\\\ =\u0026amp; (\\mathbb{x} \\cdot \\mathbb{x}) + 2 (\\mathbb{x} \\cdot \\mathbb{y} )+ ( \\mathbb{y} \\cdot \\mathbb{y} ) \\\\ \u0026amp;= \\left\\| \\mathbb{x} \\right\\|^2 + 2 (\\mathbb{x} \\cdot \\mathbb{y} ) + \\left\\| \\mathbb{y} \\right\\|^2 \\end{align*} $$\n内角の定義により $(\\mathbb{x} \\cdot \\mathbb{y} ) = \\cos{\\theta} \\left\\| \\mathbb{x} \\right\\|^2 \\left\\| \\mathbb{y} \\right\\|^2$ であり、$\\mathbb{x}$ と $\\mathbb{y}$ が互いに垂直であるため、$\\cos{\\theta} = 0$\n■\nより一般化されたピタゴラスの定理 $\\mathbf{a}_1,\\ \\mathbf{a}_2,\\ \\cdots,\\ \\mathbf{a}_n$ が互いに垂直なベクトルであるとする。すると、以下の式が成立する。\n$$ \\left\\| \\mathbf{a}_1+\\mathbf{a}_2+\\cdots+\\mathbf{a}_n \\right\\|^2 = \\left\\| \\mathbf{a}_1 \\right\\|^2 + \\left\\| \\mathbf{a}_2 \\right\\|^2 +\\cdots+ \\left\\| \\mathbf{a}_n \\right\\|^2 $$\n上の定理は$n$個のベクトルに対して一般化したものである。\n証明 上で定義された通り、以下が成り立つ。\n$$ \\left\\| \\mathbf{a}_1+\\mathbf{a}_2+\\cdots+\\mathbf{a}_n \\right\\| ^2=\\left\u0026lt; \\mathbf{a}_1 + \\mathbf{a}_2 + \\cdots + \\mathbf{a}_n,\\ \\mathbf{a}_1 + \\mathbf{a}_2+\\cdots+\\mathbf{a}_n \\right\u0026gt; $$\n内積の加算性により、上の内積を展開すると以下のようになる。\n$$ \\begin{array} {l} \\left\u0026lt; \\mathbf{a}_1,\\ \\mathbf{a}_1 \\right\u0026gt;+\\left\u0026lt; \\mathbf{a}_1,\\ \\mathbf{a}_2 \\right\u0026gt;+\\cdots+\\left\u0026lt; \\mathbf{a}_1,\\ \\mathbf{a}_n \\right\u0026gt; \\\\ + \\left\u0026lt; \\mathbf{a}_2,\\ \\mathbf{a}_1 \\right\u0026gt; + \\left\u0026lt; \\mathbf{a}_2,\\ \\mathbf{a}_2 \\right\u0026gt;+\\cdots+\\left\u0026lt; \\mathbf{a}_2,\\ \\mathbf{a}_n \\right\u0026gt; \\\\ +\\cdots \\\\ + \\left\u0026lt; \\mathbf{a}_n,\\ \\mathbf{a}_1 \\right\u0026gt;+\\left\u0026lt; \\mathbf{a}_n,\\ \\mathbf{a}_2 \\right\u0026gt;+\\cdots +\\left\u0026lt; \\mathbf{a}_n,\\ \\mathbf{a}_n \\right\u0026gt; \\end{array} $$\n仮定により$\\left\u0026lt; \\mathbf{a}_i,\\ \\mathbf{a}_j \\right\u0026gt;=\\delta_{ij}$ なので、同じベクトル同士の内積のみ残り、その他は$0$ である。したがって、\n$$ \\begin{align*} \\left\\| \\mathbf{a}_1+\\mathbf{a}_2+\\cdots+\\mathbf{a}_n \\right\\|^2 =\u0026amp; \\left\u0026lt; \\mathbf{a}_1,\\ \\mathbf{a}_1 \\right\u0026gt; + \\left\u0026lt; \\mathbf{a}_2,\\ \\mathbf{a}_2 \\right\u0026gt;+\\cdots+\\left\u0026lt; \\mathbf{a}_n,\\ \\mathbf{a}_n \\right\u0026gt; \\\\ =\u0026amp; \\left\\| \\mathbf{a}_1 \\right\\|^2 + \\left\\| \\mathbf{a}_2 \\right\\|^2 +\\cdots+\\left\\| \\mathbf{a}_n \\right\\|^2 \\end{align*} $$\n■\n","id":255,"permalink":"https://freshrimpsushi.github.io/jp/posts/255/","tags":null,"title":"ユークリッド空間における内積"},{"categories":"수리물리","contents":"定義 $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^3$ に対して**$\\mathbf{x}$と$\\mathbf{y}$の外積**cross product を定義する。\n$$ \\begin{align*} \\mathbf{x} \\times \\mathbf{y} =\u0026amp; (x_{2}y_{3} - x_{3}y_{2}, x_{3}y_{1} - x_{1}y_{3}, x_{1}y_{2} - x_{2}y_{1}) \\\\ =\u0026amp; \\det \\begin{bmatrix} \\mathbf{i} \u0026amp; \\mathbf{j} \u0026amp; \\mathbf{k} \\\\ x_{1} \u0026amp; x_{2} \u0026amp; x_{3} \\\\ y_{1} \u0026amp; y_{2} \u0026amp; y_{3} \\end{bmatrix} \\\\ =\u0026amp; \\begin{bmatrix} 0 \u0026amp; -x_{3} \u0026amp; x_{2} \\\\ x_{3} \u0026amp; 0 \u0026amp; -x_{1} \\\\ -x_{2} \u0026amp; x_{1} \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ y_{3} \\end{bmatrix} \\end{align*} $$\n説明 ちなみに$\\mathbf{i} = (1,0,0)$、$ \\mathbf{j} = (0,1,0)$、$\\mathbf{k} = (0,0,1)$ だ。内積と同様に外積ももっと一般的な定義が可能だけど、実用的な面では通常三次元に限って考えることが多い。この三次元空間での定義はベクトル積という名前もあるけど、厳格に区別する時だけ使われる。最も使われるのは物理学で、トルクやローレンツ力などを表すときに頻繁に登場する。幾何学的な形も右ねじの法則を思い出せば簡単に想像できる。外積の性質をいくつか証明なしで紹介する。\n性質 $\\mathbf{x}, \\mathbf{y}, \\mathbb{z} \\in \\mathbb{R}^3$ と $k \\in \\mathbb{R}$ に対して以下が成り立つ。\n(1) $\\mathbf{x} \\times \\mathbf{x} = 0$\n(2) 反交換性anti commutativity: $\\mathbf{x} \\times \\mathbf{y} = -\\mathbf{y} \\times \\mathbf{x} $\n(3) $(k \\mathbf{x}) \\times \\mathbf{y} = k (\\mathbf{x} \\times \\mathbf{y}) = \\mathbf{x} \\times (k \\mathbf{y})$\n(4) $\\mathbf{x} \\times ( \\mathbf{y}+ \\mathbb{z} )= (\\mathbf{x} \\times \\mathbf{y}) + (\\mathbf{x} \\times \\mathbb{z})$\n(5) スカラー三重積: $(\\mathbf{x} \\times \\mathbf{y}) \\cdot \\mathbb{z} = \\mathbf{x} \\cdot ( \\mathbf{y} \\times \\mathbb{z})$\n(6) ベクトル三重積(bac-cab公式): $\\mathbf{x} \\times ( \\mathbf{y} \\times \\mathbb{z} ) = (\\mathbf{x} \\cdot \\mathbb{z}) \\mathbf{y} - (\\mathbf{x} \\cdot \\mathbf{y}) \\mathbb{z} $\n(7) $|| \\mathbf{x} \\cdot \\mathbf{y} ||^2 = (\\mathbf{x} \\cdot \\mathbf{x} ) ( \\mathbf{y} \\cdot \\mathbf{y} ) - ( \\mathbf{x} \\cdot \\mathbf{y} )^2$\n(8) $|| \\mathbf{x} \\times \\mathbf{y} || = || \\mathbf{x} || || \\mathbf{y} || \\sin{\\theta} $\n(9) $\\mathbf{x} \\times \\mathbf{y} \\ne \\mathbb{0}$ の場合、$\\mathbf{x} \\times \\mathbf{y} $は$\\mathbf{x}$と$\\mathbf{y}$に垂直である。\n交換法則が成り立たないため、直感的に理解しにくい性質が多い。問題を解いたり、紙に書きながら慣れるようにしよう。\n","id":256,"permalink":"https://freshrimpsushi.github.io/jp/posts/256/","tags":null,"title":"三次元ユークリッド空間における外積"},{"categories":"행렬대수","contents":"要旨1 行列$A$の行空間と列空間の次元は同じである。\n証明 $R$を$A$の行階段形行列とする。基本的な行操作は$A$の行空間と列空間の次元を変えないため、次の式が成立する。\n$$ \\begin{align*} \\dim \\big( \\mathcal{R}(A) \\big) \u0026amp;= \\dim \\big( \\mathcal{R}(R) \\big) \\\\ \\dim \\big( \\mathcal{C}(A) \\big) \u0026amp;= \\dim \\big( \\mathcal{C}(R) \\big) \\end{align*} $$\n従って、$R$の行空間と列空間の次元が同じであることを示せば十分である。しかし$R$の行空間は先頭1がある行、$R$の列空間は先頭1がある列で生成されるため、$R$の行空間と列空間の次元は同じである。\n■\n定義 行列$A$の行空間(列空間)の次元を ランクrankといい、次のように表示する。\n$$ \\text{rank}(A) = \\dim \\mathcal{R}(A) = \\dim \\mathcal{C}(A) $$\n行列$A$の零空間の次元を 無効次元nullityといい、次のように表示する。\n$$ \\text{nullity}(A) = \\dim \\mathcal{N}(A) $$\n説明 ランクは係数、無効次元は劣化次元と訳されることもある。\n一方で$\\text{rank}(A)$は、$A$を行階段形にしたときのピボットの数としても定義することができる。\n正方行列でない$m \\times n$の行列$A$を考えると、行空間は最大で$n$次元、列空間は最大で$m$次元になる。しかし、これら2つの値が同じで、それがランクであるため、次の式が成立する。\n$$ \\rank(A) \\le \\min(m,n) $$\n$\\rank(A) = \\min(m,n)$の場合、$A$がフルランクfull rankを持っていると言われる。フルランクを持たない場合はランク不足rank deficientと言われる。\n直感的に理解が難しい場合は、連立方程式の未知数を数えることから導き出された概念と考えると良い。定義自体は全く難しくないが、$ m \\ne n$の場合、特に零空間や係数、劣化次元などの概念が原書で学んだ人には意味を推測するのが非常に難しいレベルである。これらの概念を学ぶ理由は、後に続く線形代数学の応用を数学の言葉で簡単に表現するためである。複雑な理論が展開されるとき、列空間や零空間などの定義は相当な面積を節約し、より複雑な現象をカバーしてくれる。\nちなみに列空間は$\\text{Im} (A)$、つまり像imageとも呼ばれる。行列$A$を関数の概念として考える場合、それは$A \\in \\mathbb{R}^{m \\times n}$に対応する関数$T_{A}$を$T_{A} : \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$としても見ることができる理由である。\n以下のランク-無効次元定理も、関数の概念として考えると理解が容易である。$\\text{rank} A = \\text{rank} A^{T}$であることを忘れないでください。\nランク-無効次元定理 行列$A \\in M_{ m \\times n }(\\mathbb{R})$に対して、次の式が成立する。\n$$ \\begin{align*} \\text{rank} (A) + \\text{nullity} (A) \u0026amp;= \\dim \\mathbb{R}^{n} = n \\\\ \\text{rank} (A^{T}) + \\text{nullity} (A^{T}) \u0026amp;= \\dim \\mathbb{R}^{m} = m \\end{align*} $$\n行列の次元定理とも呼ばれる。線形変換について一般化すると、次のようになる。\nベクター空間$V, W$と線形変換$T : V \\to W$に対して、次の式が成立する。\n$$ \\text{rank} (T) + \\text{nullity} (T) = \\dim (V) $$\n証明 $A$を$m \\times n$行列とする。すると、$A$の列が$n$個であるため、斉次線形システム$A \\mathbf{x} = \\mathbf{0}$は$n$個の未知数を持つ。従って、「先導変数の数 + 自由変数の数 = $n$」が成立する。先導変数の数は先導1の数と同じであり、これは行空間の次元と同じである。また、自由変数の数はパラメータの数と同じであり、これは零空間の次元と同じである。したがって、定理が成立する。\n■\n関連する話題 抽象代数学における核 零空間は$\\ker A$と表され、核Kernelとも呼ばれる。これは抽象代数学で扱われる一般的な核の概念を線形代数で特殊化した表現であり、これもまた$A$を関数として見ることに由来する。\nHoward Anton, Elementary Linear Algebra: Applications Version (12th Edition, 2019), p278\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3021,"permalink":"https://freshrimpsushi.github.io/jp/posts/3021/","tags":null,"title":"行列のランク、零化次元"},{"categories":"행렬대수","contents":"定義1 $$ A = \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{bmatrix} $$\n行列 $A$に対して、$A$の行から作られる$m$個の$\\mathbb{R}^{n}$ベクターは\n$$ \\begin{align*} \\mathbf{r}_{1} =\u0026amp; \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\end{bmatrix} \\\\ \\mathbf{r}_{2} =\u0026amp; \\begin{bmatrix} a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\end{bmatrix} \\\\ \u0026amp;\\vdots \\\\ \\mathbf{r}_{m} =\u0026amp; \\begin{bmatrix} a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{bmatrix} \\end{align*} $$\n$A$の行ベクトルrow vectorsと呼ばれる。$A$の列から作られる$n$個の$\\mathbb{R}^{m}$ベクターは\n$$ \\mathbf{c}_{1} = \\begin{bmatrix} a_{11} \\\\ a_{21} \\\\ \\vdots \\\\ a_{m1} \\end{bmatrix},\\quad \\mathbf{c}_{2} = \\begin{bmatrix} a_{12} \\\\ a_{22} \\\\ \\vdots \\\\ a_{m2} \\end{bmatrix},\\quad \\dots,\\quad \\mathbf{c}_{n} = \\begin{bmatrix} a_{1n} \\\\ a_{2n} \\\\ \\vdots \\\\ a_{mn} \\end{bmatrix} $$\n$A$の列ベクトルcolumn vectorsと呼ばれる。\n$$ \\begin{align*} A =\u0026amp; \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{r}_{1} \\\\ \\mathbf{r}_{2} \\\\ \\vdots \\\\ \\mathbf{r}_{m} \\end{bmatrix} \\\\ =\u0026amp; \\begin{bmatrix} \\mathbf{c}_{1} \u0026amp; \\mathbf{c}_{2} \u0026amp; \\cdots \u0026amp; \\mathbf{c}_{n} \\end{bmatrix} \\end{align*} $$\n$A$の行ベクトル$\\mathbf{r}_{1}, \\mathbf{r}_{2},\\dots,\\mathbf{r}_{m}$によって生成される$\\mathbb{R}^{n}$の部分空間を$A$の行空間row spaceと言い、以下のように表記する。\n$$ \\mathcal{R} (A) \\quad \\text{or} \\quad \\text{row}(A) $$\n$A$の列ベクトル$\\mathbf{c}_{1}, \\mathbf{c}_{2},\\dots,\\mathbf{c}_{n}$によって生成される$\\mathbb{R}^{m}$の部分空間を$A$の列空間column spaceと言い、以下のように表記する。\n$$ \\mathcal{C} (A) \\quad \\text{or} \\quad \\text{col}(A) $$\n同次連立一次方程式 $A \\mathbf{x} =\\mathbf{0}$の解集合を$A$の零空間null spaceと言い、以下のように表記する。\n$$ \\mathcal{N}(A) \\quad \\text{or} \\quad \\text{null}(A) $$\n説明 上記の概念は\n$$ \\begin{equation} A\\mathbf{x} = \\mathbf{b} \\end{equation} $$\n連立一次方程式を解くために考案された。つまり、線形代数学では$(1)$の解と$A$の行空間、列空間、零空間の関係に興味があるのだ。具体的には行空間の基底を求めることが線形システムを解くことに関連している。特に、行空間と列空間の次元をランクと言い、零空間の次元を無効次元と言う。\nなお、列空間は$\\text{Im} (A)$、すなわち像imageとも呼ばれる。行列$A$を関数の概念として考えるならば、$A \\in \\mathbb{R}^{m \\times n}$に対応する関数$T_{A}$は$T_{A} : \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$としても見ることができるからだ。\n定理1 線形システム$A \\mathbf{x} = \\mathbf{b}$が解を持つための必要十分条件は$\\mathbf{b} \\in \\mathcal{C}(A)$である。\n定理2 $\\mathbf{x}_{0}$が$A\\mathbf{x} = \\mathbf{b}$のある解だとしよう。$S= \\left\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{k} \\right\\}$を$\\mathcal{N}(A)$の基底としよう。そうすると、$A\\mathbf{x} = \\mathbf{b}$の全ての解は下記のように表現できる。\n$$ \\begin{equation} \\mathbf{x} = \\mathbf{x}_{0} + c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + \\cdots + c_{k}\\mathbf{v}_{k} \\end{equation} $$\n逆に、全ての定数$c_{1}, c_{2}, \\dots, c_{k}$に対して、上記の$\\mathbf{x}$は$A\\mathbf{x} = \\mathbf{b}$の解である。\n$(2)$を$A \\mathbf{x} = \\mathbf{b}$の一般解general solutionという。$\\mathbf{x}_{0}$を$A \\mathbf{x} = \\mathbf{b}$の特殊解particular solutionという。そして、$c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + \\cdots + c_{k}\\mathbf{v}_{k}$を$A \\mathbf{x} = \\mathbf{0}$の一般解という。\nこれらの定理から、非同次線形システムの一般解はある特殊解と同次線形システムの一般解の和として表されることが分かる。\n参照 抽象代数学における核 零空間は$\\ker A$と書き、核kernelとも呼ばれる。これは抽象代数学で扱われる一般的な核の概念を線形代数学で特殊化した表現であり、これも$A$を関数と見なしたものだ。\nHoward Anton, Chris Rorres, Anton Kaul, Elementary Linear Algebra: Applications Version(12th Edition). 2019, p263-267\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":254,"permalink":"https://freshrimpsushi.github.io/jp/posts/254/","tags":null,"title":"行空間、列空間、零空間"},{"categories":"물리학","contents":"定義 粒子の軌跡を時空間で表した線を世界線world lineと呼ぶ。\n説明 まず一方向に等速運動する座標系だけを考えよう。$A$座標系では、原点に静止した粒子がいる。この粒子の世界線は以下の通りだ。\nそして$A$座標系に対して$+x$方向に$v_{0}$の速度で動く$A^{\\prime}$座標系がある。\n同じ粒子の運動を$A^{\\prime}$座標系から観察すると、世界線は以下の通りだ。\n$A^{\\prime}$系が$x$方向にのみ動くので、$y, z$の値は変わらない。つまり、以下の通りだ。\n$$ \\begin{align*} t^{\\prime}\u0026amp;= t \\\\ x^{\\prime} \u0026amp;= -v_{0}t \\\\ y^{\\prime} \u0026amp;= 0 \\\\ z^{\\prime} \u0026amp;= 0 \\end{align*} $$\n粒子が原点ではない任意の点$P(x,y,z,)$に静止しているとすると、以下の通りだ。\n$$ \\begin{align*} t^{\\prime} \u0026amp;= t \\\\ x^{\\prime} \u0026amp;= x-v_{0}t \\\\ y^{\\prime} \u0026amp;= y \\\\ z^{\\prime} \u0026amp;= z \\end{align*} $$\nすると、二つの座標系間の時空間では以下のような式が成立し、これをガリレイ変換Galilean transformationと呼ぶ。\n$$ \\begin{pmatrix} t^{\\prime} \\\\ x^{\\prime} \\\\ y^{\\prime} \\\\ z^{\\prime} \\end{pmatrix} = \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ -v_{0} \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} t \\\\ x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} t \\\\ x-v_{0}t \\\\ y \\\\ z \\end{pmatrix} $$\nガリレイ変換には以下のような特徴があることがわかる。\n二つの座標系間で時刻は絶対的で、変わらない。 粒子の速度は、二つの座標系間の速度差分だけ異なる。 さらに、座標系が動かない方向では、速度の差が生じない。数式で表されると、以下の通りだ。 $$ \\begin{align*} t^{\\prime} \u0026amp;= t \\\\ v_{x}^{\\prime} \u0026amp;= v_{x}-v_{0} \\\\ v_{y}^{\\prime} \u0026amp;= v_{y} \\\\ v_{z}^{\\prime}\u0026amp;= v_{z} \\end{align*} $$ 参照 [ローレンツ変換] ガリレイ変換は、相対性理論の効果を考慮しない変換式である。光の速度に近くないとき、この近似は現実とかなりうまく一致する。一方で光の速度に近づくと、相対性理論の効果を考慮する必要があり、これを反映したものがローレンツ変換である。\n","id":250,"permalink":"https://freshrimpsushi.github.io/jp/posts/250/","tags":null,"title":"世界線とガリレイ変換"},{"categories":"복소해석","contents":"定義 $z$ を $z=a+ib(a,b\\in \\mathbb{R})$ の複素数としよう。\n$\\overline{z}$ を以下のように定義し、$z$ の共役複素数と呼ぶ。 $$ \\overline{z}:=\\overline{a+ib}=a-ib $$\n説明 元の複素数に $i$ ではなく $-i$ を代入したもの、複素平面で実数軸に対する反射として説明できる。共役という言葉は、足したときに実数を作り出す一対であるという点から名付けられたようだ。共役複素数は複素解析を学ぶ際に最初に出会う概念の一つだが、実際にはすぐに使うことはないため、学習を怠ることが多い。しかし、これらの性質は単に学ぶだけではなく、反復を通じて習得することが重要である。通常、[5] 以降は本でも言及されないので、生さえずし屋で学んでおき、利益を得るようにしよう。\n性質 $z_{1}$, $z_{2}$, $z \\in \\mathbb{C}$ とする。すると以下の等式が成り立つ。\n[1]: $(z+\\overline{z}) = 2 \\Re(z) \\in \\mathbb{R}$ [2]: $\\overline{z_{1} + z_{2}} = \\overline{z_{1}} + \\overline{z_{2}}$ [3]: $\\overline{z_{1} z_{2}} = \\overline{z_{1}} \\cdot \\overline{z_{2}}$ [4]: $\\overline{ \\overline{z} } = z$ [5]: $z \\overline{z} = |z|^2$ [6]: $\\overline{ \\left( { \\dfrac{1}{z} } \\right) } = \\dfrac{1}{\\overline{z}}$ [7]: $\\overline{ \\left( \\dfrac{z_{1}}{z_{2}} \\right) } = \\dfrac{\\overline{z_{1}}}{\\overline{z_{2}}}$ [8]: $\\overline{ \\sin{ z } } = \\sin{\\overline{z}}$ [9]: $\\overline{ \\cos{ z } } = \\cos{\\overline{z}}$ [10]: $\\overline{ e^{ z } } = e^{\\overline{z}}$ [11]: $\\overline{ \\cosh{z} } = e^{\\overline{z}}$ [12]: $\\overline{ \\tan{ z } } = \\tan{\\overline{z}}$ 証明 証明に移る前に変数を $z_{1} = x_{1} + i y_{1}$, $z_{2} = x_{2} + i y_{2}$, $z = x + i y$ としよう。\n[1] $$ z +\\overline{z} = (x+iy)+(x-iy) = 2x $$\nよって、$(z+\\overline{z})=2x\\in \\mathbb{R}$である。\n■\n[2] $$ \\begin{align*} \\overline{z_{1} + z_{2}} =\u0026amp; \\overline{ ( x_{1} + i y_{2} ) + ( x_{2} + i y_{2} ) } \\\\ =\u0026amp; \\overline{ ( x_{1} + x_{2} ) + i ( y_{1} + y_{2} ) } \\\\ =\u0026amp; ( x_{1} + x_{2} ) - i ( y_{1} + y_{2} ) \\\\ \u0026amp;=(x_{1} - i y_{1}) + (x_{2} - i y_{2}) \\\\ =\u0026amp; \\overline{z_{1}} + \\overline{z_{2}} \\end{align*} $$\n■\n[3] $$ \\begin{align*} \\overline{z_{1} z_{2}} =\u0026amp; \\overline{ ( x_{1} + i y_{1} ) ( x_{2} + i y_{2} ) } \\\\ =\u0026amp; \\overline{( x_{1} x_{2} - y_{1} y_{2} ) + i ( x_{1} y_{2} + y_{1} x_{2} )} \\\\ =\u0026amp; ( x_{1} x_{2} - y_{1} y_{2} ) - i ( x_{1} y_{2} + y_{1} x_{2} ) \\\\ =\u0026amp; ( x_{1} - i y_{1} ) ( x_{2} - i y_{2} ) \\\\ =\u0026amp; \\overline{( x_{1} + i y_{1} )} \\ \\overline{( x_{2} + i y_{2} )} \\\\ =\u0026amp; \\overline{z_{1}} \\cdot \\overline{z_{2}} \\end{align*} $$\n■\n[4] $$ \\overline{ \\overline{ z } } = \\overline{ x - i y } = x + i y =z $$\n■\n[5] $$ z \\overline{z} = (x + iy ) ( x - i y )= x^2 + y^2 =|z|^2 $$\n■\n[6] $$ \\overline{ \\left( { {1} \\over {z} } \\right) } = \\overline{ \\left( {1} \\over { x + iy } \\right) } = \\overline{{x - i y} \\over {x ^ 2 + y^2 }} = {{x + i y} \\over {x ^ 2 + y^2 }} = {{1} \\over {x - i y }} = {{1} \\over { \\overline{z} }} $$\n■\n[7] [3], [6]によると $$ \\overline{ \\left( { {z_{1}} \\over { z_{2} } } \\right) } = \\overline{ z_{1} { {1} \\over { z_{2} } }}=\\overline{z_{1}}\\cdot \\overline{ \\left( {1} \\over { z_{2} } \\right) } = \\overline{z_{1}}\\cdot { 1 \\over \\overline{z_{2}} } = {{\\overline{z_{1}}} \\over { \\overline{z_{2}} }} $$ ■\n[8] [9] [8]と[9]の証明は本質的に同じなので、[9]の証明は省略する。\n$$ \\begin{align*} \\overline{ \\sin{ z } } =\u0026amp; \\overline{\\sin{(x+ i y)}} \\\\ =\u0026amp; \\overline{ \\sin{x} \\cosh{y} - i \\cos{x} \\sinh{y} } \\\\ =\u0026amp; { \\sin{x} \\cosh{y} + i \\cos{x} \\sinh{y} } \\\\ =\u0026amp; \\sin{(x-iy)} \\\\ =\u0026amp; \\sin{ \\overline{z} } \\end{align*} $$\n■\n[10] オイラーの公式によると、以下が成り立つ。\n$$ \\overline{e^z} = \\overline{\\cos{x} + i \\sin{y}} = \\cos{x} - i \\sin{y} =e^{\\overline{z}} $$\n■\n[11] [7], [10]によると、以下が成り立つ。\n$$ \\overline{\\cosh{z}} = \\overline{\\left( \\dfrac{e^{z} + e^{-z}}{2} \\right)} = \\dfrac{ e^{\\overline{z}} + e^{-\\overline{z}} } {2} = \\cosh\\overline{z} $$\n■\n[12] [7], [8], [9]によると、以下が成り立つ。\n$$ \\overline{ \\tan{z} } = \\overline{ \\left( { \\sin{z} } \\over { \\cos{z} }\\right) } = {{ \\overline{\\sin{z}} } \\over { \\overline{\\cos{z}} }} = {{ \\sin{ \\overline{z} } } \\over { \\cos { \\overline{z} } }} = \\tan{ \\overline{z} } $$\n■\n補足 証明の過程から分かるように、[11]は他の双曲関数に対しても適用できる。特定の関数だけが重要で別に証明が必要なわけではなく、このような良い性質を容易に導出できることを示すために証明を残した。三角関数も同様に、これらの良い性質を自ら簡単に導き出せるので、自分で試してみよう。\n[13]: $\\dfrac{1 + i}{1 - i } = i$ [14]: $\\dfrac{1 - i}{1 + i } = -i$ [15]: $\\dfrac{1}{i } = -i$ [16]: $i \\cdot (- i ) = 1$ 公式ではないが、これらの虚数の計算は非常に頻繁に使用されるため、マスターすると計算量を劇的に減らすことができる。特に[15]は、約分や両辺に虚数を掛ける状況で非常に便利だ。\n","id":245,"permalink":"https://freshrimpsushi.github.io/jp/posts/245/","tags":null,"title":"共役複素数"},{"categories":"고전역학","contents":"慣性モーメント $$ \\begin{align*} I \u0026amp;= \\sum_{i} m_{i} {r_{i}}^2 \\\\ I \u0026amp;= \\int r^2 dm \\end{align*} $$\n慣性モーメントmoment of inertiaは（粒子の質量）$\\times$（回転軸から粒子までの距離）で定義され、物体が回転運動を続けようとする性質を表す物理量である。記号は$I$で、英語のInertiaの最初の文字から取られたものと思われる。単位は$[kg \\cdot m^2]$である。並進運動における質量と同じ役割をすると見なすことができる。つまり、角運動量$L=I \\omega $が一定のとき、慣性モーメントが大きいほど角速度は小さくなる。複数の粒子がある場合、粒子系の慣性モーメントは、各粒子の慣性モーメントをすべて足し合わせて計算する。質点が連続的に分布する物体の場合は、積分により計算する。\n回転半径 慣性モーメントを全質量で割ると、回転軸からの距離の二乗の平均値を得る。これを回転半径radius of gyrationと言い、$k$で表記される。\n$$ k=\\sqrt{\\frac{I}{m}} $$\n","id":234,"permalink":"https://freshrimpsushi.github.io/jp/posts/234/","tags":null,"title":"慣性モーメントと旋回半径"},{"categories":"함수","contents":"定義 シーケンス $\\left\\{ a_{n} \\right\\}$ に対して、 $$ g(x) =\\sum \\limits _{n=0}^{\\infty}a_{n}x^{n}= a_{0} + a_{1} x + a_{2} x^2 + \\cdots $$ の形で表される関数 $g$ を数列 $\\left\\{ a_{n}\\right\\}$ の生成関数または単に生成関数という。数列が $a_{n}=a_{n}(x)$ の場合、下記のように表記されることもある。 $$ G(x,t)=\\sum \\limits _{n=0}^{\\infty}a_{n}(x)t^{n} $$\n説明 鋭い読者なら気づいただろうが、テイラー級数も同様の形をしている。鋭くない読者でも、高校時代に「冪級数」という名前でこの似た形に接していただろう。等差数列と等比数列の合計を求める過程を数学的に理解しているかを確認するのに適しているからこそ、試験問題にはとてもふさわしいのだ。\n歴史的に「生成関数」という名前はラプラスが名付けたものと知られている。両辺を$n$回微分した後$x=0$を代入すると$g^{(n)} (x) = a_{n} n!$となり、したがって$\\displaystyle a_{n} = {{g^{(n)} (x) } \\over {n!}}$を求めることができる。その意味で、生成関数は元々与えられた数列を復元あるいは「生成できる関数」と考えることができるものだ。\n特別なケースとしては前述したようにマクローリン級数 $\\displaystyle f(x) = \\sum_{n=0}^{\\infty} {{f^{(n)} (0)}\\over{n!}} {x}^n$がある。マクローリン級数は、与えられた関数の$n$階微分係数として数列 $a_{n}$を取っている場合だ。\n$1 = a_{0} = a_{1} = a_{2} = \\cdots$すなわち、全ての項の係数が1の場合、喜んでも無限等比級数 $$ g(x) = 1 + x + x^2 + \\cdots = {{1} \\over {1-x}} $$ になる。一方$a_{n} = n+1$、すなわち自然数の数列の場合は$\\displaystyle g(x) = {{1} \\over {1-x}}$の両面を$x$で微分したものと同じだ。その結果はもちろん以下の通り。 $$ g ' (x) = 1 + 2 x + 3 x^2 + \\cdots = {{1} \\over {(1-x)^2 }} $$ 生成関数に対する研究は一見全く無駄に思えるかもしれないが、統計学をはじめとする様々な応用数学に冪関数として一般化されて幅広く使用されている。(驚くことではないが、このような名前が付く前からすでに、オイラーは解析学や数論の問題を解くために使用していたと言われている。)\n","id":232,"permalink":"https://freshrimpsushi.github.io/jp/posts/232/","tags":null,"title":"生成関数とは何か？"},{"categories":"복소해석","contents":"要約 1 $n$次の多項式 $P(x) = a_{0} + a_{1} x + a_{2} x^2 + \\cdots + a_{n} x^{n}$は、重根を含む$n$個の根を正確に持つ。\n説明 実際、多項式を解くとき、解が存在すると当然のように思うけど、それが必ずしもそうとは限らない。例えば、2次の多項式$x^2+1 = 0$には実根が存在しない。しかし、ここで複素数を許容すると、$\\pm i$という2つの解が存在することがわかる。\n事実として、多項式を解く際に虚根を許容すると、解は必ず存在し、その数も正確にその次数と同じだ。すべての基本定理の重要性は、言うまでもない。核心的なアイデアはリウビルの定理であり、自然数$n$に対して一般化するために数学的帰納法が使用される。\n証明 まず、$P(z) = 0$を満たす解が存在しないと仮定すると、$\\displaystyle {{1} \\over {P(z)}}$は全解析関数で、$\\displaystyle \\lim_{|z| \\to \\infty} \\left| {{1} \\over {P(z)}} \\right| = 0$なので、有界だ。\nリウビルの定理: $f$が全解析関数で、有界ならば、$f$は定数関数だ。\nリウビルの定理により、$P$は定数関数でなければならないが、これは仮定に矛盾するので、$P(z) = 0$は少なくとも一つの解を持つ。\n今、自然数に対して一般化しよう。$P(z) = 0$が少なくとも一つの解を持つとしたら、その解を$z = \\alpha$とすると、 $$ P(z) = (z-\\alpha) Q(z) $$ ここで$Q(z) = b_{0} + b_{1} x + b_{2} x^2 + \\cdots + b_{n-1} x^{n-1} = 0$も少なくとも一つの解を持つ。このプロセスを繰り返すことにより、数学的帰納法により、$n$次の多項式$P(z) = 0$は正確に$n$個の解を持つ。\n■\n関連項目 抽象代数の用語で表された代数学の基本定理 Osborne (1999). 複素変数及びその応用: p94.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":223,"permalink":"https://freshrimpsushi.github.io/jp/posts/223/","tags":null,"title":"代数学の基本定理の証明"},{"categories":"보조정리","contents":"要約 ガウス関数 $f(x) := e^{-x^2}$ の全範囲に及ぶ積分は、以下の通りである。\n$$ \\int_{-\\infty}^{\\infty} e^{-x^2} dx= \\sqrt{\\pi} $$\n説明 物理学者ケルビンは、「この積分を当たり前だと思う人は数学者だ」という言葉を残したそうだ。他にも ガウス積分、または オイラー・ポアソン積分 などとも呼ばれる。\n高校生にとっては衝撃的な積分で、特に統計学では非常に重要な積分でもある。それもそのはず、高校の範囲では原始関数を求めることができず、計算が不可能なのに、統計の部分では正規分布の確率を暗黙に使っているからだ。\n証明 戦略: $x$ と $y$ を互いに独立させ、極座標系に変換して閉区間の積分に変える。高校生レベルで理解できると言われるパップス・ギュルディンの定理を通じて、回転体の体積を求める方法で証明する方法があるが、本質的にこの証明と同じであり、この方法も異常積分を含んでいるため、高校生レベルとは言いがたい。\n$\\displaystyle I = \\int_{-\\infty}^{\\infty} e^{-x^2} dx$ とすると、$\\displaystyle I = \\int_{-\\infty}^{\\infty} e^{-y^2} dy$ としても表せる。$x$ と $y$ は独立しているので、\n$$ \\begin{align*} I^2 =\u0026amp; \\int_{-\\infty}^{\\infty} e^{-x^2} dx \\int_{-\\infty}^{\\infty} e^{-y^2} dy \\\\ =\u0026amp; \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} e^{-( x^2 + y^2 ) } dxdy \\end{align*} $$\n極座標系に変換すると、\n$$ \\begin{align*} I^2 =\u0026amp; \\int_{0}^{2 \\pi} \\int_{0}^{\\infty} e^{-r^2 } r dr d\\theta \\\\ =\u0026amp; \\int_{0}^{2 \\pi} \\left[ {{-e^{-r^2}} \\over {2}}\\right]_{0}^{\\infty} d\\theta \\\\ =\u0026amp; \\int_{0}^{2 \\pi} {{1} \\over {2}} d\\theta \\\\ =\u0026amp; \\pi \\end{align*} $$\n従って、\n$$ I =\\sqrt{\\pi} $$\n■\n帰結 半直線上の異常積分 $$ \\int_{0}^{\\infty} e^{-x^2} dx= {{\\sqrt{\\pi}} \\over {2}} $$\n積分範囲が $0$ から $\\infty$ までの場合、極座標は使えない。しかし、ガウス関数の形を見れば、$x=0$ に対して偶関数なので、計算せずとも半分になると予想できるが、範囲が無限に長い異常積分だからこそ、正確に確認しよう。\n証明 $$ \\int_{0}^{\\infty} e^{-x^2} dx $$\n$x :=-y$ のように置換すると、\n$$ x \\rightarrow 0,\\ y \\rightarrow 0 \\\\ x \\rightarrow \\infty,\\ y \\rightarrow -\\infty \\\\ x^2=y^2 $$\n$dx=-dy$ なので、\n$$ \\int_{0}^{\\infty} e^{-x^2} dx = -\\int_{0}^{-\\infty} e^{-y^2} dy=\\int_{-\\infty}^{0} e^{-y^2} dy $$\n積分変数は定積分に影響を与えないので、$\\displaystyle \\int_{-\\infty}^{0} e^{-y^2} dy=\\int_{-\\infty}^{0} e^{-x^2} dx$ と書けて、従って、\n$$ \\int_{0}^{\\infty} e^{-x^2} dx + \\int_{-\\infty}^{0} e^{-x^2} dx= 2\\int_{0}^{\\infty} e^{-x^2} dx=\\int_{-\\infty}^{\\infty} e^{-x^2} dx $$\n上記の結果を使用すると、\n$$ \\int_{0}^{\\infty} e^{-x^2} dx=\\frac{1}{2}\\int_{-\\infty}^{\\infty} e^{-x^2} dx=\\frac{1}{2}\\sqrt{\\pi} $$\n一般化 以下のように一般化された公式が広く用いられている。\n$$ \\int_{-\\infty}^{\\infty} e^{-\\alpha x^2} dx= \\sqrt{\\dfrac{\\pi}{\\alpha}} $$\n参照 ガウス積分の一般化 ■\n","id":219,"permalink":"https://freshrimpsushi.github.io/jp/posts/219/","tags":null,"title":"e^-x^2型の定積分、ガウス積分、オイラー-ポアソン積分"},{"categories":"교과과정","contents":"概要 $$ (x+y)^{n} = \\sum_{r=0}^{n} {_n C _r} x^{r} y^{n-r} $$ ここで、${_n C _r}$ を 二項係数Binomial Coefficientと定義する。 $$ {_n C _r} = \\binom{n}{r} = {{ n! } \\over { r ! (n-r)! }} $$\n説明 高校で学ぶにはとても役に立つもので、学んだ直後から多くの場面で使用される定理だ。その柔軟性から、多くの公式を一気に導出でき、分野を問わず広く使用される。\n証明 $(x+y)^{n}$ を展開するとき、$x^{r} y^{n-r}$ の係数は $$ (x+y)^{n} = (x+y)(x+y)(x+y) \\cdots (x+y) $$ $(x+y)$ の各 $x$ を $n$ 回、$y$ を $n-r$ 回選択することと同じである。したがって、組合せ $_n C _r$ が $x^{r} y^{n-r}$ の係数になるので、 $$ (x+y)^{n} = \\sum_{r=0}^{n} {_n C _r} x^{r} y^{n-r} $$\n■\n","id":218,"permalink":"https://freshrimpsushi.github.io/jp/posts/218/","tags":null,"title":"二項定理の証明"},{"categories":"복소해석","contents":"要約 1 複素関数 $f: A \\subseteq \\mathbb{C} \\to \\mathbb{C}$ が 単連結領域 $\\mathscr{R}$ で 解析的 だとしよう。\n$\\mathscr{R}$ に含まれる 単純閉曲線 $\\mathscr{C} \\subset \\mathscr{R}$ がある点 $\\alpha$ を囲んでいる場合、以下が成り立つ。 $$ f(\\alpha) = {{1} \\over {2 \\pi i }} \\int_{\\mathscr{C}} {{f(z)} \\over { z - \\alpha }} dz $$\n導出 まず、$\\displaystyle 2 \\pi i = \\int_{\\mathscr{C} '} {{1} \\over { z - \\alpha }} dz$ であることを示そう。\n複素経路積分の収縮補助定理: $\\mathscr{C}$内部で $\\alpha$ を中心とする 円 $\\mathscr{C} '$ について $$\\int_{\\mathscr{C}} f(z) dz = \\int_{\\mathscr{C} '} f(z) dz$$\n$\\displaystyle \\int_{\\mathscr{C}} {{1} \\over { z - \\alpha }} dz$ の積分区間を 円 $\\mathscr{C} ': | z - \\alpha | = \\rho$ に収縮すると、$z(\\theta) = \\rho e^{i \\theta} + \\alpha, -\\pi \\le \\theta \\le \\pi$ なので $$ \\int_{\\mathscr{C} '} {{1} \\over { z - \\alpha }} dz = \\int_{-\\pi}^{\\pi} {{ i \\rho e^{i \\theta}} \\over { \\rho e^{i \\theta} }} d\\theta = 2 \\pi i $$ 次に、$\\displaystyle I = \\int_{\\mathscr{C}} {{f(z)} \\over { z - \\alpha }} dz$ と置いて、$I$ を求めると $$ \\begin{align*} \\int_{\\mathscr{C}} {{f(z)} \\over { z - \\alpha }} dz =\u0026amp; \\int_{\\mathscr{C} '} {{f(\\alpha)} \\over { z - \\alpha }} dz + \\int_{\\mathscr{C} '} {{f(z) - f(\\alpha)} \\over { z - \\alpha }} dz \\\\ =\u0026amp; f(\\alpha) \\int_{\\mathscr{C} '} {{1} \\over { z - \\alpha }} dz + \\int_{\\mathscr{C} '} {{f(z) - f(\\alpha)} \\over { z - \\alpha }} dz \\\\ =\u0026amp; f(\\alpha) 2 \\pi i + \\int_{\\mathscr{C} '} {{f(z) - f(\\alpha)} \\over { z - \\alpha }} dz \\end{align*} $$ $\\displaystyle \\int_{\\mathscr{C} '} {{f(z) - f(\\alpha)} \\over { z - \\alpha }} dz = 0$ であることを示せば、証明は完了する。\n$f(z)$ は $z = \\alpha$ で微分可能であるため、ある $M\u0026gt;0$ に対して $$ \\left| {{f(z) - f(\\alpha)} \\over { z - \\alpha }} \\right| \\le M $$ $\\mathscr{C} ' : | z - \\alpha | = \\rho$ なので、$\\mathscr{C} '$ の長さは $2 \\pi \\rho$ である。\nML補助定理: $|f(z)| \\le M$ を満たす正の数 $M$ と $\\mathscr{C}$ の長さ $L$ について $$ \\left| \\int_{\\mathscr{C}} f(z) dz \\right| \\le ML $$\nML補助定理によって、 $$ \\left| \\int_{\\mathscr{C} '} {{f(z) - f(\\alpha)} \\over { z - \\alpha }} dz \\right| \\le 2 \\pi \\rho M $$ ここで、$z = \\alpha$ を中心に複素経路積分の収縮補助定理を繰り返し使ってみると、 $$\\mathscr{C}_n : | z - \\alpha | = \\rho_n \\\\ \\mathscr{C}_{n+1} : | z - \\alpha | = \\rho_{n+1} \\\\ \\rho_{n} \u0026gt; \\rho_{n+1} $$ その場合、$n \\to \\infty$ の時、$\\rho_{n} \\to 0$ である。全ての $\\rho_{n} \u0026gt;0$ について $$ \\left| \\int_{\\mathscr{C}_{n}} {{f(z) - f(\\alpha)} \\over { z - \\alpha }} dz \\right| \\le 2 \\pi \\rho_{n} M $$ このように、 $$ \\left| \\int_{\\mathscr{C} '} {{f(z) - f(\\alpha)} \\over { z - \\alpha }} dz \\right| = 0 $$ 最終的に以下を得る。 $$ \\int_{\\mathscr{C}} {{f(z)} \\over { z - \\alpha }} dz = f(\\alpha) 2 \\pi i $$\n■\n説明 盲目が目を開き、不具足が起き上がる公式だ。数学的な美しさは言うまでもなく、その有用性は非常に深いため、その影響を計り知ることが難しい。特に積分に関しては、絶え間なく豊かな数学的成果が提供されるため、複素解析の花とも言われる。\n系 一方で、コーシー積分公式は、$n$次微分係数に対しても一般化することが可能である。一般化のために数学的帰納法を利用する点を除いて、コーシー積分公式の証明と本質的に異なるわけではない。この公式は自体として非常に有用だが、それ以上に重要な意味を持っている。\n微分に対するコーシー積分公式の一般化 関数 $f: A \\subseteq \\mathbb{C} \\to \\mathbb{C}$ が 単連結領域 $\\mathscr{R}$ で 解析的 だとしよう。\n$\\mathscr{R}$ に含まれる 単純閉曲線 $\\mathscr{C} \\subset \\mathscr{R}$ がある点 $\\alpha$ を囲んでいる場合、自然数 $n$ に対して、以下が成り立つ。 $$ f^{(n)} (\\alpha) = {{n!} \\over {2 \\pi i }} \\int_{\\mathscr{C}} {{f(z)} \\over { (z - \\alpha)^{n+1} }} dz $$\nしかし、条件を読むと、$f$ が何回も微分可能であるという話はないが、$n$次微分係数を使用している。つまり複素解析では 一度微分可能な関数は無限に微分可能 であるという意味になる。これは証明過程で保証され、実関数では簡単に言えない非常に強力な利点である。このように、複素解析は、微分であれ積分であれ、あらゆる制限を取り除いてくれるため、驚くべき数学的結果が容易に導かれるのである。\n無限の微分可能性 2 解析的な複素関数の導関数は解析的である。つまり、$f$ が $z \\in \\mathbb{C}$ で 解析的 であれば、全ての $n \\in \\mathbb{N}$ に対して、$n$次導関数 $f^{(n)}$ もまた $z$ で解析的である。\nOsborne (1999). 複素変数とその応用: p87~89.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOsborne (1999). 複素変数とその応用: p91.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":215,"permalink":"https://freshrimpsushi.github.io/jp/posts/215/","tags":null,"title":"コーシーの積分公式の導出"},{"categories":"해석개론","contents":"まとめ 閉区間$[a,b]$で関数$f$が連続であるとすると、$\\displaystyle f(c) = {{1}\\over {b-a} } \\int_{a}^{b} f(x) dx$を満たす$c$が$(a,b)$に少なくとも一つ存在する。\n説明 平均値の定理に似ているが、積分に使用されるので、このような名前がついている。使用法も非常に似ており、その有用性は平均値の定理に全く劣らない。一方で、関数の平均値を右側のように定義することを考えると、むしろこれが平均値の定理であり、広く知られている平均値の定理が「微分の平均値の定理」と呼ばれるのが妥当かもしれない。\n証明 戦略: $f$の連続性が仮定されているので、最大最小値の定理と中間値の定理を使用する。\n$f$が$[a,b]$で連続であり、最大最小値の定理によって最小値$m$と最大値$M$が存在するので、\n$$ \\int_{a}^{b} m dx \\le \\int_{a}^{b} f(x) dx \\le \\int_{a}^{b} M dx $$\n$$ \\implies m \\le {{1}\\over {b-a} } \\int_{a}^{b} f(x) dx \\le M $$\nもう一度、$f$が$[a,b]$で連続であるため、中間値の定理によれば、$m$と$M$の間に、$\\displaystyle {{1}\\over {b-a} } \\int_{a}^{b} f(x) dx$に対して$f(c) = \\displaystyle {{1}\\over {b-a} } \\int_{a}^{b} f(x) dx$を満たす$c$が$a$と$b$の間に少なくとも一つ存在する。\n■\n同様に、重み$w$に対して一般化することができる。上で紹介された形式は$w(x) = 1$の場合であり、$\\displaystyle \\int_{a}^{b} dx = b - a$となり、以下の定理によくカバーされている。\n従論 閉区間$[a,b]$で関数$f$が連続であり、$w(x) \\ge 0$が積分可能である場合、$\\displaystyle \\int_{a}^{b} f(x) w(x) dx = f( \\xi ) \\int_{a}^{b} w(x) dx$を満たす$\\xi$が$(a,b)$に少なくとも一つ存在する。\n参考 平均値の定理 コーシーの平均値の定理 ガウスの平均値の定理 ","id":212,"permalink":"https://freshrimpsushi.github.io/jp/posts/212/","tags":null,"title":"積分の平均値定理"},{"categories":"해석개론","contents":"まとめ1 関数 $f$ が閉区間 $[a,b]$ で連続だとしよう。\n(1) 関数 $\\displaystyle F(x) = \\int_{a}^{x} f(t) dt$ は $[a,b]$ で連続で、$(a,b)$ で微分可能で、$\\displaystyle {{dF(x)} \\over {dx}} = f(x)$ を満たす。\n(2) $f$ の任意の不定積分 $F$ について、$\\displaystyle \\int_{a}^{b} f(x) dx = F(b) - F(a)$\n説明 もちろん、微分、積分という言葉を使うからには、これらの関係を簡単に推測できる。しかし、英語では differential と integral で全く関係ない上に、概念も特に似ていない。\n微積分学の基本定理は、この微分と積分が、実際には互いに逆の演算であることを示している。\n証明 (1) 積分の平均値定理により、$\\displaystyle f(c) = {{1} \\over {h}} \\int_{x}^{x+h} f(t) dt$ を満たす $c$ が $x, x+h$ の間に存在する。\n$h \\to 0$ のとき、$c \\to x$ になるので、\n$$ \\lim_{h \\to 0} {{1} \\over {h}} \\int_{x}^{x+h} f(t) dt = \\lim_{h \\to 0} f(c) = f(x) $$\n一方、$\\displaystyle F(x+h) - F(x) = \\int_{a}^{x+h} f(t) dt - \\int_{a}^{x} f(t) dt = \\int_{x}^{x+h} f(t) dt$ ので、\n$$ {{1} \\over {h}} \\int_{x}^{x+h} f(t) dt = { {F(x+h) - F(x)} \\over {h} } $$\n従って、\n$$ \\lim_{h \\to 0} { {F(x+h) - F(x)} \\over {h} } = F ' (x) = f(x) $$\n■\n(2) $F$ は $f$ の不定積分なので $\\displaystyle \\int_{a}^{b} f(t) dt = F(b) + C$ となり、\n$$ \\int_{a}^{a} f(t) dt = F(a) + C $$\n両辺を引くと、\n$$ \\int_{a}^{b} f(x) dx = F(b) - F(a) $$\n■\n参照 解析学における微積分学の基本定理 (1) 解析学における微積分学の基本定理 (2) 慶北国立大学基礎教育院, 理工学生のための大学数学 (2012), p108-109\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":213,"permalink":"https://freshrimpsushi.github.io/jp/posts/213/","tags":null,"title":"積分学の基本定理の証明"},{"categories":"복소해석","contents":"総括 1 $\\mathscr{C}$が単純閉路であり、その内側で$f: A \\subseteq \\mathbb{C} \\to \\mathbb{C}$が解析的で、$f '$が連続だとする。そうすると、 $$ \\int_{\\mathscr{C}} f(z) dz = 0 $$\n証明 $a \\le t \\le b$について、 $$ z(t) = x(t) + i y(t) \\\\ f(z) = u(x,y) + i v(x,y) $$ とすると、$\\displaystyle {{dz} \\over {dt}} = x ' + i y '$だから、 $$ \\begin{align*} f(z)dz =\u0026amp; f(z) ( x ' + i y ' ) dt \\\\ =\u0026amp; (u + i v ) ( x ' + i y ' ) dt \\\\ =\u0026amp; (u x ' - v y ' ) + i (v x ' + u y ' ) dt \\end{align*} $$ $\\displaystyle x ' = {{dx} \\over {dt}}$であり、$\\displaystyle y ' = {{dy} \\over {dt}}$なので、 $$ \\begin{align*} \\int_{\\mathscr{C}} f(z) dz =\u0026amp; \\int_{a}^{b} (u x ' - v y ' ) dt + i \\int_{a}^{b} (v x ' + u y ' ) dt \\\\ =\u0026amp; \\int_{\\mathscr{C}} (u dx - v dy ) + i \\int_{\\mathscr{C}} (v dx + u dy) \\end{align*} $$ ここで、導関数が連続であるという条件が使われる。\nグリーンの定理：$P,Q$が連続で、その導関数も連続ならば、 $$\\int_{\\mathscr{C}} (Pdx + Qdy) = \\iint_{S} (Q_{x} - P_{y}) dx dy$$\nグリーンの定理により、 $$ \\int_{\\mathscr{C}} f(z) dz = - \\iint_{S} (v_x + u_y) dxdy + i \\iint_{S} (u_x - v_y) dxdy $$ 一方で、$u,v$はコーシー・リーマンの方程式を満たす解なので、$u_y = -v_x$であり、$u_x = v_y$だ。だから、 $$ \\int_{\\mathscr{C}} f(z) dz = 0 $$\n■\n説明 言ってしまえば、特定の条件を満たした場合、定積分を計算する必要が全く無いということだ。「解析学の父」として知られるコーシーだが、その名前が示すように非常に、とても重要な定理だ。見ての通り、関数$f$の条件を満たすのはそれほど難しくないので、多くの場で使うことができる。\n実用的であるだけでなく、とてもシンプルなので、数学的な美しさも感じることができる。\n微分係数と積分係数を扱う時、厳密ではないが直感的に理解できるように野良解析学を使った。結果的には同じだが、厳密には間違った過程なので注意が必要だ。\nもう一つ有用な定理を証明無しで紹介する。\n一般化 コーシー・グルサの定理The Cauchy-Goursat Theorem 単連結領域 $\\mathscr{R}$で、$f$が解析的ならば、$\\mathscr{R}$内側の単純閉路 $\\mathscr{C}$に対して、 $$ \\int_{\\mathscr{C}} f(z) dz = 0 $$\nフランスの数学者グルサは、$f$の導関数に関する条件をなくすというセンスで一般化を達成した。ファクトとしては、コーシーの定理よりも明らかに有用なので、覚えておくこと。\n関連項目 群論でのコーシーの定理 Osborne (1999). Complex variables and their applications: p82.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":210,"permalink":"https://freshrimpsushi.github.io/jp/posts/210/","tags":null,"title":"複素解析におけるコーシーの定理の証明"},{"categories":"수리통계학","contents":"まとめ 確率変数$X_{1} , \\cdots , X_{n}$が相互に独立してるとしよう。\n[1] 二項分布: $X_i \\sim \\text{Bin} ( n_{i}, p)$ならば $$ \\sum_{i=1}^{m} X_{i} \\sim \\text{Bin} \\left( \\sum_{i=1}^{m} n_{i} , p \\right) $$ [2] ポアソン分布: $X_i \\sim \\text{Poi}( m_{i} )$ならば $$ \\sum_{i=1}^{n} X_{i} \\sim \\text{Poi} \\left( \\sum_{i=1}^{n} m_{i} \\right) $$ [3] ガンマ分布: $X_i \\sim \\Gamma ( k_{i}, \\theta)$ならば $$ \\sum_{i=1}^{n} X_{i} \\sim \\Gamma \\left( \\sum_{i=1}^{n} k_{i} , \\theta \\right) $$ [4] カイ二乗分布: $X_i \\sim \\chi^2 ( r_{i} )$ならば $$ \\sum_{i=1}^{n} X_{i} \\sim \\chi ^2 \\left( \\sum_{i=1}^{n} r_{i} \\right) $$ [5] 正規分布: $X_i \\sim N( \\mu_{i}, \\sigma_{i}^{2} )$ならば、与えられたベクトル$(a_{1} , \\cdots , a_{n}) \\in \\mathbb{R}^{n}$に対して $$ \\sum_{i=1}^{n} a_{i} X_{i} \\sim N \\left( \\sum_{i=1}^{n} a_{i } \\mu_{i} , \\sum_{i=1}^{n} a_{i }^2 \\sigma_{i}^2 \\right) $$ 証明 戦略：積率生成関数を使用して導く。変数が相互に独立している条件は、以下の定理を適用するために必須である。\n$X_{1} , \\cdots , X_{n}$が相互に独立しており、それぞれの積率生成関数が$M_{i}(t) \\qquad , -h_{i} \u0026lt; t \u0026lt; h_{i}$である場合、その線形組み合わせ$\\displaystyle T := \\sum_{i=1}^{n} a_{i} X_{i}$の積率生成関数は $$ M_{T} (t) = \\prod_{i=1}^{n} M_{i} \\left( a_{i} t \\right) \\qquad , -\\text{min}_{i=1, \\cdots, n}^{n} h_{i} \u0026lt; t \u0026lt; \\text{min}_{i=1, \\cdots, n} h_{i} $$\n[1]1 二項分布の積率生成関数: $$ m(t) = \\left[ (1-p) + pe^{t} \\right]^{n} \\qquad , t \\in \\mathbb{R} $$\n$\\displaystyle Y := \\sum_{i=1}^{m} X_{i}$と置くと、$X_{1} , \\cdots , X_{m}$が相互に独立なので $$ \\begin{align*} M_{Y} (t) =\u0026amp; M_{1} (t) \\cdots M_{m} (t) \\\\ =\u0026amp; \\left[ (1-p) + pe^{t} \\right]^{n_{1}} \\cdots \\left[ (1-p) + pe^{t} \\right]^{n_{m}} \\\\ =\u0026amp; \\left[ (1-p) + pe^{t} \\right]^{\\sum_{i=1}^{m} n_{i}} \\end{align*} $$ したがって $$ Y \\sim \\text{Bin} \\left( \\sum_{i=1}^{m} n_{i} , p \\right) $$\n■\n[2]2 ポアソン分布の積率生成関数: $$ m(t) = \\exp \\left[ \\lambda \\left( e^{t} - 1 \\right) \\right] \\qquad , t \\in \\mathbb{R} $$\n$\\displaystyle Y := \\sum_{i=1}^{n} X_{i}$と置くと、$X_{1} , \\cdots , X_{n}$が相互に独立なので $$ \\begin{align*} M_{Y} (t) =\u0026amp; M_{1} (t) \\cdots M_{n} (t) \\\\ =\u0026amp; \\exp \\left[ m_{1} \\left( e^{t} - 1 \\right) \\right] \\cdots \\exp \\left[ m_{n} \\left( e^{t} - 1 \\right) \\right] \\\\ =\u0026amp; \\exp \\left[ \\sum_{i=1}^{n} m_{i} \\left( e^{t} - 1 \\right) \\right] \\end{align*} $$ したがって $$ Y \\sim \\text{Poi} \\left( \\sum_{i=1}^{m} m_{i} \\right) $$\n■\n[3]3 ガンマ分布の積率生成関数: $$ m(t) = \\left( 1 - \\theta t\\right)^{-k} \\qquad , t \u0026lt; {{ 1 } \\over { \\theta }} $$\n$\\displaystyle Y := \\sum_{i=1}^{n} X_{i}$と置くと、$X_{1} , \\cdots , X_{n}$が相互に独立なので $$ \\begin{align*} M_{Y} (t) =\u0026amp; M_{1} (t) \\cdots M_{n} (t) \\\\ =\u0026amp; \\left( 1 - \\theta t\\right)^{-k_{1}} \\cdots \\left( 1 - \\theta t\\right)^{-k_{n}} \\\\ =\u0026amp; \\left( 1 - \\theta t\\right)^{-\\sum_{i=1}^{n} k_{i}} \\end{align*} $$ したがって $$ Y \\sim \\Gamma \\left( \\sum_{i=1}^{n} k_{i} , \\theta \\right) $$\n■\n[4]4 ガンマ分布とカイ二乗分布の関係: $$ \\Gamma \\left( { r \\over 2 } , 2 \\right) \\iff \\chi ^2 (r) $$\n$\\displaystyle Y := \\sum_{i=1}^{n} X_{i}$で、$\\displaystyle \\sum_{i=1}^{n} k_{i} := {{ r_{i} } \\over { 2 }}$と$\\theta := 2$と置くと、定理[3]に従って $$ Y \\sim \\Gamma \\left( \\sum_{i=1}^{n} {{ r_{i} } \\over { 2 }} , 2 \\right) $$\n■\n[5]5 正規分布の積率生成関数: $$ m(t) = \\exp \\left( \\mu t + {{ \\sigma^{2} t^{2} } \\over { 2 }} \\right) \\qquad , t \\in \\mathbb{R} $$\n$\\displaystyle Y := \\sum_{i=1}^{n} a_{i} X_{i}$と置くと、$X_{1} , \\cdots , X_{n}$が相互に独立なので $$ \\begin{align*} M_{Y} =\u0026amp; M_{1} (t) \\cdots M_{n} (t) \\\\ =\u0026amp; \\prod_{i=1}^{n} \\exp \\left[ t a_{i} \\mu_{i} + {{ t^{2} a_{i}^{2} \\sigma_{i}^{2} } \\over { 2 }} \\right] \\\\ =\u0026amp; \\exp \\left[ t \\sum_{i=1}^{n} a_{i} \\mu_{i} + {{ t^{2} \\sum_{i=1}^{n} a_{i}^{2} \\sigma_{i}^{2} } \\over { 2 }} \\right] \\end{align*} $$ したがって $$ Y \\sim N \\left( \\sum_{i=1}^{n} a_{i } \\mu_{i} , \\sum_{i=1}^{n} a_{i }^2 \\sigma_{i}^2 \\right) $$\n■\n注意点 確率変数の加算という言葉が実際には存在しないことに注意が必要だ。正確には、確率変数の線形結合の中でも特殊なケースを指す。当然ながら、iidのようなより強い条件が与えられた場合、その分布をより簡単に見つけることができる。\nHogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p145.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p155.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p163.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p163.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p176.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":202,"permalink":"https://freshrimpsushi.github.io/jp/posts/202/","tags":null,"title":"特定の分布に従う確率変数の加算の総括"},{"categories":"수리물리","contents":"定義 自然数$n \\in \\mathbb{N}$に対し、実数の集合$\\mathbb{R}$のデカルト積$\\mathbb{R}^{n}$をユークリッド空間と呼ぶ。\n$$ \\mathbb{R}^{n} = \\mathbb{R} \\times \\cdots \\times \\mathbb{R} $$\n$\\mathbb{R}^{1}$は実数空間または数直線と呼ばれる。 $\\mathbb{R}^{2}$は平面と呼ばれる。 $\\mathbb{R}^{3}$は**$3$次元空間**と呼ばれる。 ここで、$\\mathbb{N} := \\left\\{ 1, 2, 3, \\cdots \\right\\}$は自然数を全部集めた集合を意味する。$\\mathbb{R}$は実数を全部集めた集合を意味する。\n説明 ユークリッド空間は、幾何学原論の著者であるユークリッドの名を冠した空間で、私たちが生きている$3$次元空間を含め、平面、数直線はもちろん、それ以上の多次元空間まで表現する空間だ。\n私たちの生活と密接に関連しているため、多くの理論で全体の空間としてユークリッド空間が仮定されることが多い。もちろん、ユークリッド空間だけでは、深く複雑な科学技術の理論を全て説明することはできず、YouTubeに生息する疑似数学者、疑似科学者たちのおもちゃになることもある。\n多次元への拡張は、時空間が$\\mathbb{R}^{1+3}$であるからとか、弦理論で$\\mathbb{R}^{11}$となるといった派手な目的がなくとも必ず必要なものだ。代表的には統計学での応用もあり、空間自体が$3$次元であっても、速度や加速度を導入するために$9$次元が必要になることもある。 もしこの文章を読んでいる読者が主に工学、または実用的な物理学までの学びを希望しているなら、実際には通常のユークリッド空間を抜け出すことはないかもしれない。式や証明なしにロマンを歌う教養書を見て夢を育てても仕方がない。しかし、それを越えた数学や物理の世界に興呀があれば、ユークリッド空間を足場と見なして早く慣れるべきだ。\n","id":205,"permalink":"https://freshrimpsushi.github.io/jp/posts/205/","tags":null,"title":"ユークリッド空間"},{"categories":"함수","contents":"式 非整数の $p$ に対して $$ {\\Gamma (1-p) \\Gamma ( p )} = { {\\pi} \\over {\\sin \\pi p } } $$\n説明 ガンマ関数を使った式の中で最も有名な式だ。\n反射公式から得られる役立つ結果には $ \\Gamma ( { 1 \\over 2} ) = \\sqrt{\\pi}$ がある。そのためか？反射公式という名前は $\\frac{1}{2}$ を反射させるという意味からつけられたと言われている。\n導出 ワイエルシュトラスの無限積: $$ {1 \\over \\Gamma (p)} = p e^{\\gamma p } \\prod_{n=1}^{\\infty} \\left( 1 + {p \\over n} \\right) e^{- {p \\over n} } $$\n$$ \\begin{align*} {{1} \\over {\\Gamma (p)}} \\cdot { 1 \\over { \\Gamma ( -p )}} =\u0026amp; p e^{\\gamma p } \\prod_{n=1}^{\\infty} \\left( 1 + {p \\over n} \\right) e^{- {p \\over n} } \\cdot (-p) e^{- \\gamma p } \\prod_{n=1}^{\\infty} \\left( 1 - {p \\over n} \\right) e^{ {p \\over n} } \\\\ =\u0026amp; -p^2 \\prod_{n=1}^{\\infty} \\left( 1 - {p^2 \\over n^2} \\right) \\end{align*} $$ 一方で ${ \\Gamma ( 1-p )} = -p \\Gamma (-p)$ だから $$ { 1 \\over {\\Gamma (1-p) \\Gamma ( p )} } = p \\prod_{n=1}^{\\infty} \\left( 1 - {p^2 \\over n^2} \\right) $$\nシンク関数のオイラー表示: $$ {{\\sin \\pi x} \\over {\\pi x}} = \\prod_{n=1}^{\\infty} \\left( 1 - {{x^2} \\over { n^2}} \\right) $$\nシンク関数のオイラー表示をうまく調整すれば、求めていた式を得ることができる。\n■\n","id":192,"permalink":"https://freshrimpsushi.github.io/jp/posts/192/","tags":null,"title":"オイラーの反射公式の導出"},{"categories":"함수","contents":"まとめ シンク関数の定義 次の関数 $\\text{sinc} : \\mathbb{R} \\to \\mathbb{R}$ をシンク関数と呼ぶ。\n$$ \\text{sinc} x := \\begin{cases} \\displaystyle {{\\sin x} \\over {x}} \u0026amp; , \\text{if } x \\ne 0 \\\\ 1 \u0026amp; , \\text{if } x = 0 \\end{cases} $$\nオイラー表現 $$ \\text{sinc} x = \\prod_{n=1}^{\\infty} \\left( 1 - {{x^2} \\over { \\pi^2 n^2}} \\right) $$\n説明 シンク関数とは、$\\sin x$ を $x$ で割った関数であり、別名がついているだけあって、非常に役に立つ関数だ。名前は知らなくても、限界や連続の部分でよく登場する。\n一方、正規化されたシンク関数は以下のように定義される。 $$ \\text{sinc} x = {{\\sin \\pi x} \\over {\\pi x}} = \\prod_{n=1}^{\\infty} \\left( 1 - {{x^2} \\over { n^2}} \\right) $$ この場合、$\\displaystyle \\text{Sa}(x) := {{\\sin x} \\over {x}}$ を非正規化シンク関数と呼ぶこともあるが、本質的には二つは同じ関数であり、厳密に区別せず、通常はその都度、目的に合わせて再定義されることが多い。\nちなみに、シンク関数の異常積分は $\\displaystyle \\int_{- \\infty}^{\\infty} {{\\sin x} \\over {x} } dx = \\pi$ で求められる。\n証明 戦略：紹介する証明は直感的ではなく、技術的な部分が多く、理解が非常に難しい。しかし、その中でも比較的簡単な側面があり、複素解析を使わないという利点がある。\n$$ I_{n} (c) := \\int_{0}^{ { {\\pi} \\over {2} } } \\cos ^{n} t \\cos ct dt $$ を定義しよう。すると $$ I_{0} (0) = \\int_{0}^{ { {\\pi} \\over {2} } } \\cos 0 dt = { {\\pi} \\over {2} } \\\\ I_{0} (2x) = \\int_{0}^{ { {\\pi} \\over {2} } } \\cos 2xt dt = \\left[ {{\\sin 2xt} \\over {2x}} \\right]_{0}^{{ {\\pi} \\over {2} }} = {{\\sin \\pi x} \\over {2 x}} $$ 従って $$ {{I_{0} (2x)} \\over {I_{0} (0)}}= {{\\sin \\pi x} \\over {\\pi x}} = \\text{sinc} x $$ 故に $$ {{I_{0} (2x)} \\over {I_{0} (0)}}= \\prod_{n=1}^{\\infty} \\left( 1 - {{x^2} \\over { n^2}} \\right) $$ を示せば良い。まず、$ I_{n} (c)$ を漸化式として表そう。 $$ \\begin{align*} I_{n} (c) =\u0026amp; \\int_{0}^{ { {\\pi} \\over {2} } } \\cos ^{n} t \\cos ct dt \\\\ =\u0026amp; \\left[ {1 \\over c} \\cos^{n} t \\sin c t \\right]_{0}^{{ {\\pi} \\over {2} }} - \\int_{0}^{ { {\\pi} \\over {2} } } {1 \\over c} n \\cos ^{n-1} t (-\\sin t) \\sin ct dt \\\\ =\u0026amp; {n \\over c} \\int_{0}^{ { {\\pi} \\over {2} } } \\cos ^{n-1} t \\sin t \\sin ct dt \\\\ =\u0026amp; {n \\over c} \\left[ {1 \\over c} \\cos^{n-1} t \\sin t (-\\cos c t ) \\right]_{0}^{{ {\\pi} \\over {2} }} \\\\ \u0026amp; - {n \\over c} \\int_{0}^{ { {\\pi} \\over {2} } } {1 \\over c} \\left\\{ (n-1) \\cos ^{n-2} t (-\\sin^2 t) + \\cos ^{n} t \\right\\} (-\\cos ct) dt \\\\ =\u0026amp; {n \\over {c^2} } \\int_{0}^{ { {\\pi} \\over {2} } } \\left\\{ (n-1) \\cos ^{n-2} t (\\cos^2 t - 1) + \\cos ^{n} t \\right\\} \\cos ct dt \\\\ =\u0026amp; {n \\over {c^2} } \\int_{0}^{ { {\\pi} \\over {2} } } \\left\\{ n \\cos ^{n} t - (n-1) \\cos^{n-2} t \\right\\} \\cos ct dt \\\\ =\u0026amp; {n \\over {c^2} } \\left\\{ n I_{n}(c) - (n-1) I_{n-2}(c) \\right\\} \\end{align*} $$ うまく整理すると $$ (n^2 - c^2) I_{n} (c) = ( n^{2} - n) I_{n-2} (c) $$ ここで$c=0$ を代入して得られた式で各辺を割ると、新しい漸化式 $$ { {(n^2 - c^2)} \\over {n^2} } {{I_{n} (c)} \\over {I_{n} (0)}} = { {I_{n-2} (c)} \\over {I_{n-2} (0)} } $$ を得る。新しい漸化式で、右辺が$\\displaystyle { {I_{0} (c)} \\over {I_{0} (0)} }$ になるまで繰り返してみると $$ \\prod_{k=1}^{m} { {(2k)^2 - c^2} \\over {(2k)^2} } {{I_{2m} (c)} \\over {I_{2m} (0)}} = { {I_{0} (c)} \\over {I_{0} (0)} } $$ ここで$c=2x$ を代入すると $$ \\prod_{k=1}^{m} { {(2k)^2 - (2x)^2} \\over {(2k)^2} } {{I_{2m} (2x)} \\over {I_{2m} (0)}} = {{I_{2m} (2x)} \\over {I_{2m} (0)}} \\prod_{k=1}^{m} { {k^2 - x^2} \\over {k^2} } = {{I_{0} (2x)} \\over {I_{0} (0)}} $$ $\\displaystyle {{I_{0} (2x)} \\over {I_{0} (0)}}= \\prod_{n=1}^{\\infty} \\left( 1 - {{x^2} \\over { n^2}} \\right)$ であるから、$\\displaystyle \\lim_{m \\to \\infty} {{I_{m} (2x)} \\over {I_{m} (0)}}=1$ を示せば証明は完了する。今 $$ I_{m} (2x) = \\int_{0}^{ { {\\pi} \\over {2} } } \\cos ^{m} t \\cos 2xt dt $$ を考えてみよう。この数列を$x$ の関数として見れば、周期が$1$ であり、奇関数なので、$\\displaystyle 0\u0026lt;x \\le {1 \\over 2}$ だけを考えれば良い。$\\cos 0 \u0026gt;\\cos 2x t$ であるから $$ I_{m} (0)\u0026gt; I_{m} (2x) \\\\ \\cos 2xt \u0026gt;\\cos^{2} t $$ そして、$ I_{m} (2x)\u0026gt; I_{m+2} (0)$ である。従って $$ I_{m} (0)\u0026gt; I_{m} (2x)\u0026gt; I_{m+2} (0) $$ ここで、各辺を$ I_{m} (0)$ で割ると $$ 1 \u0026gt; {{I_{m} (2x)} \\over {I_{m} (0)} }\u0026gt; {{I_{m+2} (0)} \\over {I_{m} (0)}} $$ ここで $$ (m+2)^2 I_{m+2}(0) = (m^2 + 3m + 2) I_{m}(0) $$ であるから $$ \\lim_{m \\to \\infty} {{I_{m+2} (0)} \\over {I_{m} (0)}} = \\lim_{m \\to \\infty} { {m+1} \\over {m+2} } = 1 $$ 従って $$ {{\\sin \\pi x} \\over {\\pi x}} = \\prod_{n=1}^{\\infty} \\left( 1 - {{x^2} \\over { n^2}} \\right) $$\n■\n実際の結果は非常に有用だが、証明自体は覚えて他のどこかで使うようなものではない。ああ、こんな証明もあるんだと認識して、深く理解しようとするよりも、通り過ぎることをお勧めする。\n証明の追従 オイラーの証明：シンク関数を用いた平方数の逆数の和を求める ","id":187,"permalink":"https://freshrimpsushi.github.io/jp/posts/187/","tags":null,"title":"シンク関数のオイラー表現の証明"},{"categories":"해석개론","contents":"サマリー 二つの実数 $a\u0026lt;b$ に対して、$a\u0026lt;r\u0026lt;b$ を満たす $r \\in \\mathbb{R}$ が存在する。\n説明 実数空間において、どんな区間を考えても、その間には必ず別の実数が存在する。どれだけ小さく分割しても、さらに分割可能な点があるということだ。明らかに思えるが、これは明らかではないだけでなく、非常に抽象的な性質であることを心に留めておこう。例えば、物理学で扱う物質やエネルギーでさえ、小さく分割していくとその限界がある。\n証明 戦略: 証明は、有理数と無理数についてそれぞれ分けて行う。2つの実数の間に有理数が存在し、同時に無理数も存在すれば、証明は完了する。「一般性を失わずに（without loss of generality）」という表現は証明に登場する正の数がいつでも実数の差で表現できるため、$0$ 以下の数を特に考える必要がないために言及される。例えば、証明が二つの負の数 $c \u0026lt; d \u0026lt; 0$ から始まっても、不等式が成り立つ限り、$d - c \u0026gt; 0$ のように正の数を作ることができる。\n必要な前提は以下の通り。\n体の公理:\n(A1) 加法に対する閉性: $a+b \\in \\mathbb{R}$ (A5) 加法に対する逆元: $a + (-a) = (-a) + a = 0$ を満たす $(-a)$ が存在 (M1) 乗法に対する閉性: $a\\cdot b \\in \\mathbb{R}$ (M5) 乗法に対する逆元: $a \\cdot a^{-1} = a^{-1} \\cdot a = 1$ を満たす ${a^{-1}}$ が存在 (D) 分配法則: $a \\cdot (b + c) = a \\cdot b + a \\cdot c$ 順序公理:\n加法性: $a\u0026lt;b$ かつ $c\\in \\mathbb{R}$ ならば $a+ c\u0026lt; b + c$ 乗法性: $a\u0026lt;b$ かつ $c\u0026gt;0$ ならば $ac\u0026lt; bc$ 、もしくは $c\u0026lt;0$ ならば $ac\u0026gt; bc$ アルキメデスの原理: 正の数 $a$ と実数 $b$ に対して、$an\u0026gt;b$ を満たす 自然数 $n$ が存在する。\nパート1. 有理数の密度 1\n$a\u0026lt;q\u0026lt;b$ を満たす $q \\in \\mathbb{Q}$ が常に存在することを示そう。一般性を失わずに、$0 \u0026lt; a \u0026lt; b$ を満たす正の数 $(b-a) \u0026gt; 0$ と実数 $1 \\in \\mathbb{R}$ を考えると、アルキメデスの原理の不等式を満たす 自然数 の 集合 $\\left\\{ n \\in \\mathbb{N} : (b-a) n \u0026gt; 1 \\right\\}$ が存在し、加法に対する逆元の存在性、閉性と分配法則、加法性により $$ bn-an \u0026gt; 1 \\implies an + 1 \u0026lt; bn \\implies an \u0026lt; an + 1 \u0026lt; bn $$ とわかる。$an$ と $an + 1$ の差が $1$ より大きいため、その間には少なくとも一つの整数が存在し、それを $m$ とすれば $$ an \u0026lt; m \u0026lt; an + 1 \u0026lt; bn $$ 両辺に $n$ の乗法に対する逆元 $n^{-1}$ を掛けると次を得る。 $$ a \u0026lt; {{ m } \\over { n }} \u0026lt; b $$ ここで、$\\displaystyle q := {{ m } \\over { n }}$ にすると、$q$ は「自然数の比」つまり有理数であり、次の不等式を得る。 $$ a \u0026lt; q \u0026lt; b $$\nパート2. 無理数の密度\n$a\u0026lt;\\xi\u0026lt;b$ を満たす $\\xi \\in \\mathbb{Q^{c}}$ が常に存在することを示そう。一般性を失わずに、$0 \u0026lt; a \u0026lt; b$ を満たす実数と無理数 $c\u0026gt;0$ を考えると、$a\u0026lt;b$ であれば $ac\u0026lt;bc$ となる。実数は乗法に対して閉じているので、$ac$ と $bc$ も実数であり、有理数の密度により、$ac\u0026lt;q\u0026lt;bc$ を満たす有理数 $q \\ne 0$ が存在する。$ac\u0026lt;q\u0026lt;bc$ の両辺に $c$ の乗法に対する逆元 $\\displaystyle {1 \\over c}$ を掛けると次のようになる。\n$$ a\u0026lt;{q \\over c}\u0026lt;b $$\nここで、$\\displaystyle \\xi := {q \\over c}$ にすると、$\\xi$ は$0$ でない有理数と無理数の積なので無理数であり、次の不等式を得る。\n$$ a\u0026lt;\\xi\u0026lt;b $$\n■\nhttps://www.math.ucdavis.edu/~hunter/m127a_19/rat_dense.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":185,"permalink":"https://freshrimpsushi.github.io/jp/posts/185/","tags":null,"title":"実数の密度の証明"},{"categories":"해석개론","contents":"概観 証明無しにいくつかの級数判定法を紹介したい。事実としてうまく活用することが大切で、大体証明の過程も退屈だからである。\nこのポストでは、以下の記法を共有する：\n$\\mathbb{N}$ は自然数を全て集めた集合である。 $\\mathbb{R}$ は全ての実数を集めた集合で、$\\overline{\\mathbb{R}}$ は実数集合に$\\pm \\infty$ を含む拡張された実数集合である。 $\\left\\{ a_{k} \\right\\}_{k \\in \\mathbb{N}}, \\left\\{ b_{k} \\right\\}_{k \\in \\mathbb{N}} \\subset \\mathbb{R}$ は実数列である。 $\\exists \\lim_{k \\to \\infty} x_{k}$ は$x_{k}$ の極限が$\\mathbb{R}$ に存在する、つまり収束するという意味である。反対に$\\not\\exists \\lim_{k \\to \\infty} x_{k}$ は$x_{k}$ の極限が$\\mathbb{R}$ に存在しない、つまり発散するという意味だ。 十分に大きい$k$ に対して$\\displaystyle \\lim_{k \\to \\infty} { {a_k} \\over {b_k} } = 1$ の時$a_k \\approx b_k$ と表される。 $b_k \\downarrow 0$ は$b_{k}$ が減少数列であり、$k \\to \\infty$ の時$0$に収束しつつ$0$以上の値を取ることを意味する。 実数列 1 発散判定法 $\\lim _{ k \\to \\infty }{ { a }_{ k }} \\ne 0$ ならば$\\sum _{ k =1 }^{ \\infty }{ { a }_{ k }}$ は発散する: $$ \\lim _{ k \\to \\infty }{ { a }_{ k }} \\ne 0 \\implies \\not\\exists \\sum _{ k =1 }^{ \\infty }{ { a }_{ k }} $$\n発散判定法Divergence Testは高校の時に接することができる唯一の判定法である。収束自体を判定できないのは残念だが、発散を示す時は最も簡単で速い。 コーシー判定法 $\\sum _{ n=1 }^{ \\infty }{ { a }_{ n }}$ が収束することは$\\lim_{n \\to \\infty} \\sum _{ k=n }^{ n+m }{ { a }_{ k }}=0$ と等価である: $$ \\exists \\sum_{k=1}^{\\infty} a_{k} \\iff \\left( \\forall \\varepsilon \u0026gt; 0 , \\exists N \\in \\mathbb{N} : m \\ge n \\ge N \\implies \\left| \\sum_{k=n}^{m} a_{k} \\right| \u0026lt; \\varepsilon \\right) $$\nこの名前がついた理由は、定理の証明に実数空間$\\mathbb{R}$で収束する数列がコーシー数列であることと等価であることを利用するからである。 非負数列 2 各項が$0$より大きいか等しい、$a_{k} \\ge 0$ を持つ数列について扱う。\n積分判定法 減少関数 $f: [1,\\infty) \\to \\mathbb{R}$ が常に$0$より大きいとする。$\\sum _{ k =1 }^{ \\infty }{ { f }( k )}$ が収束することは$\\int_{1}^{\\infty} f(x) dx \u0026lt; \\infty$ と等価である: $$ \\exists \\sum_{k=1}^{\\infty} f(k) \\iff \\int_{1}^{\\infty} f(x) dx \u0026lt; \\infty $$\n積分判定法Integral Testは$f(n+1) \\le \\int_{n}^{n+1} f(x) dx \\le f(n)$ を利用して証明される。証明過程が面白い稀有な判定法である。 $p$級数判定法 $\\sum _{ k=1 }^{ \\infty } k^{-p}$ が収束することは$p\u0026gt;1$ と等価である: $$ \\exists \\sum_{k=1}^{\\infty} {{ 1 } \\over { k^{p} }} \\iff p \u0026gt; 1 $$\n$p$級数判定法$p$-Series Testは簡単に言えば、調和級数で少しでも指数を上げると収束し、そうでなければ発散するというものである。幾何級数を積分判定法に入れて導かれる推論だが、非常にシンプルで有用なので、積分判定法よりもよく使われる。 比較判定法 十分に大きい$k$ に対して$0 \\le a_k \\le b_k$ としよう。$\\sum _{ k=1 }^{ \\infty }{ { b }_{ k }}$ が収束するならば、$\\sum _{ k=1 }^{ \\infty }{ { a }_{ k }}$ も収束する: $$ \\begin{align*} \\sum_{k=1}^{\\infty} b_{k} \u0026lt; \\infty \\implies \u0026amp; \\sum_{k=1}^{\\infty} a_{k} \u0026lt; \\infty \\\\ \\sum_{k=1}^{\\infty} a_{k} = \\infty \\implies \u0026amp; \\sum_{k=1}^{\\infty} b_{k} = \\infty \\end{align*} $$\n比較判定法Comparison Testは名前の通り、既に収束することがわかっている別の級数と比較して収束することを示す時に使う。対偶を使えば、同様に級数が発散するかを確認することができる。 極限比較判定法 十分に大きい$k$ に対して$a_k \\ge 0$ とし、$b_k\u0026gt;0$ とする。$L := \\lim_{k \\to \\infty} { {a_k} \\over {b_k} } \\in \\overline{\\mathbb{R}}$ は何らかの拡張された実数である。 (1) $0\u0026lt;L\u0026lt;\\infty$ ならば、$\\sum _{ n=1 }^{ \\infty }{ { a }_{ n }}$ と$\\sum _{ n=1 }^{ \\infty }{ { b }_{ n }}$ は共に収束するか、あるいは共に発散する。 (2) $L=0$ で$\\sum _{ n=1 }^{ \\infty }{ { b }_{ n }}$ が収束するなら、$\\sum _{ n=1 }^{ \\infty }{ { a }_{ n }}$ も収束する。 (3) $L=\\infty$ で$\\sum _{ n=1 }^{ \\infty }{ { b }_{ n }}$ が発散するなら、$\\sum _{ n=1 }^{ \\infty }{ { a }_{ n }}$ も発散する: $$ \\begin{align*} 0 \u0026lt; L \u0026lt; \\infty \\implies \u0026amp; \\left( \\exists \\sum_{k=1}^{n} a_{k} \\iff \\exists \\sum_{k=1}^{n} b_{k} \\right) \\\\ L = 0 \\implies \u0026amp; \\left( \\exists \\sum_{k=1}^{n} b_{k} \\implies \\exists \\sum_{k=1}^{n} a_{k} \\right) \\\\ L = \\infty \\implies \u0026amp; \\left( \\not\\exists \\sum_{k=1}^{n} b_{k} \\implies \\not\\exists \\sum_{k=1}^{n} a_{k} \\right) \\end{align*} $$\n極限比較判定法Limit Comparison Testは比較判定法と同様に、元の級数が収束することを示すのが難しいので、別の収束する級数と比較するものである。条件は厳しそうに見えるが、実際には収束性だけを示す場合には満たしやすいので非常に有用である。 絶対収束 3 無限級数$S = \\sum_{k=1}^{\\infty} a_{k}$ に対して、$\\sum_{k=1}^{\\infty} \\left| a_{k} \\right|$ が収束するならば、$S$ を絶対収束Converge Absolutelyと定義する。これに従って絶対収束しないが$S$ 自体は収束する級数を条件付き収束Converge Conditionallyとも言う。\n根判定法 $\\left\\{ \\left| a_{k} \\right|^{1/k} \\right\\}$ のリミットスープリーム $r = \\limsup_{k \\to \\infty} {{|a_k|} ^ {1 / k}}$ に対して、$r\u0026lt;1$ ならば$\\sum _{ n=1 }^{ \\infty }{ { a }_{ k }}$ は絶対収束、$r\u0026gt;1$ ならば$\\sum _{ n=1 }^{ \\infty }{ { a }_{ k }}$ は発散する: $$ \\begin{align*} r \u0026lt; 1 \\implies \u0026amp; \\exists \\sum_{k=1}^{\\infty} \\left| a_{k} \\right| \\\\ r \u0026gt; 1 \\implies \u0026amp; \\not\\exists \\sum_{k=1}^{\\infty} a_{k} \\end{align*} $$\n比判定法 $a_{k} \\ne 0$ とし、$r = \\lim_{k \\to \\infty} { {|a_{k+1}|} \\over {|a_{k}|} } \\in \\overline{\\mathbb{R}}$ を拡張された実数とする。$r\u0026lt;1$ ならば$\\sum _{ k=1 }^{ \\infty }{ { a }_{ k }}$は絶対収束、$r\u0026gt;1$ ならば$\\sum _{ k=1 }^{ \\infty }{ { a }_{ k }}$は発散する: $$ \\begin{align*} r \u0026lt; 1 \\implies \u0026amp; \\exists \\sum_{k=1}^{\\infty} \\left| a_{k} \\right| \\\\ r \u0026gt; 1 \\implies \u0026amp; \\not\\exists \\sum_{k=1}^{\\infty} a_{k} \\end{align*} $$\n根判定法Root Testと比判定法Ratio Testは条件がやや厳しいが絶対収束を一発で示すためによく使われる。一方で$r=1$ の場合は、ディリクレ判定法Dirichlet\u0026rsquo;s Testや交代級数判定法Alternating Series Testなどが使われることがある。交代級数判定法はディリクレ判定法から直接導かれ、収束性だけを判定したい場合は交代調和級数を例に考えると役立つ。 ディリクレ判定法 部分和$s_n = \\sum_{k=1}^{n} a_k$ が有界で、$k \\to \\infty$ の時$b_k \\downarrow 0$ ならば、$\\sum _{ k=1 }^{ \\infty }{ { a }_{ k } {b}_{k}}$ は収束する: $$ \\left| s_n \\right| \u0026lt; \\infty , b_k \\downarrow 0 \\implies \\exists \\sum _{ k=1 }^{ \\infty }{ { a }_{ k } {b}_{k}} $$\n交代級数判定法 $k \\to \\infty$ の時$b_k \\downarrow 0$ ならば、$\\sum _{ n=1 }^{ \\infty }{ (-1)^{k} {b}_{k}}$ は収束する。 $$ b_k \\downarrow 0 \\implies \\exists \\sum _{ k=1 }^{ \\infty }{ (-1)^{-k} {b}_{k}} $$\nWade. (2013). An Introduction to Analysis(4th Edition): p186, 188。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWade. (2013). An Introduction to Analysis(4th Edition): p193, 194, 196。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWade. (2013). An Introduction to Analysis(4th Edition): p198, 201, 210。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":186,"permalink":"https://freshrimpsushi.github.io/jp/posts/186/","tags":null,"title":"解析学における様々な級数判定法の総整理"},{"categories":"해석개론","contents":"公理1 集合 $E \\subset \\mathbb{R}$ が空集合ではなく、もし $E$ が上界を持つならば、上限 $\\sup(E) \u0026lt; \\infty$ が存在する。\n説明 体の公理や順序の公理は、知っていることを複雑に書き直した感じだが、完備性の公理は一見そうではない。まずはここで登場する単語に対する定義が必要だろう。\n定義 $E$ の全ての元 $a$ に対して $a \\le M$ が成り立つならば、$E$ を上に有界bounded aboveと呼ぶ。このような条件を満たす $M$ を全て、$E$ の上界upper boundと呼ぶ。$\\sup(E)$ は$E$ の最小の上界であり、全ての$E$ の上界 $M$ に対して $\\sup (E) \\le M$ を満たす数である。これを$E$ の最小上界supremum, 上限と呼ぶ。\n反対の不等号の場合は、以下のようになる。\n$E$ の全ての元 $a$ に対して $a \\ge m$ が成り立つならば、$E$ を下に有界bounded belowと呼ぶ。このような条件を満たす $m$ を全て、$E$ の下界lower boundと呼ぶ。$\\inf(E)$ は$E$ の最大の下界であり、全ての$E$ の下界 $m$ に対して $\\inf (E) \\ge m$ を満たす数である。これを$E$ の最大下界infimum, 下限と呼ぶ。\n突然定義がたくさん出てきて混乱するかもしれないが、根本的に我々の概念を揺るがすわけではない。ただどんな集合がいつ限界を持つか、その時限界を何と呼ぶかを定義するだけだ。\n完備性公理に戻ってみると、上で紹介された定義を繰り返す感じがする。違いは簡単だ。定義では存在する時に何と呼ぶかだけを言っていて、本当に存在するかについては話していない。完備性公理では、その「存在」について話している。\nしかし、これが公理であるべきかという疑問が生じるだろう。公理として必要なほど基本的な事実なのか？証明できないのか？定義を読むと、定義によって当然のようにこのような上限が存在し、証明できるように思えるが、そうではない。\n反証 $E$ が上に有界としたならば、条件を満たす上界 $M$ があり、その中で最も小さい値が存在して、上限 $\\sup(E)$ が存在するだろう。しかし、逆に考えてみると、$\\sup(E)$ は$-M$ の中で最も大きい値、つまり上限である。そもそも最も小さい値が存在するという主張自体が上限の存在性を根拠にしている。これにより、循環論法に陥るしかない。\n■\n上でも下でも、大きさに関係なく、議論の方向は逆になる。結局、我々はこのような上限や下限の存在性を明らかにできない。だから、新しい公理として完備性公理を作り出すしかなかった。\n定理 整数の集合 $\\mathbb{Z}$ の部分集合 $E$ が上限を持つならば $\\sup(E) \\in E$\n完備性公理がなければ、こんなにも自明な事実でさえ、その仮定が疑わしいために信じられない。\n完備？ 完備とは、Completeの和訳で、実数空間 $\\mathbb{R}$ を超えて距離空間で一般化されるとき、コーシー列の収束点を含む空間を完備空間と定義する。ただし、日常の中でCompleteという英語は完全に(完)備える(備)という意味で使われることは少なく、「完成」や「完結」など、何か続いているもののその終わりと共に使われることが多い。これは、述べられたように、収束点の存在(その空間内に)を保証する点から、completeという表現が適切であることが分かる。\nもちろん、コーシー列を捉えることと$E \\subset \\mathbb{R}$ を捉えることは異なるが、$\\mathbb{R}$ が可分性を持つといった説明はまだ早い。後でそんなことをまた学ぶんだと思って、先に進んでもいい。\nWilliam R. Wade, An Introduction to Analysis (4th Edition, 2010), p16-18\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":180,"permalink":"https://freshrimpsushi.github.io/jp/posts/180/","tags":null,"title":"解析学の三つの公理：完備性公理"},{"categories":"해석개론","contents":"公理1 実数 $a,b,c \\in \\mathbb{R}$ と演算 $+,\\cdot$ に対して、以下の性質が成立すると受け入れよう。\n(A1) 加算に対する閉性: $a+b \\in \\mathbb{R}$\n(A2) 加算に対する結合律: $(a+b) + c = a + (b+c)$\n(A3) 加算に対する交換律: $ a+ b= b + a$\n(A4) 加算に対する単位元: すべての実数 $a$ に対して、$a+0=0+a=a$を満たす$0$が一意に存在する。\n(A5) 加算に対する逆元: すべての実数 $a$ に対して、$a + (-a) = (-a) + a = 0$を満たす$(-a)$が一意に存在する。\n(M1) 乗算に対する閉性: $a\\cdot b \\in \\mathbb{R}$\n(M2) 乗算に対する結合律: $(a\\cdot b) \\cdot c = a \\cdot (b\\cdot c)$\n(M3) 乗算に対する交換律: $a\\cdot b= b \\cdot a$\n(M4) 乗算に対する単位元: すべての実数 $a$ に対して、$a\\cdot 1=1\\cdot a=a$を満たす$1$が一意に存在する。\n(M5) 乗算に対する逆元: $0$ を除くすべての実数 $a$ に対して、$a \\cdot a^{-1} = a^{-1} \\cdot a = 1$を満たす${a^{-1}}$が一意に存在する。\n(D) 分配法則: $a \\cdot (b + c) = a \\cdot b + a \\cdot c$\n説明 解析学入門は基本的に実数集合 $\\mathbb{R}$ での関数を扱う科目だ。そしてこの解析学で最も戸惑う過程の一つが、こういった当然の事実について勉強することだ。今まで当然だったものを「厳密な」あるいは学生が感じるには「無駄な」レベルまで掘り下げる。厳密さにいつも渇望していた学生は興味を感じるだろうし、そうでない場合は少し力が入るだろう。\n実数は義務教育過程で少しずつ拡張しながら自然に、直感的に受け入れてきた概念だ。そして、そんな明らかに知っていた「実数」が、演算に対して閉じているかどうかなんて話しているのはかなり退屈だ。特にその11の性質は私たちがあまりにも当然と思っているので、試験を前にして急いで暗記しているうちに数学者としての疑問感もなかなかなものだろう。\nしかし、これら全てはより大きな学問を学ぶために乗り越えなければならない試練の部分だ。そして、しっかりと暗記する必要もない。しばらくすると抽象代数学を学ぶと、その11の性質をわざわざ記憶せずともスラスラと話せるようになる。そして、もう少しすると、それらの性質がいかに当たり前ではないが有益な性質なのかを理解することになる。例えば、次のような当たり前の事実も意外と証明が必要だ。\n定理 任意の実数と $0$ を掛け合わせると $0$ である。\n証明 $a \\in \\mathbb{R}$ とする。$0 \\in \\mathbb{R}$ であるから、$0 + 0= 0$ で、\n$$ a \\cdot 0 = a \\cdot [ 0 + 0 ] $$\n(D) 分配法則 によって、\n$$ a \\cdot 0 = a \\cdot 0 + a \\cdot 0 $$\n$a \\cdot 0 \\in \\mathbb{R}$であるから、(A5) 加算に対する逆元が存在して、\n$$ a \\cdot 0 + \\left[ -(a \\cdot 0) \\right] = a \\cdot 0 + a \\cdot 0 + \\left[ -(a \\cdot 0) \\right] $$\nすると$a \\cdot 0 + \\left[ -(a \\cdot 0) \\right] = 0$であるから、\n$$ 0 = a \\cdot 0 $$\n■\n関連項目を見る 抽象代数学における体 William R. Wade, An Introduction to Analysis (第4版, 2010), p5-6\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":178,"permalink":"https://freshrimpsushi.github.io/jp/posts/178/","tags":null,"title":"解析学の三つの公理：1 体の公理"},{"categories":"양자역학","contents":"導出 一次元で考えよう。私たちが探している関数は、運動量が$p$の波動関数$\\psi (x,t) = e^{i(kx - \\omega t)}$に対して次の式を満たすオペレータだ。\n$$ p_{\\text{op}} \\psi = p \\psi $$\n量子力学では、運動量とエネルギーはそれぞれ$p = \\hbar k$であるため、波動関数は、\n$$ \\psi (x,t) = e^{i(px - \\hbar \\omega t)/\\hbar} $$\n運動量$p$を得るには、$x$に関して微分すればよい。\n$$ \\dfrac{\\partial }{\\partial x} e^{i(px - \\hbar \\omega t)/\\hbar} = \\dfrac{i}{\\hbar} p e^{i(px - \\hbar \\omega t)/\\hbar} $$\nしたがって、\n$$ \\begin{align*} \u0026amp;\u0026amp; \\dfrac{\\hbar}{i}\\dfrac{\\partial }{\\partial x} e^{i(px - \\hbar \\omega t)/\\hbar} \u0026amp;= p e^{i(px - \\hbar \\omega t)/\\hbar} \\\\ \\implies \u0026amp;\u0026amp; \\dfrac{\\hbar}{i}\\dfrac{\\partial }{\\partial x} \\psi (x, t) \u0026amp;= p \\psi (x, t) \\\\ \\implies \u0026amp;\u0026amp; p_{\\text{op}} = \\dfrac{\\hbar}{i}\\dfrac{\\partial }{\\partial x} \\end{align*} $$\n■\n","id":100,"permalink":"https://freshrimpsushi.github.io/jp/posts/100/","tags":null,"title":"量子力学における運動量演算子"},{"categories":"교과과정","contents":"数式 初項が$a$で、公比が$r$の等比数列$a_{n} = a r^{n-1}$について、 $$ \\sum_{k=1}^{n} a_{k}= {{a (1- r^{n} ) } \\over {1-r}} $$\n証明 $\\displaystyle S= \\sum_{k=1}^{n} a_{k}$としよう。すると、 $$ S= a + ar + \\cdots + ar^{n-2} + ar^{n-1} $$ 両辺に$r$をかけると、 $$ rS= ar + a r^2 + \\cdots + ar^{n-1} + ar^{n} $$ ここで、上の二つの式から両辺を引くと、 $$ S - rS = (1-r)S = a- a r^n $$ 右辺の二つの式を$1-r$で割ると、 $$ S=\\sum_{k=1}^{n} a_{k}= {{a (1- r^{n} ) } \\over {1-r}} $$\n■\n説明 等差数列の和とは異なり、この公式自体が非常によく使われる。証明法も少し異なるが、別にもっと勉強するほど難しいわけではない。\n等比級数は幾何級数Geometric Seriesとも言われる。人々がよく「幾何級数的に」と言うが、それがこの言葉だ。つまり、ほとんどの数学に詳しくない人々が「幾何級数的に」という言葉を間違って使っているのだ。\n等比級数で$n$が無限大になるとどうなるか？$|r|\u0026lt;1$ならば収束し、$|r|\u0026gt;1$ならば発散するだろう。等比級数でこのように$n \\to \\infty$を考えることを「無限等比級数」という。\n無限等比級数 $|r|\u0026lt;1$の時、 $$ \\sum_{n=1}^{\\infty} a r^{n-1} = { a \\over {1-r}} $$\n$n \\to \\infty$から$ar^n \\to 0$になるので、等比級数から自然に導かれる。\n","id":170,"permalink":"https://freshrimpsushi.github.io/jp/posts/170/","tags":null,"title":"等比数列の和を求める"},{"categories":"해석개론","contents":"要約1 平面 $S = [a,b] \\times [c,d]$ 上で反時計回りに単純かつスムーズな閉曲線を描く $\\mathcal{C}$ としよう。関数 $P,Q : \\mathbb{R}^2 \\to \\mathbb{R}$ が $\\mathcal{C}$ 上で連続であり、その導関数も連続であれば、\n$$ \\int_{\\mathcal{C}} (Pdx + Qdy) = \\iint_{S} (Q_{x} - P_{y}) dx dy $$\n説明 線積分を面積分に変換する定理として考えられる。平面に限定したケルビン・ストークス定理の系として広く知られている。より一般化された定理が存在するにもかかわらず、その名前が残される限り、多くの分野でその地位を失っていない定理である。\n証明 $$ I_{1} := \\int_{\\mathcal{C}} P dx \\\\ \\displaystyle I_{2} := \\int_{\\mathcal{C}} Q dy $$ とすると、 $$ \\int_{\\mathcal{C}} (Pdx + Qdy) = I_{1} + I_{2} $$ となる。まず $I_{1}$ から求めよう。\n$I_{1}$ を計算する領域は上のように表されるだろう。この時、$\\mathcal{C}$ を囲む領域は、 $$ S = \\left\\{ (x,y) \\in \\mathbb{R} \\ | \\ a \\le x \\le b, y_{1}(x) \\le y \\le y_{2}(x) \\right\\} $$ であるため、 $$ \\begin{align*} I_{1} =\u0026amp; \\int_{\\mathcal{C}} Pdx \\\\ =\u0026amp; \\int_{a}^{b} P(x,y_{1} (x))dx + \\int_{b}^{a} P(x,y_{2} (x)) dx \\\\ =\u0026amp; - \\int_{a}^{b} \\left\\{ P(x,y_{2} (x))-P(x,y_{1} (x)) \\right\\} dx \\\\ =\u0026amp; - \\int_{a}^{b} \\int_{y_{1}(x)}^{y_{2}(x)} {{\\partial P(x,y)} \\over {\\partial y}} dy dx \\\\ =\u0026amp; - \\iint_{S} P_{y} dy dx \\end{align*} $$ 次に $I_{2}$ を求めよう。通常、このような証明では「同じ方法で求めることができる」と終わることが多いが、グリーンの定理では、直接計算しなければならない。方法は似ているが、結果的に符号が反対方向になるため、必ず確認するようにしよう。\n$I_{2}$ を計算する領域は上のように表されるだろう。この時、$\\mathcal{C}$ を囲む領域は、 $$ S = \\left\\{ (x,y) \\in \\mathbb{R}^{2} \\ | \\ c \\le y \\le d, x_{1}(y) \\le x \\le x_{2}(y) \\right\\} $$ であるため、 $$ \\begin{align*} I_{2} =\u0026amp; \\int_{\\mathcal{C}} Qdy \\\\ =\u0026amp; \\int_{d}^{c} Q(x_{1}(y),y) dy + \\int_{c}^{d} Q(x_{2}(y),y) dy \\\\ =\u0026amp; \\int_{c}^{d} Q(x_{2}(y),y) dy - \\int_{c}^{d} Q(x_{1}(y),y) dy \\\\ =\u0026amp; \\int_{c}^{d} \\left\\{ Q(x_{2}(y),y) dy - Q(x_{1}(y),y) \\right\\} dy \\\\ =\u0026amp; \\int_{c}^{d} \\int_{x_{1}(x)}^{x_{2}(x)} {{\\partial Q(x,y)} \\over {\\partial x}} dx dy \\\\ =\u0026amp; \\iint_{S} Q_{x} dx dy \\end{align*} $$ $I_{2}$ と $I_{1}$ の結果を足すと、 $$ \\int_{\\mathcal{C}} (Pdx + Qdy) = I_{2} + I_{1} = \\iint_{S} Q_{x} dx dy - \\iint_{S} P_{y} dy dx $$\nフビニの定理：$R : [a,b] \\times [c,d]$ とする。$f(x,\\cdot)$ が $[c,d]$ 上で、$f(\\cdot,y)$ が $[a,b]$ 上で、$f$ が $R$ 上で積分可能であれば、 $$ \\iint _{R} f dA = \\int_{a}^{b} \\int_{c}^{d} f(x,y) dy dx = \\int_{c}^{d} \\int_{a}^{b} f(x,y) dx dy $$\n前提条件から $P$ の導関数 $P_{y}$ も連続であるため積分可能であり、フビニの定理を適用できる。積分の順序を次のように変えると、 $$ \\iint_{S} P_{y} dy dx = \\iint_{S} P_{y} dx dy $$ そして積分順序を $dx dy$ に統一すると、 $$ \\int_{\\mathcal{C}} (Pdx + Qdy) = \\iint_{S} ( Q_{x} - P_{y} ) dx dy $$\n■\n上では長方形 $S$ について示したが、これを小さな正方形 $[\\alpha, \\alpha + \\varepsilon] \\times [\\beta, \\beta + \\varepsilon]$ に特殊化し、さらに一般的な有界領域 $\\mathcal{R}$ を一辺の長さが $\\varepsilon$ の小さな正方形に分割し、$\\varepsilon \\to 0$ で極限をとると、次のような一般化された定理を簡単に得ることができる。\n条件や表現は異なるかもしれないが、本質的には大きな違いはない。一般化に意味を置くよりも、教科書によって詳細が異なる程度で受け入れて進むとよい。\n一般化 1 $\\mathcal{R}$ で定義された2つの関数 $P,Q$ が $\\mathcal{R}$ 上で微分可能であれば、 $$ \\int_{\\mathcal{C}} (Pdx + Qdy) = \\iint_{\\mathcal{R}} (Q_{x} - P_{y}) dx dy $$\n曲線 $C^{2}$ は2回微分可能で、その導関数も全て微分可能である。 参照 グリーンの定理の一般化 Millman. (1977). Elements of Differential Geometry: p51. 反時計回りに単純な閉曲線 $C^{2}$ $\\mathcal{C}$ が有界領域 $\\mathcal{R}$ を囲んでいるとしよう。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":166,"permalink":"https://freshrimpsushi.github.io/jp/posts/166/","tags":null,"title":"グリーンの定理の証明"},{"categories":"해석개론","contents":"定理1 2 2次元領域$R : [a,b] \\times [c,d]$に対して関数$f : R \\to \\mathbb{R}$を定義しよう。$f(x,\\cdot)$が$[c,d]$で積分可能であり、$f(\\cdot,y)$が$[a,b]$で積分可能であり、$f$が$R$で積分可能なら\n$$ \\iint _{R} f dA = \\int_{a}^{b} \\int_{c}^{d} f(x,y) dy dx = \\int_{c}^{d} \\int_{a}^{b} f(x,y) dx dy $$\n説明 積分領域の$R$は当然Rectangleから来ている。解析学ではいつもそうだけど、長い話を読みたくないみんなのために要約すると、直行する二つの方向それぞれで積分可能ならば、$f$の重積分を求める際に積分の順序を変えても問題ないということだ。他の専攻では、この条件を満たす関数を大体扱っているから、たいしたことないように思えるかもしれないけど、非常に重要な定理だ。ただの重積分の性質ではなく、人の名前が付いた定理として残っているのは、それなりの理由があるんだ。\n証明 $$ \\begin{align*} (L) \\iint _{R} f dA \\le \u0026amp; (L) \\int_{a}^{b} \\left( \\int_{c}^{d} f(x,y) dy \\right) dx \\\\ \\le \u0026amp; (U) \\int_{a}^{b} \\left( \\int_{c}^{d} f(x,y) dy \\right) dx \\\\ \\le \u0026amp; (U) \\iint _{R} f dA \\end{align*} $$\n関数$g : [a,b] \\to \\mathbb{R}$を$\\displaystyle g(x) := \\int_{c}^{d} f(x,y) dy$として定義しよう。$f$は$R$で積分可能、つまり$\\displaystyle (L) \\iint _{R} f dA = (U) \\iint _{R} f dA$であるから、\n$$ \\iint _{R} f dA = (U) \\int_{a}^{b} g(x) dx = (L) \\int_{a}^{b} g(x) dx $$\n$\\displaystyle (U) \\int_{a}^{b} g(x) dx = (L) \\int_{a}^{b} g(x) dx$より$g$は$[a,b]$で積分可能である。もう一度表現すると、\n$$ \\begin{align*} \\iint _{R} f dA =\u0026amp; (U) \\int_{a}^{b} g(x) dx \\\\ =\u0026amp; (L) \\int_{a}^{b} g(x) dx \\\\ =\u0026amp; \\int_{a}^{b} \\int_{c}^{d} f(x,y) dy dx \\end{align*} $$\nここで$x$と$y$を入れ替えて、同じ過程を繰り返すと、\n$$ \\iint _{R} f dA = \\int_{c}^{d} \\int_{a}^{b} f(x,y) dx dy $$\nを得ることができるから、\n$$ \\iint _{R} f dA = \\int_{a}^{b} \\int_{c}^{d} f(x,y) dy dx = \\int_{c}^{d} \\int_{a}^{b} f(x,y) dx dy $$\n■\nWilliam R. Wade, An Introduction to Analysis (4th Edition, 2010), p477-478\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n경북대학교 기초교육원, 이공학도를 위한 대학수학 (2012), p317-318\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":165,"permalink":"https://freshrimpsushi.github.io/jp/posts/165/","tags":null,"title":"フビニの定理の証明"},{"categories":"복소해석","contents":"要約 1 関数$f$が積分経路$\\mathscr{C}: z = z(t), t \\in [a,b]$で部分的に連続だとしよう。正数$\\displaystyle L = \\int_{a}^{b} |z\u0026rsquo;(t)| dt$が$\\mathscr{C}$の長さであり、$\\mathscr{C}$上の全ての点において$|f(z)| \\le M$を満たす正数$M$が存在する場合、 $$ \\left| \\int_{\\mathscr{C}} f(z) dz \\right| \\le ML $$\n証明 関数$z\u0026rsquo;: [a,b] \\to \\mathbb{C}$に対して$\\displaystyle \\left| \\int_{a}^{b} z\u0026rsquo;(t) dt \\right| = r$とする。\nもし$r \\ne 0$ならば、$\\displaystyle \\int_{a}^{b} z\u0026rsquo;(t) dt = r e^{i \\theta}$と表せる。その結果、$\\theta$は定数であるため、 $$ r = \\int_{a}^{b} e^{- i \\theta} z\u0026rsquo;(t) dt \\le \\int_{a}^{b} \\left| e^{- i \\theta} z\u0026rsquo;(t) \\right| dt = \\int_{a}^{b} \\left| e^{- i \\theta} \\right| \\left| z\u0026rsquo;(t) \\right| dt $$ ここで、複素数の実数根の絶対値は常に1であるため、$\\left| e^{ - i \\theta} \\right| = 1$である。つまり、 $$ \\left| \\int_{a}^{b} z\u0026rsquo;(t) dt \\right| = r \\le \\int_{a}^{b} \\left| z\u0026rsquo;(t) \\right| dt $$ これによって、実関数で成立する定積分の性質が複素関数でも成立することがわかる。上で導かれた不等式を使用すると、 $$ \\begin{align*} \\left| \\int_{\\mathscr{C}} f(z) dz \\right| =\u0026amp; \\left| \\int_{a}^{b} f(z(t)) z\u0026rsquo;(t) dt \\right| \\\\ \\le \u0026amp; \\int_{a}^{b} \\left| f(z(t)) \\right| \\left| z\u0026rsquo;(t) \\right| dt \\\\ =\u0026amp; \\int_{a}^{b} M \\left| z\u0026rsquo;(t) \\right| dt \\\\ =\u0026amp; ML \\end{align*} $$\n■\n説明 ML補題では、MはMaximum、LはLengthを意味する。\nML補題を使用する際に混乱しやすいのが、どのように$L$を選ぶかである。元の積分で$\\mathscr{C}$が半径$r$の円として与えられ、置換すると積分区間が$[0,2\\pi]$になることが多い。置換してからML補題を用いる場合、$L$は元の円の周囲である$2\\pi r$ではなく、置換後の積分区間の長さである$2 \\pi$を使用する必要がある。つまり、置換によって新たに生じた曲線（線分）である$\\mathscr{C} ' $に対してML補題を適用する必要があるという事実を忘れないように注意が必要である。\nOsborne (1999). Complex variables and their applications: p76.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":162,"permalink":"https://freshrimpsushi.github.io/jp/posts/162/","tags":null,"title":"ML補助定理の証明"},{"categories":"교과과정","contents":"概要 直角三角形において、斜辺の長さを$c$、残りの二辺の長さを$a,b$とすると、以下の式が成立する。 $$ a^2 + b^2 = c^2 $$\n解説 あちこちで使われるのは二の次で、それ自体が非常に実用的な定理だ。最古の「証明」を残したのがピタゴラスだからその名前がついているが、実際に文明を成したと言える古代人たちのほとんどが、ファクト自体は知っていたと推測される。\nピタゴラスの定理には、知られているだけで400種類以上の証明がある。その中から、最古の定理、即ちピタゴラス本人が残した証明を学んでみよう。\n証明 外側の正方形の一辺の長さは$(a+b)$で、内側の正方形の一辺の長さは$c$である。外側の正方形の面積は$(a+b)^2 = a^2 + 2ab + b^2$である。それぞれの頂点を持つ直角三角形の面積は$\\displaystyle {ab \\over 2}$である。従って、内側の正方形の面積は $$ (a+b)^2 - 4{ab \\over 2} = a^2 + 2ab + b^2 - 2ab $$ と言える。一方、内側の正方形の面積は$c^2$でもあるため、 $$ a^2 + b^2 = c^2 $$\n■\n誰かがこの証明を「見てみろ」と要約できると言った。それくらい直感的で簡単なので、しっかりと見て、忘れないようにしよう。\n","id":161,"permalink":"https://freshrimpsushi.github.io/jp/posts/161/","tags":null,"title":"ピタゴラスの定理の証明"},{"categories":"고전역학","contents":"極座標系での速度と加速度 $$ \\begin{align*} \\mathbf{v}\u0026amp;=\\dot{r} \\hat{\\mathbf{r}} + r \\dot{\\theta} \\hat{\\boldsymbol{\\theta}} \\\\ \\mathbf{a}\u0026amp;= (\\ddot r -r\\dot{\\theta} ^2)\\hat{\\mathbf{r}} + (2\\dot{r} \\dot{\\theta} + r\\ddot{\\theta})\\hat{\\boldsymbol{\\theta}} \\end{align*} $$\n導出 極座標系では、単位ベクトルは以下のようである。\n$$ \\begin{align*} \u0026amp;\u0026amp; \\mathbf{r}\u0026amp;=r\\hat{\\mathbf{r}}=x\\hat{\\mathbf{x}} + y \\hat{\\mathbf{y}} \\\\ \\implies \u0026amp;\u0026amp; \\hat{\\mathbf{r}} \u0026amp;= \\frac{x}{r}\\hat{\\mathbf{x}} +\\frac{y}{r} \\hat{\\mathbf{y}}=\\cos\\theta \\hat{\\mathbf{x}} + \\sin\\theta \\hat{\\mathbf{y}} = \\hat{\\mathbf{r}} (\\theta) \\\\ {} \\\\ \u0026amp;\u0026amp; \\hat \\theta \u0026amp;= \\hat{\\mathbf{r}}(\\theta+\\pi/2)= -\\sin\\theta \\hat{\\mathbf{x}} + \\cos\\theta \\hat{\\mathbf{y}} \\end{align*} $$\n速度は位置を時間で微分して、加速度は速度を時間で微分して求められる。ちなみに$\\dot{r}$は「アルドット」と読む。物理学で文字の上の点は時間に関する微分を意味する。\n$$ \\dot{r}=\\frac{dr}{dt} $$\n速度 $\\mathbf{r}$を$t$に関して微分すると次のようになる。\n$$ \\mathbf{v}=\\frac{d \\mathbf{r}}{dt}=\\frac{d}{dt}(r \\hat{\\mathbf{r}})=\\frac{d r}{dt}\\hat{\\mathbf{r}} + r\\frac{d \\hat{\\mathbf{r}}}{dt} =\\dot{r} \\hat{\\mathbf{r}} +r \\dot{\\hat{\\mathbf{r}}} $$\n$\\dot{\\hat{\\mathbf{r}}}$を計算しよう。$\\hat{\\mathbf{x}}$と$\\hat{\\mathbf{y}}$は時間に関して変わらないので$\\dfrac{d \\hat{\\mathbf{x}}}{dt}=0$が成り立つ。したがって次のようになる。\n$$ \\begin{align*} \\dot{\\hat{\\mathbf{r}}} = \\frac{d}{dt}(\\hat{\\mathbf{r}}) \u0026amp;= \\frac{d}{dt}(\\cos\\theta \\hat{\\mathbf{x}}) + \\frac{d}{dt}(\\sin\\theta \\hat{\\mathbf{y}}) \\\\ \u0026amp;= \\frac{\\cos\\theta}{dt}\\hat{\\mathbf{x}} + \\frac{\\sin\\theta}{dt}\\hat{\\mathbf{y}} \\\\ \u0026amp;= \\frac{\\cos\\theta}{d \\theta}\\frac{d \\theta}{dt}\\hat{\\mathbf{x}}+\\frac{\\sin\\theta}{d \\theta}\\frac{d \\theta}{dt}\\hat{\\mathbf{y}} \\\\ \u0026amp;= -\\sin\\theta \\frac{d \\theta}{dt}\\hat{\\mathbf{x}}+\\cos\\theta \\frac{d \\theta}{dt}\\hat{\\mathbf{y}} \\\\ \u0026amp;= \\frac{d \\theta }{dt}(-\\sin\\theta \\hat{\\mathbf{x}}+\\cos\\theta \\hat{\\mathbf{y}}) \\\\ \u0026amp;= \\dot{\\theta} \\hat{\\boldsymbol{\\theta}} \\end{align*} $$ したがって、極座標系での速度は次のようになる。\n$$ \\mathbf{v}=\\dot{r} \\hat{\\mathbf{r}} + r \\dot{\\theta} \\hat{\\boldsymbol{\\theta}} $$\n■\n加速度 $\\mathbf{v}$を$t$に関して微分すると次のようになる。\n$$ \\mathbf{a} = \\dfrac{d\\mathbf{v}}{dt} = \\dfrac{d(\\dot{r} \\hat{\\mathbf{r}} + r \\dot{\\theta} \\hat{\\boldsymbol{\\theta}})}{dt} = \\ddot{r} \\hat{\\mathbf{r}} + \\dot{r} \\dot{\\hat{\\mathbf{r}}} + \\dot{r} \\dot{\\theta} \\hat{\\boldsymbol{\\theta}} + r \\ddot{\\theta} \\hat{\\boldsymbol{\\theta}} + r \\dot{\\theta} \\dot{\\hat{\\boldsymbol{\\theta}}} $$\n$\\dot{ \\hat{\\boldsymbol{\\theta}}}$を計算すると以下のようになる。\n$$ \\begin{align*} \\dot{ \\hat{\\boldsymbol{\\theta}}} = \\frac{d}{dt}(\\hat{\\boldsymbol{\\theta}}) \u0026amp;= \\frac{d}{dt}(-\\sin\\theta \\hat{\\mathbf{x}})+\\frac{d}{dt}(\\cos\\theta \\hat{\\mathbf{y}}) \\\\ \u0026amp;= -\\frac{d \\sin\\theta}{dt}\\hat{\\mathbf{x}} +\\frac{d\\cos\\theta}{dt}\\hat{\\mathbf{y}} \\\\ \u0026amp;= -\\frac{d\\sin\\theta}{d \\theta}\\frac{d \\theta}{dt}\\hat{\\mathbf{x}}+\\frac{d\\cos\\theta}{d \\theta}\\frac{d \\theta}{dt}\\hat{\\mathbf{y}} \\\\ \u0026amp;= \\dfrac{d\\theta}{dt} (-\\cos\\theta \\hat{\\mathbf{x}}-\\sin\\theta \\hat{\\mathbf{y}}) \\\\ \u0026amp;= - \\dot{\\theta} \\hat{\\mathbf{r}} \\end{align*} $$\n$\\dot{\\hat{\\mathbf{r}}}$は速度を求める時に計算したので、代入して整理すると次の結果を得る。\n$$ \\begin{align*} \\mathbf{a} \u0026amp;= \\ddot r \\hat{\\mathbf{r}} +\\dot{r} \\dot{ \\hat{\\mathbf{r}}} + \\dot{r} \\dot{\\theta} \\hat{\\boldsymbol{\\theta}} + r \\ddot{\\theta} \\hat{\\boldsymbol{\\theta}} + r \\dot{\\theta} \\dot{ \\hat{\\boldsymbol{\\theta}}} \\\\ \u0026amp;= \\ddot r \\hat{\\mathbf{r}} +\\dot{r} \\dot{\\theta} \\hat{\\theta} + \\dot{r} \\dot{\\theta} \\hat{\\boldsymbol{\\theta}} + r \\ddot{\\theta} \\hat{\\boldsymbol{\\theta}} -r \\dot{\\theta} \\dot{\\theta} \\hat{\\mathbf{r}} \\\\ \u0026amp;= (\\ddot r -r\\dot{\\theta} ^2)\\hat{\\mathbf{r}} + (2\\dot{r} \\dot{\\theta} + r\\ddot{\\theta})\\hat{\\boldsymbol{\\theta}} \\end{align*} $$\n■\n参照 直交座標系での速度と加速度 円筒座標系での速度と加速度 球座標系での速度と加速度 ","id":158,"permalink":"https://freshrimpsushi.github.io/jp/posts/158/","tags":null,"title":"極座標系における速度と加速度"},{"categories":"복소해석","contents":"定義 1 複素関数としての双曲関数 $\\sinh, \\cosh : \\mathbb{C} \\to \\mathbb{C}$ を下記のように定義する。 $$ \\sinh z := { {e^{z} - e^{-z}} \\over 2 } \\\\ \\cosh z := { {e^{z} + e^{-z}} \\over 2 } $$\n定理 2 $$ \\begin{align*} \\sinh (iz) =\u0026amp; i \\sin z \\\\ \\sin (iz) =\u0026amp; i \\sinh z \\\\ \\cosh (iz) =\u0026amp; \\cos z \\\\ \\cos (iz) =\u0026amp; \\cosh z \\end{align*} $$\n説明 双曲関数に初めて接する時、最も理解できないのが「なぜこんな定義を使うのか」という点だ。実数上で三角関数は単位円の三角比で定義され、双曲関数は指数関数の線形組み合わせで表される。しかし、定義だけを見ても、なぜ双曲関数を三角関数の一種と呼ぶのか納得が難しい。複素数上でこれらの関数を見ると、この体系がいかによく組み立てられ、直感的か理解できる。\n上記の性質は、三角関数で元々使われていた性質ともある程度文脈を共有している。\nサイン関数は奇関数、コサイン関数は偶関数 $$ \\sin (-\\theta) = - \\sin \\theta \\\\ \\cos (-\\theta) = \\cos \\theta $$\n$-1$ が$\\sin$ の内外を自由に行き来し、$\\cos$ に影響を与えないように、$i$ も$\\sin$ と $\\sinh$ の内外を自由に行き来し、$\\cos$ と$\\cosh$ に影響を与えない。違いは $\\sin$ であれ $\\cos$ であれ $\\text{h}$ の存在が反転することだ。複素数が正でも負でもない $i$ と $-i$ の世界を作り出したと考えれば、三角関数も $\\sin$ か $\\cos$ を超えて $\\sinh$ と $\\cosh$ が必要なのだと気づくだろう。\n双曲関数の周期性 $$ \\sinh (ix) = i \\sin x \\\\ \\cosh (ix) = \\cos x $$\n一方で、三角関数と双曲関数の関係から、純虚数で双曲関数が周期性を持つことを容易に確認できる。少し考えればすぐにわかるが、その性質に慣れていない時は自分一人で気づくのは難しい。\n証明 複素解析における三角関数: $$ \\sin z = { {e^{iz} - e^{-iz}} \\over 2 i } \\\\ \\cos z = { {e^{iz} + e^{-iz}} \\over 2 } $$\n$$ \\sinh (iz) = { { e^{iz} - e^{-iz} } \\over 2 } = i { { e^{iz} - e^{-iz} } \\over {2 i} } = i \\sin z $$\n$$ \\sin (iz) = { {e^{iiz} - e^{-iiz}} \\over 2 i } = - i { {e^{-z} - e^{z}} \\over 2 } = i \\sinh z $$\n$$ \\cosh (iz) = { { e^{iz} + e^{-iz} } \\over 2 } = \\cos z $$\n$$ \\cos (iz) = { { e^{iiz} + e^{-iiz} } \\over 2 } = { { e^{-z} + e^{z} } \\over 2 } = \\cosh z $$\n■\nOsborne (1999). Complex variables and their applications: p27.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOsborne (1999). Complex variables and their applications: p29.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":157,"permalink":"https://freshrimpsushi.github.io/jp/posts/157/","tags":null,"title":"複素解析における三角関数と双曲線関数の関係"},{"categories":"수리물리","contents":"球面座標系の単位ベクトル $$ \\begin{align*} \\hat{\\mathbf{r}} \u0026amp;= \\cos\\phi \\sin\\theta\\hat{\\mathbf{x}} + \\sin\\phi \\sin\\theta\\hat{\\mathbf{y}} + \\cos\\theta\\hat{\\mathbf{z}} \\\\ \\hat{\\boldsymbol{\\theta}} \u0026amp;= \\cos\\phi \\cos\\theta \\hat{\\mathbf{x}} + \\sin\\phi \\cos\\theta \\hat{\\mathbf{y}} - \\sin\\theta\\hat{\\mathbf{z}} \\\\ \\hat{\\boldsymbol{\\phi}} \u0026amp;= -\\sin\\phi \\hat{\\mathbf{x}} + \\cos\\phi \\hat{\\mathbf{y}} \\end{align*} $$\n導出 まず、$\\hat{\\mathbf{r}}$を計算してから、残りの二つを求める。\n半径方向の単位ベクトル $\\hat{\\mathbf{r}}$ $$ \\hat{\\mathbf{r}}=r\\hat{\\mathbf{r}}=x\\hat{\\mathbf{x}}+y\\hat{\\mathbf{y}}+z\\hat{\\mathbf{z}} $$\nしたがって、両辺を$r$で割ると、\n$$ \\begin{align*} \\hat{\\mathbf{r}}\u0026amp;=\\frac{x}{r}\\hat{\\mathbf{x}}+\\frac{y}{r}\\hat{\\mathbf{y}}+\\frac{z}{r}\\hat{\\mathbf{z}} \\\\ \u0026amp;= \\frac{x}{r \\sin\\theta}\\sin\\theta\\hat{\\mathbf{x}}+\\frac{y}{r \\sin\\theta}\\sin\\theta\\hat{\\mathbf{y}}+\\cos\\theta\\hat{\\mathbf{z}} \\\\ \u0026amp;= \\cos\\phi \\sin\\theta \\hat{\\mathbf{x}} + \\sin\\phi \\sin\\theta\\hat{\\mathbf{y}} + \\cos\\theta\\hat{\\mathbf{z}} =\\hat{\\mathbf{r}}(\\theta,\\phi) \\end{align*} $$\n極角方向の単位ベクトル $\\hat{\\boldsymbol{\\theta}}$ $\\hat{\\boldsymbol{\\theta}}$は、$\\hat{\\mathbf{r}}$方向から$\\phi$はそのままで$\\theta$だけ$\\dfrac{\\pi}{2}$増加するから、以下のようになる。\n$$ \\begin{align*} \\hat{\\boldsymbol{\\theta}} \u0026amp;= \\hat{\\mathbf{r}} \\left(\\theta+\\dfrac{\\pi}{2}, \\phi \\right) \\\\ \u0026amp;= \\cos\\phi \\sin\\left(\\theta+\\dfrac{\\pi}{2}\\right) \\hat{\\mathbf{x}} + \\sin\\phi \\sin\\left(\\theta+\\dfrac{\\pi}{2}\\right)\\hat{\\mathbf{y}} + \\cos\\left(\\theta+\\dfrac{\\pi}{2}\\right)\\hat{\\mathbf{z}} \\\\ \u0026amp;= \\cos\\phi \\cos\\theta \\hat{\\mathbf{x}} + \\sin\\phi \\cos\\theta\\hat{\\mathbf{y}} - \\sin\\theta\\hat{\\mathbf{z}} \\end{align*} $$\n方位角方向の単位ベクトル $\\hat{\\boldsymbol{\\phi}}$ $\\hat{\\boldsymbol{\\phi}}=\\hat{\\mathbf{r}} \\times \\hat{\\boldsymbol{\\theta}}$であるから、次のようになる。\n$$ \\begin{align*} \\hat{\\boldsymbol{\\phi}} \u0026amp;= \\begin{vmatrix} \\hat{\\mathbf{x}} \u0026amp; \\hat{\\mathbf{y}} \u0026amp; \\hat{\\mathbf{z}} \\\\ \\cos\\phi \\sin\\theta \u0026amp; \\sin\\phi \\sin\\theta\\ \u0026amp; \\cos\\theta \\\\ \\cos\\phi \\cos\\theta \u0026amp; \\sin\\phi \\cos\\theta \u0026amp; -\\sin\\theta \\end{vmatrix} \\\\ \u0026amp;= (-\\sin\\phi \\sin^2\\theta-\\sin\\phi \\cos^2\\theta)\\hat{\\mathbf{x}}+ (\\cos\\phi \\cos^2\\theta + \\cos\\phi \\sin^2\\theta)\\hat{\\mathbf{y}} \\\\ \u0026amp;\\quad +(\\cos\\phi \\sin\\theta \\sin\\phi \\cos\\theta -\\cos\\phi \\sin\\theta \\sin\\phi \\cos\\theta)\\hat{\\mathbf{z}} \\\\ \u0026amp;= -\\sin\\phi (\\sin^2\\theta + \\cos^2\\theta) \\hat{\\mathbf{x}} + \\cos\\phi (\\cos^2 \\theta + \\sin^2\\theta) \\hat{\\mathbf{y}} \\\\ \u0026amp;= -\\sin\\phi \\hat{\\mathbf{x}} + \\cos\\phi \\hat{\\mathbf{y}} \\end{align*} $$\nまた、このように考えることもできる。$\\hat{\\boldsymbol{\\phi}}$の方向を決める際に、$\\theta$は影響を与えない。$\\theta$の値に関わらず、唯一$r$と$\\phi$の値に基づいて方向が決定される。また、$\\hat{\\boldsymbol{\\phi}}$の方向は、$\\hat{\\mathbf{r}}$方向から$\\phi$が$\\dfrac{\\pi}{2}$増加したものである。したがって、$\\hat{\\mathbf{r}}$から$\\theta$項が消え、$\\phi$の代わりに$\\phi + \\dfrac{\\pi}{2}$を代入した形になる。\n$$ \\begin{align*} \\hat{\\boldsymbol{\\phi}} \u0026amp;= \\cos{(\\phi+\\dfrac{\\pi}{2} )}\\hat{\\mathbf{x}} + \\sin{(\\phi + \\dfrac{\\pi}{2})}\\hat{\\mathbf{y}} \\\\ \u0026amp;= -\\sin \\phi \\hat{\\mathbf{x}}+ \\cos \\phi \\hat{\\mathbf{y}} \\end{align*} $$\n■\n","id":152,"permalink":"https://freshrimpsushi.github.io/jp/posts/152/","tags":null,"title":"直交座標系の単位ベクトルで表された球面座標系の単位ベクトル"},{"categories":"수리물리","contents":"定義 次の式をスカラー三重積scalar triple productと言う。\n$$ \\mathbf{A}\\cdot (\\mathbf{B} \\times \\mathbf{C} ) $$\n説明 スカラー三重積は、3つのベクトルを掛け合わせる操作で、結果がスカラーになるものを指す。結果がベクトルになるものは、ベクトル三重積と言う。結果をスカラーにするためには、最初に2つのベクトルの外積を行い、その結果得られたベクトルと別のベクトルを内積しなければならない。\n下記の交換可能な性質により、次のような表記も使用され、これをグラスマン記号1と言う。\n$$ \\mathbf{A} \\cdot (\\mathbf{B} \\times \\mathbf{C} ) = [\\mathbf{A}, \\mathbf{B}, \\mathbf{C}] = [\\mathbf{A} \\mathbf{B} \\mathbf{C}] $$\n平行六面体 スカラー三重積の大きさは、3つのベクトルが作る平行六面体の体積と同じである。\n$$ \\mathbf{A}\\cdot (\\mathbf{B} \\times \\mathbf{C} )=|\\mathbf{A}||\\mathbf{B}\\times \\mathbf{C}| \\cos\\theta $$\n$ |\\mathbf{B} \\times \\mathbf{C}|$は平行六面体の底面の面積、$|\\mathbf{A} \\cos\\theta|$は高さである。つまり、スカラー三重積は底面の面積と高さの積なので、六面体の体積である。\nこの特徴を詳しく見ると、どのような順序で操作しても同じ値が出なければならないことがわかる。なぜなら、3つのベクトルが作る平行六面体は一意だからである。したがって、次の式が成立する。\n交換可能 スカラー三重積の値は、循環形式で交換可能である。\n$$ \\mathbf{A}\\cdot (\\mathbf{B} \\times \\mathbf{C} ) = \\mathbf{B}\\cdot (\\mathbf{C} \\times \\mathbf{A} ) =\\mathbf{C}\\cdot (\\mathbf{A} \\times \\mathbf{B} ) $$\nレヴィ-チヴィタ記号を使って、簡単に証明できる。\n$$ \\mathbf{A} \\cdot (\\mathbf{B} \\times \\mathbf{C} ) = A_{i} (B \\times C)_{i} =A_{i} \\epsilon_{ijk} B_{j}C_{k} =\\epsilon_{ijk}A_{i}B_{j}C_{k} $$\n$$ \\mathbf{B} \\cdot (\\mathbf{C} \\times \\mathbf{A} ) = B_{i} (C \\times A)_{i} =B_{i} \\epsilon_{ijk} C_{j}A_{k} =\\epsilon_{ijk}B_{i}C_{j}A_{k} $$\n$$ \\mathbf{C} \\cdot (\\mathbf{A} \\times \\mathbf{B} ) = C_{i} (A \\times B)_{i} =C_{i} \\epsilon_{ijk} A_{j}B_{k} =\\epsilon_{ijk}C_{i}A_{j}B_{k} $$\nレヴィ-チヴィタ記号の性質により、上記の三つの式が同じ値であることがわかる。ABCでも、BCAでも、CABでも、順序が正しければどう計算しても同じ値が得られる。逆に言えば、順序が違うと異なる値になる。操作に含まれる外積の結果がベクトルであるため、方向が重要だからである。\n$$ \\mathbf{A}\\cdot (\\mathbf{B} \\times \\mathbf{C} ) \\neq \\mathbf{A}\\cdot (\\mathbf{C} \\times \\mathbf{B} ) $$\n$$ \\mathbf{A}\\cdot (\\mathbf{B} \\times \\mathbf{C} ) \\neq \\mathbf{B}\\cdot (\\mathbf{A} \\times \\mathbf{C} ) $$\nスカラー三重積は行列式の形で表現することもできる。直交座標系の場合、以下のようになる。\n$$ \\mathbf{A}\\cdot (\\mathbf{B} \\times \\mathbf{C} ) = \\epsilon_{ijk}A_{i}B_{j}C_{k}=\\begin{vmatrix} A_{i} \u0026amp; A_{j} \u0026amp; A_{k} \\\\ B_{i}\u0026amp;B_{j}\u0026amp;B_{k} \\\\ C_{i}\u0026amp;C_{j}\u0026amp;C_{k} \\end{vmatrix}=\\begin{vmatrix} A_{x} \u0026amp; A_{y} \u0026amp; A_{z} \\\\ B_{x}\u0026amp;B_{y}\u0026amp;B_{z} \\\\ C_{x}\u0026amp;C_{y}\u0026amp;C_{z} \\end{vmatrix} $$\n三重積 -ウィキペディア\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":144,"permalink":"https://freshrimpsushi.github.io/jp/posts/144/","tags":null,"title":"スカラー三重積"},{"categories":"수리물리","contents":"定義1 原点から観察点までのベクトルを分離ベクトルseparation vectorという。\n$$ \\bcR = \\mathbf{r} - \\mathbf{r}^{\\prime} $$\n説明 原点ベクトルsource vector $\\mathbf{r}^{\\prime}$: 電荷や電流が存在する場所。つまり、電磁場を生成する起源の座標を表すベクトル。 位置ベクトルposition vector $\\mathbf{r}$: 電場 $\\mathbf{E}$や磁場 $\\mathbf{B}$を測定する場所の座標を表すベクトル。 分離ベクトル $\\bcR$: 位置ベクトルと原点ベクトル（起源ベクトル）の差。 分離ベクトルの表示には標準がなく、ばらばらである。記号を特に定めずに$\\mathbf{r} - \\mathbf{r}^{\\prime}$と書く場合もある。エビ寿司屋では、グリフィスの電磁気学と同様に、筆記体$r$(Kaufmannフォント) $\\bcR$で表示する。その他に使用される文字には、ギリシャ文字のエータ$\\eta$などがある。分離ベクトルの大きさと単位ベクトルは次のとおりである。\n$$ \\left| \\bcR \\right| = \\cR = \\left| \\mathbf{r} - \\mathbf{r}^{\\prime} \\right| $$\n$$ \\crH = \\dfrac{\\bcR}{\\cR} = \\dfrac{\\mathbf{r} - \\mathbf{r}^{\\prime}}{ \\left| \\mathbf{r} - \\mathbf{r}^{\\prime} \\right|} $$\n直交座標系では、以下のようになる。\n$$ \\bcR = (x-x^{\\prime})\\hat {\\mathbf{x}} + (y-y^{\\prime})\\hat{\\mathbf{y}} + (z-z^{\\prime})\\hat{\\mathbf{z}} $$ $$ \\cR = \\sqrt{ (x-x^{\\prime})^2 + (y-y^{\\prime})^2 + (z-z^{\\prime})^2 } $$ $$ \\crH = \\dfrac{ (x-x^{\\prime})\\hat {\\mathbf{x}} + (y-y^{\\prime})\\hat{\\mathbf{y}} + (z-z^{\\prime})\\hat{\\mathbf{z}}}{\\sqrt{ (x-x^{\\prime})^2 + (y-y^{\\prime})^2 + (z-z^{\\prime})^2 }} $$\n例 原点(2,8,7)から観察点(4,6,8)までの分離ベクトル$\\bcR$を求めなさい。また、その大きさと単位ベクトルを求めなさい。\n$$ \\bcR=(4,6,8)-(2,8,7)=(2,-2,1)=2\\hat{\\mathbf{x}} -2\\hat{\\mathbf{y}}+\\hat{\\mathbf{z}} $$\n$$ \\cR=\\sqrt{ 2^2+ (-2)^2+1^2}=\\sqrt{4+4+1}=\\sqrt{9}=3 $$\n$$ \\crH=\\left( \\dfrac{2}{3}, - \\dfrac{2}{3},\\dfrac{1}{3} \\right) = \\dfrac{2}{3}\\hat{\\mathbf{x}} -\\dfrac{2}{3}\\hat{\\mathbf{y}}+\\dfrac{1}{3}\\hat{\\mathbf{z}} $$\nデイビッド・J・グリフィス, 『基礎電磁気学』（金進丞 訳）(第4版). 2014, p9-10\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":141,"permalink":"https://freshrimpsushi.github.io/jp/posts/141/","tags":null,"title":"分離ベクトル"},{"categories":"수리물리","contents":"数式 分離ベクトル $\\bcR$の大きさの$n$乗、$\\cR ^{n}$の勾配は以下の通りだ。\n$$ \\nabla (\\cR^n)=n\\cR^{n-1}\\crH $$\n説明 多項関数の微分と同じやり方で計算した後に、単位ベクトル $\\crH$を付けるだけだ。\n分離ベクトルは $\\bcR=\\mathbf{r}-\\mathbf{r}^{\\prime}$なので、$(x,y,z)$と$(x^{\\prime},y^{\\prime},z^{\\prime})$を変数に持つ。だから、微分するときはこれに注意が必要だ。上付き標数がある座標とない座標に対する勾配を以下のように表す。\n$$ \\begin{align*} \\nabla f\u0026amp;= \\dfrac{\\partial f}{\\partial x}\\hat {\\mathbf{x}} + \\dfrac{\\partial f}{\\partial y} \\hat{\\mathbf{y}} + \\dfrac{\\partial f} {\\partial z} \\hat{\\mathbf{z}} \\\\ \\nabla^{\\prime} f\u0026amp;= \\dfrac{\\partial f}{\\partial x^{\\prime}}\\hat {\\mathbf{x}} + \\dfrac{\\partial f}{\\partial y^{\\prime}} \\hat{\\mathbf{y}} + \\dfrac{\\partial f} {\\partial z^{\\prime}} \\hat{\\mathbf{z}} \\end{align*} $$\n直交座標系では、分離ベクトルは以下の通りだ。\n$$ \\begin{align*} \\bcR \u0026amp;= (x-x^{\\prime})\\hat {\\mathbf{x}} + (y-y^{\\prime})\\hat{\\mathbf{y}} + (z-z^{\\prime})\\hat{\\mathbf{z}} \\\\ \\cR \u0026amp;= \\sqrt{ (x-x^{\\prime})^{2} + (y-y^{\\prime})^{2} + (z-z^{\\prime})^{2} } \\\\ \\crH \u0026amp;= \\dfrac{ (x-x^{\\prime})\\hat {\\mathbf{x}} + (y-y^{\\prime})\\hat{\\mathbf{y}} + (z-z^{\\prime})\\hat{\\mathbf{z}}}{\\sqrt{ (x-x^{\\prime})^{2} + (y-y^{\\prime})^{2} + (z-z^{\\prime})^{2} }} \\end{align*} $$\n$n=2$、$n=-1$の場合の結果を先に見て、一般的な場合について証明する。式が長すぎる場合は、同じ部分を赤い角括弧${\\color{red}[ \\ \\ ]}$で示して省略した。\n証明 $\\nabla \\cR^{2} = 2\\bcR=2\\cR\\crH$ $$ \\begin{align*} \\nabla(\\cR ^{2}) =\u0026amp;\\ \\frac{\\partial }{\\partial x} {\\color{red} \\left[ (x-x^{\\prime})^{2}+(y-y^{\\prime})^{2}+(z-z^{\\prime})^{2} \\right]} \\hat{\\mathbf{x}} +\\frac{\\partial }{\\partial y}{\\color{red}[ \\ \\ ]}\\hat{\\mathbf{y}} +\\frac{\\partial }{\\partial z}{\\color{red}[ \\ \\ ]}\\hat{\\mathbf{z}} \\\\ =\u0026amp;\\ 2(x-x^{\\prime})\\hat{\\mathbf{x}}+2(y-y^{\\prime})\\hat{\\mathbf{y}}+2(z-z^{\\prime})\\hat{\\mathbf{z}} \\\\ =\u0026amp;\\ 2 \\left( (x-x^{\\prime})\\hat {\\mathbf{x}} + (y-y^{\\prime})\\hat{\\mathbf{y}} + (z-z^{\\prime})\\hat{\\mathbf{z}} \\right) \\\\ =\u0026amp;\\ 2\\bcR \\\\ =\u0026amp;\\ 2\\cR\\crH \\end{align*} $$\n■\n$\\nabla \\dfrac{1}{\\cR} = -\\dfrac{1}{\\cR^{2}}\\crH$ $$ \\begin{align*} \\nabla \\dfrac{1}{\\cR} \u0026amp;= \\dfrac{\\partial }{\\partial x} {\\color{red} \\left[ (x-x^{\\prime})^{2}+(y-y^{\\prime})^{2}+(z-z^{\\prime})^{2} \\right]}^{-\\frac{1}{2}} \\hat{\\mathbf{x}} +\\dfrac{\\partial }{\\partial y}{\\color{red}[ \\ \\ ]}^{-\\frac{1}{2}} \\hat{\\mathbf{y}} +\\dfrac{\\partial }{\\partial z}{\\color{red}[ \\ \\ ]}^{-\\frac{1}{2}} \\hat{\\mathbf{z}} \\\\ \u0026amp;= -\\frac{1}{2}\\dfrac{2(x-x^{\\prime})}{ {\\color{red} \\left[(x-x^{\\prime})^{2}+(y-y^{\\prime})^{2}+(z-z^{\\prime})^{2} \\right]}^{\\frac{3}{2}} }\\hat{\\mathbf{x}} - \\frac{1}{2}\\dfrac{2(y-y^{\\prime})}{ {\\color{red}[ \\ \\ ]}^{\\frac{3}{2}} } \\hat{\\mathbf{y}} -\\frac{1}{2}\\dfrac{2(z-z^{\\prime})}{ {\\color{red}[ \\ \\ ]}^{\\frac{3}{2}} } \\\\ \u0026amp;= -\\dfrac{1}{ {\\color{red} \\left[(x-x^{\\prime})^{2}+(y-y^{\\prime})^{2}+(z-z^{\\prime})^{2} \\right]} } \\left[ \\dfrac{(x-x^{\\prime})}{ {\\color{red}[ \\ \\ ]}^{\\frac{1}{2}} } \\hat{\\mathbf{x}} + \\dfrac{(y-y^{\\prime})}{ {\\color{red}[ \\ \\ ]}^{\\frac{1}{2}} } \\hat{\\mathbf{y}} + \\dfrac{(z-z^{\\prime})}{ {\\color{red}[ \\ \\ ]}^{\\frac{1}{2}} } \\right] \\\\ \u0026amp;= -\\dfrac{1}{ {\\color{red}\\cR^{2}} } \\left[ \\dfrac{(x-x^{\\prime})}{ {\\color{red} \\left[ (x-x^{\\prime})^{2}+(y-y^{\\prime})^{2}+(z-z^{\\prime})^{2} \\right]} ^{\\frac{1}{2}}} \\hat{\\mathbf{x}} + \\dfrac{(y-y^{\\prime})}{ {\\color{red}[ \\ \\ ]}^{\\frac{1}{2}}} \\hat{\\mathbf{y}} +\\dfrac{(z-z^{\\prime})}{ {\\color{red}[ \\ \\ ]}^{\\frac{1}{2}}} \\hat{\\mathbf{z}} \\right] \\\\ \u0026amp;= -\\dfrac{1}{\\cR^{2}} \\dfrac{ (x-x^{\\prime})\\hat{\\mathbf{x}} + (y-y^{\\prime})\\hat{\\mathbf{y}} +(z-z^{\\prime})\\hat{\\mathbf{z}}}{\\sqrt{(x-x^{\\prime})^{2}+(y-y^{\\prime})^{2}+(z-z^{\\prime})^{2}}} \\\\ \u0026amp;= -\\dfrac{1}{\\cR^{2}}\\crH \\end{align*} $$ ■\n$\\nabla (\\cR^n)=n\\cR^{n-1}\\crH $ $$ \\begin{align*} \\nabla (\\cR^n) \u0026amp;= \\frac{\\partial}{\\partial x}(\\cR^n)\\hat{\\mathbf{x}}+ \\frac{\\partial}{\\partial y}(\\cR^n)\\hat{\\mathbf{y}}+\\frac{\\partial}{\\partial z}(\\cR^n)\\hat{\\mathbf{z}} \\\\ \u0026amp;= \\frac{\\partial}{\\partial \\cR}(\\cR^n)\\frac{\\partial \\cR}{\\partial x}\\hat{\\mathbf{x}}+ \\frac{\\partial}{\\partial \\cR}(\\cR^n)\\frac{\\partial \\cR}{\\partial y}\\hat{\\mathbf{y}}+\\frac{\\partial}{\\partial \\cR}(\\cR^n)\\frac{\\partial \\cR}{\\partial z}\\hat{\\mathbf{z}} \\end{align*} $$\n2番目の等号は連鎖律によって成り立つ。この時、以下の式が成り立つ。\n$$ \\begin{align*} \\frac{\\partial \\cR}{\\partial x} \u0026amp;=\\frac{\\partial }{\\partial x}[(x-x^{\\prime})^{2} + (y-y^{\\prime})^{2} +(z-z^{\\prime})^{2})]^{\\frac{1}{2}} \\\\ \u0026amp;= \\frac{1}{2}[2(x-x^{\\prime})][(x-x^{\\prime})^{2}+(y-y^{\\prime})^{2}+(z-z^{\\prime})^{2}]^{-\\frac{1}{2}} \\\\ \u0026amp;= \\frac{x-x^{\\prime}}{\\cR} \\end{align*} $$\n同様に$\\dfrac{\\partial \\cR}{\\partial y}= \\dfrac {y-y^{\\prime}}{\\cR}$、$\\dfrac{\\partial \\cR}{\\partial z} = \\dfrac {z-z^{\\prime}}{\\cR}$だ。従って、まとめると以下の通りだ。\n$$ \\begin{align*} \\nabla (\\cR^n) \u0026amp;= \\frac{\\partial}{\\partial \\cR}(\\cR^n)\\frac{x-x^{\\prime}}{\\cR}\\hat{\\mathbf{x}}+ \\frac{\\partial}{\\partial \\cR}(\\cR^n)\\frac{y-y^{\\prime}}{\\cR}\\hat{\\mathbf{y}}+\\frac{\\partial}{\\partial \\cR}(\\cR^n)\\frac{z-z^{\\prime}}{\\cR}\\hat{\\mathbf{z}} \\\\ \u0026amp;= \\frac{\\partial}{\\partial \\cR}(\\cR^n) \\left( \\frac{x-x^{\\prime}}{\\cR}\\hat{\\mathbf{x}}+ \\frac{y-y^{\\prime}}{\\cR}\\hat{\\mathbf{y}}+\\frac{z-z^{\\prime}}{\\cR}\\hat{\\mathbf{z}} \\right) \\\\ \u0026amp;= n\\cR^{n-1}\\crH \\end{align*} $$\n■\n","id":142,"permalink":"https://freshrimpsushi.github.io/jp/posts/142/","tags":null,"title":"分離ベクトルの大きさの勾配"},{"categories":"해석개론","contents":"まとめ オイラーの公式: $$ { e }^{ ix }= \\cos x + i \\sin x $$\nオイラーの等式: $$ { e }^{ i\\pi }+1=0 $$\n説明 オイラーの公式Euler\u0026rsquo;s Formulaは、それ自体の形がすごく奇妙で、オイラー自身もどこで使われるか分からなかったけど、今では多くの分野で使われていて、その有用性を要約するのが難しい程だ。虚数が学界でまだうまく受け入れられていなかった当時の発見だと考えれば、さらに驚くべきだ。導出自体は指数関数、サイン関数、コサイン関数のテイラー展開を通じて簡単にできる。\n$$ { { e ^ x } }=\\sum _{ n=0 }^{ \\infty }{ \\frac { { x } ^{ n } }{ n! } } $$\n$$ \\sin x=\\sum _{ n=0 }^{ \\infty }{ \\frac { { x } ^{ 2n+1 } }{ (2n+1)! }{ { (-1) }^{ n } } } $$\n$$ \\cos x=\\sum _{ n=0 }^{ \\infty }{ \\frac { { x } ^{ 2n } }{ (2n)! }{ { (-1) }^{ n } } } $$\n導出（オイラーの公式） $$ \\begin{align*} { e }^{ ix } =\u0026amp; \\sum _{ n=0 }^{ \\infty }{ \\frac { { (ix) } ^{ n } }{ n! } } \\\\ =\u0026amp;\\frac { { (ix) } ^{ 0 } }{ 0! }+\\frac { { (ix) } ^{ 1 } }{ 1! }+\\frac { { (ix) } ^{ 2 } }{ 2! }+\\frac { { (ix) } ^{ 3 } }{ 3! }+\\frac { { (ix) } ^{ 4 } }{ 4! }+ \\cdots \\\\ =\u0026amp;\\frac { 1 }{ 0! }+\\frac { ix }{ 1! }-\\frac { { x }^{ 2 } }{ 2! }-\\frac { i { x }^{ 3 } }{ 3! }+\\frac { { x } ^{ 4 } }{ 4! }+ \\cdots \\\\ =\u0026amp; \\left( \\frac { 1 }{ 0! } - \\frac { { x } ^{ 2 } }{ 2! }+\\frac { { x } ^{ 4 } }{ 4! }-\\frac { { x } ^{ 6 } }{ 6! }+\\cdots \\right) + i\\left( \\frac { x }{ 1! } - \\frac { { x } ^{ 3 } }{ 3! }+\\frac { { x } ^{ 5 } }{ 5! }-\\frac { { x } ^{ 7 } }{ 7! }+\\cdots \\right) \\\\ =\u0026amp; \\cos x + i \\sin x \\end{align*} $$\nしたがって、\n$$ { e }^{ ix }= \\cos x + i \\sin x $$\n■\n特に$x=\\pi$を代入すると、いわゆる\u0026rsquo;世界で最も美しい等式\u0026rsquo;であるオイラーの等式を得る。また、オイラーの等式をうまくいじると、虚数単位$i$の$i$乗、つまり$i^i$の値も求めることができる。驚くべきことに、その値は実数で、証明は次の通り。\n証明 $$ \\begin{align*} \u0026amp;\u0026amp; { e }^{ i\\pi }+1 =\u0026amp; 0 \\\\ \\implies \u0026amp;\u0026amp; { e }^{ i\\pi }=\u0026amp;-1 \\\\ \\implies \u0026amp;\u0026amp; { e }^{ \\frac { i\\pi }{ 2 } } =\u0026amp; \\sqrt { -1 } \\\\ \\implies \u0026amp;\u0026amp; { \\left( { e } ^{ \\frac { i\\pi }{ 2 } } \\right) }^{ i } =\u0026amp; { \\sqrt { -1 } }^{ i } \\\\ \\implies \u0026amp;\u0026amp; { e }^{ \\frac { i\\pi }{ 2 }i } =\u0026amp; { i } ^{ i } \\\\ \\implies \u0026amp;\u0026amp; { i }^{ i } =\u0026amp; { e }^{ -\\frac { \\pi }{ 2 } } \\\\ \\implies \u0026amp;\u0026amp; { i }^{ i } =\u0026amp; \\frac { 1 }{ \\sqrt { { e }^{ \\pi } } } \\end{align*} $$\n■\n","id":112,"permalink":"https://freshrimpsushi.github.io/jp/posts/112/","tags":null,"title":"微積分学におけるオイラーの公式"},{"categories":"정수론","contents":"定義 1 整数 $a \\equiv b \\pmod{m}$, $\\iff$, $a$, $b$, $m$ に対して、$a = b + mk$ を満たす整数 $k$ が存在する。\n定理 $a_{1} \\equiv b_{1} \\pmod{m}$ と $a_{2} \\equiv b_{2} \\pmod{m}$ が成り立つとしよう。\n[1] 加算: $a_{1} + a_{2} \\equiv b_{1} + b_{2} \\pmod{m}$ [2] 減算: $a_{1} - a_{2} \\equiv b_{1} - b_{2} \\pmod{m}$ [3] 乗算: $a_{1} a_{2} \\equiv b_{1} b_{2} \\pmod{m}$ [4] 除算: $\\gcd ( c , m ) = 1$ の場合、 $$ ac \\equiv bc \\pmod{m} \\implies a \\equiv b \\pmod{m} $$ [5] モジュロ同士の乗算: $\\gcd ( m_{1} , m_{2} ) = 1$ の場合、 $$ \\begin{cases} a \\equiv b \\pmod{ m_{1} } \\\\ a = b \\pmod{ m_{2} } \\end{cases} \\implies a \\equiv b \\pmod{ m_{1} m_{2} } $$ [6] モジュロのべき乗: $a \\equiv b \\pmod{m^n}$ の場合は $a \\equiv b \\pmod{m}$ [7] モジュロを含む約分: $ax \\equiv ay \\pmod{am} \\iff x \\equiv y \\pmod{m}$ 説明 式 $a \\equiv b \\pmod{m}$ を合同式と呼び、法（モジュロ）$m$ で $a$ は $b$ に合同であるという。ただし、方程式の中で $\\pmod{}$ がそのまま使われているので、モジュロ $m$ と発音する方が一般的だ。\n定理 [4]で、$\\gcd(c,m) = 1$ という条件がない場合、両辺から $c$ を割ることはできない。例えば $ 8 \\equiv 20 \\pmod{12}$ は成り立つが、両辺から $4$ を割った $ 2 \\equiv 5 \\pmod{12}$ は成り立たない。\n通常、簡単な整数論の問題を扱う場合は $m$ を素数とするが、複雑になると $m$ を合成数とする。したがって、合同方程式全体を大きく見て、その性質も理解しておく必要がある。\n証明 [5] $a \\equiv b \\pmod{m_{1}}$ であるから、次を満たす整数 $n_{1}$ が存在する。 $$ a = b + n_{1} m_{1} $$ $a \\equiv b \\pmod{m_{2}}$ であるから、次を満たす整数 $n_{2}$ が存在する。 $$ a = b + n_{2} m_{2} $$ $m_{1}$ と $m_{2}$ は互いに素であるため、二つの式が同時に成り立つには、$n_{1}$ は $m_{2}$ の倍数であり、$n_{2}$ は $m_{1}$ の倍数でなければならない。したがって、次を満たす整数 $n_{3}$ が存在する。 $$ a = b + n_{3} m_{1} m_{2} $$ すると、合同式の定義により $$ a \\equiv b \\pmod{ m_{1} m_{2} } $$\n■\n[6] $a \\equiv b \\pmod{m^n}$ の場合、次を満たす整数 $k$ が存在する。 $$ a = b + m^n k $$ $m^{n-1}$ を取り除くと、 $$ a = b + m^{n} k \\cdot n = b + m \\cdot (m^{n-1} k) $$ したがって、次を満たす整数 $m^{n-1} k$ が存在する。 $$ a \\equiv b \\pmod{m} $$\n■\n[7] ある整数 $k$ について、 $$ \\begin{align*} ax \\equiv ay \\pmod{am} \u0026amp; \\iff ax = ay + amk \\\\ \u0026amp; \\iff x = y + mk \\\\ \u0026amp; \\iff x \\equiv y \\pmod{m} \\end{align*} $$\n■\nSilverman. (2012). A Friendly Introduction to Number Theory (4th Edition): p55.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":106,"permalink":"https://freshrimpsushi.github.io/jp/posts/106/","tags":null,"title":"整数論における合同"},{"categories":"함수","contents":"定義 以下の二つの条件を満たす関数をディラックデルタ関数という。\n$$ \\delta (x) = \\begin{cases} 0, \u0026amp; x\\neq 0 \\\\ \\infty , \u0026amp; x=0 \\end{cases} $$\n$$ \\int_{-\\infty}^{\\infty}{\\delta (x) dx}=1 $$\n説明 ※クロネッカーデルタと間違えないように注意が必要だ。\n工学では、単位インパルス関数unit impulse functionと呼ばれる。正確に言うと、数学的にディラックデルタ関数は関数ではない。それは0で無限大に発散するからだが、グリフィスの教科書ではこのように説明されている。「デルタ関数は$x=0$で値が無限大になるため、技術的には関数ではない。数学文献では、一般化した関数generalized functionまたは分布distributionと呼ばれる」上の式だけを見てもデルタ関数が何かを一度に理解するのは難しい。以下の図を見れば、その幾何学的な意味を把握するのに役立つだろう。 もう少し直感的に説明すると以下のようだ。高さが$n$、幅が$\\displaystyle \\frac{1}{n}$の長方形$R_{n}(x)$、または高さが$n$、底辺が$\\displaystyle \\frac{2}{n}$の二等辺三角形$T_{n}(x)$のような関数列の極限\nデルタ関数が何か理解できたら、デルタ関数の特性を見てみよう。関数$f(x)$がデルタ関数ではない一般的な関数だとすると、$f(x)\\delta (x)$の値は$x=0$を除くすべての場所で$0$である。（$\\because$ $\\delta (x)$が$x=0$を除くすべての場で$0$だから）つまり、$x=0$でのみ値が存在する。したがって、下記の式が成り立つ。\n$$ f(x)\\delta (x) = f(0) \\delta (x) $$\n積分形で表すと\n$$ \\displaystyle{ \\int_{-\\infty}^{\\infty} f(x) \\delta (x) dx = f(0) \\int_{-\\infty}^{\\infty} \\delta (x) dx = f(0)} $$\n一般的な場合を表すために、デルタ関数の峰を$x=0$から$x=a$に移すと、以下のようになる。\n$$ \\delta (x-a) = \\begin{cases} 0, \u0026amp; x\\neq a \\\\ \\infty , \u0026amp; x=a \\end{cases} $$\n$$ \\displaystyle{ \\int_{-\\infty}^{\\infty}{\\delta (x-a) dx}=1 } $$\n$$ f(x)\\delta (x-a) = f(a) \\delta (x-a) $$\n$$ \\displaystyle{ \\int_{-\\infty}^{\\infty} f(x) \\delta (x-a) dx = f(a)} $$\n3次元では、デルタ関数は以下のようになる。\n$$ \\int f( \\mathbf{r} ) \\delta ^3 (\\mathbf{r}-\\mathbf{a}) d\\tau = f(\\mathbf{a}) $$\nこの時、\n$$ \\int \\delta ^3 (\\mathbf{r} ) d\\tau =1 $$\n$$ \\delta ^3 (\\mathbf{r})=\\delta (\\mathbf{x}) \\delta (\\mathbf{y}) \\delta (\\mathbf{z}) $$\n","id":103,"permalink":"https://freshrimpsushi.github.io/jp/posts/103/","tags":null,"title":"ディラックのデルタ関数"},{"categories":"함수","contents":"定義 次のように定義された関数 $\\Gamma : (0, \\infty) \\to \\mathbb{R}$ をガンマ関数と言う。 $$ \\Gamma (x) := \\int_{0}^{\\infty} t^{x-1} e^{-t} dt $$\n説明 上の式において積分に焦点を置くと、オイラー積分とも呼ばれる。ガンマ関数は、純粋数学だけでなく物理学、統計学などでも非常に重要な関数として有名である。非常に多くの興味深い性質を持っているが、最も代表的なのは階乗を実数に対して一般化する概念である点である。\n定理 階乗の一般化としてのガンマ関数 自然数 $n \\in \\mathbb{N}$ に対して $\\Gamma (n) = (n-1)!$ が成り立つ。\n証明 戦略：ガンマ関数が階乗の形で表されることだけを示せば、一般化に関しては十分である。\nガンマ関数の定義により $$ \\Gamma (n) = \\int_{0}^{\\infty} t^{n-1} e^{-t} dt $$\nCase 1. $n=1$ $$ \\Gamma (1) = \\int_{0}^{\\infty} e^{-t} dt = 1 $$ これは、$0! = 1$ と同じ意味で受け取ることができる。\nCase 2. $n\u0026gt;1$\n部分積分法により $$ \\begin{align*} \\Gamma (n) =\u0026amp; \\int_{0}^{\\infty} t^{n-1} e^{-t} dt \\\\ =\u0026amp; \\left[ -t^{n-1} e^{-t} \\right] _{0} ^{\\infty} - \\int_{0}^{\\infty} -(n-1)t^{n-2} e^{-t} dt \\\\ =\u0026amp; (n-1) \\int_{0}^{\\infty} t^{n-2} e^{-t} dt \\\\ =\u0026amp; (n-1) \\Gamma (n-1) \\end{align*} $$\n両方のケースをまとめると $$ \\begin{align*} \\Gamma (n) =\u0026amp; (n-1) \\cdot (n-2) \\cdots 2\\cdot\\Gamma (2) \\\\ =\u0026amp; (n-1) \\cdot (n-2) \\cdots 2\\cdot 1\\cdot \\Gamma (1) \\\\ =\u0026amp; (n-1)! \\end{align*} $$\n■\n参考 ガンマ関数の導出 物理学でのガンマ関数 多項式のラプラス変換の結果としてのガンマ関数 ガンマ関数に対するオイラーの反射公式 ガンマ関数に対するワイエルシュトラスの無限積 ","id":95,"permalink":"https://freshrimpsushi.github.io/jp/posts/95/","tags":null,"title":"ガンマ関数"},{"categories":"수리물리","contents":"ノーテーション ２回以上繰り返される添字については、合計記号$\\sum$を省略する。\n説明 アインシュタインの合計規約Einstein summation conventionとも呼ばれる。公式のようなものではなく、文字通りの規則である。ベクトル計算をしていくと、一つの式の中で$\\sum$を何重にも書かなければならない場面がよくあるが、これでは式が汚くなり、手で書く時も非常に面倒である。したがって、添字が２回以上繰り返される場合は、合計記号を省略するという約束である。もちろん、意味を混同しないように注意が必要である。\n混乱する場合は、左辺にどのようなインデックスがあるかを確認すればよい。左辺に明らかにインデックス$i$がなければ、右辺ではアインシュタインのノーテーションにより$\\sum \\limits_{i}$が省略されていることになる。逆に、左辺にインデックス$j$がある場合、右辺では$j$に対する$\\sum$が省略されているわけではなく、単にないのである。\n例 $1,2,3$がそれぞれ$x,y,z$を表すとする。ベクトル$\\mathbf{A} = (A_{1}, A_{2}, A_{3})$と$\\mathbf{B} = (B_{1}, B_{2}, B_{3})$が与えられたとする。\nベクトル $$ \\begin{align*} \\mathbf{A} \u0026amp;= \\hat{\\mathbf{e}}_{1}A_{1} + \\hat{\\mathbf{e}}_{2}A_{2} + \\hat{\\mathbf{e}}_{3}A_{3} \\\\ \u0026amp;= \\sum \\limits_{i=1}^{3} \\hat{\\mathbf{e}}_{i}A_{i} \\\\ \u0026amp;= \\hat{\\mathbf{e}}_{i}A_{i} \\end{align*} $$\n２つのベクトルの内積 $$ \\begin{align*} \\mathbf{A} \\cdot \\mathbf{B} \u0026amp;= A_{1}B_{1} + A_{2}B_{2} + A_{3}B_{3} \\\\ \u0026amp;= \\sum \\limits_{i=1}^{3} A_{i}B_{i} \\\\ \u0026amp;= A_{i}B_{i} \\end{align*} $$\nクロネッカーのデルタを使って、次のように表せる。\n$$ \\mathbf{A} \\cdot \\mathbf{B} = A_{i}B_{i} = \\delta_{ij}A_{i}B_{j} $$\nベクトル関数の発散 $\\dfrac{\\partial }{\\partial x_{i}} = \\nabla_{i}$とする。そうすると、２つのベクトルの内積から得られる結果と似た結果が得られる。\n$$ \\begin{align*} \\nabla \\cdot \\mathbf{A} \u0026amp;= \\dfrac{\\partial A_{1}}{\\partial x_{1}} + \\dfrac{\\partial A_{2}}{\\partial x_{2}} + \\dfrac{\\partial A_{3}}{\\partial x_{3}} \\\\ \u0026amp;= \\nabla_{1} A_{1} + \\nabla_{2} A_{2} + \\nabla_{3} A_{3} \\\\ \u0026amp;= \\sum \\limits_{i=1}^{3} \\nabla_{i} A_{i} \\\\ \u0026amp;= \\nabla_{i}A_{i} \\\\ \u0026amp;= \\delta_{ij}\\nabla_{i}A_{j} \\end{align*} $$\n２つのベクトルの外積 $$ \\begin{align*} \u0026amp; \\mathbf{A} \\times \\mathbf{B} \\\\ =\u0026amp;\\ \\hat{\\mathbf{e}}_{1} \\left( A_{2} B_{3} - A_{3} B_{2} \\right) + \\hat{\\mathbf{e}}_{2} \\left( A_{3} BA_{1} - A_{1} B_{3} \\right) + \\hat{\\mathbf{e}}_{3} \\left( A_{1} B_{2} - A_{2} B_{1} \\right) \\\\ =\u0026amp;\\ \\hat{\\mathbf{e}}_{1} A_{2} B_{3} - \\hat{\\mathbf{e}}_{1} A_{3} B_{2} + \\hat{\\mathbf{e}}_{2} A_{3} B_{1} - \\hat{\\mathbf{e}}_{2} A_{1} B_{3} + \\hat{\\mathbf{e}}_{3} A_{1} B_{2} - \\hat{\\mathbf{e}}_{1} A_{2} B_{1} \\\\ =\u0026amp;\\ \\epsilon_{123} \\hat{\\mathbf{e}}_{1} A_{2} B_{3} + \\epsilon_{132} \\hat{\\mathbf{e}}_{1} A_{3} B_{2} + \\epsilon_{231} \\hat{\\mathbf{e}}_{2} A_{3} B_{1} + \\epsilon_{213} \\hat{\\mathbf{e}}_{2} A_{1} B_{3} + \\epsilon_{312} \\hat{\\mathbf{e}}_{3} A_{1} B_{2} + \\epsilon_{321} \\hat{\\mathbf{e}}_{3} A_{2} B_{1} \\\\ =\u0026amp;\\ \\sum\\limits_{i=1}^{3} \\sum\\limits_{j=1}^{3} \\sum\\limits_{k=1}^{3} \\epsilon_{ijk} \\hat{\\mathbf{e}}_{i} A_{j} B_{k} \\\\ =\u0026amp;\\ \\epsilon_{ijk} \\hat{\\mathbf{e}}_{i} A_{j}B_{k} \\end{align*} $$\nこの時、$\\epsilon_{ijk}$はレヴィ=チヴィタ記号である。上記の結果により、次の式が成り立つ。\n$$ (\\mathbf{A} \\times \\mathbf{B} )_{i} = \\epsilon_{ijk} A_{j}B_{k} $$\nベクトル関数の回転 再び$\\dfrac{\\partial }{\\partial x_{i}} = \\nabla_{i}$とする。そうすると、２つのベクトルの外積から得られる結果と似た結果が得られる。\n$$ \\begin{align*} \u0026amp; \\nabla \\times \\mathbf{A} \\\\ =\u0026amp;\\ \\hat{\\mathbf{e}}_{1} \\left( \\nabla_{2} A_{3} - \\nabla_{3} A_{2} \\right) + \\hat{\\mathbf{e}}_{2} \\left( \\nabla_{3} A_{1} - \\nabla_{1} A_{3} \\right) + \\hat{\\mathbf{e}}_{3} \\left( \\nabla_{1} A_{2} - \\nabla_{2} A_{1} \\right) \\\\ =\u0026amp;\\ \\hat{\\mathbf{e}}_{1} \\nabla_{2} A_{3} - \\hat{\\mathbf{e}}_{1} \\nabla_{3} A_{2} + \\hat{\\mathbf{e}}_{2} \\nabla_{3} A_{1} - \\hat{\\mathbf{e}}_{2} \\nabla_{1} A_{3} + \\hat{\\mathbf{e}}_{3} \\nabla_{1} A_{2} - \\hat{\\mathbf{e}}_{1} \\nabla_{2} A_{1} \\\\ =\u0026amp;\\ \\epsilon_{123} \\hat{\\mathbf{e}}_{1} \\nabla_{2} A_{3} + \\epsilon_{132} \\hat{\\mathbf{e}}_{1} \\nabla_{3} A_{2} + \\epsilon_{231} \\hat{\\mathbf{e}}_{2} \\nabla_{3} A_{1} + \\epsilon_{213} \\hat{\\mathbf{e}}_{2} \\nabla_{1} A_{3} + \\epsilon_{312} \\hat{\\mathbf{e}}_{3} \\nabla_{1} A_{2} + \\epsilon_{321} \\hat{\\mathbf{e}}_{3} \\nabla_{2} A_{1} \\\\ =\u0026amp;\\ \\sum\\limits_{i=1}^{3} \\sum\\limits_{j=1}^{3} \\sum\\limits_{k=1}^{3} \\epsilon_{ijk} \\hat{\\mathbf{e}}_{i} \\nabla_{j} A_{k} \\\\ =\u0026amp;\\ \\epsilon_{ijk} \\hat{\\mathbf{e}}_{i} \\nabla_{j} A_{k} \\end{align*} $$\nここで、$\\nabla_{i}$が微分であることを常に意識しなければならない。通常のベクトル成分は順番を変えても大きな問題はない。\n$$ A_{1}A_{2}A_{3} = A_{2}A_{1}A_{3} $$\nしかし、$\\nabla_{i}$は微分であるため、ベクトルの成分と順序を絶対に入れ替えてはいけない。\n$$ A_{1}\\nabla_{2}A_{3} \\ne \\nabla_{2}A_{1}A_{3} $$\n例えば、$\\mathbf{A} = (y,xy,xyz)$とすると、次の結果が得られる。\n$$ A_{1}\\nabla_{2}A_{3} = y \\dfrac{\\partial (xyz)}{\\partial y} = xyz \\ne 2xyz = \\dfrac{\\partial (xy^{2}z)}{\\partial y} = \\nabla_{2}A_{1}A_{3} $$\nもちろん、$\\dfrac{\\partial^{2} }{\\partial x\\partial y} = \\dfrac{\\partial^{2} }{\\partial y\\partial x}$であるため、$\\nabla_{1}\\nabla_{2}=\\nabla_{2}\\nabla_{1}$が成立する。\n","id":90,"permalink":"https://freshrimpsushi.github.io/jp/posts/90/","tags":null,"title":"アインシュタインの記法"},{"categories":"수리물리","contents":"まとめ 次のように定義される $\\epsilon_{ijk}$ を レビ-チビタ記号 と呼ぶ。\n$$ \\epsilon_{ijk} = \\begin{cases} +1 \u0026amp; \\text{if} \\ \\epsilon_{123}, \\epsilon_{231}, \\epsilon_{312} \\\\ -1 \u0026amp; \\text{if} \\ \\epsilon_{132}, \\epsilon_{213}, \\epsilon_{321} \\\\ 0 \u0026amp; \\text{if} \\ i=j \\ \\text{or} \\ j=k \\ \\text{or} \\ k=i \\end{cases} $$\n次のように定義される $\\delta_{ij}$ を クロネッカーのデルタ と呼ぶ。\n$$ \\delta_{ij} := \\begin{cases} 1,\u0026amp;i=j \\\\ 0, \u0026amp; i\\ne j \\end{cases} $$\n二つのレビ-チビタ記号の積とクロネッカーのデルタとの間には、次の関係が成り立つ。\n(a) 一つのインデックスが同じ場合: $\\epsilon_{ijk}\\epsilon_{ilm} = \\delta_{jl}\\delta_{km} - \\delta_{jm}\\delta_{kl}$\n(b) 二つのインデックスが同じ場合: $\\epsilon_{ijk}\\epsilon_{ijm}=2\\delta_{km}$\n(c) 三つのインデックスが同じ場合: $\\epsilon_{ijk}\\epsilon_{ijk}=6$\n説明 文章全体で $\\sum$ を省略する アインシュタインの記法 を使用していることに注意してください。これは上記の式においても同じです。 (a)は使用頻度が高いため、覚えておくと便利です。簡単に覚える方法は次のとおりです。\n証明 (a) $\\mathbf{e}_{i}$ $(i=1,2,3)$ を3次元における 標準単位ベクトル としよう。\n$$ \\mathbf{e}_{1} = (1, 0, 0),\\quad \\mathbf{e}_{2} = (0, 1, 0),\\quad \\mathbf{e}_{3} = (0, 0, 1) $$\n$P_{ijk}$ を1行目が $\\mathbf{e}_{i}$、2行目が $\\mathbf{e}_{j}$、3行目が $\\mathbf{e}_{k}$ の $3 \\times 3$ 行列 とする。\n$$ P_{ijk} = \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{j} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{k} \\text{ \u0026mdash;} \\end{bmatrix} $$\nすると、行列式の性質により $\\det P_{ijk} = \\epsilon_{ijk}$ と簡単にわかる。まず $P_{123}$ は 単位行列 であるため、行列式は $1$ である。また、異なる行の順序を偶数回変えると行列式の値は変わらないため、\n$$ \\det P_{123} = \\det P_{231} = \\det P_{312} = 1 $$\n異なる行の順序を奇数回変えると行列式の符号が逆になるため、\n$$ \\det P_{132} = \\det P_{213} = \\det P_{321} = -1 $$\n同じ行を二つ以上含む行列の行列式は $0$ であるため、残りの場合は全て $0$ となる。したがって $\\det P_{ijk} = \\epsilon_{ijk}$ が成立する。一つのインデックスが同じ二つのレビ-チビタ記号の積は、 行列式の性質 をうまく使うと、次のようになる。\n$$ \\begin{align*} \\epsilon_{ijk}\\epsilon_{ilm} \u0026amp;= \\det \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{j} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{k} \\text{ \u0026mdash;} \\end{bmatrix} \\det \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{l} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{m} \\text{ \u0026mdash;} \\end{bmatrix} \\\\ \u0026amp;= \\det \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{j} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{k} \\text{ \u0026mdash;} \\end{bmatrix} \\det \\begin{bmatrix} \\vert \u0026amp; \\vert \u0026amp; \\vert \\\\ \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{m} \\\\ \\vert \u0026amp; \\vert \u0026amp; \\vert \\end{bmatrix} \u0026amp; (\\because \\det A = \\det A^{T}) \\\\ \u0026amp;= \\det \\left( \\begin{bmatrix} \\text{\u0026mdash; } \\mathbf{e}_{i} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{j} \\text{ \u0026mdash;} \\\\ \\text{\u0026mdash; } \\mathbf{e}_{k} \\text{ \u0026mdash;} \\end{bmatrix} \\begin{bmatrix} \\vert \u0026amp; \\vert \u0026amp; \\vert \\\\ \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{m} \\\\ \\vert \u0026amp; \\vert \u0026amp; \\vert \\end{bmatrix} \\right) \u0026amp; \\Big(\\because (\\det A) (\\det B) = \\det (AB) \\Big) \\\\ \u0026amp;= \\det \\begin{bmatrix} \\mathbf{e}_{i} \\cdot \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{i} \\cdot \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{i} \\cdot \\mathbf{e}_{m} \\\\ \\mathbf{e}_{j} \\cdot \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{j} \\cdot \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{j} \\cdot \\mathbf{e}_{m} \\\\ \\mathbf{e}_{k} \\cdot \\mathbf{e}_{i} \u0026amp; \\mathbf{e}_{k} \\cdot \\mathbf{e}_{l} \u0026amp; \\mathbf{e}_{k} \\cdot \\mathbf{e}_{m} \\end{bmatrix} \\end{align*} $$\n$\\mathbf{e}_{i}$ は標準単位ベクトルであるため、$\\mathbf{e}_{i} \\cdot \\mathbf{e}_{j} = \\delta_{ij}$ が成立する。\n$$ \\epsilon_{ijk}\\epsilon_{ilm} = \\det \\begin{bmatrix} \\delta_{ii} \u0026amp; \\delta_{il} \u0026amp; \\delta_{im} \\\\ \\delta_{ji} \u0026amp; \\delta_{jl} \u0026amp; \\delta_{jm} \\\\ \\delta_{ki} \u0026amp; \\delta_{kl} \u0026amp; \\delta_{km} \\end{bmatrix} $$\nこのとき、$i$ が $j, k, l, m$ と全て異なる場合のみを考えていることに注意しよう。なぜなら $j, k, l, m$ のいずれかが $i$ と同じであれば、$\\epsilon_{ijk}\\epsilon_{ilm} = 0$ となり、意味のない結果だからである。したがって結果\nは次のようになる。\n$$ \\epsilon_{ijk}\\epsilon_{ilm} = \\det \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\delta_{jl} \u0026amp; \\delta_{jm} \\\\ `` 0 \u0026amp; \\delta_{kl} \u0026amp; \\delta_{km} \\end{bmatrix} = \\delta_{jl}\\delta_{km} - \\delta_{jm}\\delta_{kl} $$\n■\n(b) (a) で $l=j$ の場合である。したがって、次のようになる。\n$$ \\epsilon_{ijk}\\epsilon_{ijm} = \\delta_{jj}\\delta_{km} - \\delta_{jm}\\delta_{kj} $$\nこのとき $\\delta_{jj}=3$ が成立し、また $\\delta_{jm}\\delta_{kj}=\\delta_{mk}$ も成立するため、結果は次のようになる。\n$$ \\epsilon_{ijk}\\epsilon_{ijm} = \\delta_{jj}\\delta_{km} - \\delta_{jm}\\delta_{kj} = 3\\delta_{km} - \\delta_{mk} = 2\\delta_{km} $$\n■\n(c) (b) で $m=k$ の場合であるため、\n$$ \\epsilon_{ijk}\\epsilon_{ijk} = \\sum_{k=1}^{3}2\\delta_{kk} = 2\\delta_{11} + 2\\delta_{22} + 2\\delta_{33} = 2 + 2 + 2 = 6 $$\nまたは、0でない全ての項を展開すると、次を得る。\n$$ \\begin{align*} \\epsilon_{ijk}\\epsilon_{ijk} \u0026amp;=\\sum \\limits _{i=1} ^{3}\\sum \\limits _{j=1} ^{3}\\sum \\limits _{k=1} ^{1} \\epsilon_{ijk}\\epsilon_{ijk} \\\\ \u0026amp;=\\epsilon_{123}\\epsilon_{123}+\\epsilon_{231}\\epsilon_{231}+\\epsilon_{312}\\epsilon_{312}+\\epsilon_{132}\\epsilon_{132}+\\epsilon_{213}\\epsilon_{213}+\\epsilon_{321}\\epsilon_{321} \\\\ \u0026amp;=6 \\end{align*} $$\n■\n","id":88,"permalink":"https://freshrimpsushi.github.io/jp/posts/88/","tags":null,"title":"二つのレビ-チビタ記号の積"},{"categories":"수리물리","contents":"定義 以下のように定義される$\\delta_{ij}$をクロネッカーのデルタKronecker deltaと呼ぶ。\n$$ \\delta_{ij} := \\begin{cases} 1,\u0026amp;i=j \\\\ 0, \u0026amp; i\\ne j \\end{cases} $$\n説明 クロネッカーのデルタは非常に多くの場所で使用され、すべての成分（要素、可能性など）の中で欲しいものだけを示すのが主な役割だ。物理学の学生なら、内積に関する表現で主に接することになる。これが何を意味するのかすぐにはわからないかもしれないから、以下の例を見て理解しよう。\n例 まず、2つのベクトル$\\mathbf{A}=(A_{1}, A_{2}, A_{3})$、$\\mathbf{B}=(B_{1}, B_{2}, B_{3})$が与えられたとしよう。すると、二つのベクトルの内積は次のようになる。\n$$ \\mathbf{A} \\cdot \\mathbf{B} = A_{1}B_{1} + A_{2}B_{2} + A_{3}B_{3} $$\nこれを合計記号$\\sum$を使って表現すると、次のようになる。\n$$ \\mathbf{A} \\cdot \\mathbf{B} = A_{1}B_{1} + A_{2}B_{2} + A_{3}B_{3} = \\sum \\limits_{i=1}^{3}A_{i}B_{i} $$\nそれでは、上の式と$\\sum \\limits_{i=1}^{3}\\sum \\limits_{j=1}^{3}\\delta_{ij}A_{i}B_{j}$が同じ式であることを次でわかる。\n$$ \\begin{align*} \\sum _{i=1}^{3}\\sum _{j=1}^{3}\\delta_{ij}A_{i}B_{j} \u0026amp;= \\delta_{11}A_{1}B_{1} + \\delta_{12}A_{1}B_{2} + \\delta_{13}A_{1}B_{3} \\\\ \u0026amp; \\quad+ \\delta_{21}A_{2}B_{1} + \\delta_{22}A_{2}B_{2} + \\delta_{23}A_{2}B_{3} \\\\ \u0026amp; \\quad+ \\delta_{31}A_{3}B_{1} + \\delta_{32}A_{3}B_{2} + \\delta_{33}A_{3}B_{3} \\\\ \u0026amp;= 1\\cdot A_{1}B_{1} + 0 \\cdot A_{1}B_{2} + 0\\cdot A_{1}B_{3} \\\\ \u0026amp; \\quad+ 0\\cdot A_{2}B_{1} + 1\\cdot A_{2}B_{2} + 0\\cdot A_{2}B_{3} \\\\ \u0026amp; \\quad+ 0\\cdot A_{3}B_{1} + 0\\cdot A_{3}B_{2} + 1\\cdot A_{3}B_{3} \\\\ \u0026amp;= A_{1}B_{1} + A_{2}B_{2} + A_{3}B_{3} \\\\ \u0026amp;= \\sum \\limits_{i=1}^{3}A_{i}B_{i} \\\\ \u0026amp;= \\mathbf{A} \\cdot \\mathbf{B} \\end{align*} $$\n一方の項に同じインデックスが2回以上出現する場合、$\\sum$を省略するアインシュタイン記法を適用すると、次のようになる。\n$$ \\delta_{ij}A_{i}B_{j} = \\mathbf{A} \\cdot \\mathbf{B} $$\nそれで$\\delta_{ij}A_{i}B_{j}$と$\\mathbf{A} \\cdot \\mathbf{B}$が同じであることはわかるが、なぜこのような表現を使うのかは理解できないかもしれない。上の例は非常に単純な式であるため、その有用性が目立たないかもしれないが、電磁気学などで多数のベクトルの内積や外積、勾配、発散、回転、ラプラシアンなどを計算すると、その便利さがわかるだろう。学部2年生なら、その便利さを自然に知ることになるので、今すぐ無理に納得する必要はない。\nまた、下添字が両方とも同じ場合のみ値があるため、複数のクロネッカーのデルタが掛けられている場合は、すべての添字が同じ場合のみ値がある。\n$$ \\delta_{ij}\\delta_{jk} $$\nこのような場合、$i=j=k$の場合のみ、$0$ではない値が存在する。また、クロネッカーのデルタは$2$次テンソルの一例である。\n公式 (a) $\\delta_{ii} = 3$\n(b) $\\delta_{ij}\\delta_{jl} = \\delta_{il}$\n(c) $\\delta_{ii}\\delta_{jj} = 9$\n(d) $\\delta_{ii}\\delta_{jj} = 6 \\quad (i \\ne j)$\n同じインデックスが項に2回以上出現する場合は$\\sum$が省略されていることを忘れないでほしい。\n証明 (a) アインシュタイン記法により、以下が成立する。\n$$ \\delta_{ii} = \\sum \\limits_{i=1}^{3} \\delta_{ii} = \\delta_{11}+\\delta_{22}+\\delta_{33}=3 $$\n■\n(b) アインシュタイン記法により、以下が成立する。\n$$ \\delta_{ij}\\delta_{jl}=\\sum\\limits_{j=1}^{3}\\delta_{ij}\\delta_{jl}=\\delta_{i1}\\delta_{1l}+\\delta_{i2}\\delta_{2l}+\\delta_{i3}\\delta_{3l} $$\nでは、上記の値が$0$でない場合について考えよう。\n$$ i=l=1 \\quad \\text{and} \\quad i=l=2 \\quad \\text{and} \\quad i=l=3 $$\n最初の場合、以下が成立する。\n$$ \\delta_{i1}\\delta_{1l} = 1 \\quad \\text{and} \\quad \\delta_{i2}\\delta_{2l}=\\delta_{i3}\\delta_{3l} = 0 \\\\ \\implies \\delta_{ij}\\delta_{jl} = \\delta_{i1}\\delta_{1l}+\\delta_{i2}\\delta_{2l}+\\delta_{i3}\\delta_{3l} = 1 $$\n二番目の場合、以下が成立する。\n$$ \\delta_{i2}\\delta_{2l} = 1 \\quad \\text{and} \\quad \\delta_{i1}\\delta_{1l}=\\delta_{i3}\\delta_{3l} = 0 \\\\ \\implies \\delta_{ij}\\delta_{jl} = \\delta_{i1}\\delta_{1l}+\\delta_{i2}\\delta_{2l}+\\delta_{i3}\\delta_{3l} = 1 $$\n三番目の場合、以下が成立する。\n$$ \\delta_{i3}\\delta_{3l} = 1 \\quad \\text{and} \\quad \\delta_{i1}\\delta_{1l}=\\delta_{i2}\\delta_{2l} = 0 \\\\ \\implies \\delta_{ij}\\delta_{jl} = \\delta_{i1}\\delta_{1l}+\\delta_{i2}\\delta_{2l}+\\delta_{i3}\\delta_{3l} = 1 $$\nしたがって、$\\delta_{ij}\\delta_{jl}$は$i=l$のときのみ$1$の値を持ち、それ以外の場合はすべて値が$0$であるので、以下の結果が得られる。\n$$ \\delta_{ij}\\delta_{jl} = \\delta_{il} $$\n■\n(c) アインシュタイン記法により、$\\sum$が省略されているので、以下のようになる。\n$$ \\begin{align*} \\delta_{ii}\\delta_{jj} \u0026amp;= \\sum\\limits_{i=1}^{3}\\sum\\limits_{j=1}^3{\\delta_{ii}\\delta_{jj}} \\\\ \u0026amp;= \\sum\\limits_{i=1}^{3}{\\delta_{ii} \\sum\\limits_{j=1}^3\\delta_{jj}} \\\\ \u0026amp;= 3\\cdot 3 \\\\ \u0026amp;= 9 \\end{align*} $$\n三番目の等号は**（a）**により成立する。\n■\n(d) アインシュタイン記法により、$\\sum$が省略されているので、以下のようになる。\n$$ \\begin{align*} \\delta_{ii}\\delta_{jj} \u0026amp;= \\sum\\limits_{i=1}^{3}\\sum\\limits_{\\substack{j=1 \\\\ j\\ne i}}^{3}{\\delta_{ii}\\delta_{jj}} \\\\ \u0026amp;= \\delta_{11}\\delta_{22} +\\delta_{11}\\delta_{33} +\\delta_{22}\\delta_{11} +\\delta_{22}\\delta_{33}+\\delta_{33}\\delta_{11}+\\delta_{33}\\delta_{22} \\\\ \u0026amp;= 6 \\end{align*} $$\n■\n","id":84,"permalink":"https://freshrimpsushi.github.io/jp/posts/84/","tags":null,"title":"クロネッカーのデルタ"},{"categories":"수리물리","contents":"定義 以下のように定義される$\\epsilon_{ijk}$を レビ-チビタ記号Levi-Civita-symbol という。\n$$ \\epsilon_{ijk} = \\begin{cases} +1 \u0026amp; \\text{if} \\ \\epsilon_{123}, \\epsilon_{231}, \\epsilon_{312} \\\\ -1 \u0026amp; \\text{if} \\ \\epsilon_{132}, \\epsilon_{213}, \\epsilon_{321} \\\\ 0 \u0026amp; \\text{if} \\ i=j \\ \\text{or} \\ j=k \\ \\text{or} \\ k=i \\end{cases} $$\n説明 クロネッカーのデルタがインデックス同士が同じかだけを考えたとするなら、レビ-チビタ記号は定義から見えるようにインデックスの順番も値に影響を与える。$\\epsilon_{ijk}$は$i$、$j$、$k$が重複なしで昇順$(1\\to 2\\to 3\\to 1)$であれば$+1$、重複なしで降順$(3\\to 2\\to 1\\to 3)$であれば$-1$、一つでも重複があれば$0$を値とする。単純に述べれば上のようで、全部の値を列挙すれば合計$3\\times 3\\times 3=27$個で、その中で$0$ではないのは$6$個である。\n$$ \\begin{array}{|c|c|c|c|}\\hline i=1 \u0026amp; k=1 \u0026amp; k=2 \u0026amp; k=3 \\\\ \\hline j=1 \u0026amp; \\epsilon_{111}=0 \u0026amp; \\epsilon_{112}=0 \u0026amp; \\epsilon_{113}=0 \\\\ j=2 \u0026amp; \\epsilon_{121}=0 \u0026amp; \\epsilon_{122}=0 \u0026amp; \\epsilon_{123}=1 \\\\ j=3 \u0026amp; \\epsilon_{131}=0 \u0026amp; \\epsilon_{132}=-1 \u0026amp; \\epsilon_{133}=0 \\\\ \\hline \\end{array}\\quad \\begin{array}{|c|c|c|c|}\\hline i=2 \u0026amp; k=1 \u0026amp; k=2 \u0026amp; k=3 \\\\ \\hline j=1 \u0026amp; \\epsilon_{211}=0 \u0026amp; \\epsilon_{212}=0 \u0026amp; \\epsilon_{213}=-1 \\\\ j=2 \u0026amp; \\epsilon_{221}=0 \u0026amp; \\epsilon_{222}=0 \u0026amp; \\epsilon_{223}=0 \\\\ j=3 \u0026amp; \\epsilon_{231}=1 \u0026amp; \\epsilon_{232}=0 \u0026amp; \\epsilon_{233}=0 \\\\ \\hline \\end{array} \\\\ {} \\\\ \\begin{array}{|c|c|c|c|}\\hline i=3 \u0026amp; k=1 \u0026amp; k=2 \u0026amp; k=3 \\\\ \\hline j=1 \u0026amp; \\epsilon_{311}=0 \u0026amp; \\epsilon_{312}=1 \u0026amp; \\epsilon_{313}=0 \\\\ j=2 \u0026amp; \\epsilon_{321}=-1 \u0026amp; \\epsilon_{322}=0 \u0026amp; \\epsilon_{323}=0 \\\\ j=3 \u0026amp; \\epsilon_{331}=0 \u0026amp; \\epsilon_{332}=0 \u0026amp; \\epsilon_{333}=0 \\\\ \\hline \\end{array} $$\n$3$次テンソルの一例である。\n例 外積 レビ-チビタ記号を使うと、二つのベクトルの外積を非常に簡単に表現できる。3次元直交座標系で二つのベクトルの外積は以下の通りである。\n$$ \\begin{align*} \\mathbf{A} \\times \\mathbf{B} =\u0026amp;\\ \\hat{\\mathbf{e}}_{x} (A_{y}B_{z}-A_{z}B_{y}) + \\hat{\\mathbf{e}}_{y} (A_{z}B_{x}-A_{x}B_{z}) + \\hat{\\mathbf{e}}_{z} (A_{x}B_{y}-A_{y}B_{x}) \\\\ =\u0026amp;\\ \\begin{vmatrix} \\hat{\\mathbf{e}}_{x} \\quad \\hat{\\mathbf{e}}_{y} \\quad \\hat{\\mathbf{e}}_{z} \\\\ A_{x} \\quad A_{y} \\quad A_{z} \\\\ B_{x} \\quad B_{y} \\quad B_{z} \\end{vmatrix} \\end{align*} $$\nここで$x=1$、$y=2$、$z=3$とすると、二つのベクトルの外積はレビ-チビタ記号を使用して以下のように表現できる。\n$$ \\sum\\limits_{i=1}^{3} \\sum\\limits_{j=1}^{3} \\sum\\limits_{k=1}^{3} {\\epsilon_{ijk} \\hat{\\mathbf{e}}_{i} A_{j}B_{k}} $$\n$0$ではない項だけを展開すると、以下のようになる。\n$$ \\begin{align*} \u0026amp; \\sum\\limits_{i=1}^{3} \\sum\\limits_{j=1}^{3} \\sum\\limits_{k=1}^{3} {\\epsilon_{ijk} \\hat{\\mathbf{e}}_{i} A_{j}B_{k}} \\\\ =\u0026amp;\\ \\epsilon_{123} \\hat{\\mathbf{e}}_{1}A_{2}B_{3} + \\epsilon_{132} \\hat{\\mathbf{e}}_{1}A_{3}B_{2} + \\epsilon_{ 231 }\\hat{\\mathbf{e}}_{2}A_{3}B_{1} + \\epsilon_{213}\\hat{\\mathbf{e}}_{2}A_{1}B_{3} + \\epsilon_{312}\\hat{\\mathbf{e}}_{3}A_{1}B_{2} + \\epsilon_{321}\\hat{\\mathbf{e}}_{3}A_{2}B_{1} \\end{align*} $$\nここで$\\epsilon_{123}=\\epsilon_{231}=\\epsilon_{312}=1$、$\\epsilon_{132}=\\epsilon_{213}=\\epsilon_{321}=-1$を代入して整理すると、以下のようになる。\n$$ \\begin{align*} \u0026amp; \\sum\\limits_{i=1}^{3} \\sum\\limits_{j=1}^{3} \\sum\\limits_{k=1}^{3} {\\epsilon_{ijk} A_{i}B_{j}}\\hat{\\mathbf{e}}_{k} \\\\ =\u0026amp;\\ \\hat{\\mathbf{e}}_{1}\\left( A_{2}B_{3} - A_{3}B_{2} \\right) + \\hat{\\mathbf{e}}_{2}\\left( A_{3}B_{1} - A_{1}B_{3} \\right) + \\hat{\\mathbf{e}}_{3}\\left( A_{1}B_{2} - A_{2}B_{1} \\right) \\end{align*} $$\n最後に$1$、$2$、$3$にそれぞれ$x$、$y$、$z$を代入すると、以下を得る。\n$$ \\hat{\\mathbf{e}}_{x}\\left( A_{y}B_{z} - A_{z}B_{y} \\right) + \\hat{\\mathbf{e}}_{y}\\left( A_{z}B_{x} - A_{x}B_{z} \\right) + \\hat{\\mathbf{e}}_{z}\\left( A_{x}B_{y} - A_{y}B_{x} \\right) $$\nしたがって、以下の結果を得る。\n$$ \\mathbf{A} \\times \\mathbf{ B } = \\sum \\limits_{i=1}^{3} \\sum \\limits_{j=1}^{3} \\sum \\limits_{k=1}^{3} \\epsilon_{ijk}\\hat{\\mathbf{e}}_{i}A_{j}B_{k} $$\nアインシュタインの記法を使えば、以下のようになる。\n$$ \\mathbf{A} \\times \\mathbf{ B } = \\epsilon_{ijk}\\hat{\\mathbf{e}}_{i}A_{j}B_{k} $$\n外積の各成分も容易に表現できるが、上の式から分かるように、$(\\mathbf{A} \\times \\mathbf{B})$の$i$成分は以下の通りである。\n$$ (\\mathbf{A} \\times \\mathbf {B})_{i}=\\epsilon_{ijk}A_{j}B_{k} $$\n行列式 $3 \\times 3$行列$A = [a_{ij}]$の行列式は、以下のように表される。\n$$ \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \\\\ \\end{vmatrix} = \\sum\\limits_{i,j,k=1}^{3} \\epsilon_{ijk}a_{1i}a_{2j}a_{3k} $$\n示す方法は簡単だ。行列式を展開し、行列の各成分の二番目のインデックスをよく見ると、インデックスのレビ-チビタ記号が各項の符号と一致することがわかる。\n$$ \\begin{align*} \u0026amp; \\det A \\\\ \u0026amp;= a_{11}(a_{22}a_{33} - a_{23}a_{32}) + a_{12}(a_{23}a_{31} - a_{21}a_{33}) + a_{13}(a_{21}a_{32} - a_{22}a_{31}) \\\\ \u0026amp;= a_{11}a_{22}a_{33} - a_{11}a_{23}a_{32} + a_{12}a_{23}a_{31} - a_{12}a_{21}a_{33} + a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31} \\\\ \u0026amp;= a_{1\\textcolor{red}{1}}a_{2\\textcolor{red}{2}}a_{3\\textcolor{red}{3}} - a_{1\\textcolor{red}{1}}a_{2\\textcolor{red}{3}}a_{3\\textcolor{red}{2}} + a_{1\\textcolor{red}{2}}a_{2\\textcolor{red}{3}}a_{3\\textcolor{red}{1}} - a_{1\\textcolor{red}{2}}a_{2\\textcolor{red}{1}}a_{3\\textcolor{red}{3}} + a_{1\\textcolor{red}{3}}a_{2\\textcolor{red}{1}}a_{3\\textcolor{red}{2}} - a_{1\\textcolor{red}{3}}a_{2\\textcolor{red}{2}}a_{3\\textcolor{red}{1}} \\\\ \u0026amp;= \\epsilon_{\\textcolor{red}{123}}a_{1\\textcolor{red}{1}}a_{2\\textcolor{red}{2}}a_{3\\textcolor{red}{3}} + \\epsilon_{\\textcolor{red}{132}}a_{1\\textcolor{red}{1}}a_{2\\textcolor{red}{3}}a_{3\\textcolor{red}{2}} + \\epsilon_{\\textcolor{red}{231}}a_{1\\textcolor{red}{2}}a_{2\\textcolor{red}{3}}a_{3\\textcolor{red}{1}} + \\epsilon_{\\textcolor{red}{213}}a_{1\\textcolor{red}{2}}a_{2\\textcolor{red}{1}}a_{3\\textcolor{red}{3}} + \\epsilon_{\\textcolor{red}{312}}a_{1\\textcolor{red}{3}}a_{2\\textcolor{red}{1}}a_{3\\textcolor{red}{2}} + \\epsilon_{\\textcolor{red}{321}}a_{1\\textcolor{red}{3}}a_{2\\textcolor{red}{2}}a_{3\\textcolor{red}{1}} \\\\ \u0026amp;= \\sum\\limits_{i,j,k=1}^{3} \\epsilon_{ijk}a_{1i}a_{2j}a_{3k} \\end{align*} $$\n公式 (a) 一つのインデックスが同じ場合: $\\epsilon_{ijk}\\epsilon_{lmk}=\\delta_{il}\\delta_{jm}-\\delta_{im}\\delta_{jl}$\n(b) 二つのインデックスが同じ場合: $\\epsilon_{ijk}\\epsilon_{ljk}=2\\delta_{il}$\n(c) 三つのインデックスが同じ場合: $\\epsilon_{ijk}\\epsilon_{ijk}=6$\n","id":83,"permalink":"https://freshrimpsushi.github.io/jp/posts/83/","tags":null,"title":"レヴィ-チヴィタ記号"},{"categories":"수리물리","contents":"説明 上の式の左辺をベクトル三重積と呼ぶ。右辺の結果を簡単に**BAC-CAB（バックキャップ）**という。ベクトル三重積は、結果がベクトルになる3つのベクトルの掛け算の操作である。結果がベクトルになるためには、式には外積が2回入る。2つのベクトルの外積はやはりベクトルなので、再び別のベクトルと外積することができる。\n結果がスカラーになる場合はスカラー三重積と呼ぶ。その形は下のようである。\n$$ \\mathbf{A}\\cdot (\\mathbf{B}\\times \\mathbf{C}) $$\nただし、内積の操作結果がスカラーであるため、内積が2回入っていたり、内積と外積の操作順序が逆の場合は間違った式になる。\n$$ \\mathbf{A}\\cdot (\\mathbf{B}\\cdot \\mathbf{C}) \\to \\text{틀린 식} $$\n$$ \\mathbf{A}\\times (\\mathbf{B}\\cdot \\mathbf{C})\\to \\text{틀린 식} $$\nまた、外積は結合法則が成り立たないため、以下のように等号が成り立たない。\n$$ \\mathbf{A}\\times (\\mathbf{B}\\times \\mathbf{C})\\neq (\\mathbf{A}\\times \\mathbf{B})\\times \\mathbf{C} $$\n証明 以下の証明は3次元直交座標系に関するものであり、証明ではアインシュタイン記法を使用して合計記号$\\sum$を省略している。\nレヴィ-チヴィタ記号を使った証明 $$ \\begin{align*} \\mathbf{A} \\times (\\mathbf{ B }\\times \\mathbf{ C })=\u0026amp;\\ \\epsilon _{ ijk }\\mathbf{e}_{i}A_{ j }(B\\times C)_{ k } \\\\ =\u0026amp;\\ \\epsilon _{ ijk }\\mathbf{e}_{i}A_{ j }(\\epsilon _{ klm }B_{ l }C_{ m }) \\\\ =\u0026amp;\\ \\epsilon _{ ijk }\\epsilon _{ klm }\\mathbf{e}_{i}A_{ j }B_{ l }C_{ m } \\\\ =\u0026amp;\\ (\\delta _{ il }\\delta _{ jm }-\\delta _{ im }\\delta _{ jl })(\\mathbf{e}_{i}A_{ j }B_{ l }C_{ m }) \\\\ =\u0026amp;\\ \\delta _{ il }\\delta _{ jm }\\mathbf{e}_{i}A_{ j }B_{ l }C_{ m }-\\delta _{ im }\\delta _{ jl }\\mathbf{e}_{i}A_{ j }B_{ l }C_{ m } \\\\ =\u0026amp;\\ \\mathbf{e}_{i}A_{ j }B_{ i }C_{ j }-\\mathbf{e}_{i}A_{ j }B_{ j }C_{ i } \\\\ =\u0026amp;\\ \\mathbf{e}_{i}B_{ i }A_{ j }C_{ j }-\\mathbf{e}_{i}C_{ i }A_{ j }B_{ j } \\\\ =\u0026amp;\\ \\mathbf{B}(\\mathbf{A} \\cdot \\mathbf{C} )-\\mathbf{C}(\\mathbf{A} \\cdot \\mathbf{B}) \\end{align*} $$\n4つ目の等号は$\\epsilon_{ijk}\\epsilon_{klm}=\\delta_{il}\\delta_{jm}-\\delta_{im}\\delta_{jl}$であるため成立する。\n■\n証明を見るとわかるが、少し練習すれば、式を忘れても試験中に導出することができるくらいである。\n直接計算 $$ \\begin{align*} \u0026amp; \\mathbf{A} \\times (\\mathbf{B} \\times \\mathbf{C}) \\\\ =\u0026amp;\\ \\mathbf{A} \\times \\begin{vmatrix} \\hat{\\mathbf{x}} \u0026amp; \\hat{\\mathbf{y}} \u0026amp; \\hat{\\mathbf{z}} \\\\ B_{x} \u0026amp; B_{y} \u0026amp; B_{z} \\\\ C_{x} \u0026amp; C_{y} \u0026amp; C_{z}\\end{vmatrix} \\\\ =\u0026amp;\\ \\mathbf{A} \\times \\Big[ (B_{y}C_{z}-B_{z}C_{y})\\hat{\\mathbf{x}} + (B_{z}C_{x}-B_{x}C_{z})\\hat{\\mathbf{y}} +(B_{x}C_{y}-B_{y}C_{x})\\hat{\\mathbf{z}} \\Big] \\\\ =\u0026amp;\\ \\begin{vmatrix} \\hat{\\mathbf{x}} \u0026amp; \\hat{\\mathbf{y}} \u0026amp; \\hat{\\mathbf{z}} \\\\ A_{x} \u0026amp; A_{y} \u0026amp; A_{z} \\\\ B_{y}C_{z}-B_{z}C_{y} \u0026amp; B_{z}C_{x}-B_{x}C_{z} \u0026amp; B_{x}C_{y}-B_{y}C_{x}\\end{vmatrix} \\\\ =\u0026amp;\\ \\Big[ A_{y}(B_{x}C_{y}-B_{y}C_{x})-A_{z}(B_{z}C_{x}-B_{x}C_{z}) \\Big]\\hat{\\mathbf{x}} \\\\ \u0026amp;\\quad +\\Big[ A_{z}(B_{y}C_{z}-B_{z}C_{y})-A_{x}(B_{x}C_{y}-B_{y}C_{x}) \\Big]\\hat{\\mathbf{y}} \\\\ \u0026amp;\\quad +\\Big[ A_{x}(B_{z}C_{x}-B_{x}C_{z})-A_{y}(B_{y}C_{z}-B_{z}C_{y}) \\Big]\\hat{\\mathbf{z}} \\\\ =\u0026amp;\\ (A_{y}B_{x}C_{y}+A_{z}B_{x}C_{z}-A_{y}B_{y}C_{x}-A_{z}B_{z}C_{x}) \\hat{\\mathbf{x}} \\\\ \u0026amp;\\quad +(A_{z}B_{y}C_{z}+A_{x}B_{y}C_{x}-A_{z}B_{z}C_{y}-A_{x}B_{x}C_{y}) \\hat{\\mathbf{y}} \\\\ \u0026amp;\\quad +( A_{x}B_{z}C_{x}+A_{y}B_{z}C_{y}-A_{x}B_{x}C_{z}-A_{y}B_{y}C_{z}) \\hat{\\mathbf{z}} \\end{align*} $$\n$$ \\begin{align*} \u0026amp; \\mathbf{B}(\\mathbf{A}\\cdot \\mathbf{C}) - \\mathbf{C}(\\mathbf{A}\\cdot \\mathbf{B}) \\\\ =\u0026amp;\\ B_{x}(A_{x}C_{x}+A_{y}C_{y}+A_{z}C_{z})\\hat{\\mathbf{x}} -C_{x}(A_{x}B_{x}+A_{y}B_{y}+A_{z}B_{z})\\hat{\\mathbf{x}} \\\\ \u0026amp;\\quad + B_{y}(A_{x}C_{x}+A_{y}C_{y}+A_{z}C_{z})\\hat{\\mathbf{y}} -C_{y}(A_{x}B_{x}+A_{y}B_{y}+A_{z}B_{z})\\hat{\\mathbf{y}} \\\\ \u0026amp;\\quad + B_{z}(A_{x}C_{x}+A_{y}C_{y}+A_{z}C_{z})\\hat{\\mathbf{z}} -C_{z}(A_{x}B_{x}+A_{y}B_{y}+A_{z}B_{z})\\hat{\\mathbf{z}} \\\\ =\u0026amp;\\ ({\\color{red}A_{x}B_{x}C_{x}}+A_{y}B_{x}C_{y}+A_{z}B_{x}C_{z}{\\color{red}-A_{x}B_{x}C_{x}}-A_{y}B_{y}C_{x}-A_{z}B_{z}C_{x})\\hat{\\mathbf{x}} \\\\ \u0026amp;\\quad + (A_{x}B_{y}C_{x}+{\\color{red}A_{y}B_{y}C_{y}}+A_{z}B_{y}C_{z}-A_{x}B_{x}C_{y} {\\color{red}-A_{y}B_{y}C_{y}}-A_{z}B_{z}C_{y})\\hat{\\mathbf{y}} \\\\ \u0026amp;\\quad + (A_{x}B_{z}C_{x}+A_{y}B_{z}C_{y}+{\\color{red}A_{z}B_{z}C_{z}}-A_{x}B_{x}C_{z}-A_{y}B_{y}C_{z} {\\color{red}-A_{z}B_{z}C_{z}})\\hat{\\mathbf{z}} \\\\ =\u0026amp;\\ (A_{y}B_{x}C_{y}+A_{z}B_{x}C_{z}-A_{y}B_{y}C_{x}-A_{z}B_{z}C_{x}) \\hat{\\mathbf{x}} \\\\ \u0026amp;\\quad +(A_{z}B_{y}C_{z}+A_{x}B_{y}C_{x}-A_{z}B_{z}C_{y}-A_{x}B_{x}C_{y}) \\hat{\\mathbf{y}} \\\\ \u0026amp;\\quad +( A_{x}B_{z}C_{x}+A_{y}B_{z}C_{y}-A_{x}B_{x}C_{z}-A_{y}B_{y}C_{z}) \\hat{\\mathbf{z}} \\end{align*} $$\n二つの式が同じであるため、次の式を得る。\n$$ \\mathbf{A} \\times (\\mathbf{B} \\times \\mathbf{C} )= \\mathbf{B}(\\mathbf{A} \\cdot \\mathbf{C} )-\\mathbf{C}(\\mathbf{A} \\cdot \\mathbf{B}) $$\n■\n従結論 $$ [\\mathbf{A} \\times (\\mathbf{B} \\times \\mathbf{C})] +[\\mathbf{B} \\times (\\mathbf{C} \\times \\mathbf{A})] +[\\mathbf{C} \\times (\\mathbf{A} \\times \\mathbf{B})] = \\mathbf{0} $$\n証明 BAC-CAB公式により、次の式が成り立つ。\n$$ \\begin{align*} \\mathbf{A} \\times (\\mathbf{B} \\times \\mathbf{C}) =\u0026amp;\\ \\mathbf{B}(\\mathbf{A} \\cdot \\mathbf{C}) -\\mathbf{C} (\\mathbf{A} \\cdot \\mathbf{B}) \\\\ \\mathbf{B} \\times (\\mathbf{C} \\times \\mathbf{B}) =\u0026amp;\\ \\mathbf{C}(\\mathbf{B} \\cdot \\mathbf{A}) -\\mathbf{A} (\\mathbf{B} \\cdot \\mathbf{C}) \\\\ \\mathbf{C} \\times (\\mathbf{A} \\times \\mathbf{B}) =\u0026amp;\\ \\mathbf{A}(\\mathbf{C} \\cdot \\mathbf{B}) -\\mathbf{B} (\\mathbf{C} \\cdot \\mathbf{A}) \\end{align*} $$\n上記の式を全部足すと、次のようになる。\n$$ \\begin{align*} \u0026amp; \\left[\\mathbf{A} \\times (\\mathbf{B} \\times \\mathbf{C})\\right] +\\left[\\mathbf{B} \\times (\\mathbf{C} \\times \\mathbf{A})\\right] +\\left[\\mathbf{C} \\times (\\mathbf{A} \\times \\mathbf{B})\\right] \\\\ =\u0026amp;\\ \\mathbf{B}(\\mathbf{A} \\cdot \\mathbf{C}) -\\mathbf{C} (\\mathbf{A} \\cdot \\mathbf{B})+\\mathbf{C}(\\mathbf{B} \\cdot \\mathbf{A}) -\\mathbf{A} (\\mathbf{B} \\cdot \\mathbf{C}) +\\mathbf{A}(\\mathbf{C} \\cdot \\mathbf{B}) -\\mathbf{B} (\\mathbf{C} \\cdot \\mathbf{A}) \\\\ =\u0026amp;\\ \\left[\\mathbf{A}(\\mathbf{C} \\cdot \\mathbf{B}) -\\mathbf{A} (\\mathbf{B} \\cdot \\mathbf{C})\\right] +\\left[\\mathbf{B}(\\mathbf{A} \\cdot \\mathbf{C}) -\\mathbf{B} (\\mathbf{C} \\cdot \\mathbf{A}) \\right] +\\left[\\mathbf{C}(\\mathbf{B} \\cdot \\mathbf{A}) -\\mathbf{C} (\\mathbf{A} \\cdot \\mathbf{B})\\right] \\\\ =\u0026amp;\\ \\mathbf{0} \\end{align*} $$\n■\n","id":71,"permalink":"https://freshrimpsushi.github.io/jp/posts/71/","tags":null,"title":"ベクトル三重積、BAC-CAB公式"},{"categories":"해석개론","contents":"要旨1 $$ \\begin{equation} { { e ^ x } }=\\sum _{ n=0 }^{ \\infty }{ \\frac { { x } ^{ n } }{ n! } } \\end{equation} $$\n$$ \\begin{equation} \\sin x=\\sum _{ n=0 }^{ \\infty }{ \\frac { { x } ^{ 2n+1 } }{ (2n+1)! }{ { (-1) }^{ n } } } \\end{equation} $$\n$$ \\begin{equation} \\cos x=\\sum _{ n=0 }^{ \\infty }{ \\frac { { x } ^{ 2n } }{ (2n)! }{ { (-1) }^{ n } } } \\end{equation} $$\n説明 指数関数、サイン関数、コサイン関数のMaclaurinシリーズは、難しい技術を使わずに簡単に求めることができる。これら３つを上手く合わせると、その有名なEulerの公式になる。ヒントとして、サインは次数が奇数の項のみ、コサインは次数が偶数の項のみ持つことを覚えておくといい。\n証明 $(1)$ ${ \\left( { { e ^ x } } \\right) ^{ (n) } }={ { e ^ x } }$ のため\n$$ { { e ^ x } }=\\frac { { x } ^{ 0 } }{ 0! } { e }^{ 0 } +\\frac { { x } ^{ 1 } }{ 1! } { e }^{ 0 } +\\frac { { x } ^{ 2 } }{ 2! } { e }^{ 0 } + \\cdots =\\sum _{ n=0 }^{ \\infty }{ \\frac { { x } ^{ n } }{ n! } } $$\n■\n$(2)$ $k=0,1,2, \\cdots$ に関して\n$$ { (\\sin x) } ^{ (n) }= \\begin{cases} \\cos x \u0026amp; , n=4k+1 \\\\ \\pm \\sin x \u0026amp; , n=2k \\\\ -\\cos x \u0026amp; , n=4k+3 \\end{cases} $$\nだから\n$$ { (\\sin 0) } ^{ (n) }=\\begin{cases} 1 \u0026amp; , n=4k+1 \\\\ 0 \u0026amp; , n=2k \\\\ -1 \u0026amp; , n=4k+3 \\end{cases} $$\nそして、シリーズ形式で表すと\n$$ \\begin{align*} \\sin x =\u0026amp; \\frac { { x } ^{ 0 } }{ 0! }0+\\left( \\frac { { x } ^{ 1 } }{ 1! }1+\\frac { { x } ^{ 2 } }{ 2! }0+\\frac { { x } ^{ 3 } }{ 3! }(-1)+\\frac { { x } ^{ 4 } }{ 4! }0 \\right) + \\cdots \\\\ =\u0026amp; \\frac { x }{ 1! }-\\frac { { x } ^{ 3 } }{ 3! }+\\frac { { x } ^{ 5 } }{ 5! }-\\frac { { x } ^{ 7 } }{ 7! }+ \\cdots \\\\ =\u0026amp; \\sum _{ n=0 }^{ \\infty }{ \\frac { { x } ^{ 2n+1 } }{ (2n+1)! }{ { (-1) }^{ n } } } \\end{align*} $$\nまとめると、$\\displaystyle \\sin x=\\sum _{ n=0 }^{ \\infty }{ \\frac { { x } ^{ 2n+1 } }{ (2n+1)! }{ { (-1) }^{ n } } }$ を得る。\n■\n$(3)$ $k=0,1,2, \\cdots$ に関して\n$$ { (\\cos x) }^{ (n) } = \\begin{cases} \\cos x \u0026amp; , n=4k \\\\ \\pm \\sin x \u0026amp; , n\\neq 2k \\\\ -\\cos x \u0026amp; , n=4k+2 \\end{cases} $$\nだから\n$$ { (\\cos 0) } ^{ (n) } = \\begin{cases} 1 \u0026amp; , n=4k \\\\ 0 \u0026amp; , n\\neq 2k \\\\ -1 \u0026amp; , n=4k+2 \\end{cases} $$\nそして、シリーズ形式で表すと\n$$ \\begin{align*} \\cos x =\u0026amp; \\left( \\frac { { x } ^{ 0 } }{ 0! }1+\\frac { { x } ^{ 1 } }{ 1! }0+\\frac { { x } ^{ 2 } }{ 2! }(-1)+\\frac { { x } ^{ 3 } }{ 3! }0 \\right) +\\frac { { x } ^{ 4 } }{ 4! }1+ \\cdots \\\\ =\u0026amp; \\frac { 1 }{ 0! }-\\frac { { x } ^{ 2 } }{ 2! }+\\frac { { x } ^{ 4 } }{ 4! }-\\frac { { x } ^{ 6 } }{ 6! }+ \\cdots \\\\ =\u0026amp; \\sum _{ n=0 }^{ \\infty }{ \\frac { { x } ^{ 2n } }{ (2n)! }{ { (-1) }^{ n } } } \\end{align*} $$\nまとめると、$\\displaystyle \\cos x=\\sum _{ n=0 }^{ \\infty }{ \\frac { { x } ^{ 2n } }{ (2n)! }{ { (-1) }^{ n } } }$ を得る。\n■\n龍谷大学基礎教育院, 工学部のための大学数学 (2012), p221-222\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":59,"permalink":"https://freshrimpsushi.github.io/jp/posts/59/","tags":null,"title":"指数関数、正弦関数、余弦関数のテイラー展開"},{"categories":"교과과정","contents":"公式 二次方程式 $ax^{2}+bx+c=0$ について(ここで $a\\neq 0$): $$ x=\\dfrac{ -b\\pm \\sqrt { b^{2}-4ac } }{2a} $$\n説明 二次方程式が与えられた場合、その根は公式を通じて簡単に見つけることができる。\n導出 戦略: 公式を導くキーは、完全平方形に変換することだ。数学が苦手な子供たちのために、できるだけ詳しく説明した。質問せずにただ従って、何度も繰り返してみること。\n$$ \\begin{align*} \u0026amp;\u0026amp; ax^{2} + bx + c =\u0026amp; 0 \\\\ \\implies \u0026amp;\u0026amp; \\ ax^{2} + bx =\u0026amp; -c \\\\ \\implies \u0026amp;\u0026amp; x^{2} + \\dfrac{b}{a}x =\u0026amp; -\\dfrac{c}{a} \\\\ \\implies \u0026amp;\u0026amp; x^{2} + \\dfrac{b}{a}x + \\left( \\dfrac{ b^{2} }{ 4a^{2} }-\\dfrac{ b^{2} }{ 4a^{2} } \\right) =\u0026amp; -\\dfrac{c}{a} \\text{(완전제곱꼴을 만들기 위한 트릭)} \\\\ \\implies \u0026amp;\u0026amp; \\left( x^{2}+\\dfrac{b}{a}x+\\dfrac{ b^{2} }{ 4a^{2} } \\right) -\\dfrac{ b^{2} }{ 4a^{2} } =\u0026amp; -\\dfrac{c}{a} \\\\ \\implies \u0026amp;\u0026amp; \\left( x^{2}+\\dfrac{b}{a}x+\\dfrac{ b^{2} }{ 4a^{2} } \\right) =\u0026amp; \\dfrac{ b^{2} }{ 4a^{2} }-\\dfrac{c}{a} \\\\ \\implies \u0026amp;\u0026amp; \\left( x^{2}+\\dfrac{b}{a}x+\\dfrac{ b^{2} }{ 4a^{2} } \\right) =\u0026amp; \\dfrac{ b^{2} }{ 4a^{2} }-\\dfrac{ 4ac }{ 4a^{2} } \\\\ \\implies \u0026amp;\u0026amp; \\left( x^{2}+\\dfrac{b}{a}x+\\dfrac{ b^{2} }{ 4a^{2} } \\right) =\u0026amp; \\dfrac{ b^{2}-4ac }{ 4a^{2} } \\\\ \\implies \u0026amp;\u0026amp; \\left( x+\\dfrac{b}{2a} \\right) \\left( x+\\dfrac{b}{2a} \\right) =\u0026amp; \\dfrac{ b^{2}-4ac }{ 4a^{2} } \\\\ \\implies \u0026amp;\u0026amp; { \\left( x+\\dfrac{b}{2a} \\right) }^{ 2 } =\u0026amp; \\dfrac{ b^{2}-4ac }{ 4a^{2} } \\\\ \\implies \u0026amp;\u0026amp; \\left( x+\\dfrac{b}{2a} \\right) =\u0026amp; \\pm \\sqrt { \\dfrac{ b^{2}-4ac }{ 4a^{2} } } \\text{(양변에 루트를 취함)} \\\\ \\implies \u0026amp;\u0026amp; x+\\dfrac{b}{2a} =\u0026amp; \\pm \\sqrt { \\dfrac{ b^{2}-4ac }{ 4a^{2} } } \\\\ \\implies \u0026amp;\u0026amp; x+\\dfrac{b}{2a} =\u0026amp; \\pm \\dfrac{ \\sqrt { b^{2}-4ac } }{2a} \\\\ \\implies \u0026amp;\u0026amp; x =\u0026amp; -\\dfrac{b}{2a}\\pm \\dfrac{ \\sqrt { b^{2}-4ac } }{2a} \\\\ \\implies \u0026amp;\u0026amp; x =\u0026amp; \\dfrac{ -b\\pm \\sqrt { b^{2}-4ac } }{2a} \\end{align*} $$\n■\n","id":56,"permalink":"https://freshrimpsushi.github.io/jp/posts/56/","tags":null,"title":"二次方程式の解の公式の導出 ステップ バイ ステップ"},{"categories":"보조정리","contents":"まとめ $$ ({a}^{2}+{b}^{2})({x}^{2}+{y}^{2})\\ge { (ax+by) }^{ 2 } $$\n証明 $$ \\begin{align*} \u0026amp; ({a}^{2}+{b}^{2})({x}^{2}+{y}^{2})-{ (ax+by) }^{ 2 } \\\\ =\u0026amp; {a}^{2}{x}^{2}+{b}^{2}{x}^{2}+{a}^{2}{y}^{2}+{b}^{2}{y}^{2}-{ (ax+by) }^{ 2 } \\\\ =\u0026amp; {b}^{2}{x}^{2}+{a}^{2}{y}^{2}-2axby \\\\ =\u0026amp; { (ay-bx) }^{ 2 } \\\\ \\ge\u0026amp; 0 \\end{align*} $$ よって、下記を得る。 $$ ({a}^{2}+{b}^{2})({x}^{2}+{y}^{2})\\ge { (ax+by) }^{ 2 } $$\n■\n説明 高校の授業から接することができる不等式で、分野を問わず多くの場所で使用されている。代数的証明は非常に簡単である。\n証明プロセスで分かるように、等号が成立するのは$ay-bx=0$の場合のみである。コーシー・シュワルツの不等式は、証明中に現れる項を含め、方程式の形で表すこともできる。\n$$ ({a}^{2}+{b}^{2})({x}^{2}+{y}^{2})={ (ax+by) }^{ 2 }+{ (ay-bx) }^{ 2 } $$\nこれはある平方数の和が他の平方数の和の積として表されることを示唆している。\n一般化 一般化された証明 ","id":51,"permalink":"https://freshrimpsushi.github.io/jp/posts/51/","tags":null,"title":"コーシー・シュヴァルツの不等式の証明"},{"categories":"교과과정","contents":"要約 $$ \\sin\\left( \\alpha +\\beta \\right) =\\sin\\alpha \\cos\\beta +\\cos\\alpha \\sin\\beta \\\\ \\sin\\left( \\alpha -\\beta \\right) =\\sin\\alpha \\cos\\beta -\\cos\\alpha \\sin\\beta \\\\ \\cos\\left( \\alpha +\\beta \\right) =\\cos\\alpha \\cos\\beta -\\sin\\alpha \\sin\\beta \\\\ \\cos\\left( \\alpha -\\beta \\right) =\\cos\\alpha \\cos\\beta +\\sin\\alpha \\sin\\beta \\\\ \\tan\\left( \\alpha +\\beta \\right) =\\frac { \\tan\\alpha +\\tan\\beta }{ 1-\\tan\\alpha \\tan\\beta } \\\\ \\tan\\left( \\alpha -\\beta \\right) =\\frac { \\tan\\alpha -\\tan\\beta }{ 1+\\tan\\alpha \\tan\\beta } $$\n証明 コサイン法則を用いた証明 ピタゴラスの定理によると $$ \\begin{align*} {\\overline { AB } } ^{ 2 } =\u0026amp; {( \\cos \\alpha -\\cos \\beta )}^{ 2 }+{(\\sin\\alpha -\\sin\\beta )}^{ 2 } \\\\ =\u0026amp; 2-2 \\cos \\alpha \\cos \\beta –2 \\sin \\alpha \\sin \\beta \\end{align*} $$\n第二コサイン法則により\n$$ \\begin{align*} { \\overline { AB } } ^{ 2 } =\u0026amp; 1^{ 2 }+1^{ 2 }-2\\cos(\\beta -\\alpha ) \\\\ =\u0026amp; 2-2\\cos(\\beta -\\alpha ) \\end{align*} $$\nこれら二つの式の右辺は等しいので\n$$ \\cos(\\beta -\\alpha )=\\cos\\alpha \\cos\\beta +\\sin\\alpha \\sin\\beta $$\n■\n最も基本的な証明法で、色々な方法があるけど、最初は普通この方法に触れる。\nベクトルの内積を用いた証明 $$ \\begin{align*} \\cos(\\beta -\\alpha ) =\u0026amp; \\frac { \\vec { OA }\\cdot \\vec { OB } }{ \\left| \\vec { OA } \\right| \\left| \\vec { OB } \\right| } \\\\ =\u0026amp; \\cos\\alpha \\cos\\beta +\\sin\\alpha \\sin\\beta \\end{align*} $$\n■\nベクトルの内積を紙に書くと実は一行になるのと同じ。アイディアもシンプルで最も簡単な方法だ。\n三角形を使った証明 (1) 三角形の面積を $S$ とすると\n$$ S=\\frac { 1 }{ 2 }ab\\sin(\\alpha +\\beta ) $$\n(2) 垂直を境にした二つの三角形の面積を足すと\n$$ S=\\frac { 1 }{ 2 }bh\\sin\\alpha +\\frac { 1 }{ 2 }ah\\sin\\beta $$\nこのとき $h=b\\cos\\alpha =a\\cos\\beta$ なので\n$$ S=\\frac { 1 }{ 2 }ab\\cos\\beta \\sin\\alpha +\\frac { 1 }{ 2 }ab\\cos\\alpha \\sin\\beta $$\n(1)と (2)で求めたものは共に $S$ なので、両辺の $\\frac { 1 }{ 2 }ab$ を削除すれば\n$$ \\sin(\\alpha +\\beta )=\\cos\\beta \\sin\\alpha +\\cos\\alpha \\sin\\beta $$\n■\n三角形の面積を利用した証明では、アイディアはシンプルだし $h$ を上手く扱うことがポイント。\n回転変換を用いた証明 点 $A$ を原点に対して $\\beta$ だけ回転変換すると\n$$ \\begin{bmatrix} \\cos(\\alpha +\\beta ) \\\\ \\sin(\\alpha +\\beta ) \\end{bmatrix} = \\begin{bmatrix} { \\cos\\beta }\u0026amp;{ -\\sin\\beta } \\\\ { \\sin\\beta }\u0026amp;{ \\cos\\beta } \\end{bmatrix} \\begin{bmatrix} { \\cos\\alpha } \\\\ { \\sin\\alpha } \\end{bmatrix} \\\\ \\implies \\begin{cases} \\cos(\\alpha +\\beta )=\\cos\\beta \\cos\\alpha -\\sin\\beta \\sin\\alpha \\\\ { \\sin(\\alpha +\\beta )=\\sin\\beta \\cos\\alpha +\\cos\\beta \\sin\\alpha } \\end{cases} $$\n■\n回転変換を用いた証明。角度を少し変える必要があるけど、コサインとサインについて同時に得られるのでいい。\n結論 これらの場合は思った以上によく使われるので、覚えておくと便利。\n$$ \\begin{align*} \\sin(\\frac { \\pi }{ 4 }+\\frac { \\pi }{ 6 })=\\cos(\\frac { \\pi }{ 4 }-\\frac { \\pi }{ 6 })=\\frac { \\sqrt { 3 }+1 }{ 2\\sqrt { 2 } } \\\\ \\sin(\\frac { \\pi }{ 4 }-\\frac { \\pi }{ 6 })=\\cos(\\frac { \\pi }{ 4 }+\\frac { \\pi }{ 6 })=\\frac { \\sqrt { 3 }-1 }{ 2\\sqrt { 2 } } \\end{align*} $$ タンジェントの加法定理： $$ \\tan ( \\theta_1 \\pm \\theta_2) = \\dfrac{\\tan\\theta_1 \\pm \\tan\\theta_2}{1 \\mp \\tan\\theta_1\\tan\\theta_2} $$ タンジェントの加法定理の証明 $$ \\tan (\\theta_1 \\pm \\theta2)=\\dfrac{\\sin ( \\theta_1 \\pm \\theta_2)}{\\cos ( \\theta_1 \\pm \\theta_2)} =\\dfrac{ \\sin \\theta_1 \\cos \\theta_2 \\pm \\sin \\theta_2 \\cos \\theta_2}{\\cos \\theta_1 \\cos\\theta_2 \\mp \\sin\\theta_1 \\sin\\theta_2} $$ 分子と分母を $\\cos\\theta_1\\cos\\theta_2$ で割ると $$ \\dfrac{ \\dfrac{\\sin \\theta_1}{ \\cos \\theta_1} \\pm \\dfrac{\\sin \\theta_2}{ \\cos \\theta_1} } { 1 \\mp \\dfrac{\\sin\\theta_1 \\sin\\theta_2}{\\cos\\theta_1\\cos\\theta_2 }} = \\dfrac{ \\tan\\theta_1 \\pm \\tan\\theta_2}{1 \\mp \\tan\\theta_1\\tan\\theta_2} $$\n■\n","id":44,"permalink":"https://freshrimpsushi.github.io/jp/posts/44/","tags":null,"title":"三角関数の加法定理：様々な証明"},{"categories":"해석개론","contents":"要約1 関数 $f(x)$ が $[a,b]$ で連続であり、$(a,b)$ で $n$ 回微分可能であれば、\n$$ \\begin{align*} f(b) =\u0026amp; \\sum_{k=0}^{n-1} {{(b-a)^{k}\\over{k!}}{f^{(k)}( a )}} + {(b-a)^{n}\\over{n!}}{f^{(n)}(\\xi)} \\\\ =\u0026amp; {f(a)} + {(b-a)f ' (a)} + \\cdots + {(b-a)^{n-1}\\over{(n-1)!}}{f^{(n-1)}(a)} + {(b-a)^{n}\\over{(n)!}}{f^{(n)}(\\xi)} \\end{align*} $$\nを満たす $\\xi \\in (a,b)$ が存在する。\n説明 数学全般で非常に重要な定理で、この定理にちなんでテイラー級数がある。$n$回微分するという意味においては、平均値の定理を一般化したものと言える。\n通常、テイラーの定理を使用する時は、$c$ではなく、$\\xi$を使用する。\n証明 $$ \\begin{align*} f(b) :=\u0026amp; {(b-a)^0\\over{0!}}{f(a)} + {(b-a)^1\\over{1!}}{f ' (a)} + {(b-a)^2\\over{2!}}{f '' (a)} \\\\ \u0026amp;+ \\cdots + {(b-a)^{n-1}\\over{(n-1)!}}{f^{(n-1)}(a)} + {(b-a)^{n}\\over{(n)!}}c \\end{align*} $$\nとしよう。$c={f^{(n)}(\\xi)}$ を示せば証明は終わる。関数 $g$ を次のように定義する。\n$$ \\begin{align*} g(x):=\u0026amp; -f(b) + f(x) + {(b-x)^1\\over{1!}}{f ' (x)} + {(b-x)^2\\over{2!}}{f '' (x)} \\\\ \u0026amp; + \\cdots + {(b-x)^{n-1}\\over{(n-1)!}}{f^{(n-1)}(x)} + {(b-x)^{n}\\over{(n)!}}c \\\\ =\u0026amp; -f(b) + \\sum_{k=0}^{n-1}{(b-x)^{k}\\over{(k)!}}{f^{(k)}(x)} + {(b-x)^{n}\\over{(n)!}}c \\end{align*} $$\n$g$ は $[a,b]$ で連続で、$(a,b)$ で微分可能であり、$c$ の定義により $g(b)=g(a)=0$である。\nロルの定理: 関数 $f(x)$が $[a,b]$で連続で、$(a,b)$で微分可能で、$f(a)=f(b)$ ならば、$(a,b)$ で少なくとも一つの $\\xi$ が $f ' (\\xi)=0$ を満たす。\n$h(x)$ を $$ \\begin{align*} h(x):=\u0026amp; \\left[ \\sum_{k=0}^{n-1}{(b-x)^{k}\\over{(k)!}}{f^{(k)}(x)} \\right] ' \\\\ =\u0026amp; \\left[ {(b-x)^{0}\\over{(0)!}}{f^{(0)}(x)} + {(b-x)^{1}\\over{(1)!}}{f^{(1)}(x)} + \\cdots + {(b-x)^{n-1}\\over{(n-1)!}}{f^{(n-1)}(x)} \\right] ' \\\\ =\u0026amp; \\left[ f (x) + {(b-x)^{1}\\over{(1)!}}{f^{(1)}(x)} + \\cdots + {(b-x)^{n-1}\\over{(n-1)!}}{f^{(n-1)}(x)} \\right] ' \\\\ =\u0026amp; f^{(1)} (x) - \\left[ f^{(1)} (x) + {(b-x)^{1}\\over{(1)!}}{f^{(2)}(x)} \\right] \\\\ \u0026amp; + \\left[ - {(b-x)^{1}\\over{(1)!}} f^{(2)} (x) + {(b-x)^{2}\\over{(2)!}}{f^{(3)}(x)} \\right] \\\\ \u0026amp; \\vdots \\\\ \u0026amp; + \\left[ - {(b-x)^{n-3}\\over{(n-3)!}} f^{(n-2)} (x) + {(b-x)^{n-2}\\over{(n-2)!}}{f^{(n-1)}(x)} \\right] \\\\ \u0026amp; + \\left[ - {(b-x)^{n-2}\\over{(n-2)!}} f^{(n-1)} (x) + {(b-x)^{n-1}\\over{(n-1)!}}{f^{(n)}(x)} \\right] \\\\ =\u0026amp; {(b-x)^{n-1}\\over{(n-1)!}}{f^{(n)}(x)} \\end{align*} $$\nとし、$\\displaystyle g ' (x) = 0 + h(x) + {(b-x)^{n-1}\\over{(n-1)!}}c$ であるため、ロルの定理により、\n$$ \\begin{align*} g ' (\\xi) =\u0026amp; h(\\xi) - {(b-\\xi)^{n-1}\\over{(n-1)!}}c \\\\ =\u0026amp; {(b-\\xi)^{n-1}\\over{(n-1)!}}{f^{(n)}(\\xi)} - {(b-\\xi)^{n-1}\\over{(n-1)!}}c \\\\ =\u0026amp; 0 \\end{align*} $$\nを満たす $\\xi$ が $(a,b)$ に少なくとも一つ存在する。したがって、\n$$ \\begin{align*} \u0026amp;\u0026amp; {(b-\\xi)^{n-1}\\over{(n-1)!}}{f^{(n)}(\\xi)} - {(b-\\xi)^{n-1}\\over{(n-1)!}}c =\u0026amp; 0 \\\\ \\implies \u0026amp;\u0026amp; {(b-\\xi)^{n-1}\\over{(n-1)!}}{f^{(n)}(\\xi)} =\u0026amp; {(b-\\xi)^{n-1}\\over{(n-1)!}}c \\\\ \\implies \u0026amp;\u0026amp; {f^{(n)}(\\xi)} =\u0026amp; c \\end{align*} $$\n$c={f^{(n)}(\\xi)}$ を示したので、証明が終了した。\n■\n上記のように証明したが、より一般的に使用される形は以下の通りである。もちろん$x \\in [a,b]$で、$x_{0} \\in (a,b)$として、実質的に$[x_{0} , x] \\subset [a,b]$となる。\nテイラーの定理\n関数 $f(x)$ が $[a,b]$ で連続であり、$(a,b)$ で $n$ 回微分可能であれば、$x_{0} \\in (a,b)$に対して、\n$$ f(x) = \\sum_{k=0}^{n-1} {{( x - x_{0} )^{k}\\over{ k! }}{f^{(k)}( x_{0} )}} + {(x - x_{0} )^{n}\\over{ n! }}{f^{(n)}(\\xi)} $$\nを満たす $\\xi \\in (a,b)$ が存在する。\n参照 多変数関数のテイラーの定理 慶北大学校基礎教育院, 理工学生のための大学数学 (2012), p67-68\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":41,"permalink":"https://freshrimpsushi.github.io/jp/posts/41/","tags":null,"title":"テイラーの定理の証明"},{"categories":"해석개론","contents":"要約1 関数$f$が点$a$の近くで無限に微分可能で、$\\displaystyle f(x) = \\sum_{n=0}^{\\infty} {{f^{(n)} (a)}\\over{n!}} {(x-a)}^n$の必要十分条件はある$\\xi \\in \\mathscr{H} \\left\\{ x , a \\right\\}$に対して\n$$ \\lim_{n \\to \\infty} {{f^{(n)} (\\xi)}\\over{n!}} {(x-a)}^n = 0 $$\n$\\xi \\in \\mathscr{H} \\left\\{ x , a \\right\\}$とは、$\\xi$が$(x,a)$または$(a,x)$にあるという表現だ。\n説明 テイラー定理は関数が無限に微分可能な場合、一般に無限級数の形で表される。これをテイラー級数と呼び、特に$a=0$の場合、マクローリン級数と呼ばれる。テイラー級数は テイラー公式, テイラー展開ともよく呼ばれる。\n証明 テイラー定理\n関数$f(x)$が$[a,b]$で連続であり、$(a,b)$で$n$回微分可能なら$x_{0} \\in (a,b)$に対して\n$$ f(x) = \\sum_{k=0}^{n-1} {{( x - x_{0} )^{k}\\over{ k! }}{f^{(k)}( x_{0} )}} + {(x - x_{0} )^{n}\\over{ n! }}{f^{(n)}(\\xi)} $$\nを満たす$\\xi \\in (a,b)$が存在する。\nテイラー定理により、\n$$ f(x) = \\sum_{k=0}^{n-1} {{( x - a )^{k}\\over{ k! }}{f^{(k)}( a )}} + {(x - a )^{n}\\over{ n! }}{f^{(n)}(\\xi)} $$\n$\\xi$が$x$と$a$の間に少なくとも1つ存在する。関数$f$は無限に微分可能なので、\n$$ f(x) =\\lim_{n \\to \\infty} \\left[ \\sum_{k=0}^{n-1} {{f^{(k)} (a)}\\over{k!}} {(x-a)}^k + {{f^{(n)} (a)}\\over{n!}} {(x-a)}^n \\right] $$\nもし$\\displaystyle \\lim_{n \\to \\infty} {{f^{(n)} (a)}\\over{n!}} {(x-a)}^n = 0$なら、\n$$ f(x) =\\lim_{n \\to \\infty} \\sum_{k=0}^{n-1} {{f^{(k)} (a)}\\over{k!}} {(x-a)}^k = \\sum_{n=0}^{\\infty} {{f^{(n)} (a)}\\over{n!}} {(x-a)}^n $$\n■\n慶北大学校基礎教育院、理工学生のための大学数学 (2012)、p220-221\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":42,"permalink":"https://freshrimpsushi.github.io/jp/posts/42/","tags":null,"title":"テイラー級数とマクローリン級数"},{"categories":"함수","contents":"定義 $f(-x) = f(x)$ を満たす関数 $f(x)$ を偶関数Evenという。 $f(-x) = -f(x)$ を満たす関数 $f(x)$ を奇関数Oddという。 説明 偶関数は座標平面で$y$ 軸に関して対称な関数、奇関数は原点$O$ に関して対称な関数だ。\n例として三角関数の中で、奇関数の$\\sin$ と偶関数の$\\cos$ を挙げることができる。$\\sin$ を微分すると$\\cos$ が、$\\cos$ を微分すると$\\sin$ が得られる。必要ないように見えるかもしれないけど、関数を正確に知らなくても使える状況で便利だ。\n導関数 $f$ が実数全体で微分可能なら、次が成り立つ。\n[1] 偶関数の導関数は奇関数だ。 [2] 奇関数の導関数は偶関数だ。 導出 $f(x)$ を任意の奇関数、$g(x)$ を任意の偶関数とする。\n$f(x)=-f(-x)$ であることから、 $$ f ' (x)=f ' (-x) $$ $g(x)=g(-x)$ であることから、 $$ g ' (x)=-g ' (-x) $$\n■\n系 もう一つ覚えておくといいことが、偶関数 $g(x)$ の導関数 $g ' (x)$ は、常に $g ' (0)=0$ である。\n証明 $$ \\begin{align*} \u0026amp; g ' (x)=-g ' (-x) \\\\ \\implies\u0026amp; g ' (0)=-g ' (-0) \\\\ \\implies\u0026amp; 2g ' (0)=0 \\\\ \\implies\u0026amp; g ' (0)=0 \\end{align*} $$\n■\n","id":40,"permalink":"https://freshrimpsushi.github.io/jp/posts/40/","tags":null,"title":"奇関数と偶関数"},{"categories":"해석개론","contents":"要約1 関数 $f(x)$ が $[a,b]$ で連続であり、$(a,b)$ で微分可能であり、$f(a)=f(b)$ の場合、$f ' (c)=0$ を満たす $c$ が $(a,b)$ に少なくとも一つ存在する。\n説明 高校の授業では、平均値の定理を証明するための補助定理としてだけ紹介され、それ以外では全く使われないが、高校のレベルを超えると、時々補助定理として使われることがある。平均値の定理がより一般的なのは事実だが、$\\displaystyle f '(c) = {{f(b) - f(a)} \\over {b - a}}$ のような複雑な形を使う必要がない場合は、証明をより簡潔にする。\n証明 戦略: $f(x)$ が定数関数の場合とそうでない場合の二つに分けて、フェルマーの定理を適用する。\nケース 1. $f(x)$ が定数関数の場合\n$f ' (x)=0$ であるため、$f ' (c)=0$ を満たす $c$ が $(a,b)$ に少なくとも一つ存在する。\nケース 2. $f(x)$ が定数関数ではない場合\n$f(x)$ は極大または極小を持ち、$(a,b)$ で微分可能なため、極点 $c$ に対して $f ' (c)$ が存在する。\nフェルマーの定理\n関数 $f(x)$ が $x=c$ で極大または極小を持ち、$f ' (c)$ が存在するならば、$f ' (c) = 0$\n極点 $c$ は、フェルマーの定理により、$f ' (c) = 0$ を満たさなければならない。\nしたがって、$f(x)$ が定数関数であろうとなかろうと、$f ' (c)=0$ を満たす $c$ が $(a,b)$ に少なくとも一つ存在する。\n■\n경북대학교 기초교육원, 이공학도를 위한 대학수학 (2012), p65\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":36,"permalink":"https://freshrimpsushi.github.io/jp/posts/36/","tags":null,"title":"微分積分学におけるロルの定理の証明"},{"categories":"해석개론","contents":"要約1 関数 $f(x)$ が $[a,b]$ で連続であり、$(a,b)$ で微分可能ならば、$\\displaystyle f '(c)={{f(b)-f(a)}\\over{b-a}}$ を満たす$c$ が $(a,b)$ 内に少なくとも一つ存在する。\n説明 ただよく使われるというだけではなく、MVTという略語を使用するほど有名な定理だ。平均値という言葉は、微分係数が全区間の平均変化率と同じになる点があるという意味から来ている。平均という概念が有用であるため、さまざまな分野に適用するための異なる変形形式が存在する。\n証明 $\\displaystyle m:= {{f(b)-f(a)}\\over{b-a}}$ とし、$g(x):=f(x)-mx$ を定義すると、$g(b)=g(a)$ であり、$g(x)$ は微分可能である。\nロルの定理\n関数 $f(x)$ が $[a,b]$ で連続であり、$(a,b)$ で微分可能で、$f(a)=f(b)$ ならば、$c$ が $(a,b)$ 内に少なくとも一つ存在して、$f ' (c)=0$ を満たす。\nロルの定理により、$g ' (c)=0$ を満たす$c$ が $(a,b)$ 内に少なくとも一つ存在し、$g ' (x)=f ' (x) - m$ なので、$g ' (c) = f '(c) - m = 0$ である。$f ' (c) -m = 0$ から$(-m)$ に項を移動すると、$\\displaystyle f '(c) = m = {{f(b)-f(a)}\\over{b-a}}$ を満たす $c$ が $(a,b)$ 内に少なくとも一つ存在することがわかる。\n■\n参照 コーシーの平均値の定理 積分の平均値の定理 ガウスの平均値の定理 慶北国立大学基礎教育院, 理工学生のための大学数学 (2012), p65-66\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":37,"permalink":"https://freshrimpsushi.github.io/jp/posts/37/","tags":null,"title":"微分積分学における平均値定理の証明"},{"categories":"수리통계학","contents":"要約 1 標本空間 $S$ と事象 $A$、確率 $P$ $\\left\\{ S_1, S_2, \\cdots ,S_n \\right\\}$ が $S$ の分割だとすると、次が成り立つ。 $$ P(S_k|A)=\\frac { P(S_k)P(A|S_k) }{ \\sum _{ k=1 }^{ n }{ P(S_k)P(A|S_k) } } $$\n定義 ベイズ定理の右辺である$P \\left( S_{k} \\right)$を事前確率Prior Probability、左辺にあたる$P \\left( S_{k} | A \\right)$を事後確率Posterior Probabilityと呼ぶ。これらの確率で構成される確率分布をそれぞれ事前分布Prior Distribution、事後分布Posterior Distribution\n説明 またベイズ法則Bayes\u0026rsquo; Ruleとも呼ばれるこの定理は、二つの法則だけで簡単に証明できるが、その応用は広大だ。いわゆるベイジアンパラダイムは、統計学を二分する思考法であり、その重要性は何度強調しても過言ではない。\n私たちが知りたいのは上式の左辺だ。既に知っているのは事象 $A$ と標本空間 $S$ の分割 $S_k$ が起こる確率、そしてそれぞれの分割が起こったときに $A$ が起こる確率だ。つまり、$S_k$とそれらが $A$ に与える影響についてすべて知っている状態から始める。ベイズの定理はここから反対に、$A$がそれぞれにどんな影響を与えるかを知ることができる定理だ。難しいと感じるなら、ただ左辺が知りたいということだけ考えても良い。\n証明 全確率の法則と確率の乗法定理によって、以下の式を得る。 $$ \\begin{align*} P(A)=\u0026amp;P(A\\cap S_1)+P(A\\cap S_2)+\u0026hellip;+P(A\\cap S_n) \\\\ =\u0026amp;P(S_1)P(A|S_1)+P(S_2)P(A|S_2)+\u0026hellip;+P(S_n)P(A|S_n) \\\\ =\u0026amp; \\sum _{ k=1 }^{ n }{ P(S_k)P(A|S_k) } \\end{align*} $$ 両辺に逆数を取ると $$ \\begin{align*} \u0026amp; \\frac { 1 }{ \\sum _{ k=1 }^{ n }{ P(S_k)P(A|S_k) } }=\\frac { 1 }{ P(A) } \\\\ \\implies\u0026amp; \\frac { P(A\\cap S_k) }{ \\sum _{ k=1 }^{ n }{ P(S_k)P(A|S_k) } }=\\frac { P(A\\cap S_k) }{ P(A) } \\\\ \\implies\u0026amp; \\frac { P(S_k)P(A|S_k) }{ \\sum _{ k=1 }^{ n }{ P(S_k)P(A|S_k) } }=P(S_k|A) \\end{align*} $$\n■\nHogg et al. (2013). Introduction to Mathematical Statistcs(7th Edition): p23.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":29,"permalink":"https://freshrimpsushi.github.io/jp/posts/29/","tags":null,"title":"ベイズの定理の証明と事前分布、事後分布"},{"categories":"복소해석","contents":"定理 1 $\\left\\{ a_{i} \\right\\}_{i=0}^{n} \\subset \\mathbb{R}$ で $a_0 \u0026gt; a_1 \u0026gt; \\cdots \u0026gt; a_n \u0026gt; 0$ としましょう。すると、多項関数 $$ P(z) := a_0 + a_1 z + \\cdots + a_{n-1} z^{n-1} + a_n z^n $$ において、あらゆる根 $z \\in \\mathbb{C}$ は $|z| \\ge 1$ を満たします。\n証明 もし $P(z) = 0$ の根が $z=1$ である場合、$\\displaystyle 0 = P(1) = \\sum_{i=0}^{n} a_{i} \u0026gt; 0$ より、根は $z \\ne 1$ でなければなりません。式 $P(z) = 0$ の両辺に $z$ を乗じて元の式から引き、$a_0$ を以下のように表せます。 $$ a_0 = (1-z)P(z) + (a_0 - a_1) z + \\cdots + (a_{n-1} - a_n) z^n + a_n z^{n+1} $$ ここで、$P(z) = 0$ の根 $z \\ne 1$ で $|z| \u0026lt; 1$ を仮定してみると、$a_0 \u0026gt; a_1 \u0026gt; \\cdots \u0026gt; a_n \u0026gt; 0$ より $$ \\begin{align*} \u0026amp; |a_0| \u0026lt; |(1-z)P(z)| + (a_0 - a_1) + \\cdots + (a_{n-1} - a_n) + a_n \\\\ \\implies\u0026amp; |a_0| \u0026lt; |(1-z)P(z)| + a_0 + (- a_1 + a_1) + \\cdots + (- a_{n-1} + a_{n-1} )+ (- a_n + a_n ) \\\\ \\implies\u0026amp; a_0 = |a_0| \u0026lt; |(1-z)P(z)| + a_0 \\\\ \\implies\u0026amp; 0 \u0026lt; |(1-z)P(z)| \\end{align*} $$ ですが、$z \\ne 1$ が $P(z) = 0$ の根であることを仮定しているので、以下の矛盾が生じます。 $$ 0 \u0026lt; |(1-z)P(z)| = 0 $$ これは、$| z | \u0026lt; 1$ という仮定が誤りであることを意味し、結果として $|z | \\ge 1$ でなければなりません。\n■\nOsborne. (1999). 複素変数とその応用: p. 6.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":5,"permalink":"https://freshrimpsushi.github.io/jp/posts/5/","tags":null,"title":"エーネストローム-カケヤ定理の証明"},{"categories":"해석개론","contents":"まとめ 調和級数は発散する。\n$$ \\sum _{ n=1 }^{ \\infty }{ \\frac { 1 }{ n } }=\\infty $$\n説明 調和級数は一見すると、その値が続けざまに小さくなるので収束しそうに見えるが、オレームはこれが発散することを非常に簡単で美しく証明した。こんな事実は主に絶対収束の概念を説明するための例としてよく使われていて、交互調和級数は$\\displaystyle \\sum_{n=1}^{\\infty} {{(-1)^{n-1}} \\over {n}} = 1- {1 \\over 2} + { 1 \\over 3} - { 1 \\over 4 }+ \\cdots = \\ln 2 \u0026lt; \\infty$として収束する一方で、その絶対値の列の級数である調和級数は$\\displaystyle \\sum_{n=1}^{\\infty} \\left| {{(-1)^{n-1}} \\over {n}} \\right| = \\sum _{ n=1 }^{ \\infty }{ \\frac { 1 }{ n } }=\\infty$が成立する。だから、「収束するからといって必ずしも絶対収束するわけではない」というのを説明する最も簡単な例になる。また、「無限級数が収束すれば、数列は0に収束する」という命題の逆が成立しないことを示す反例としてもいい。\n証明 証明の核心は、無限に多くの$1/2$の和が発散するが、その和よりも調和級数が大きいという事実にある。全く難しくなく、きれいな証明法だ。\n$$ \\begin{align*} \u0026amp; \\frac { 1 }{ 1 }+\\frac { 1 }{ 2 }+\\frac { 1 }{ 3 }+\\frac { 1 }{ 4 }+\\frac { 1 }{ 5 }+\\frac { 1 }{ 6 }+\\frac { 1 }{ 7 }+\\frac { 1 }{ 8 }+\\frac { 1 }{ 9 }+\\cdots \\\\ =\u0026amp; \\frac { 1 }{ 1 }+\\frac { 1 }{ 2 }+\\left( \\frac { 1 }{ 3 }+\\frac { 1 }{ 4 } \\right) +\\left( \\frac { 1 }{ 5 }+\\frac { 1 }{ 6 }+\\frac { 1 }{ 7 }+\\frac { 1 }{ 8 } \\right) +\\frac { 1 }{ 9 }+\\cdots \\\\ \u0026gt;\u0026amp; 1+\\frac { 1 }{ 2 }+\\left( \\frac { 1 }{ 4 }+\\frac { 1 }{ 4 } \\right) +\\left( \\frac { 1 }{ 8 }+\\frac { 1 }{ 8 }+\\frac { 1 }{ 8 }+\\frac { 1 }{ 8 } \\right) +\\cdots \\\\ =\u0026amp; 1+\\frac { 1 }{ 2 }+\\frac { 1 }{ 2 }+\\frac { 1 }{ 2 }+\\cdots \\end{align*} $$\n■\n一緒に見る 交互調和級数の収束性 ","id":17,"permalink":"https://freshrimpsushi.github.io/jp/posts/17/","tags":null,"title":"オイラーの調和級数の発散性に関する証明"},{"categories":"교과과정","contents":"式 $$ d=\\frac {|2k|}{\\sqrt{m^2+1}} $$\n説明 双曲線の接線の問題を解いていると、二つの接線の間の距離を求めることがよくあります。点から直線までの距離を求める公式があるため、それ自体を解くことは難しくありません。しかし、その距離を簡単かつ迅速に計算できる公式を知っていれば、少しでも計算量を減らすことができるでしょう。\n導出 二つの平行な直線の方程式を $y=mx\\pm k$ とします。ある点 $(x,y)$ から直線 $y=mx+k$ までの距離は $$ \\frac {|mx-y+k|}{\\sqrt{m^2+1}} $$ 直線 $y=mx-k$ 上の点 $(x_1,y_1)$ に対しては $$ k=mx_1-y_1 $$ これを距離の公式に代入すると $$ \\frac {|mx_1-y_1+k|}{\\sqrt{m^2+1}} = \\frac {|k+k|}{\\sqrt{m^2+1}} $$ したがって、二つの平行な直線 $y=mx\\pm k$ の間の距離は $$ \\frac {|2k|}{\\sqrt{m^2+1}} $$\n■\n","id":4,"permalink":"https://freshrimpsushi.github.io/jp/posts/4/","tags":null,"title":"二本の平行な直線の間の距離を求める公式の導出"},{"categories":"보조정리","contents":"定義 $n$ 個の正数 ${x}_1,{x}_2,\\cdots,{x}_n$ に対して算術平均、幾何平均、調和平均は以下のように定義される。\n算術平均 : $$ \\sum_{ k=1 }^{ n }{ \\frac { {x}_k }{ n } }=\\frac { {x}_1+{x}_2+\\cdots+{x}_n }{ n } $$ 幾何平均 : $$ \\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } }=\\sqrt [ n ]{ {x}_1{x}_2\\cdots{x}_n } $$ 調和平均 : $$ \\left( \\frac { \\sum_{ k=1 }^{ n }{ \\frac { 1 }{ {x}_k } } }{ n } \\right)^{-1}=\\frac { n }{ \\frac { 1 }{ {x}_1 }+\\frac { 1 }{ n{x}_2 }+\\cdots+\\frac { 1 }{ n{x}_n } } $$ 定理 これらの平均に対して、次の不等式が成り立つ。\n$$ \\frac { {x}_1+{x}_2+\\cdots+{x}_n }{ n }\\ge \\sqrt [ n ]{ {x}_1{x}_2\\cdots{x}_n }\\ge \\frac { n }{ \\frac { 1 }{ {x}_1 }+\\frac { 1 }{ n{x}_2 }+\\cdots+\\frac { 1 }{ n{x}_n } } $$\n説明 高校生であれば、算術・幾何平均について一度は耳にするかもしれないが、特定の名称で定義されることはあまりなく、通常は「算術幾何」という略称で口伝えにされることが一般的である。$n=2$ の場合には証明も簡単で、高校レベルの問題解決にも役立つ。高校生レベルで一般的な証明には複雑な式を使った数学的帰納法を用いる必要があるが、より洗練されたが難しい証明を紹介する。\n証明 戦略：次の補助定理を利用する。\nジェンセンの不等式： $f$ が 凸関数 で、$E(X) \u0026lt; \\infty$ の場合、以下の不等式が成り立つ。 $$ E{f(X)}\\ge f{E(X)} $$\n算術-幾何 $f(x)=-\\ln x$ とすると、$f$ は区間 $(0,\\infty )$ で凸関数である。確率変数 $X$ が確率質量関数\n$$ p(X=x)=\\begin{cases}{1 \\over n} \u0026amp; , x={x}_1,{x}_2, \\cdots ,{x}_n \\\\ 0 \u0026amp; , その他の場合\\end{cases} $$\nを持つとする。すると $E(X)$ は\n$$ \\frac { {x}_1+{x}_2+\u0026hellip;+{x}_n }{ n }\u0026lt;\\infty $$\nであり有限である。これはジェンセンの不等式に必要な全ての条件を満たすため、次を得る。\n$$ E(-\\ln X)\\ge –\\ln E(X) $$\n左辺は\n$$ \\begin{align*} E(-\\ln X)\u0026amp;=-E(\\ln X) \\\\ \u0026amp;=-\\frac { 1 }{ n } \\sum_{ k=1 }^{ n }{ \\ln{x}_k } \\\\ \u0026amp;=-\\frac { 1 }{ n }\\ln \\prod_{ k=1 }^{ n }{ {x}_k } \\\\ \u0026amp;=-\\ln { \\left( \\prod_{ k=1 }^{ n }{ {x}_k } \\right) }^{ \\frac { 1 }{ n } } \\\\ \u0026amp;=-\\ln\\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } } \\end{align*} $$\n右辺は\n$$ \\begin{align*} -\\ln E(X)=-\\ln\\frac { 1 }{ n }\\sum_{ k=1 }^{ n }{ {x}_k } \\end{align*} $$\nこの両者を定理すると\n$$ \\begin{align*} -\\ln\\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } } \\ge\u0026amp; -\\ln\\frac { 1 }{ n }\\sum_{ k=1 }^{ n }{ {x}_k } \\\\ \\implies \\ln\\frac { 1 }{ n }\\sum_{ k=1 }^{ n }{ {x}_k } \\ge\u0026amp; \\ln\\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } } \\\\ \\implies \\frac { 1 }{ n }\\sum_{ k=1 }^{ n }{ {x}_k } \\ge\u0026amp; \\prod_{ k=1 }^{ n }{ { {x}_k }^{ \\frac { 1 }{ n } } } \\\\ \\implies \\frac { {x}_1+{x}_2+\u0026hellip;+{x}_n }{ n } \\ge\u0026amp; \\sqrt [ n ]{ {x}_1{x}_2\u0026hellip;{x}_n } \\end{align*} $$\n■\nこれにより、算術平均と幾何平均の間の不等式が証明された。これを用いて、幾何平均と調和平均の間の不等式を証明しよう。\n幾何-調和 $$ \\frac { {x}_1+{x}_2+\u0026hellip;+{x}_n }{ n }\\ge \\sqrt [ n ]{ {x}_1{x}_2\u0026hellip;{x}_n } $$\n$\\displaystyle {x}_k=\\frac { 1 }{ n{y}_k }$ と置くと、\n$$ \\begin{align*} \\frac { \\frac { 1 }{ n{y}_1 }+\\frac { 1 }{ n{y}_2 }+\u0026hellip;+\\frac { 1 }{ n{y}_n } }{ n }\\ge \\sqrt [ n ]{ \\frac { 1 }{ n{y}_1 }\\frac { 1 }{ n{y}_2 }\u0026hellip;\\frac { 1 }{ n{y}_n } } \\\\ \\implies \\frac { 1 }{ n\\sqrt [ n ]{ \\frac { 1 }{ n{y}_1 }\\frac { 1 }{ n{y}_2 }\u0026hellip;\\frac { 1 }{ n{y}_n } } }\\ge \\frac { n }{ n\\frac { 1 }{ n{y}_1 }+\\frac { 1 }{ n{y}_2 }+\u0026hellip;+\\frac { 1 }{ n{y}_n } } \\\\ \\implies \\sqrt [ n ]{ {y}_1{y}_2\u0026hellip;{y}_n }\\ge \\frac { n }{ n\\frac { 1 }{ n{y}_1 }+\\frac { 1 }{ n{y}_2 }+\u0026hellip;+\\frac { 1 }{ n{y}_n } } \\end{align*} $$ ■\n","id":3,"permalink":"https://freshrimpsushi.github.io/jp/posts/3/","tags":null,"title":"算術平均と幾何平均、調和平均の間の不等式"},{"categories":"머신러닝","contents":"説明 PyTorchには、多くのニューラルネットワーク関連の関数がtorch.nnとtorch.nn.functionalに同じ名前で含まれています。 nn の関数はニューラルネットワークを関数として返し、 nn.functional の関数はニューラルネットワークそのものです。\n例えば nn.MaxPool2d はカーネルサイズを入力として受け取り、プーリング層を返す。\nimport torch import torch.nn as nn pool = nn.MaxPool2d(kernel_size = 2) # MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) A = torch.arange(16.).reshape(1, 4, 4) # tensor([[[ 0., 1., 2., 3.], # [ 4., 5., 6., 7.], # [ 8., 9., 10., 11.], # [12., 13., 14., 15.]]]) pool(A) # tensor([[[ 5., 7.], # [13., 15.]]]) 一方で nn.functional.MaxPool2d はそのものが2次元マックスプーリングレイヤーです。そのため、この関数はプーリングを適用するテンソルとプーリングの条件をすべて入力として受け取り、実際に入力テンソルをプーリングした結果を返す。\nimport torch import torch.nn.functional as F A = torch.arange(16.).reshape(1, 4, 4) F.max_pool2d(A, kernel_size=2) #tensor([[[ 5., 7.], # [13., 15.]]]) つまり nn.MaxPool2d(kernel_size=(n,m)) が返す関数の forward が max_pool2d( ,kernel_size(n,m)) として定義されているわけです。コードを見ると、実際には次のようになっています。\nclass MaxPool2d(_MaxPoolNd): kernel_size: _size_2_t stride: _size_2_t padding: _size_2_t dilation: _size_2_t def forward(self, input: Tensor): return F.max_pool2d(input, self.kernel_size, self.stride, self.padding, self.dilation, ceil_mode=self.ceil_mode, return_indices=self.return_indices) パラメータが含まれるレイヤー、例えば線形層であれば、F.linear(input, weight, bias) のようにパラメータも入力として受け取ります。\n環境 OS: Windows11 バージョン: Python 3.11.5, torch==2.0.1+cu118 ","id":3626,"permalink":"https://freshrimpsushi.github.io/jp/posts/3626/","tags":null,"title":"パイトーチでtorch.nnとtorch.nn.functionalの違い"},{"categories":"편미분방정식","contents":"整理 次のような波動方程式が与えられたとする。 この時、$\\Delta_{\\mathbf{x}}$は変数$\\mathbf{x}$に対するラプラシアンである。\n$$ \\begin{align} \\partial_{t}^{2} p(\\mathbf{x}, t) \u0026amp;= \\Delta_{\\mathbf{x}} p(\\mathbf{x}, t) \u0026amp;\\text{on } \\mathbb{R} \\times [0, \\infty) \\\\ p(\\mathbf{x}, 0) \u0026amp;= f(\\mathbf{x}) \u0026amp;\\text{on } \\mathbb{R} \\\\ \\partial_{t} p(\\mathbf{x}, 0) \u0026amp;= 0 \u0026amp;\\text{on } \\mathbb{R} \\end{align} $$\n上辺微分方程式の解は次の通りである。\n$$ \\begin{equation} p(\\mathbf{x}, t) = \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\cos (t \\left| \\boldsymbol{\\xi} \\right|) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} \\end{equation} $$\nこのとき$\\hat{f}$は$f$の「フーリエ変換」（../1086）である。 今回は初期条件が以下のように与えられた波動方程式を考えてみよう。\n$$ \\begin{align*} \\partial_{t}^{2} p(\\mathbf{x}, t) \u0026amp;= \\Delta_{\\mathbf{x}} p(\\mathbf{x}, t) \u0026amp;\\text{on } \\mathbb{R} \\times [0, \\infty) \\\\ p(\\mathbf{x}, 0) \u0026amp;= 0 \u0026amp;\\text{on } \\mathbb{R} \\\\ \\partial_{t} p(\\mathbf{x}, 0) \u0026amp;= g(\\mathbf{x}) \u0026amp;\\text{on } \\mathbb{R} \\end{align*} $$\n上辺微分方程式の解は次の通りである。\n$$ p(\\mathbf{x}, t) = \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{g} (\\boldsymbol{\\xi}) \\dfrac{\\sin (t \\left| \\boldsymbol{\\xi} \\right|)}{\\left| \\boldsymbol{\\xi} \\right|} e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} $$\n説明 「フーリエ変換」(../1086)と「逆変換」(../1112)の定義を以下のようにしておこう。\n$$ \\hat{f}(\\boldsymbol{\\xi}) = \\int\\limits_{\\mathbb{R}^{n}} f(\\mathbf{x}) e^{\\mathrm{i} \\boldsymbol{\\xi} \\cdot \\mathbf{x}} \\mathrm{d} \\mathbf{x}, \\qquad f(\\mathbf{x}) = \\dfrac{1}{(2\\pi)^{n}}\\int\\limits_{\\mathbb{R}^{n}} f(\\mathbf{x}) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} $$\n後者の証明法は前者と大同小異なので省略する。\n証明 $(4)$が$(1)$、$(2)$、$(3)$を満足させるか確認するだけだ。 まず、時間に対する2階導関数を計算してみると、\n$$ \\partial_{t}^{2} p(\\mathbf{x}, t) = -\\left| \\boldsymbol{\\xi} \\right|^{2} \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\cos (t \\left| \\boldsymbol{\\xi} \\right|) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} $$\nラプラシアンを計算してみると次のようになる。\n$$ \\begin{align*} \\Delta_{\\mathbf{x}} p(\\mathbf{x}, t) \u0026amp;= \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\cos (t \\left| \\boldsymbol{\\xi} \\right|) (\\Delta_{\\mathbf{x}} e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}}) \\mathrm{d} \\boldsymbol{\\xi} \\\\ \u0026amp;= (- \\left| \\boldsymbol{\\xi} \\right|^{2}) \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\cos (t \\left| \\boldsymbol{\\xi} \\right|) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} \\\\ \\end{align*} $$\nしたがって$(1)$が成立する。 $p(\\mathbf{x}, 0)$を計算してみると次のようになるので$(2)$が成立する。\n$$ \\begin{align*} p(\\mathbf{x}, 0) \u0026amp;= \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\cos ( 0 \\left| \\boldsymbol{\\xi} \\right|) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} \\\\ \u0026amp;= \\dfrac{1}{(2\\pi)^{n}} \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} \\\\ \u0026amp;= f(\\mathbf{x}) \\end{align*} $$\n$(3)$が成立することも容易に確認できる。\n$$ \\begin{align*} \\partial_{t}p(\\mathbf{x}, 0) \u0026amp;= - \\left| \\boldsymbol{\\xi} \\right| \\int\\limits_{\\mathbb{R}^{n}} \\hat{f} (\\boldsymbol{\\xi}) \\sin ( 0 \\left| \\boldsymbol{\\xi} \\right|) e^{\\mathrm{i} \\mathbf{x} \\cdot \\boldsymbol{\\xi}} \\mathrm{d} \\boldsymbol{\\xi} \\\\ \u0026amp;= 0 \\end{align*} $$\n■\n","id":3623,"permalink":"https://freshrimpsushi.github.io/jp/posts/3623/","tags":null,"title":"初期条件が0の波動方程式の解。"},{"categories":"머신러닝","contents":"説明 AdaBeliefは2020年にJ. Zhuangらによって紹介されたオプティマイザで、Adamの変形の一つです1。PyTorchではこのオプティマイザをデフォルトで提供していないため、別途インストールして使用する必要があります。\nコード2 インストール cmdで以下のコードでインストールできます。\npip install adabelief-pytorch==0.2.0 使用方法 以下のコードで読み込んで使用できます。\nfrom adabelief_pytorch import AdaBelief\roptimizer = AdaBelief(model.parameters(), lr=1e-3, eps=1e-16, betas=(0.9,0.999), weight_decouple = True, rectify = False) 環境 OS: Windows11 Version: Python 3.11.5, torch==2.0.1+cu118, adabelief-pytorch==0.2.0 https://arxiv.org/abs/2010.07468\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/juntang-zhuang/Adabelief-Optimizer?tab=readme-ov-file#installation-and-usage\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3620,"permalink":"https://freshrimpsushi.github.io/jp/posts/3620/","tags":null,"title":"파이토치에서 AdaBelief 옵티마이저 사용하는 방법"},{"categories":"줄리아","contents":"説明 カラーグラディエントは、Juliaの視覚化パッケージ Plots.jlがサポートする2つのカラースキームのうちの1つ（もう1つはパレット）で、私たちが一般的にグラデーションと呼んでいるものと同じだ。つまり、簡単に言うとグラデーションが実装されているタイプが ColorGradientである。\nグラディエントは、heatmap(), surface(), contour()などの図表を描くのに使われる。複数のグラフの色をそれぞれ変えたい場合は、グラディエントではなくパレットを使用する。\nコード シンボル cgrad(シンボル)で使用できる。デフォルトのグラディエントは cgrad(:inferno)で、色は以下の通りである。\nusing Plots\rcgrad(:inferno) heatmap(reshape(1:25, (5, 5))) Plots.jlに事前定義されているパレットとグラディエントは、公式ドキュメントで確認できる。(パッケージ ColorSchemes.jlの公式ドキュメントでさらに多様なパレットとグラディエントを探すことができる。)\nPythonのmatplotlibで imshowのデフォルトカラーマップでグラディエントに似たものは :viridisである。\nheatmap(reshape(1:25, (5, 5)), fillcolor = cgrad(:viridis)) 直接定義 cgrad([開始色, 終了色])で直接パレットを定義することができる。色が変わるポイントを設定するには、オプション引数として$0$と$1$の間の値を要素に持つベクトルを入力する。\ncgrad([:blue, :orange]) cgrad([:blue, :orange], [0.1, 0.9]) cgrad([:blue, :orange], [0.5, 0.50001]) キーワード rev キーワード引数として rev = trueを入力すると、順序が逆になる。\ncgrad(:darktest) cgrad(:darktest, rev = true) scale キーワードscaleは、グラディエントのスケールを指定する。:logまたは:expを入力できる。\ncgrad(:rainbow) cgrad(:rainbow, scale = :log) cgrad(:rainbow, scale = :exp) 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 参照 色の使い方 パレットの使い方 カラーグラディエント（グラデーション）の使い方 色処理のためのパッケージ Colors.jl RGBコードの使い方 RGB(1,0,0) HEXコードの使い方 \u0026quot;#000000\u0026quot; グラフ要素の色指定の仕方 サブプロット毎にグラフの色を指定する方法 軸、軸の名前、目盛、目盛の値の色指定の仕方 背景色の指定の仕方 ","id":3608,"permalink":"https://freshrimpsushi.github.io/jp/posts/3608/","tags":null,"title":"ジュリアプロットでカラーグラデーションを使用する方法"},{"categories":"줄리아","contents":"説明 パレットとは、予め絞り出された絵の具がおかれている板のことを指します。数学的に説明すると、「色の集合」や「色の数列」と言えるでしょう。1つの絵に複数のグラフを描く際、最も一般的な方法は異なる色を使って区別することですが、その目的のためにJuliaでは、様々な色を集めたColorPaletteというタイプが実装されています。色のベクトルとして理解すると便利です。実際に、デフォルトのパレット：defaultを読み込んでみると、とても複雑に見えますが、中を見れば、ただの色のベクトルです。\nヒートマップを描くときは、パレットではなくグラデーションを使用します。\njulia\u0026gt; using Plots\rjulia\u0026gt; palette(:default)\rColorPalette(ColorSchemes.ColorScheme{Vector{RGB{Float64}}, String, String}(RGB{Float64}[RG\rB{Float64}(0.0,0.6056031611752245,0.9786801175696073), RGB{Float64}(0.8888735002725198,0.43\r564919034818994,0.2781229361419438), RGB{Float64}(0.2422242978521988,0.6432750931576305,0.3\r044486515341153), RGB{Float64}(0.7644401754934356,0.4441117794687767,0.8242975359232758), R\rGB{Float64}(0.6755439572114057,0.5556623322045815,0.09423433626639477), RGB{Float64}(4.8211\r81644776295e-7,0.6657589812923561,0.6809969518707945), RGB{Float64}(0.930767491919665,0.367\r4771896571412,0.5757699667547829), RGB{Float64}(0.7769816661712932,0.5097431319944513,0.146\r4252569555497), RGB{Float64}(3.8077343661790943e-7,0.6642678029460116,0.5529508754522481), RGB{Float64}(0.558464964115081,0.5934846564332882,0.11748125233232104), RGB{Float64}(5.9476\r23898072685e-7,0.6608785231434254,0.7981787608414297), RGB{Float64}(0.6096707676128648,0.49\r918492100827777,0.9117812665042642), RGB{Float64}(0.3800016049820351,0.5510532724353506,0.9\r665056985227146), RGB{Float64}(0.942181647954218,0.37516423354097583,0.4518168202944593), R\rGB{Float64}(0.8684020893043971,0.3959893639954845,0.7135147524811879), RGB{Float64}(0.42314\r674364630817,0.6224954944199981,0.19877060252130468)], \u0026#34;\u0026#34;, \u0026#34;\u0026#34;))\rjulia\u0026gt; palette(:default).colors.colors\r16-element Array{RGB{Float64},1} with eltype RGB{Float64}:\rRGB{Float64}(0.0,0.6056031611752245,0.9786801175696073)\rRGB{Float64}(0.8888735002725198,0.43564919034818994,0.2781229361419438)\rRGB{Float64}(0.2422242978521988,0.6432750931576305,0.3044486515341153)\rRGB{Float64}(0.7644401754934356,0.4441117794687767,0.8242975359232758)\rRGB{Float64}(0.6755439572114057,0.5556623322045815,0.09423433626639477)\rRGB{Float64}(4.821181644776295e-7,0.6657589812923561,0.6809969518707945)\rRGB{Float64}(0.930767491919665,0.3674771896571412,0.5757699667547829)\rRGB{Float64}(0.7769816661712932,0.5097431319944513,0.1464252569555497)\rRGB{Float64}(3.8077343661790943e-7,0.6642678029460116,0.5529508754522481)\rRGB{Float64}(0.558464964115081,0.5934846564332882,0.11748125233232104)\rRGB{Float64}(5.947623898072685e-7,0.6608785231434254,0.7981787608414297)\rRGB{Float64}(0.6096707676128648,0.49918492100827777,0.9117812665042642)\rRGB{Float64}(0.3800016049820351,0.5510532724353506,0.9665056985227146)\rRGB{Float64}(0.942181647954218,0.37516423354097583,0.4518168202944593)\rRGB{Float64}(0.8684020893043971,0.3959893639954845,0.7135147524811879)\rRGB{Float64}(0.42314674364630817,0.6224954944199981,0.19877060252130468) 既に定義されたパレットのシンボルを入力するか、色と長さを入力してpalette()関数でパレットを読み込んだり作ったりすることができます。図表を描くときは、plot()関数のpaletteキーワードに代入すればOKです。\nコード シンボル palette(シンボル)のように使用します。デフォルトのパレットのシンボルは:defaultで、色は以下の通りです。\n1つの絵に複数のグラフを描くと、上記の色が順番に適用されます。色を使い切った後は、再び最初から循環します。\nusing Plots x = 0:0.01:2π plot([x -\u0026gt; sin(x - a) for a in range(0, π, length = 5)], 0, 2π) Plots.jlに予め定義されたパレットやグラデーションは、公式ドキュメントで確認できます。(パッケージColorSchemes.jlの公式ドキュメントでも、より多様なパレットやグラデーションを見つけることができます。)\n:rainbowで描いてみると、\npalette(:rainbow) plot([x -\u0026gt; sin(x - a) for a in range(0, π, length = 5)], 0, 2π,\rpalette = palette(:rainbow)) 直接定義 palette([開始色、終了色], 長さ)で、直接パレットを定義することができます。また、range()を使用して色を補間することもできます。\npalette([:blue, :orange], 10) palette([RGB(0.5, 0.6, 0.2), RGB(1.0, 0.2, 0.9)], 10) 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 併せて見る 色の使用方法 パレットの使用方法 カラーグラデーション(グラデーション)の使用方法 色処理のためのパッケージ Colors.jl RGBコードの使用方法 RGB(1, 0, 0) HEXコードの使用方法 \u0026quot;#000000\u0026quot; グラフ要素の色の指定方法 サブプロットごとにグラフの色を指定する方法 軸、軸名、目盛り、目盛り値の色の指定方法 背景色の指定方法 ","id":3607,"permalink":"https://freshrimpsushi.github.io/jp/posts/3607/","tags":null,"title":"ジュリアプロットでパレットを使用する方法"},{"categories":"줄리아","contents":"コード 大きくスケールの異なる2つのデータを同じプロットに描いた場合、下の図のようにスケールが小さい方が完全に無視されてしまう。\nusing Plots\rx = 0:0.01:2π\rplot(x, sin.(x))\rplot!(x, exp.(x)) 2つ目のデータをプロットするとき、twinx()を最初の引数に入力すれば、$x$軸を共有し、新しい$y$軸に対してグラフが描かれる。\nplot(x, sin.(x), ylabel = \u0026#34;sin x\u0026#34;)\rplot!(twinx(), x, exp.(x), ylabel = \u0026#34;exp x\u0026#34;) 逆に、$y$軸を共有して描くときは、twiny()を最初の引数に入力すればいい。\n環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 ","id":3606,"permalink":"https://freshrimpsushi.github.io/jp/posts/3606/","tags":null,"title":"ジュリアプロットで異なるスケールの2つのデータ軸を共有して描く方法"},{"categories":"줄리아","contents":"説明 JuliaのPlots.jlでは、プロットも一つのオブジェクトだ。空のプロットを描いてタイプを確認すると、以下のようになる。\njulia\u0026gt; using Plots\rjulia\u0026gt; p = plot()\rjulia\u0026gt; p |\u0026gt; typeof\rPlots.Plot{Plots.GRBackend} Plots.を外してみると、Plot{GRBackend}となり、要素のデータタイプがFloat64のベクターがVector{Float64}と表示されるのと同様に、バックエンドがGRのプロットだという意味だ。Plotのプロパティを確認してみると、以下のようになる。\njulia\u0026gt; p |\u0026gt; propertynames\r(:backend, :n, :attr, :series_list, :o, :subplots, :spmap, :layout, :inset_subplots, :init) 各プロパティは、図の属性を含むベクターか、辞書か、そういうものだ。\np.backend プロットのバックエンドだ。\njulia\u0026gt; p.backend\rPlots.GRBackend() p.attr 図の属性に関する辞書だ。以下のような30個のキー・バリューを含んでいる。\njulia\u0026gt; plot(rand(10, 4), layout = 4).attr\rRecipesPipeline.DefaultsDict with 30 entries:\r:dpi =\u0026gt; 100\r:background_color_outside =\u0026gt; :match\r:plot_titlefontvalign =\u0026gt; :vcenter\r:warn_on_unsupported =\u0026gt; true\r:background_color =\u0026gt; RGBA{Float64}(1.0,1.0,1.0,1.0)\r:inset_subplots =\u0026gt; nothing\r:size =\u0026gt; (600, 400)\r:display_type =\u0026gt; :auto\r:overwrite_figure =\u0026gt; true\r:html_output_format =\u0026gt; :auto\r:plot_titlefontfamily =\u0026gt; :match\r:plot_titleindex =\u0026gt; 0\r:foreground_color =\u0026gt; RGB{N0f8}(0.0,0.0,0.0)\r:window_title =\u0026gt; \u0026#34;Plots.jl\u0026#34;\r:plot_titlefontrotation =\u0026gt; 0.0\r:extra_plot_kwargs =\u0026gt; Dict{Any, Any}()\r:pos =\u0026gt; (0, 0)\r:plot_titlefonthalign =\u0026gt; :hcenter\r:tex_output_standalone =\u0026gt; false\r:extra_kwargs =\u0026gt; :series\r:thickness_scaling =\u0026gt; 1\r:layout =\u0026gt; 4\r:plot_titlelocation =\u0026gt; :center\r:plot_titlefontsize =\u0026gt; 16\r:plot_title =\u0026gt; \u0026#34;\u0026#34;\r:show =\u0026gt; false\r:link =\u0026gt; :none\r:plot_titlefontcolor =\u0026gt; :match\r:plot_titlevspan =\u0026gt; 0.05\r:fontfamily =\u0026gt; \u0026#34;sans-serif\u0026#34;\rjulia\u0026gt; plot(rand(10, 4), layout = 4).attr[:size]\r(600, 400) p.series_list 各データのグラフに関する属性の辞書を要素とするベクターだ。\njulia\u0026gt; plot(rand(10,5)).series_list\r5-element Vector{Plots.Series}:\rjulia\u0026gt; plot(plot(rand(10, 4)), plot(rand(10, 3))).series_list\r7-element Vector{Plots.Series}: 各辞書に含まれるキー・バリューは、以下の通りだ。\njulia\u0026gt; plot(rand(10, 2)).series_list[1].plotattributes\rRecipesPipeline.DefaultsDict with 62 entries:\r:plot_object =\u0026gt; Plot{Plots.GRBackend() n=2}\r:subplot =\u0026gt; Subplot{1}\r:label =\u0026gt; \u0026#34;y1\u0026#34;\r:fillalpha =\u0026gt; nothing\r:linealpha =\u0026gt; nothing\r:linecolor =\u0026gt; RGBA{Float64}(0.0,0.605603,0.97868,1.0)\r:x_extrema =\u0026gt; (NaN, NaN)\r:series_index =\u0026gt; 1\r:markerstrokealpha =\u0026gt; nothing\r:markeralpha =\u0026gt; nothing\r:seriestype =\u0026gt; :path\r:z_extrema =\u0026gt; (NaN, NaN)\r:x =\u0026gt; Base.OneTo(10)\r:markerstrokecolor =\u0026gt; RGBA{Float64}(0.0,0.0,0.0,1.0)\r:fillcolor =\u0026gt; RGBA{Float64}(0.0,0.605603,0.97868,1.0)\r:clims_calculated =\u0026gt; (NaN, NaN)\r:seriescolor =\u0026gt; RGBA{Float64}(0.0,0.605603,0.97868,1.0)\r:extra_kwargs =\u0026gt; Dict{Symbol, Any}()\r:z =\u0026gt; nothing\r:series_plotindex =\u0026gt; 1\r:y =\u0026gt; [0.477103, 0.00362131, 0.864524, 0.391488, 0.663659, 0.89787, 0.157973, 0.964416, 0.806635, 0.243531]\r:markercolor =\u0026gt; RGBA{Float64}(0.0,0.605603,0.97868,1.0)\r:y_extrema =\u0026gt; (0.00362131, 0.964416)\r:linewidth =\u0026gt; 1\r:group =\u0026gt; nothing\r:stride =\u0026gt; (1, 1)\r:permute =\u0026gt; :none\r:marker_z =\u0026gt; nothing\r:show_empty_bins =\u0026gt; false\r:seriesalpha =\u0026gt; nothing\r:smooth =\u0026gt; false\r:zerror =\u0026gt; nothing\r:arrow =\u0026gt; nothing\r:normalize =\u0026gt; false\r:linestyle =\u0026gt; :solid\r:contours =\u0026gt; false\r:bar_width =\u0026gt; nothing\r:bins =\u0026gt; :auto\r:markerstrokestyle =\u0026gt; :solid\r:weights =\u0026gt; nothing\r:z_order =\u0026gt; :front\r:fill_z =\u0026gt; nothing\r:markershape =\u0026gt; :none\r:markerstrokewidth =\u0026gt; 1\r:xerror =\u0026gt; nothing\r:bar_position =\u0026gt; :overlay\r:contour_labels =\u0026gt; false\r:hover =\u0026gt; nothing\r:primary =\u0026gt; true\r:yerror =\u0026gt; nothing\r:ribbon =\u0026gt; nothing\r:fillstyle =\u0026gt; nothing\r:line_z =\u0026gt; nothing\r:orientation =\u0026gt; :vertical\r:markersize =\u0026gt; 4\r:bar_edges =\u0026gt; false\r:quiver =\u0026gt; nothing\r:fillrange =\u0026gt; nothing\r:colorbar_entry =\u0026gt; true\r:series_annotations =\u0026gt; nothing\r:levels =\u0026gt; 15\r:connections =\u0026gt; nothing\rjulia\u0026gt; plot(rand(10, 2)).series_list[1][:label]\r\u0026#34;y1\u0026#34; 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 ","id":3605,"permalink":"https://freshrimpsushi.github.io/jp/posts/3605/","tags":null,"title":"ジュリアプロッツでのプロットのプロパティリスト"},{"categories":"줄리아","contents":"概要 Plots.jlでの図の背景の格子に関連するキーワードは次の通りだ。\nキーワード名 機能 grid 格子表示 gridalpha, ga, gα 格子の透明度指定 foreground_color_grid, fgcolor_grid 格子の色指定 gridlinewidth, grid_lw 格子の太さ指定 gridstyle, grid_ls 格子の線スタイル指定 minorgrid 補助格子表示 minorgridalpha 補助格子の透明度指定 foreground_color_minor_grid, fgcolor_minorgrid 補助格子の色指定 minorgridlinewidth, minorgrid_lw 補助格子の太さ指定 minorgridstyle, minorgrid_ls 補助格子の線スタイル指定 コード 格子表示 格子を表示するキーワードはgridだ。:xや:yを入力するとそれぞれ$x$軸の目盛り補助線、$y$軸の目盛り補助線のみを表示する。falseを入力すると格子を表示しない。\nplot(plot(rand(10)), plot(rand(10), grid = :x), plot(rand(10), grid = :y), plot(rand(10), grid = false)) 透明度 背景の格子は基本的に0.1の透明度で描かれる。格子の透明度を調節するキーワードはgridalpha(=ga)(=gα)だ。\nplot(rand(10, 3), layout = (3, 1), gridalpha = [0.1 0.5 1]) 色 格子の基本色は黒で、キーワードforeground_color_grid(=fgcolor_grid)で他の色を指定できる。\nplot(rand(10, 3), layout = (3, 1), gridalpha = 1, fgcolor_grid = [:red :green :orange]) 太さ 格子の太さを指定するキーワードはgridlinewidth(=grid_lw)で、基本値は0.5だ。\nplot(rand(10, 3), layout = (3, 1), grid_lw = [0.5 5 10]) 格子スタイル キーワードgridstyle(=grid_ls)で格子の線スタイルを指定できる。可能なシンボルは:auto, :solid, :dash, :dot, :dashdot, :dashdotdot。\nplot(rand(10, 2), layout = 2, ga = 1, gridstyle = [:solid :dash]) 補助格子 キーワード引数としてminorgrid = trueを入力すると補助格子を描く。補助格子の透明度、色、線の太さ、線スタイルを指定するキーワードはそれぞれminorgridalpha, foreground_color_minor_grid minorgrid_lw, minorgrid_lsだ。\nplot(plot(rand(10)), plot(rand(10), minorgrid = true), gridalpha = 0.8, minorgridalpha = 0.2) 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 ","id":3604,"permalink":"https://freshrimpsushi.github.io/jp/posts/3604/","tags":null,"title":"ジュリアプロットで背景のグリッドを飾る方法"},{"categories":"줄리아","contents":"概要 Plots.jlで図の背景色に関連するキーワードは次の通りです。\nキーワード名 機能 background_color, bg_color 全体の背景の色を指定 background_color_outside, bg_color_outside グラフが描かれた外側の領域の色を指定 background_subplot, bg_subplot グラフが描かれた領域の色を指定 background_inside, bg_inside 凡例を除いたグラフが描かれた領域の色を指定 コード 背景色を指定するキーワードはbackground_color(=bg_color)です。凡例、グラフが描かれた場所とその他の全ての背景色を入力した値で指定します。\nplot(rand(10), bg_color = :tomato) グラフが描かれた領域外側の色を指定するキーワードはbackground_color_outside(=bg_color_outside)です。\nplot(rand(10), bg_color_outside = :palegreen) グラフが描かれた領域の色を指定するキーワードはbackground_subplot(=bg_subplot)です。\nplot(rand(10), bg_subplot = :violet) 凡例を除いてグラフが描かれた領域の色を指定するキーワードはbackground_inside(=bg_inside)です。\nplot(rand(10), bg_inside = :brown4) サブプロット 複数のサブプロットがある場合、bg_subplotやbg_insideで色を指定する必要があり、全体のプロットにまとめたときそれぞれの背景色を維持します。\np₁ = plot(rand(10), bg_subplot = :tomato)\rp₂ = scatter(rand(10), bg_inside = :yellow)\rp = plot(p₁, p₂) 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 参照 色の使用方法 パレットの使用方法 カラーグラデーションの使用方法 色処理のためのパッケージ Colors.jl RGBコードの使用方法 RGB(1, 0, 0) HEXコードの使用方法 \u0026quot;#000000\u0026quot; グラフ要素の色の指定方法 サブプロットごとにグラフ色を指定する方法 軸、軸名、目盛り、目盛り値の色の指定方法 背景色の指定方法 ","id":3603,"permalink":"https://freshrimpsushi.github.io/jp/posts/3603/","tags":null,"title":"ジュリア・プロットで背景色を指定する方法"},{"categories":"줄리아","contents":"概要 サブプロットごとにグラフの色を指定する3つの方法を紹介する。グラフ要素に色を指定する方法はここを参照してください。\n方法 1 サブプロットのグラフの色を指定する最初の方法は、各サブプロットを定義するときにあらかじめ色を指定することです。Juliaでは、1枚の絵が各オブジェクトであるため、属性を異にする絵を複数定義してから、それらを1つのプロットにまとめればよいです。\np₁ = plot(rand(10), lc = :red)\rp₂ = scatter(rand(10), mc = :blue)\rp₃ = bar(rand(10), fc = :green)\rplot(p₁, p₂, p₃,\rlayout = (3, 1),\rtitle = [\u0026#34;p₁\u0026#34; \u0026#34;p₂\u0026#34; \u0026#34;p₃\u0026#34;],\r) 方法 2 2つ目の方法は、全体のプロットを定義するときに、キーワード引数として色の行ベクトルを入力することです。列ベクトルではなく行ベクトルでなければならないことに注意してください。\np₄ = plot(rand(10))\rp₅ = plot(rand(10))\rp₆ = plot(rand(10))\rplot(p₄, p₅, p₆,\rlayout = (3, 1),\rlinecolor = [:brown :purple :orange],\rtitle = [\u0026#34;p₄\u0026#34; \u0026#34;p₅\u0026#34; \u0026#34;p₆\u0026#34;],\r) 方法 3 3つ目の方法は、全体のプロットを定義した後に、各サブプロットのプロパティ値を直接変更することです。プロパティ.series_listは、各サブプロットのシリーズ属性情報を含む辞書のベクトルです。すなわち、p.series_list[1]は、最初のサブプロットのシリーズ属性辞書を返します。この辞書に:linecolorキーを入力してその値を変更することで、最初のサブプロットの線の色が変わります。\np₇ = plot(rand(10))\rp₈ = scatter(rand(10))\rp₉ = bar(rand(10))\rp = plot(p₇, p₈, p₉,\rlayout = (3, 1),\rtitle = [\u0026#34;p₇\u0026#34; \u0026#34;p₈\u0026#34; \u0026#34;p₉\u0026#34;],\r)\rp.series_list[1][:linecolor] = :goldenrod1\rp.series_list[2][:markercolor] = :olivedrab3\rp.series_list[3][:fillcolor] = :hotpink3 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 参照 色の使い方 パレットの使い方 カラーグラデーションの使い方 色処理のためのパッケージ Colors.jl RGBコードの使い方 RGB(1, 0, 0) HEXコードの使い方 \u0026quot;#000000\u0026quot; グラフ要素の色の指定方法 サブプロットごとのグラフの色の指定方法 軸、軸の名前、目盛り、目盛り値の色の指定方法 背景色の指定方法 ","id":3602,"permalink":"https://freshrimpsushi.github.io/jp/posts/3602/","tags":null,"title":"Julia Plotsで各サブプロットごとにグラフの色を指定する方法"},{"categories":"줄리아","contents":"要約 Plots.jlでは、グラフの各構成要素の色を指定するキーワードは以下の通りだ。\nキーワード 機能 markercolor, mc マーカー内部の色を指定 markerstrokecolor, msc マーカーの縁の色を指定 linecolor, lc 線の色を指定 fillcolor, fc 面積の色を指定 seriescolor, c すべての要素の色を指定 キーワード 機能 markeralpha, ma, mα マーカー内部の透明度を指定 markerstrokealpha, msa, msα マーカーの縁の透明度を指定 linealpha, la, lα 線の透明度を指定 fillalpha, fa, fα 面積の透明度を指定 seriesalpha, a, α すべての要素の透明度を指定 色 Plots.jlでは、変更可能な対象は点、線、面の三つだ。それぞれの色を指定するキーワード引数は、markercolor(=mc)、linecolor(=lc)、そしてfillcolor(=fc)である。これらのキーワードで指定された属性は互いに影響を与えないので、線グラフを描いてmc = :redと入力しても、線の色が赤になることはない。実際にp = plot(rand(10), mc = :red)のプロパティを確認すると、以下のようになる。\njulia\u0026gt; p = plot(rand(10), mc = :red)\rjulia\u0026gt; p.series_list[1][:linecolor]\rRGBA{Float64}(0.0,0.6056031611752245,0.9786801175696073,1.0)\rjulia\u0026gt; p.series_list[1][:markercolor]\rRGBA{Float64}(1.0,0.0,0.0,1.0) プロットされた線グラフの線色は依然としてデフォルトの色であり、赤ではない。\nだから、複数のサブプロットを描き、上記の三つのキーワードで色を指定すると、それぞれが適用される。点（マーカー）を紫色の:purple、線をダークグリーンの:darkgreen、面をスカイブルーの:skyblueで色付けすると、以下のようになる。\nst = [:line :scatter :barhist :steppre :scatterhist :bar]\rx = rand(20)\ry = repeat(x, outer = (1, length(st)))\rplot(y, seriestype = st, layout = 6, mc = :purple,\rlc = :darkgreen,\rfc = :skyblue\r) 透明度 色の透明度を決定するキーワードは、色を指定するキーワード名でcolorをalphaに置き換えたものだ。また、ギリシャ文字のαを直接使用してもよい。\nまたは、RGBAのように、透明度が含まれる色コードを、色を指定するキーワードに入力してもよい。\nplot(rand(20, 3), layout = (3,1), seriestype = [:scatter :line :bar],\rmc = :red,\rlc = :green,\rfc = :blue\r) plot(rand(20, 3), layout = (3,1), seriestype = [:scatter :line :bar],\rmarkeralpha = 0.5, mc = :red,\rla = 0.5, lc = :green,\rfα = 0.5, fc = :blue\r) 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0 参考 色の使い方 パレットの使い方 カラーグラデーション（グラデーション）の使い方 色処理のためのパッケージ Colors.jl RGBコードの使い方 RGB(1, 0, 0) HEXコードの使い方 \u0026quot;#000000\u0026quot; グラフ要素の色の指定方法 サブプロットごとにグラフの色の指定方法 軸、軸の名前、目盛り、目盛り値の色の指定方法 背景色の指定方法 ","id":3601,"permalink":"https://freshrimpsushi.github.io/jp/posts/3601/","tags":null,"title":"ジュリアプロットでグラフ要素の色を指定する方法"},{"categories":"줄리아","contents":"コード Juliaで色を扱うために提供されるパッケージはColors.jlだ。視覚化パッケージのPlots.jlを読み込むと、Colors.jlの中の機能も一緒に使用できる。RGB空間を表す色コードにはRGB, BGR, RGB24, RGBX, XRGBがサポートされており、これらはAbstractRGBのサブタイプだ。RGBAはRGBに透明度が加わったものだ。\njulia\u0026gt; using Plots\rjulia\u0026gt; subtypes(AbstractRGB)\r5-element Vector{Any}:\rBGR\rRGB\rRGB24\rRGBX\rXRGB\rjulia\u0026gt; subtypes(AbstractRGBA)\r2-element Vector{Any}:\rBGRA\rRGBA 文字列 関数plot()の色を指定するキーワードに文字列で\u0026quot;rgb(255, 0, 0)\u0026quot;のように入力すれば、RGBコードが(255, 0, 0)の色を使用できる。下を見ればわかるが、文字列を入力してもいい理由は、plot()が文字列を自動でパースしてくれるからのようだ。名前がある色の場合は、\u0026quot;red\u0026quot;や:redのように文字列やシンボルで使用できる。\nusing Plots\rr = \u0026#34;rgb(255, 0, 0)\u0026#34; # 정수로 표현된 RGB 빨간색\rg = \u0026#34;rgba(0, 255, 0, 0.2)\u0026#34; # 투명도가 0.2인 RGB 초록색\rp = \u0026#34;rgb(50%, 0%, 100%)\u0026#34; # 퍼센테이지로 표현된 RGB 보라색\rplot(\rplot(rand(15), lc = r),\rbar(rand(15), fc = g),\rscatter(rand(15), mc = p),\rlayout = (3, 1)\r) パース colorant\u0026quot;rgb(0, 0, 0)\u0026quot;のようにRGB色コードをパースできる。\njulia\u0026gt; r = colorant\u0026#34;rgb(255, 0, 0)\u0026#34; # 정수로 표현된 RGB 빨간색\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; g = colorant\u0026#34;rgba(0, 255, 0, 0.2)\u0026#34; # 투명도가 0.2인 초록색\rRGBA{N0f8}(0.0,1.0,0.0,0.502)\rjulia\u0026gt; p = colorant\u0026#34;rgb(50%, 0%, 100%)\u0026#34; # 퍼센테이지로 표현된 RGB 보라색\rRGB{N0f8}(0.502,0.0,1.0)\rjulia\u0026gt; plot(\rplot(rand(15), lc = r),\rbar(rand(15), fc = g),\rscatter(rand(15), mc = p),\rlayout = (3, 1)) parse(RGB, \u0026quot;rgb(0, 255, 255)\u0026quot;)と同様にパースすることができる。\njulia\u0026gt; parse(RGB, \u0026#34;rgb(0, 255, 255)\u0026#34;)\rRGB{N0f8}(0.0,1.0,1.0)\rjulia\u0026gt; parse(RGBA, \u0026#34;rgba(0, 255, 0, 0.5)\u0026#34;)\rRGBA{N0f8}(0.0,1.0,0.0,0.502) 直接定義 関数RGB(), RGBA()などを使えば、色を直接定義できる。\njulia\u0026gt; RGB(1, 0, 0)\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; RGB(1.0, 0.0, 0.0)\rRGB{Float64}(1.0,0.0,0.0)\rjulia\u0026gt; RGBA(1, 0, 0.5, 0.5)\rRGBA{Float64}(1.0,0.0,0.5,0.5) colorantでパースされた色と正確に同じタイプを得るには、入力としてN0f8タイプの数を入力する必要がある。これを使用するためにはFixedPointNumbers.jlが必要だ。または、直接1.0N0f8のように定義してもいい。以下は、colorant\u0026quot;rgb(255, 0, 0)\u0026quot;と同じ、赤色の色を返すコードだ。\njulia\u0026gt; using FixedPointNumbers\r# RGB 빨간색 RGB\rjulia\u0026gt; colorant\u0026#34;rgb(255, 0, 0)\u0026#34;\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; RGB(1.0, 0.0, 0.0)\rRGB{Float64}(1.0,0.0,0.0)\rjulia\u0026gt; RGB(1.0N0f8, 0N0f8, 0N0f8)\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; RGB(reinterpret(N0f8, UInt8(255)), reinterpret(N0f8, UInt8(0)), reinterpret(N0f8, UInt8(0)))\rRGB{N0f8}(1.0,0.0,0.0) 他の色空間からの変換 関数convert()は、他の色空間の色コードをRGBコードに変換する。\njulia\u0026gt; using Colors julia\u0026gt; convert(RGB{N0f8}, HSL(270, 0.5, 0.5)) ERROR: UndefVarError: `N0f8` not defined julia\u0026gt; using FixedPointNumbers julia\u0026gt; convert(RGB{N0f8}, HSL(270, 0.5, 0.5)) RGB{N0f8}(0.502,0.251,0.749) 色名を得る 関数rgb_string()とrgba_string()はそれぞれ色のRGB、RGBAコードを文字列で返す。\njulia\u0026gt; rgb_string(colorant\u0026#34;rgba(255, 0, 0, 0.5)\u0026#34;)\r\u0026#34;rgb(255, 0, 0)\u0026#34;\rjulia\u0026gt; rgba_string(colorant\u0026#34;rgba(255, 0, 0, 0.5)\u0026#34;)\r\u0026#34;rgba(255, 0, 0, 0.502)\u0026#34;\rjulia\u0026gt; rgb_string(colorant\u0026#34;red\u0026#34;)\r\u0026#34;rgb(255, 0, 0)\u0026#34;\rjulia\u0026gt; rgb_string(parse(RGB, :blue))\r\u0026#34;rgb(0, 0, 255)\u0026#34;\rjulia\u0026gt; rgb_string(colorant\u0026#34;#00FF00\u0026#34;)\r\u0026#34;rgb(0, 255, 0)\u0026#34; 併せて見る Plotsで色を使う方法 RGB色コードの使い方 HEX色コードの使い方 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0, Colors v0.12.10, FixedPointNumbers v0.8.4 併せて見る 色を使う方法 パレットの使い方 カラーグラデーションの使い方 色の処理のためのパッケージColors.jl RGBコードの使い方 RGB(1, 0, 0) HEXコードの使い方 \u0026quot;#000000\u0026quot; グラフ要素の色の指定方法 サブプロットごとにグラフ色を指定する方法 軸、軸名、目盛り、目盛り値の色の指定方法 背景色の指定方法 ","id":3600,"permalink":"https://freshrimpsushi.github.io/jp/posts/3600/","tags":null,"title":"ジュリアでRGBカラーコードを使用する方法"},{"categories":"줄리아","contents":"概要1 Juliaで色処理のためのパッケージであるColors.jlの機能について紹介する。視覚化パッケージであるPlots.jlを使う場合はColors.jlを別に読み込む必要はない。以下の機能を提供する。\n色のパースと変換 色マップ 色スケール パースと変換 strを色情報を表した文字列とすると、@colorant_strまたはparse(Colorant, str)を通じて文字列を特定の色空間の色コードにパースできる。なお、colorantは染料、色素などの意味を持つ英単語である。\nRGBコードの使い方 HEXコードの使い方 julia\u0026gt; using Colors\rjulia\u0026gt; colorant\u0026#34;red\u0026#34;\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; colorant\u0026#34;rgb(255, 0, 0)\u0026#34;\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; parse(Colorant, \u0026#34;rgba(0, 255, 0, 0.5)\u0026#34;)\rRGBA{N0f8}(0.0,1.0,0.0,0.502)\rjulia\u0026gt; parse(Colorant, \u0026#34;#FF0000\u0026#34;)\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; parse(Colorant, \u0026#34;hsl(120, 100%, 25%)\u0026#34;)\rHSL{Float32}(120.0f0,1.0f0,0.25f0) convert()関数で他の色空間の色コードに変換できる。\njulia\u0026gt; convert(RGB, HSL(270, 0.5, 0.5))\rRGB{Float64}(0.5,0.25,0.75) 色の補間 range()関数を通じて色を補間interpolationできる。この動作は非常に論理的で直感的である。例えば、RGBコードを考えてみよう。赤はRGBコードで$(255, 0, 0)$または$(1, 0, 0)$で表され、これは実質的に3次元ベクトルと同じだ。したがって、二つのベクトル間を補間するrange()関数の引数として色コードを使う理由はない。\njulia\u0026gt; v1 = [1.0, 0.0, 0.0];\rjulia\u0026gt; v2 = [0.0, 0.5, 0.0];\rjulia\u0026gt; collect(range(v1, v2, length = 15))\r15-element Vector{Vector{Float64}}:\r[1.0, 0.0, 0.0]\r[0.9285714285714286, 0.03571428571428571, 0.0]\r[0.8571428571428572, 0.07142857142857142, 0.0]\r[0.7857142857142857, 0.10714285714285714, 0.0]\r[0.7142857142857143, 0.14285714285714285, 0.0]\r[0.6428571428571428, 0.17857142857142858, 0.0]\r[0.5714285714285714, 0.21428571428571427, 0.0]\r[0.5, 0.25, 0.0]\r[0.4285714285714286, 0.2857142857142857, 0.0]\r[0.3571428571428571, 0.32142857142857145, 0.0]\r[0.2857142857142857, 0.35714285714285715, 0.0]\r[0.2142857142857143, 0.39285714285714285, 0.0]\r[0.1428571428571429, 0.42857142857142855, 0.0]\r[0.0714285714285714, 0.4642857142857143, 0.0]\r[0.0, 0.5, 0.0]\rjulia\u0026gt; c1 = colorant\u0026#34;rgb(255, 0, 0)\u0026#34;\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; c2 = colorant\u0026#34;rgb(0, 128, 0)\u0026#34;\rRGB{N0f8}(0.0,0.502,0.0)\rjulia\u0026gt; range(colorant\u0026#34;rgb(255,0,0)\u0026#34;, colorant\u0026#34;rgb(0,128,0)\u0026#34;, length=15)\r15-element Array{RGB{N0f8},1} with eltype RGB{FixedPointNumbers.N0f8}:\rRGB{N0f8}(1.0,0.0,0.0)\rRGB{N0f8}(0.929,0.035,0.0)\rRGB{N0f8}(0.859,0.071,0.0)\rRGB{N0f8}(0.784,0.11,0.0)\rRGB{N0f8}(0.714,0.145,0.0)\rRGB{N0f8}(0.643,0.18,0.0)\rRGB{N0f8}(0.573,0.216,0.0)\rRGB{N0f8}(0.502,0.251,0.0)\rRGB{N0f8}(0.427,0.286,0.0)\rRGB{N0f8}(0.357,0.322,0.0)\rRGB{N0f8}(0.286,0.357,0.0)\rRGB{N0f8}(0.216,0.392,0.0)\rRGB{N0f8}(0.141,0.431,0.0)\rRGB{N0f8}(0.071,0.467,0.0)\rRGB{N0f8}(0.0,0.502,0.0) 上記の色レンジの視覚化は、VS CodeでJulia拡張機能をインストールして実行すると以下のように得られる。\nまた、range()の戻り値はパレットとして使用できる。\nmy_palette = range(colorant\u0026#34;rgb(255,0,0)\u0026#34;, colorant\u0026#34;rgb(0,128,0)\u0026#34;, length=15)\rplot(rand(10, 15), palette = my_palette) 環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0, Colors v0.12.10 関連項目 色の使い方 パレットの使い方 カラーグラデーションの使い方 色処理のためのパッケージColors.jl RGBコードの使い方 RGB(1, 0, 0) HEXコードの使い方 \u0026quot;#000000\u0026quot; グラフ要素の色の指定方法 サブプロット毎にグラフの色を指定する方法 軸、軸ラベル、目盛り、目盛り値の色の指定方法 背景色の設定方法 https://juliagraphics.github.io/Colors.jl/stable/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3599,"permalink":"https://freshrimpsushi.github.io/jp/posts/3599/","tags":null,"title":"ジュリアのカラー処理のためのパッケージ"},{"categories":"줄리아","contents":"概要 Juliaで色を便利に使うためのパッケージにはColors.jlがある。「Plots.jl」という視覚化パッケージを読み込めば一緒に使うことができる。\nシンボルと文字列 名前がついた色のリストを確認する方法は、コンソールにColors.color_namesを入力するか、公式ドキュメントを確認することだ。\njulia\u0026gt; using Plots\rjulia\u0026gt; Colors.color_names\rDict{String, Tuple{Int64, Int64, Int64}} with 666 entries:\r\u0026#34;darkorchid\u0026#34; =\u0026gt; (153, 50, 204)\r\u0026#34;chocolate\u0026#34; =\u0026gt; (210, 105, 30)\r\u0026#34;chocolate2\u0026#34; =\u0026gt; (238, 118, 33)\r\u0026#34;grey69\u0026#34; =\u0026gt; (176, 176, 176)\r\u0026#34;grey97\u0026#34; =\u0026gt; (247, 247, 247)\r\u0026#34;olivedrab3\u0026#34; =\u0026gt; (154, 205, 50)\r\u0026#34;deeppink2\u0026#34; =\u0026gt; (238, 18, 137)\r\u0026#34;mediumpurple2\u0026#34; =\u0026gt; (159, 121, 238)\r\u0026#34;ivory1\u0026#34; =\u0026gt; (255, 255, 240)\r⋮ =\u0026gt; ⋮ 色を指定できるキーワード引数には基本的にシンボルと文字列が使用できる。色名をシンボル、文字列で入力すれば、その色が反映される。入力するものが何であれ、Colors.parse(Colorant, 色名)に渡されるため、シンボルでも文字列でも結果は同じである。\njulia\u0026gt; Colors.parse(Colorant, :red)\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; Colors.parse(Colorant, \u0026#34;red\u0026#34;)\rRGB{N0f8}(1.0,0.0,0.0) 様々なグラフに色を指定してみると、その結果は次のようになる。\nplot(randn(50, 6),\rseriescolor = [:red :hotpink1 :purple3 \u0026#34;blue\u0026#34; \u0026#34;lime\u0026#34; \u0026#34;brown4\u0026#34;],\rseriestype = [:line :scatter :histogram :shape :sticks :steppre],\rlayout = (3,2)\r) RGB RGB色コードはcolorant\u0026quot;rgb(255, 0, 0)\u0026quot;で使用できる。rgb()には$[0, 255]$内の整数のみ入力できる。\njulia\u0026gt; colorant\u0026#34;rgb(255, 0, 0)\u0026#34; # rgb() notation with integers in [0, 255]\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; colorant\u0026#34;rgba(0, 0, 255, 0.5)\u0026#34; # with alpha in [0, 1]\rRGBA{N0f8}(0.0,0.0,1.0,0.502)\rplot(rand(20, 2),\rseriescolor = [colorant\u0026#34;rgb(255, 0, 0)\u0026#34; colorant\u0026#34;rgba(0, 0, 255, 0.5)\u0026#34;],\rlayout = 2\r) RGB色コードを扱う詳細はこちらを参照。\nHEX 6桁のHEXコードはcolorant\u0026quot;#FF0000\u0026quot;、3桁のHEXコードはcolorant\u0026quot;#00f\u0026quot;のように使用できる。\njulia\u0026gt; colorant\u0026#34;#FF0000\u0026#34; # 6-digit hex notation\rRGB{N0f8}(1.0,0.0,0.0)\rjulia\u0026gt; colorant\u0026#34;#00f\u0026#34; # 3-digit hex notation\rRGB{N0f8}(0.0,0.0,1.0)\rjulia\u0026gt; plot(rand(20, 2),\rseriescolor = [colorant\u0026#34;#FF0000\u0026#34; colorant\u0026#34;#00f\u0026#34;],\rlayout = 2\r) HEX色コードを扱う詳細はこちらを参照。\n環境 OS: Windows11 Version: Julia 1.9.4, Plots v1.39.0, Colors v0.12.10 一緒に見る 色の使い方 パレットの使い方 カラーグラデーションの使い方 色処理のためのパッケージ Colors.jl RGBコードの使い方 RGB(1, 0, 0) HEXコードの使い方 \u0026quot;#000000\u0026quot; グラフ要素の色の指定の仕方 サブプロットごとのグラフ色の指定の仕方 軸、軸名、目盛り、目盛り値の色の指定の仕方 背景色の指定の仕方 ","id":3598,"permalink":"https://freshrimpsushi.github.io/jp/posts/3598/","tags":null,"title":"ジュリアプロットでの色の使用方法"},{"categories":"줄리아","contents":"コード 関数 printstyled(文字列; color = 色)を使用すると、出力される関数を装飾できる。キーワード引数 colorの入力としては、シンボル、自然数$(0 \\le n \\le 255)$が可能である。文字列は不可能であることに注意。\n利用可能なシンボルには、:blink、:reverse 等の色ではないものも含まれている。これらはキーワード引数として blink = true、bold = true等と入力して適用することもできる。\n:normal :default :blink :bold :hidden :nothing :reverse :underline :white :light_white :black :light_black :blue :light_blue :cyan :light_cyan :green :light_green :magenta :light_magenta :red :light_red :yellow :light_yellow symbols = [:normal :default :blink :bold :hidden :nothing :reverse :underline :white :light_white :black :light_black :blue :light_blue :cyan :light_cyan :green :light_green :magenta :light_magena :red :light_red :yellow :light_yellow]\rfor i ∈ 1:length(symbols)\rprintstyled(\u0026#34;Hello ($(symbols[i]))\\n\u0026#34;, color = symbols[i])\rend Base.text_colorsを入力すると、キーワード引数（シンボルを含む）で可能な全ての値を返す。\njulia\u0026gt; Base.text_colors\rDict{Union{Int64, Symbol}, String} with 280 entries:\r56 =\u0026gt; \u0026#34;\\e[38;5;56m\u0026#34;\r35 =\u0026gt; \u0026#34;\\e[38;5;35m\u0026#34;\r60 =\u0026gt; \u0026#34;\\e[38;5;60m\u0026#34;\r220 =\u0026gt; \u0026#34;\\e[38;5;220m\u0026#34;\r:blink =\u0026gt; \u0026#34;\\e[5m\u0026#34;\r67 =\u0026gt; \u0026#34;\\e[38;5;67m\u0026#34;\r215 =\u0026gt; \u0026#34;\\e[38;5;215m\u0026#34;\r73 =\u0026gt; \u0026#34;\\e[38;5;73m\u0026#34;\r251 =\u0026gt; \u0026#34;\\e[38;5;251m\u0026#34;\r115 =\u0026gt; \u0026#34;\\e[38;5;115m\u0026#34;\r⋮ =\u0026gt; ⋮ 参照 パッケージ Crayons.jlも使用できる。\n環境 OS: Windows11 Version: Julia 1.9.4 ","id":3597,"permalink":"https://freshrimpsushi.github.io/jp/posts/3597/","tags":null,"title":"ジュリアでテキスト出力を装飾する組み込み関数"},{"categories":"데이터과학","contents":"定義 データセット$X \\subset \\mathbb{R}^{n}$が与えられたとする。$m \\lt n$に対して、次のようなマッピングを次元削減dimension reductionという。\n$$ r : X \\to \\mathbb{R}^{m} $$\nまたは、主に機械学習で、パフォーマンスを可能な限り維持しながら入力変数を減らす方法の全体を次元削減技術という。\n説明 次元削減とは、その名の通りベクトルの次元を減らすことを言う。データをより簡単に、より直感的に理解するために使用されることが多い。次元を減らす方法はアルゴリズムによって異なる。特定の成分をそのまま削除することもあれば、既存のデータを用いて定められたルールに従って次元が小さい新しいデータを生成することもある。以下のような技術がある。\n主成分分析PCA 数理統計学での主成分分析 目的 可視化 我々は4次元以上のデータを効率的に可視化することが事実上不可能である。さらに3次元のデータでさえ、その形状によっては可視化に困難を感じることがある。可視化に困難を感じるとは、データの特徴をよく表す図を描くことが難しいということである。3次元データであれば、視点によってその形状が異なって見えるだろう。このような時に次元を減らして描いてみると、データの特徴を把握しやすくなるかもしれない。以下の図は、同じデータだが見る方向によってその形状が著しく異なる例を示している。右の図は左のデータを$xy$-平面に射影したものである。\n4次元データであるアイリスデータセットを以下のように複数の2次元図に分けて可視化することが、多くのデータサイエンスの教科書で紹介されている。\n選択と集中 あまり重要でない情報を排除して、より重要な情報に集中するために次元削減を使用することができる。ここで言う「あまり重要でない情報」とは、ノイズとして扱われるか、重複した情報を指す。例えば、下の左の表を見ると、最初の列がすべてのデータに対して同じ値であることがわかる。また、2番目の列と3番目の列は異なる値だが、実質的には同じ値であることがわかる。したがって、最初の列と2番目（または3番目）の列を削除することで次元削減を行うことができる。また、右の表は大邱の天気情報をまとめたものである。一見すると、ここで不要な情報はないように見えるが、「日差 = 最高気温 - 最低気温」であるため、この3つは線形独立ではなく、実際には回帰分析時にエラーを引き起こす可能性がある。したがって、この場合には多重共線性を除去するために4番目の列を削除することが次元削減である。\n学校\r学年\r所属\r名前\rハイブ高等学校\r3学年\rプロミスナイン\nfromis_9\rイ・ナギョン\n이나경\rハイブ高等学校\r3学年\rプロミスナイン\nfromis_9\rペク・ジホン\n백지헌\rハイブ高等学校\r2学年\rル・セラフィム\nLE SSERAFIM\rキム・チェウォン\n김채원\rハイブ高等学校\r2学年\rル・セラフィム\nLE SSERAFIM\rホ・ユンジン\n허윤진\rハイブ高等学校\r1学年\rニュージンス\nNewJeans\rヘリン\n해린\rハイブ高等学校\r1学年\rニュージンス\nNewJeans\rミンジ\n민지\r日付\r最高気温\r最低気温\r日差\r降水確率\r19일\r32º\r24º\r8º\r60%\r20일\r33º\r22º\r11º\r0%\r21일\r32º\r23º\r9º\r30%\r22일\r30º\r21º\r9º\r60%\r23일\r31º\r24º\r7º\r60%\r24일\r33º\r25º\r8º\r60%\r軽量化 データの次元が減ると、それだけ保存しなければならない数字が少なくなるため、データ自体の容量が減る。人工神経網の場合、MLPは線形層で構成されており、入力データの次元がモデルのパラメータ数に影響を与える。この場合、次元削減を使用してモデルのパラメータを減らすことができる。CNNのように、入力データの次元がモデルのパラメータ数に影響を与えない場合でも、計算速度での利点をもたらすことができる。\n過学習防止 適切な次元削減は、オーバーフィッティングをある程度防ぐことができるとされている。\n","id":3563,"permalink":"https://freshrimpsushi.github.io/jp/posts/3563/","tags":null,"title":"データサイエンスにおける次元削減"},{"categories":"머신러닝","contents":"概要 TensorFlowでは、Kerasを使用して簡単にニューラルネットワークを定義することができます。以下では、Sequential()と関数型APIを使用してシンプルなMLPを定義し、訓練する方法を紹介します。ただし、Sequential()はモデルの定義自体は簡単ですが、それを使用して複雑な構造を設計するには適していません。同様に、関数型APIを使用して複雑な構造を設計する場合は、keras.Modelクラスの使用が適しており、より複雑で自由なカスタマイズを求める場合は、Kerasを使用せずに低レベルで実装する方が良いでしょう。どのような作業にディープラーニングを使用するかによって異なりますが、もし自分が理工学の研究者であり、専門分野にディープラーニングを応用したい場合は、以下の方法を主に使用する可能性は低いでしょう。ディープラーニングを初めて学び、実践する際は、「これが使用法だ」と感じ取る程度だと考えられます。\nシーケンシャルモデル モデル定義 サイン関数 $\\sin : \\mathbb{R} \\to \\mathbb{R}$ の近似のために、入力と出力の次元が1のMLPを次のように定義しましょう。\nimport tensorflow as tf\rfrom tensorflow.keras import Sequential\rfrom tensorflow.keras.layers import Dense\r# モデル定義\rmodel = Sequential([Dense(10, input_dim = 1, activation = \u0026#34;relu\u0026#34;),\rDense(10, input_dim = 10, activation = \u0026#34;relu\u0026#34;),\rDense(1, input_dim = 10)])\rmodel.summary() # output↓\r# Model: \u0026#34;sequential_3\u0026#34;\r# _________________________________________________________________\r# Layer (type) Output Shape Param # # =================================================================\r# dense_9 (Dense) (None, 10) 20 # # dense_10 (Dense) (None, 10) 110 # # dense_11 (Dense) (None, 1) 11 # # =================================================================\r# Total params: 141\r# Trainable params: 141\r# Non-trainable params: 0\r# _________________________________________________________________ keras.layers.Dense()の特徴の一つに、入力の次元を記述する必要がないという点があります。なぜこのような許容がされているのかは分かりませんが、コードの可読性のためには（特に他の人が見る可能性があるコードであれば）入力の次元を明示的に記述することが良いでしょう。このために、出力の次元が左、入力の次元が右に記述されるという特徴があります。したがって、モデルの構造を読むためには、アラビア語ではなく、右から左に読む必要があります。もし線形層を線形変換としての行列と考えた場合、$\\mathbf{y} = A\\mathbf{x}$ なので、入力が右、出力が左に来るのが自然です。しかし、TensorFlowはこのような数学的な厳密さを考慮して設計された言語ではないので、この理由だけでそう設計されたとは考えにくいです。数学的な厳密さを非常に重視するJuliaでも、線形層は Dense(in, out) のように実装されています。これは、左から右へ読む方が便利で分かりやすいためです。元々、$X$ から $Y$ への関数 $f$ の記述自体が $f : X \\to Y$ であり、（Kerasを除いて）世界のどこにも右から左へのマッピングで記述される関数はありません。\nデータ生成 サイン関数を訓練するため、データをサイン関数の関数値とし、モデルの出力とサイン関数のグラフを比較すると以下のようになります。\n# データ生成\rfrom math import pi\rx = tf.linspace(0., 2*pi, num=1000) # 入力データ\ry = tf.sin(x) # 出力データ(label)\r# モデルの出力確認\rimport matplotlib.pyplot as plt\rplt.plot(x, model(x), label=\u0026#34;model\u0026#34;)\rplt.plot(x, y, label=\u0026#34;sin\u0026#34;)\rplt.legend()\rplt.show() 訓練及び結果 from tensorflow.keras.optimizers import Adam\rmodel.compile(optimizer=Adam(learning_rate=0.001), loss=\u0026#39;mse\u0026#39;) model.compile(optimizer, loss, metric) .compile() メソッドでオプティマイザと損失関数を指定します。他の主要なオプションには metric があり、これはモデルを評価する関数を意味します。これは loss と同じになることもありますし、異なることもあります。例えば、MLPで MNISTデータセット を学習する場合、lossは出力とラベルのMSEであり、metricは全データの中で予測に成功した割合になるでしょう。\n\u0026gt; model.fit(x, y, epochs=10000, batch_size=1000, verbose=\u0026#39;auto\u0026#39;)\r.\r.\r.\rEpoch 9998/10000\r1/1 [==============================] - 0s 8ms/step - loss: 6.2260e-06\rEpoch 9999/10000\r1/1 [==============================] - 0s 4ms/step - loss: 6.2394e-06\rEpoch 10000/10000\r1/1 [==============================] - 0s 3ms/step - loss: 6.2385e-06 .fit() メソッドに入力とラベル、エポック数、バッチサイズなどを入力すると訓練が実行されます。verboseは訓練の進行状況をどのように表示するかを決めるオプションで、0、1、2の中から選択でき、0は何も表示しません。他のオプションは以下のフォーマットで表示されます。 # verbose=1\rEpoch (現在のエポック)/(全エポック)\r(現在のバッチ)/(全体のバッチ) [==============================] - 0s 8ms/step - loss: 0.7884\r# verbose=2\rEpoch (現在のエポック)/(全エポック)\r(現在のバッチ)/(全体のバッチ) - 0s - loss: 0.7335 - 16ms/epoch - 8ms/step 訓練が終わり、サイン関数とモデルの関数値を比較すると、学習がうまく行われたことがわかります。\n関数型API Input() 関数と Model() 関数でレイヤーを直接連結する方法です。MLPのようなシンプルなモデルであれば、上記のシーケンシャルモデルで定義する方がはるかに簡単です。上のシーケンシャルモデルで定義したニューラルネットワークと同じ構造のモデルを定義する方法は次のようになります。\nfrom tensorflow.keras import Model\rfrom tensorflow.keras.layers import Input, Dense\rinput = Input(shape=(10)) # 変数は \u0026#34;出力の次元 = 最初の層の入力の次元\u0026#34;\rdense1 = Dense(10, activation = \u0026#34;relu\u0026#34;)(input)\rdense2 = Dense(10, activation = \u0026#34;relu\u0026#34;)(dense1)\routput = Dense(1)(dense2)\rmodel = Model(inputs=input, outputs=output)\rmodel.summary() # output↓\r# Model: \u0026#34;model_10\u0026#34;\r# _________________________________________________________________\r# Layer (type) Output Shape Param #\r# =================================================================\r# input_13 (InputLayer) [(None, 1)] 0\r# # dense_19 (Dense) (None, 10) 20\r# # dense_20 (Dense) (None, 10) 110\r# # dense_21 (Dense) (None, 1) 11\r# # =================================================================\r# Total params: 141\r# Trainable params: 141\r# Non-trainable params: 0\r# _________________________________________________________________ Inputはインプットレイヤーを定義する関数です。正確にはレイヤーではなくテンソルですが、重要な点ではないので、ただの入力層として受け入れても良いでしょう。混乱する点は、出力の次元を入力する必要があるという点です。つまり、最初の層の入力の次元を入力する必要があります。これを定義した後、Dense関数の入力として入力し、明示的に直接各層を連結します。最後に、Model関数で入力と出力を引数に入れると、モデルを定義することができます。\nその後、モデルを .compile() メソッドでコンパイルし、.fit() メソッドで訓練するプロセスは、上で紹介した通りです。\n環境 OS: Windows11 Version: Python 3.9.13, tensorflow==2.12.0, keras==2.12.0 ","id":3562,"permalink":"https://freshrimpsushi.github.io/jp/posts/3562/","tags":null,"title":"TensorFlow-Kerasでシーケンスモデル、関数型APIでMLPを定義してトレーニングする方法"},{"categories":"줄리아","contents":"コード サイズ plot(x, y, size=(600,400)) Juliaでは、図のサイズは size オプションで設定する。Tuple{Integer, Integer} 型で入力する必要があり、各整数はそれぞれ横ピクセルと縦ピクセルを意味する。デフォルト値は (600,400) だ。\nusing Plots\rx = rand(10)\rplot(x)\rsavefig(\u0026#34;size_default.png\u0026#34;)\rplot(x, size=(1200,800))\rsavefig(\u0026#34;size_(1200,800).png\u0026#34;) 1800x1200の画像 (左)、600x400の画像 (右) 解像度 plot(x, y, dpi=100) 画像の解像度は dpi オプションで設定し、デフォルト値は 100 だ。論文やレポート、PowerPointなどに添付する場合は、300くらいにするのがいい。\nusing Plots\rx = rand(10)\rplot(x)\rsavefig(\u0026#34;dpi_default.png\u0026#34;)\rplot(x, dpi=300) savefig(\u0026#34;dpi_300.png\u0026#34;) dpi=100の画像 (上)、dpi=300の画像 (下) また、サイズを増やす時は、解像度も一緒に増やさないと、画像が綺麗さを保てないので注意。dpiを300にして保存すると、画像のサイズが1800x1200に増えるが、サイズだけを増やしてdpiをデフォルト値の100のままにしておくと、画像が醜くなるから気をつけて。\ndpi=300, size=1800x1200の画像 (左)、dpi=100, size=1800x1200の画像 (右) 環境 OS: Windows11 Version: Julia 1.9.0, Plots v1.38.12 ","id":3559,"permalink":"https://freshrimpsushi.github.io/jp/posts/3559/","tags":null,"title":"ジュリアで画像のサイズと解像度を調整する方法"},{"categories":"줄리아","contents":"コード plot!([x1, x2], [y1, y2], arrow=:true) このコードは、プロット上に点$(x1, y1)$から点$(x2, y2)$までの矢印を描く。当然ながら、矢印の先端は終点$(x2, y2)$にある。サイン関数の最大値は以下のように示すことができる。\nusing Plots\rx = range(0, 2π, 100)\rplot(x, sin.(x), label=\u0026#34;\u0026#34;, ylims=(-1.3,1.3))\rplot!([π/2, 3], [1, 1.1], arrow=:true, color=:black, label=\u0026#34;\u0026#34;)\rannotate!(3.7, 1.1, \u0026#34;maximum\u0026#34;) 矢印の先 矢印の先のスタイルは:openか:closedで選ぶことができる。\n指定しないか:trueの場合：折れ線$\\to$ plot!([3π/2, 3], [-1, -1.1], arrow=:open, color=:red, label=\u0026#34;\u0026#34;)\rannotate!(2.3, -1.1, \u0026#34;minimum\u0026#34;) 矢印の方向 矢印の先の方向は:head、:tail、:bothで設定でき、:headがデフォルトである。\nplot!([π/2, π/2], [0, 1], arrow=(:closed, :both), color=:purple, label=\u0026#34;\u0026#34;)\rannotate!(0.75π, 0.5, \u0026#34;amplitude\u0026#34;) 公式ドキュメントで1、headlengthとheadwidthのオプションについての説明があるが、使ってみるとエラーしか出なくてどう使うかよくわからない。\n環境 OS: Windows11 Version: Julia 1.9.0, Plots v1.38.12 https://docs.juliaplots.org/v1.38/api/#Plots.arrow-Tuple\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3558,"permalink":"https://freshrimpsushi.github.io/jp/posts/3558/","tags":null,"title":"ジュリアでグラフィックスに矢印を描く方法"},{"categories":"줄리아","contents":"説明1 Juliaでは、ランダムシードは以下のように固定する。\nseed!([rng=default_rng()], seed) -\u0026gt; rng seed!([rng=default_rng()]) -\u0026gt; rng 入力変数rngはランダムナンバージェネレータの略で、乱数を抽出するアルゴリズムを意味する。Randomパッケージでは、以下のオプションを提供している。\nTaskLocalRNG：デフォルトの設定値だ。 Xoshiro RandomDevice MersenneTwister コード シードを0に固定して三回抽出した後、再び0に固定して三回抽出すると、同じ値が得られることが確認できる。\njulia\u0026gt; using Random\rjulia\u0026gt; Random.seed!(0)\rTaskLocalRNG()\rjulia\u0026gt; rand(1)\r1-element Vector{Float64}:\r0.4056994708920292\rjulia\u0026gt; rand(1)\r1-element Vector{Float64}:\r0.06854582438651502\rjulia\u0026gt; rand(1)\r1-element Vector{Float64}:\r0.8621408571954849\rjulia\u0026gt; Random.seed!(0)\rTaskLocalRNG()\rjulia\u0026gt; rand(3)\r3-element Vector{Float64}:\r0.4056994708920292\r0.06854582438651502\r0.8621408571954849 https://docs.julialang.org/ja/v1/stdlib/Random/index.html#Random.seed!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":3555,"permalink":"https://freshrimpsushi.github.io/jp/posts/3555/","tags":null,"title":"ジュリアでランダムシードを固定する方法"},{"categories":"줄리아","contents":"日本語訳 説明 ボックスプロットを描くには、統計的可視化パッケージであるStatsPlots.jlを使用する必要がある。\nboxplot([data], labels=[label]) コード using StatsPlots\rx = rand(0:100, 100)\ry = rand(50:100, 100)\rz = cat(x,y, dims=1)\rboxplot(x, label=\u0026#34;x\u0026#34;)\rboxplot!(y, label=\u0026#34;y\u0026#34;)\rboxplot!(z, label=\u0026#34;z\u0026#34;) または、boxplot([x,y,z], label=[\u0026quot;x\u0026quot; \u0026quot;y\u0026quot; \u0026quot;z\u0026quot;])も同じ図を描く。lableにはコンマがないことに注意しよう。つまり、$3 \\times 1$ベクターではなく、$1 \\times 3$配列である必要がある。\nx軸の目盛り 文字列でx軸の目盛りを表したい場合、\nboxplot([x, y, z], xticks=(1:3, [\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;]), label=[\u0026#34;x\u0026#34; \u0026#34;y\u0026#34; \u0026#34;z\u0026#34;]) または、以下のコードも同じ図を描く。違いは、実際の座標がどうなっているかだ。上のコードでは、実際に各ボックスが描かれるx座標が1, 2, 3だが、グラフの目盛り値だけx, y, zと見えるように変えたものだ。下のコードは、実際に\u0026quot;x\u0026quot;, \u0026ldquo;y\u0026rdquo;, \u0026ldquo;z\u0026quot;の座標上にボックスを描く。\n2次元配列で描く a = rand(100, 3)\rboxplot(a, xticks=(1:3, [\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;]), label=[\u0026#34;x\u0026#34; \u0026#34;y\u0026#34; \u0026#34;z\u0026#34;]) データフレームで描く データフレームそのもので描くことはできず、配列に変換する必要がある。\nusing MLDatasets\rusing DataFrames\rdf = Iris().features\rboxplot(Array(df), xticks=(1:4, names(df)), label=reshape(names(df), (1,4))) 平均 平均を表示するオプションは別にない。scatterで打ってみよう。\nusing Statistics\rboxplot(fill(\u0026#34;x\u0026#34;, length(x)), x, labels=\u0026#34;x\u0026#34;)\rboxplot!(fill(\u0026#34;y\u0026#34;, length(y)), y, labels=\u0026#34;y\u0026#34;)\rboxplot!(fill(\u0026#34;z\u0026#34;, length(z)), z, labels=\u0026#34;z\u0026#34;)\rscatter!([\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;], [mean(x), mean(y), mean(z)], color=palette(:default)[1:3], label=\u0026#34;\u0026#34;) または、次のコードも同じ図を描く。\nboxplot([x, y, z], xticks=(1:3, [\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;]), label=[\u0026#34;x\u0026#34; \u0026#34;y\u0026#34; \u0026#34;z\u0026#34;])\rscatter!([1, 2, 3], [mean(x), mean(y), mean(z)], color=palette(:default)[1:3], label=\u0026#34;\u0026#34;) 環境 OS: Windows11 Version: Julia 1.9.0, Plots v1.38.12, StatsPlots v0.15.5, DataFrames v1.5.0, MLDatasets v0.7.11 併せて見る Python matplotlibで描く方法 ","id":3553,"permalink":"https://freshrimpsushi.github.io/jp/posts/3553/","tags":null,"title":"ジュリアでボックスプロットを描く方法"},{"categories":"줄리아","contents":"概要 ジュリアで決定木Decision Treeを実装したDecisionTree.jlパッケージを紹介する1。\nコード 例としては、Rの組み込みデータであるirisデータセットを使う。目標は、四つの変数SepalLength, SepalWidth, PetalLength, PetalWidthを用いてSpeciesを予測する決定木を作り、そのパフォーマンスを評価することである。\njulia\u0026gt; iris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\r150×5 DataFrame\rRow │ SepalLength SepalWidth PetalLength PetalWidth Speci ⋯\r│ Float64 Float64 Float64 Float64 Cat… ⋯\r─────┼──────────────────────────────────────────────────────────\r1 │ 5.1 3.5 1.4 0.2 setos ⋯\r2 │ 4.9 3.0 1.4 0.2 setos 3 │ 4.7 3.2 1.3 0.2 setos 4 │ 4.6 3.1 1.5 0.2 setos 5 │ 5.0 3.6 1.4 0.2 setos ⋯\r6 │ 5.4 3.9 1.7 0.4 setos 7 │ 4.6 3.4 1.4 0.3 setos 8 │ 5.0 3.4 1.5 0.2 setos 9 │ 4.4 2.9 1.4 0.2 setos ⋯\r10 │ 4.9 3.1 1.5 0.1 setos 11 │ 5.4 3.7 1.5 0.2 setos 12 │ 4.8 3.4 1.6 0.2 setos 13 │ 4.8 3.0 1.4 0.1 setos ⋯\r14 │ 4.3 3.0 1.1 0.1 setos 15 │ 5.8 4.0 1.2 0.2 setos ⋮ │ ⋮ ⋮ ⋮ ⋮ ⋮ ⋱\r136 │ 7.7 3.0 6.1 2.3 virgi 137 │ 6.3 3.4 5.6 2.4 virgi ⋯\r138 │ 6.4 3.1 5.5 1.8 virgi 139 │ 6.0 3.0 4.8 1.8 virgi 140 │ 6.9 3.1 5.4 2.1 virgi 141 │ 6.7 3.1 5.6 2.4 virgi ⋯\r142 │ 6.9 3.1 5.1 2.3 virgi 143 │ 5.8 2.7 5.1 1.9 virgi 144 │ 6.8 3.2 5.9 2.3 virgi 145 │ 6.7 3.3 5.7 2.5 virgi ⋯\r146 │ 6.7 3.0 5.2 2.3 virgi 147 │ 6.3 2.5 5.0 1.9 virgi 148 │ 6.5 3.0 5.2 2.0 virgi 149 │ 6.2 3.4 5.4 2.3 virgi ⋯\r150 │ 5.9 3.0 5.1 1.8 virgi 1 column and 120 rows omitted モデル作成 julia\u0026gt; using DecisionTree\rjulia\u0026gt; model = DecisionTreeClassifier(max_depth=2)\rDecisionTreeClassifier\rmax_depth: 2\rmin_samples_leaf: 1\rmin_samples_split: 2\rmin_purity_increase: 0.0\rpruning_purity_threshold: 1.0\rn_subfeatures: 0\rclasses: nothing\rroot: nothing モデルを作る。DecisionTreeClassifier()を通じて、決定木で使われるパラメーターを指定できる。\nモデルフィッティング julia\u0026gt; features = Matrix(iris[:, Not(:Species)]);\rjulia\u0026gt; labels = iris.Species;\rjulia\u0026gt; fit!(model, features, labels) DecisionTreeClassifier\rmax_depth: 2\rmin_samples_leaf: 1\rmin_samples_split: 2\rmin_purity_increase: 0.0\rpruning_purity_threshold: 1.0\rn_subfeatures: 0\rclasses: [\u0026#34;setosa\u0026#34;, \u0026#34;versicolor\u0026#34;, \u0026#34;virginica\u0026#34;]\rroot: Decision Tree Leaves: 3\rDepth: 2 データを独立変数と従属変数に分けて、fit!()関数でモデルを学習させる。\nパフォーマンス確認 julia\u0026gt; print_tree(model)\rFeature 3 \u0026lt; 2.45 ?\r├─ setosa : 50/50\r└─ Feature 4 \u0026lt; 1.75 ?\r├─ versicolor : 49/54\r└─ virginica : 45/46 学習が終わったモデルは、print_tree()関数を通じてどんな構造をしているか確認できる。\njulia\u0026gt; sum(labels .== predict(model, features)) / length(labels)\r0.96 簡単に正解率を確認した結果、96%程度でかなり良いことがわかった。\n全コード using RDatasets\riris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\rusing DecisionTree\rmodel = DecisionTreeClassifier(max_depth=2)\rfeatures = Matrix(iris[:, Not(:Species)]);\rlabels = iris.Species;\rfit!(model, features, labels)\rprint_tree(model)\rsum(labels .== predict(model, features)) / length(labels) 環境 OS: Windows julia: v1.9.0 https://github.com/JuliaAI/DecisionTree.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2618,"permalink":"https://freshrimpsushi.github.io/jp/posts/2618/","tags":null,"title":"ジュリアで決定木を使う方法"},{"categories":"줄리아","contents":"概要 ジュリアでコレクションの重複をなくし、チェックする方法を紹介する。重複をなくすunique()関数は、アルゴリズム的に見て難しくないが、自分で実装しようとすると面倒で、効率的でないかもしれない。重複要素がないかをチェックするallunique()関数は、実装も簡単なほどに見つけることがない関数だから、この機会にしっかり覚えておくべきだ。\nコード unique() julia\u0026gt; x = [3, 1, 4, 1, 5, 9, 2, 6, 5];\rjulia\u0026gt; y = unique(x)\r7-element Vector{Int64}:\r3\r1\r4\r5\r9\r2\r6 allunique() 1 実はこのポストは、allunique()を紹介することが目的だ。コレクションが重複要素を持っているかをチェックする常識的な方法の一つは、length(unique(x)) == length(x)としてunique()を適用し、要素が減ったかをチェックすることだ。\nこの方法はあまりにも簡単で、効率に対して過信しやすいが、一旦unique()関数は長さ$n$の配列を少なくとも一度は全ての要素を見る必要があるので、時間複雑度は$O (n)$だ。これは、コード内で要素の重複チェックを頻繁に行う場合、確かに負担になるレベルのコストであり、allunique()は配列の長さによって実装が異なり、途中で重複要素が見つかれば計算を中断し、チェックに成功するなど、性能面で明らかな利点がある。\njulia\u0026gt; allunique(x)\rfalse\rjulia\u0026gt; allunique(y)\rtrue 完全なコード x = [3, 1, 4, 1, 5, 9, 2, 6, 5];\ry = unique(x)\rallunique(x)\rallunique(y) 環境 OS: Windows julia: v1.9.0 https://docs.julialang.org/en/v1/base/collections/#Base.allunique\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2616,"permalink":"https://freshrimpsushi.github.io/jp/posts/2616/","tags":null,"title":"ジュリアでコレクションの重複を削除する方法"},{"categories":"줄리아","contents":"概要 Juliaでは、クラスタリング用のパッケージとしてClustering.jlが提供されている1。実装されているアルゴリズムは次の通りです:\nK-means K-medoids Affinity Propagation Density-based spatial clustering of applications with noise (DBSCAN) Markov Clustering Algorithm (MCL) Fuzzy C-Means Clustering 階層クラスタリング Single Linkage Average Linkage Complete Linkage Ward\u0026rsquo;s Linkage コード DBSCAN DBSCAN (Density-based spatial clustering of applications with noise)はdbscan()関数で実装されています。$p$次元のデータが$n$個ある場合、$p \\times n$サイズの行列と半径Radiusが引数として与えられなければなりません。\njulia\u0026gt; points = [iris.PetalLength iris.PetalWidth]\u0026#39;\r2×150 adjoint(::Matrix{Float64}) with eltype Float64:\r1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 1.5 … 5.4 5.6 5.1 5.1 5.9 5.7 5.2 5.0 5.2 5.4 5.1 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 0.2 2.1 2.4 2.3 1.9 2.3 2.5 2.3 1.9 2.0 2.3 1.8 julia\u0026gt; dbscaned = dbscan(points, 0.5)\rDbscanResult(DbscanCluster[DbscanCluster(50, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10 … 41, 42, 43, 44, 45, 46, 47, 48, 49, 50], Int64[]), DbscanCluster(100, [51, 52, 53, 54, 55, 56, 57, 58, 59, 60 … 141, 142, 143, 144, 145, 146, 147, 148, 149, 150], Int64[])], [1, 51], [50, 100], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1 … 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\rjulia\u0026gt; dbscaned |\u0026gt; propertynames\r(:clusters, :seeds, :counts, :assignments) DBSCANの結果はDbscanResultという構造体で返されます。.assignmentsと.clusterが重要です。\n各クラスタにどのデータポイントが属しているかは、次のようにgetproperty()関数を通じて得ることができます。\njulia\u0026gt; getproperty.(dbscaned.clusters, :core_indices)\r2-element Vector{Vector{Int64}}:\r[1, 2, 3, 4, 5, 6, 7, 8, 9, 10 … 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\r[51, 52, 53, 54, 55, 56, 57, 58, 59, 60 … 141, 142, 143, 144, 145, 146, 147, 148, 149, 150] 各データポイントがどのクラスタに属しているかは、次のように.assignmentsプロパティを通じて知ることができます。\njulia\u0026gt; dbscaned.assignments\r150-element Vector{Int64}:\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\r⋮\r2\r2\r2\r2\r2\r2\r2\r2\r2\r2\r2\r2\r2\r2 視覚化のコツとして、クラスタは任意の整数に割り当てられるので、散布図を描く際に*.assignmentsをそのままcolorオプションに入れると、次のように各クラスタに対応する色が指定されます。\nscatter(iris.PetalLength, iris.PetalWidth,\rxlabel = \u0026#34;PetalLength\u0026#34;, ylabel = \u0026#34;PetalWidth\u0026#34;,\rcolor = dbscaned.assignments) クラスタリングがうまく実行されたことが確認できます。\n全コード using Clustering\rusing RDatasets\riris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\rscatter(iris.PetalLength, iris.PetalWidth, xlabel = \u0026#34;PetalLength\u0026#34;, ylabel = \u0026#34;PetalWidth\u0026#34;)\rpng(\u0026#34;iris\u0026#34;)\rpoints = [iris.PetalLength iris.PetalWidth]\u0026#39;\rdbscaned = dbscan(points, 0.5)\rdbscaned |\u0026gt; propertynames\rgetproperty.(dbscaned.clusters, :core_indices)\rdbscaned.assignments\rscatter(iris.PetalLength, iris.PetalWidth,\rxlabel = \u0026#34;PetalLength\u0026#34;, ylabel = \u0026#34;PetalWidth\u0026#34;,\rcolor = dbscaned.assignments) 環境 OS: Windows julia: v1.9.0 Clustering v0.15.4 https://github.com/JuliaStats/Clustering.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2613,"permalink":"https://freshrimpsushi.github.io/jp/posts/2613/","tags":null,"title":"ジュリアでクラスタリングパッケージを使用する方法"},{"categories":"줄리아","contents":"概要 ジュリアでは、マシンラーニング、特にディープラーニングに関連した自動微分Automatic DifferentiationのためにZygote.jlというパッケージを使っている1。開発者たちは、このパッケージは次世代の自動微分システムとして、ジュリアで微分可能プログラミングDifferentiable Programmingができるようにすると宣伝していて、実際に使ってみると驚くほど直感的だと分かる。\n自動微分ではなく、導関数に関連したパッケージ自体が気になるなら、Calculus.jl パッケージを参照してほしい。\nコード 単変数関数 信じられないくらい簡単だ。普段私たちが微分するのと同じように、関数名の後ろにプライム'をつけると、まるで本当に導関数を使って計算しているように、微分係数が計算される。\njulia\u0026gt; using Zygote\rjulia\u0026gt; p(x) = 2x^2 + 3x + 1\rp (generic function with 1 method)\rjulia\u0026gt; p(2)\r15\rjulia\u0026gt; p\u0026#39;(2)\r11.0\rjulia\u0026gt; p\u0026#39;\u0026#39;(2)\r4.0 多変数関数 gradient() 関数を使う。\njulia\u0026gt; g(x,y) = 3x^2 + 2y + x*y\rg (generic function with 1 method)\rjulia\u0026gt; gradient(g, 2,-1)\r(11.0, 4.0) もう少し直感的にコードを書きたいなら、次のように\\nabla、すなわち∇で再び関数を定義して試してみるのも良い。\njulia\u0026gt; ∇(f, v...) = gradient(f, v...)\r∇ (generic function with 1 method)\rjulia\u0026gt; ∇(g, 2, -1)\r(11.0, 4.0) 全体のコード using Zygote\rp(x) = 2x^2 + 3x + 1\rp(2)\rp\u0026#39;(2)\rp\u0026#39;\u0026#39;(2)\rg(x,y) = 3x^2 + 2y + x*y\rgradient(g, 2,-1)\r∇(f, v...) = gradient(f, v...)\r∇(g, 2, -1) 環境 OS: Windows julia: v1.9.0 Zygote: v0.6.62 https://github.com/FluxML/Zygote.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2609,"permalink":"https://freshrimpsushi.github.io/jp/posts/2609/","tags":null,"title":"ジュリアの自動微分パッケージZygote.jl"},{"categories":"줄리아","contents":"概要 Juliaで構造体のプロパティを参照する方法は主に二つある。文法的な便宜または実際の用途に応じて適切に使用するべきだ。\nコード 例として、Juliaで//演算子は以下のように有理数(Rational)タイプの数を作る。有理数が持つプロパティの名前には、分子Numeratorを意味する:numと分母Denominatorを意味する:denがある。\njulia\u0026gt; q = 7 // 12\r7//12\rjulia\u0026gt; q |\u0026gt; typeof\rRational{Int64}\rjulia\u0026gt; q |\u0026gt; propertynames\r(:num, :den) getproperty(x, :y)とx.y julia\u0026gt; getproperty(q, :den)\r12\rjulia\u0026gt; q.den\r12 基本的には、getproperty()関数の二番目の引数にそのプロパティの名前をシンボルで与えればいい。あるいは、普通のプログラミング言語でのフィールド、プロパティへの参照のように、オブジェクト変数名の後ろに点を付けてそのプロパティにアクセスできる。\n配列に対するプロパティの参照 一方、上記の方法は一回だけ必要な時に使用できる方法だが、配列にある複数の要素にアクセスする必要があれば、以下のようにブロードキャストを使用しなければならない。あるいは、性能が重要でなくてただ早くコーディングが必要なら、Pythonみたいにリストコンプリヘンションを使用することも一つの方法だ。\njulia\u0026gt; Q = [k // 12 for k in 1:12]\r12-element Vector{Rational{Int64}}:\r1//12\r1//6\r1//4\r1//3\r5//12\r1//2\r7//12\r2//3\r3//4\r5//6\r11//12\r1//1\rjulia\u0026gt; getproperty.(Q, :num)\r12-element Vector{Int64}:\r1\r1\r1\r1\r5\r1\r7\r2\r3\r5\r11\r1\rjulia\u0026gt; [q.num for q in Q]\r12-element Vector{Int64}:\r1\r1\r1\r1\r5\r1\r7\r2\r3\r5\r11\r1 全コード q = 7 // 12 q |\u0026gt; typeof q |\u0026gt; propertynames getproperty(q, :den) q.den Q = [k // 12 for k in 1:12] getproperty.(Q, :num) [q.num for q in Q] 環境 OS: Windows julia: v1.9.0 ","id":2607,"permalink":"https://freshrimpsushi.github.io/jp/posts/2607/","tags":null,"title":"ジュリアで関数として構造体のプロパティを参照する方法"},{"categories":"줄리아","contents":"コード quiver(, quiver=) Juliaでは、quiver()関数を使ってベクトルフィールドを視覚化することができる。\nθ = 0:0.2:2π\rquiver(cos.(θ),sin.(θ), quiver = (-sin.(θ), cos.(θ)), size = (600,600), lims = (-2,2)); png(\u0026#34;1\u0026#34;) 矢印の長さの変更 矢印の大きさを変えるもっといい方法があるかもしれないけど、基本的にはquiver=オプションで提供されているベクトルの長さを伸ばしたり縮めたりして、もっといい図を描くことができる。\nquiver(cos.(θ),sin.(θ), quiver = (-sin.(θ), cos.(θ)) ./ 2, size = (600,600), lims = (-2,2)); png(\u0026#34;2\u0026#34;) 環境 OS: Windows julia: v1.9.0 ","id":2605,"permalink":"https://freshrimpsushi.github.io/jp/posts/2605/","tags":null,"title":"ジュリアでベクトル場を描く方法"},{"categories":"줄리아","contents":"概要 複数の配列が与えられた時、例えば、それぞれの配列の3番目の要素にアクセスしたいという状況は意外と多い。Juliaでは、getindex()関数のブロードキャストを通じてこれを実装できる。\nコード getindex.() julia\u0026gt; seq_ = [collect(1:k:100) for k in 1:10]\r10-element Vector{Vector{Int64}}:\r[1, 2, 3, 4, 5, 6, 7, 8, 9, 10 … 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\r[1, 3, 5, 7, 9, 11, 13, 15, 17, 19 … 81, 83, 85, 87, 89, 91, 93, 95, 97, 99]\r[1, 4, 7, 10, 13, 16, 19, 22, 25, 28 … 73, 76, 79, 82, 85, 88, 91, 94, 97, 100]\r[1, 5, 9, 13, 17, 21, 25, 29, 33, 37 … 61, 65, 69, 73, 77, 81, 85, 89, 93, 97]\r[1, 6, 11, 16, 21, 26, 31, 36, 41, 46, 51, 56, 61, 66, 71, 76, 81, 86, 91, 96]\r[1, 7, 13, 19, 25, 31, 37, 43, 49, 55, 61, 67, 73, 79, 85, 91, 97]\r[1, 8, 15, 22, 29, 36, 43, 50, 57, 64, 71, 78, 85, 92, 99]\r[1, 9, 17, 25, 33, 41, 49, 57, 65, 73, 81, 89, 97]\r[1, 10, 19, 28, 37, 46, 55, 64, 73, 82, 91, 100]\r[1, 11, 21, 31, 41, 51, 61, 71, 81, 91]\rjulia\u0026gt; getindex.(seq_, 3)\r10-element Vector{Int64}:\r3\r5\r7\r9\r11\r13\r15\r17\r19\r21 first(), last() first()はgetindex(, 1)と同じだが、last()はgetindex(, end)と同じ表現が存在しないため、特別な関数と言える。プログラムが繰り返される中で最後にある結果が必要な場合が多く、その最後の要素のインデックスは様々な場合が多いため、last()関数は知っておくべきだ。\njulia\u0026gt; first.(seq_)\r10-element Vector{Int64}:\r1\r1\r1\r1\r1\r1\r1\r1\r1\r1\rjulia\u0026gt; last.(seq_)\r10-element Vector{Int64}:\r100\r99\r100\r97\r96\r97\r99\r97\r100\r91 環境 OS: Windows julia: v1.9.0 ","id":2603,"permalink":"https://freshrimpsushi.github.io/jp/posts/2603/","tags":null,"title":"ジュリアで配列の特定の位置を関数で参照する方法"},{"categories":"줄리아","contents":"概要 ジュリアでパッケージを読み込む方法はusingを使うことだけど、プログラムが大きくなるとそれを一つ一つ書くのも大変だ。ループを通してパッケージを読み込む方法を紹介する1。\nコード メタプログラミング packages = [:CSV, :DataFrames, :LinearAlgebra, :Plots]\rfor package in packages\r@eval using ▷eq1◁(package)\rend 実際の使い方としては、プログレスバーだけ別に読み込んで、その他のパッケージの読み込みは目で確認する方がいい。\n環境 OS: Windows julia: v1.9.0 https://discourse.julialang.org/t/programmatically-load-packages/52435/2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2601,"permalink":"https://freshrimpsushi.github.io/jp/posts/2601/","tags":null,"title":"ジュリアからRへのパッケージのインポート方法"},{"categories":"줄리아","contents":"概要 Juliaで行列の正規化を簡単にするヒントを紹介する1。基本的には行列を行ごと、列ごとにスカラー倍する方法とeachcol()関数、LinearAlgebraモジュールのnorm()関数を混ぜて使っただけだが、一行で終わり、使う機会が多いので覚えておくと役に立つ。\nコード julia\u0026gt; using LinearAlgebra\rjulia\u0026gt; X = reshape(1:15, 5, :)\r5×3 reshape(::UnitRange{Int64}, 5, 3) with eltype Int64:\r1 6 11\r2 7 12\r3 8 13\r4 9 14\r5 10 15 与えられた行列Xを列ごとに正規化Normalizeするのは、X ./ norm.(eachcol(X))'の一行で可能だ。実行自体と実際にうまく正規化されたかを確認した結果は以下の通りだ。\njulia\u0026gt; Z = X ./ norm.(eachcol(X))\u0026#39;\r5×3 Matrix{Float64}:\r0.13484 0.330289 0.376192\r0.26968 0.385337 0.410391\r0.40452 0.440386 0.444591\r0.53936 0.495434 0.47879\r0.6742 0.550482 0.512989\rjulia\u0026gt; norm.(eachcol(Z))\r3-element Vector{Float64}:\r1.0\r1.0\r1.0 全コード using LinearAlgebra\rX = reshape(1:15, 5, :)\rZ = X ./ norm.(eachcol(X))\u0026#39;\rnorm.(eachcol(Z)) 環境 OS: Windows julia: v1.9.0 https://stackoverflow.com/a/72627341/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2599,"permalink":"https://freshrimpsushi.github.io/jp/posts/2599/","tags":null,"title":"ジュリアで列ごとに行列を正規化する方法"},{"categories":"행렬대수","contents":"定義 1 2 置換行列 $P^{T}$ と 可逆行列 $A \\in \\mathbb{R}^{n \\times n}$ に対し、その 行列の積 $P^{T} A$ は $LU$ を与える。この分解を $A$ の PLU分解Permutation LU Decomposition と言う。$P$ は置換行列であるため、直交行列 となり、すなわち $P^{-1} = P^{T}$ であり、次のように表すことができる。 $$ P^{T} A = LU \\iff A = PLU $$\n説明 LU分解のアルゴリズム：$(a_{ij}) \\in \\mathbb{R}^{n \\times n}$ を可逆行列とする。\nStep 1. $k = 1$\n$u_{1j} = a_{1j}$ とし、$\\displaystyle l_{i1} = {{1} \\over {u_{11}}} a_{i1}$ を計算する。\nStep 2. $k = 2, 3, \\cdots , n-1$\nStep 2-1. 以下を計算する。 $$ u_{kk} = a_{kk} - \\sum_{s = 1}^{k-1} l_{ks} u_{sk} $$ Step 2-2. $j = k+1 , k+2 , \\cdots , n-1$ に対し以下を計算する。 $$ u_{kj} = a_{kj} - \\sum_{s = 1}^{k-1} l_{ks} u_{sj} $$ Step 2-3. $i = k+1 , k+2 , \\cdots , n-1$ に対し以下を計算する。 $$ l_{ik} = {{1} \\over {u_{kk}}} \\left{ a_{ik} - \\sum_{s = 1}^{k-1} l_{is} u_{sk} \\right} $$ Step 3. $k = n$ に対し以下を計算する。 $$ u_{nn} = a_{nn} - \\sum_{s = 1}^{n-1} l_{ns} u_{sn} $$\n行列のLU分解を行うには $u_{11} = a_{11}$ や $u_{kk}$ の逆数をとることが可能でなければならないが、 $$ A = \\begin{bmatrix} 0 \u0026amp; 3\\\\ 2 \u0026amp; 1 \\end{bmatrix} $$ のような 行列でもこのアルゴリズムを適用することはできない。LU分解を可能にするためにある置換行列 $P^{T}$ を乗じて $A$ を $PLU$ として表すことを PLU分解 と呼ぶ。もちろん、左または右、行または列が重要というわけではないため、 $$ A P^{T} = LU \\iff A = LUP $$ と書き LUP分解 と呼んでも差し支えない。\nhttps://www.cfm.brown.edu/people/dobrush/cs52/Mathematica/Part2/PLU.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://math.unm.edu/~loring/links/linear_s08/LU\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2,"permalink":"https://freshrimpsushi.github.io/jp/posts/2/","tags":null,"title":"PLU分解"},{"categories":"행렬대수","contents":"定義 1 各行で成分が一つだけ$1$で、残りがすべて$0$である正方行列$P \\in \\mathbb{R}^{n \\times n}$を順列行列と呼ぶ。\n基本的性質 直交性 すべての順列行列は直交行列である: $$P^{-1} = P^{T}$$\nスパース性 十分に大きな$n$に対して、$P \\in \\mathbb{R}^{n \\times n}$はスパース行列となる。\n説明 順列行列はその名前が示す通り、行列の乗算によって行と列の順列を与える。次の例では、左側に乗算すると行の順列となり、右側に乗算すると列の順列となることがわかる。 $$ \\begin{align*} \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; 0 \\\\ 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \\end{bmatrix} = \u0026amp; \\begin{bmatrix} a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\\\ a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \\end{bmatrix} \\\\ \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \\end{bmatrix} \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; 0 \\\\ 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} = \u0026amp; \\begin{bmatrix} a_{12} \u0026amp; a_{11} \u0026amp; a_{13} \\\\ a_{22} \u0026amp; a_{21} \u0026amp; a_{23} \\\\ a_{32} \u0026amp; a_{31} \u0026amp; a_{33} \\end{bmatrix} \\end{align*} $$\nhttps://www.cfm.brown.edu/people/dobrush/cs52/Mathematica/Part2/PLU.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1,"permalink":"https://freshrimpsushi.github.io/jp/posts/1/","tags":null,"title":"順列行列"},{"categories":"줄리아","contents":"エラー Juliaでデータフレームを使っていると、文字列データがString7やString15、String31などと読み込まれて様々なエラーを発生させることがある。具体的にどんなエラーが発生するというより、普段からよく使っている関数がここでは通用しなくて色々な問題を起こす。\n原因 パフォーマンス上の理由で、Stringをより早く処理できるString7などに変更したのだ。意図通りに設計されているので、仕方がない。\n解決法 CSV.read()にオプションとしてstringtype = Stringを渡せばいい。\n実際に使ってみるとかなり不便だけど、我慢して使うしかない。\n環境 OS: Windows julia: v1.8.5 ","id":2574,"permalink":"https://freshrimpsushi.github.io/jp/posts/2574/","tags":null,"title":"ジュリアでString7, String15なしでデータフレームを呼び出す方法"},{"categories":"다변수벡터해석","contents":"質問 偏微分では、通常の微分と異なり、$\\displaystyle {{ d f } \\over { d t }}$ の代わりに $\\displaystyle {{ \\partial f } \\over { \\partial t }}$ のような表現を使用します。$\\partial$ は[ラウンドディー]Round Deeまたは[パーシャル]Partialと読み、歴史的にも$d$ を丸めて書いた[カーリーディー]Curly Deeから由来しています。1 $\\TeX$ のコードでは \\partial であり、韓国では[ラウンドディー]さえも長いと考えるのか、単に[ラウンド]と読む人も多いです。\nなぜ $d$ を $\\partial$ で書くのか？ 問題は、偏微分が単に他の変数に関して微分するだけなのに、なぜ記号を異なるものにする必要があるのかということが納得できないということです。学部の授業レベルでは、偏微分が初めて登場するたびに必ず出てくる質問ですが、実際の答えは、数学科でなければ「そんなことは数学科で考えることだ」または数学科であっても「ただの表記の違いとして受け入れても問題ない」という程度で返ってくることがあります。これが決して間違っているわけではないのは、$d$ で書こうが $\\partial$ で書こうが、数学科でなければそれが特に重要なわけではなく、数学科であっても式の意味自体が変わるわけではないからです。 例えば、熱方程式を学ぶ場合、 $$ {{ \\partial u } \\over { \\partial t }} = {{ \\partial u } \\over { \\partial x^{2} }} $$ の $\\partial$ を通常の微分表記 $d$ に変えて $$ {{ d u } \\over { d t }} = {{ d u } \\over { d x^{2} }} $$ と書いた場合、2つの方程式が同じかどうかを尋ねることができます。非常に混乱することに、その答えは「実際には同じ」なので、この時点で多くの学生が $d$ と $\\partial$ の区別に意味がないと感じたり、定義レベルで受け入れてしまったりすることになります。\n回答 ニュートンとライプニッツ 本格的な偏微分の話に入る前に、微分の2人の父、ニュートンNewtonとライプニッツLeibnizの話を面白い読み物として取り上げたいと思います。現代において、両者は独自に微分の概念および記法を考案したと認められていますが、関数$y = f(x)$ の導関数を表す際、ニュートンは $$ y ' = f ' (x) $$ のような表記を使用し、ライプニッツは $$ {{ dy } \\over { dx }} = {{ d f(x) } \\over { dx }} $$ のような表記を使用しました。同じ微分であっても、このように表現の違いが生じるのは、両者の思考方法や微積分に対する見方自体が異なっていたためです。現在では、同時代に独自に微分を考案した人がもう一人いても良かったと思えるほど、幸運なことです。ニュートンは古典力学の巨匠として、「位置を一度微分すると速度、二度微分すると加速度」といった話をしなければならず、この時 $$ \\begin{align*} v =\u0026amp; x ' \\\\ a =\u0026amp; v ' = x '' \\end{align*} $$ のような表現は非常にすっきりして効率的です。ライプニッツは幾何学的Geometricな観点から見るとより理にかなっており、直線の傾きが横と縦の変化量の比として定義されるため、曲線では非常に小さな単位を与えて $$ {{ \\Delta y } \\over { \\Delta x }} \\approx {{ d y } \\over { d x }} $$ のように接線の傾きに自然に近づくことができます。興味深いことに、ここまで述べたのは全て通常の微分に関するものであり、分野によっては以下のような表記の分化が起こり、ニュートンとライプニッツの表記が共存することができるという事実です。\n微分幾何学における$s$ に対する微分と$t$ に対する微分の表記： $$ {{ df } \\over { ds }} = f^{\\prime} \\quad \\text{and} \\quad {{ df } \\over { dt }} = \\dot{f} $$ ドット$\\dot{}$ やプライム$'$ はどちらも微分を表していますが、微分幾何学の文脈では上記のように記号を区別することができます。通常、$s$ は単位速度曲線のパラメータであり、$t = t(s)$ は曲線の長さの再パラメータ化によって表されます。\nこの表記は、微分という概念が変形されて出てきたわけではありません。微分幾何学では、単に$s$ で多くの微分を行い、$t$ でも多くの微分を行う必要があるため、ニュートンの表記では何に対して微分しているのか区別できず、ライプニッツの表記では数式が複雑すぎるため、両者の長所を取り入れるために新たな表記を作成したものです。\n本当に興味深いのは、このように幾何学的な観点から$s$ や $t$ は単なるパラメータに過ぎないにもかかわらず、常微分方程式の中でも特に時間timeによる変化を表す場合には、その頭文字を取って$t$ に対する$v$ の導関数を$v '$ ではなく$\\dot{v}$ と書くようになりました。これにより、ダイナミクスなどのほとんどのシステムで時間による変化を記述する際には、$v '$ の代わりに $$ \\dot{v} = f(v) $$ という表現を好んで使用するようになりました。ポイントは、「何によって微分するか」を明確かつすっきりと表現するための検討自体が、偏微分という枠組みに縛られなくても自然に浮かぶことができるということです。\n多変数関数の暗示 前節では、$f '$ と $\\dot{f}$ が単に表現の違いだけで、どの変数によって微分されたかを区別できること、特にダイナミクスシステムでは、時間$\\dot{v} = f(v)$ が現れなくても、一般的な規約とコンテキストからそれが時間による微分であることを暗示できることを指摘しました。このように表現によって暗黙的Implicitにわかる情報についてもう少し話してみたいと思います。\n再び偏微分に戻ると、$d$ と $\\partial$ の表記がどのように異なるかを実感するのが難しいのは、その式自体が示す偏導関数に違いがないからです。例えば、$f$ を$t$ で微分した導関数が$g$ である場合、その$g$ は $$ g = {{ d f } \\over { d t }} = {{ \\partial f } \\over { \\partial t }} $$ のように$d$ で表されても$\\partial$ で表されてもあまり関係がない。記号がどうであれ、$t$ で微分された「結果」である$g$ が同じだからです。しかし、$\\partial$ が暗黙的に与える情報は$g$ ではなく$f$ に関するものです。ある関数$h$ が$H$ に関して$x$ で微分された結果だとすると、次のように2つの表現を比較してみましょう：\n偏微分表現を使用しない場合：$\\displaystyle h = {{ d H } \\over { d x }} \\implies$ $H$ を微分すると$h$ になるらしい。\n偏微分表現を使用する場合：$\\displaystyle h = {{ \\partial H } \\over { \\partial x }} \\implies$ なぜこれだけ？何か$y$ があって$H = H (x , y)$ になるんだろう？\rつまり、$\\partial$ という記号は、与えられた関数が多変数関数であることを暗示しているのです。多くの場合、偏微分に初めて本格的に触れるのは通常偏微分方程式であり、 $$ {{ \\partial u } \\over { \\partial t }} = {{ \\partial u } \\over { \\partial x^{2} }} $$ のような方程式があれば、私たちは$u$ を$t$ で微分した偏導関数$u_{t}$ が気になるわけではなく、$u$ を$x$ で2回微分した2階偏導関数$u_{xx}$ が気になるわけでもなく、その両方が等しいときの$t$ と$x$ の関数$u = u (t,x)$ が何であるかが気になるのです。この観点から、偏微分に使用される$\\partial$ が偏微分方程式の記述に使用されるのは妥当で自然だと主張することができます。\n一方で、このような慣習が広く受け入れられることにより、$d$ 自体の意味も変わります。多変数関数ではない関数をわざわざ$\\partial$ で微分することは意味がないため、導関数の表現に$d$ が使用されていれば、それは多変数関数ではないことを暗示することになります。例えば、2変数関数$u = u (t,x)$ に対して位置を一点に固定して$u = u \\left( t , x_{0} \\right)$ とすると、 $$ \\left. {{ \\partial u } \\over { \\partial t }} \\right|_{x = x_{0} } = {{ d u } \\over { d t }} = \\dot{u} $$ のような式は、$\\partial$ と$d$ の暗黙的な情報伝達を非常にうまく活用しています。これは、単なる表現の違いにとどまらず、実際に式を扱う思考方法にも影響を与え、偏微分方程式の問題を比較的簡単な常微分方程式に変換して解くといったアイデアにつながることもあります。\n✅ 全微分における混乱を避けるために $$ df = \\frac{ \\partial f}{ \\partial x_{1} }dx_{1} + \\frac{ \\partial f}{ \\partial x_{2} }dx_{2} + \\cdots + \\frac{ \\partial f}{ \\partial x_{n} }dx_{n} $$ 多変数関数$f : \\mathbb{R}^{n} \\to \\mathbb{R}$ に対する数理物理学などで使用される全微分は、通常上記のような形で表され、もう少し直感的に書くために$n = 3$ のとき次のように$t,x,y,z$ のみを書き、$x,y,z$ は互いに独立であるとしましょう。 $$ df = {{ \\partial f } \\over { \\partial x }} dx + {{ \\partial f } \\over { \\partial y }} dy + {{ \\partial f } \\over { \\partial z }} dz $$ 一見すると、$d$ と$\\partial$ が混在していて複雑に見えますが、ライプニッツの遺産に従って「両辺を$dt$ や$dx$ で割る」ような操作を行うと、 $$ \\begin{align*} df =\u0026amp; {{ \\partial f } \\over { \\partial x }} dx + {{ \\partial f } \\over { \\partial y }} dy + {{ \\partial f } \\over { \\partial z }} dz \\\\ {{ d f } \\over { d t }} =\u0026amp; {{ \\partial f } \\over { \\partial x }} {{ d x } \\over { d t }} + {{ \\partial f } \\over { \\partial y }} {{ d y } \\over { d t }} + {{ \\partial f } \\over { \\partial z }} {{ d z } \\over { d t }} \\\\ {{ d f } \\over { d x }} =\u0026amp; {{ \\partial f } \\over { \\partial x }} {{ d x } \\over { d x }} + {{ \\partial f } \\over { \\partial y }} {{ d y } \\over { d x }} + {{ \\partial f } \\over { \\partial z }} {{ d z } \\over { d x }} = {{ \\partial f } \\over { \\partial x }} \\end{align*} $$ のように$f$ を$t$ で微分する意味と$x$ で偏微分する意味が同時によく表現されていることがわかります。これは全微分の形が数式的に扱う上で非常に便利であることを示していますが、全微分で$\\partial$ をすべて取り除いて$d$ で統一して再度書き直すと次のようになります。 $$ df = {{ d f } \\over { d x }} dx + {{ d f } \\over { d y }} dy + {{ d f } \\over { d z }} dz $$ もちろん、ライプニッツの微分記法が分数の分子と分母を扱うときのように非常に直感的であることは事実ですが、この記事を読んでいる皆さんであれば、$dx$ や$dy$、$dz$ を本当にそのように扱ってはいけないことを知っているでしょう。それにもかかわらず、皆さんの内なる本能はこのように約分するように叫ぶでしょう。 $$ \\begin{align*} df =\u0026amp; {{ d f } \\over { dx }} dx + {{ d f } \\over { dy }} dy + {{ d f } \\over { dz }} dz \\\\ \\overset{?}{=} \u0026amp; {{ d f } \\over { \\cancel{dx} }} \\cancel{dx} + {{ d f } \\over { \\cancel{dy} }} \\cancel{dy} + {{ d f } \\over { \\cancel{dz} }} \\cancel{dz} \\\\ =\u0026amp; df + df + df \\\\ \\overset{???}{=}\u0026amp; 3 df \\end{align*} $$ このような惨事は、$d$ が$\\partial$ と同じになる条件を見落としたために起こった循環論法と見ることができます。\u0026rsquo;$\\partial$ をすべて取り除いて$d$ で統一して再度書き直す\u0026rsquo;という展開を無造作に行うことがあまりにも大胆であるため、何らかの方法で$\\partial$ を$d$ で置き換えてもよいと考えること自体が、$x,y,z$ が独立である場合 $$ df = {{ \\partial f } \\over { \\partial x }} dx + {{ \\partial f } \\over { \\partial y }} dy + {{ \\partial f } \\over { \\partial z }} dz \\implies {{ d f } \\over { d x }} = {{ \\partial f } \\over { \\partial x }} \\implies d \\equiv \\partial $$ から出てきたものです。その一方で、$d \\equiv \\partial$ の根拠となる$df = {{ \\partial f } \\over { \\partial x }} dx + {{ \\partial f } \\over { \\partial y }} dy + {{ \\partial f } \\over { \\partial z }} dz$ を無闇にいじると、どのような方法でも必ず問題が発生します。$d$ と$\\partial$ が等しくなるには、例で仮定したように多変数関数の変数が互いに独立である場合や、何らかの特別な条件の下での何らかの驚くべき定理を通じて、$d$ と$\\partial$ が本当に同じである必要があります。\nこれまでの考察から、偏微分で$d$ の代わりに$\\partial$ を使用する理由は、実際にそれらが異なるためであると要約することができます。これまで見てきた、$d$ と$\\partial$ が同じだったすべての例は、必ずそのための仮定を暗黙的に含んでいます。その良い仮定の中で、$\\partial$ が実質的に$d$ と同じになるかもしれませんが、だからといってわざわざ$\\partial$ を$d$ で書き直す必要もないのです。\n❌ 微分する変数以外は定数とみなすために？ 結論から言うと、間違った答えです。\nもっと正確に言うと、現象を説明する因果関係が逆転しています。例えば、$f(t,x) = \\left( t^{2} + x^{2} \\right)$ であれば、形式的にFormally$\\partial t$ 以外の変数を定数として $$ {{ \\partial f } \\over { \\partial t }} = 2t + 0 = 2t = {{ d f } \\over { d t }} $$ ではないのは、前節で見たように、$t$ と$t$ が独立であるという仮定$x$ の下で $$ \\begin{align*} \u0026amp; df = {{ \\partial f } \\over { \\partial t }} dt + {{ \\partial f } \\over { \\partial x }} dx \\\\ \\implies \u0026amp; {{ d f } \\over { d t }} = {{ \\partial f } \\over { \\partial t }} {{ dt } \\over { dt }} + {{ \\partial f } \\over { \\partial x }} {{ dx } \\over { dt }} \\\\ \\implies \u0026amp; {{ d f } \\over { d t }} = {{ \\partial f } \\over { \\partial t }} \\cdot 1 + {{ \\partial f } \\over { \\partial x }} \\cdot 0 \\\\ \\implies \u0026amp; {{ d f } \\over { d t }} = {{ \\partial f } \\over { \\partial t }} \\end{align*} $$ が成立するからです。偏微分$\\displaystyle {{ dx } \\over { dt }} = 0$ 自体が$\\partial$ という結果をもたらしたのではなく、$\\displaystyle {{ dx } \\over { dt }} = 0$ という原因が$\\displaystyle {{ dx } \\over { dt }} = 0$ という結果をもたらしたのです。このように「偏微分は微分する変数以外を定数として扱う」という説明は、まるで通常の微分$\\partial \\equiv d$ と異なり、偏微分$d$ がより強力なオペレーターであるかのような印象と誤解を与えます。また、$\\partial$ を定数として扱った場合、$x$ で微分した後には消えるはずですが、単純に$t$ のような例を考えると、$f(t,x) = t^{2} + x^{2} + 2tx$ は依然として変数が$\\dfrac{\\partial f}{\\partial t}$ である2変数関数です。\nこのような誤解がなくならない理由は、これがかなりもっともらしいからです。実際には、変数間に$(t,x)$ のような関係があると仮定する場合、そもそも$x = x(t)$ で偏微分するという表現自体を使用する必要がありません。チェーンルールに従えば、 $$ \\begin{align*} {{ d f } \\over { d t }} =\u0026amp; {{ d } \\over { d t }} \\left( t^{2} + x^{2} \\right) \\\\ =\u0026amp; 2t + {{ d x^{2} } \\over { d x }} {{ dx } \\over { dt }} \\\\ =\u0026amp; 2t + 2x \\dot{x} \\end{align*} $$ のように最初から誤解の余地なく式を展開することができます。少なくともこの例では、$t$ は実質的に$f = f(t,x)$ と同じか、むしろ難しいですし、結局のところ教科書ではこのような無意味なケースをすべて排除して、変数間が独立でありながらも依然として多変数関数である形だけが残ります。通常はきれいな例だけを見ながら学び、時間が経ち、偏微分に慣れ、誤った直感が定着し、他の人もそうです。しかし、違うものは違うものです。単に微分の記号を変えるだけで与えられた関数の従属関係を勝手に変えることはできません。\nhttps://math.stackexchange.com/a/2000353/459895\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2573,"permalink":"https://freshrimpsushi.github.io/jp/posts/2573/","tags":null,"title":"偏微分の記号を使い分ける理由"},{"categories":"줄리아","contents":"概要 ジュリアで図を描く時、titleでタイトルを入れるとサブプロット全てに適用されるので、plot_titleを使うべきだ1。これはプロットがサブプロットを含む場合、つまり\nplot(\rplot1, plot2, ...\r) のような形で構成された時、最も外側にあるplot()関数の引数が内部のサブプロットに継承Inheritanceみたいなことが起こるからだ。これを明確に区分するためにtitleとplot_titleが別々にある。\nコード plot(p1, p2, title = \u0026#34;Two Plots\u0026#34;) 見ての通り、title = \u0026quot;Two Plots\u0026quot;をすると全てのサブプロットにそのタイトルが適用される。\nplot(p1, p2, plot_title = \u0026#34;Two Plots\u0026#34;) plot_title = \u0026quot;Two Plots\u0026quot;では全体の図にただ一つのタイトルが適用されているのが確認できる。\n全体コード using Plots\rp1 = scatter(rand(100))\rp2 = histogram(rand(100))\rplot(p1, p2, title = \u0026#34;Two Plots\u0026#34;)\rplot(p1, p2, plot_title = \u0026#34;Two Plots\u0026#34;) 環境 OS: Windows julia: v1.8.5 https://stackoverflow.com/a/69713616/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2572,"permalink":"https://freshrimpsushi.github.io/jp/posts/2572/","tags":null,"title":"Juliaのサブプロットにメインタイトルを追加する方法"},{"categories":"줄리아","contents":"概要 Juliaで、カラーバー、軸、目盛り、グリッドなどの図のグラフィック要素を消す方法があるけれども、グラフィカルな要素をいじるから数字だけをきれいに消すことができず、formatterというオプションを使わないといけない。\nformatter = (_...) -\u0026gt; \u0026quot;\u0026quot; plot()関数のオプションにformatter = (_...) -\u0026gt; \u0026quot;\u0026quot;を与えるといい。\nusing Plots\rx = rand(10)\ry = rand(10)\rplot(\rplot(x,y)\r,plot(x,y, formatter = (_...) -\u0026gt; \u0026#34;\u0026#34;)\r) 上の画像では、左が普通の画像で、右が値を全部消した画像だ。元々formatterはこう使うだけじゃなくて、もっと多機能を持っている。原理を簡単に説明すると、元の画像に表示されるべきだった値に与えられた関数を適用する方式だ。上の例では、(_...) -\u0026gt; \u0026quot;\u0026quot;というラムダ式を受け取って、どんな数値が入っても空白の文字列を返して、軸の値を消した1。\nxformatter, yformatter 当然、xformatter、yformatterがあり、軸別に指定もできる。x軸だけを消したいなら、yformatterに、y軸だけを消したいなら、xformatterに(_...) -\u0026gt; \u0026quot;\u0026quot;を伝えればいい。\n環境 OS: Windows julia: v1.8.5 foreground_color_text = false plot()関数のキーワードにforeground_color_text = falseを入力すればいい。目盛り値(名前)の色を指定するキーワードだけど、falseを入力すると値が全然表示されなくなる。\nx_foreground_color_textとy_foreground_color_textでも軸別に指定できる。\nusing Plots\rplot(\rplot(rand(10)),\rplot(rand(10), foreground_color_text = false)\r) 環境 OS: Windows11 Version: Julia 1.9.3, Plots v1.39.0 https://stackoverflow.com/questions/74842089/remove-only-axis-values-in-plot-julia\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2570,"permalink":"https://freshrimpsushi.github.io/jp/posts/2570/","tags":null,"title":"ジュリアプロットで軸の値を削除する方法"},{"categories":"줄리아","contents":"概要 Juliaで有限差分法を使うには、特に有限差分の係数を求めるためには、FiniteDifferences.jlを使うのがいいだろう1。ノイズに弱い場合は、TVDiffとして知られるTotal Variation Regularized Numerical Differentiationを使ってみる価値がある。これはNoiseRobustDifferentiation.jlに実装されている。\nコード FiniteDifferenceMethod() julia\u0026gt; f′ = FiniteDifferenceMethod([-2, 0, 5], 1)\rFiniteDifferenceMethod:\rorder of method: 3\rorder of derivative: 1\rgrid: [-2, 0, 5]\rcoefficients: [-0.35714285714285715, 0.3, 0.05714285714285714]\rjulia\u0026gt; typeof(f′)\rFiniteDifferences.UnadaptedFiniteDifferenceMethod{3, 1} 上記の例では、関数の中央の点、左から2番目の点、右から5番目の点を使って1次微分を計算して、これら3点の重みを求める。基本的にFiniteDifferenceMethod()を通じて得られるのは、これらの係数である。\njulia\u0026gt; propertynames(f′)\r(:grid, :coefs, :coefs_neighbourhood, :condition, :factor, :max_range, :∇f_magnitude_mult, :f_error_mult)\rjulia\u0026gt; f′.grid\r3-element StaticArraysCore.SVector{3, Int64} with indices SOneTo(3):\r-2\r0\r5\rjulia\u0026gt; f′.coefs\r3-element StaticArraysCore.SVector{3, Float64} with indices SOneTo(3):\r-0.35714285714285715\r0.3\r0.05714285714285714 構造体で使用するプロパティには、主に.gridと.coefsの2つがある。\njulia\u0026gt; f′(sin, π/2)\r-1.2376571459669071e-11\rjulia\u0026gt; f′(cos, π/2)\r-1.0000000000076525 データではなく関数自体が与えられた場合は、上記のように直接関数形式で書いても問題ない。\n_fdm() central_fdm(2, 1)\rcentral_fdm(3, 2)\rbackward_fdm(4, 1)\rforward_fdm(5, 1) 特にカスタマイズせずに一般的に知られているFDMが必要な場合は、上記のようにすでに用意されている関数を使う方が便利だ。第1引数は、関数の種類に応じて何点を使用するかに依存し、第2引数$n$は$n$次微分を計算することを決定する。central_fdm()の第1引数は奇数が与えられた場合、中心にある点の係数が確実に0であることは言うまでもない。\n全コード using FiniteDifferences\rf′ = FiniteDifferenceMethod([-2, 0, 5], 1)\rtypeof(f′)\rpropertynames(f′)\rf′.grid\rf′.coefs\rf′(sin, π/2)\rf′(cos, π/2)\rcentral_fdm(2, 1)\rcentral_fdm(3, 2)\rbackward_fdm(4, 1)\rforward_fdm(5, 1) 環境 OS: Windows julia: v1.8.5 https://github.com/JuliaDiff/FiniteDifferences.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2568,"permalink":"https://freshrimpsushi.github.io/jp/posts/2568/","tags":null,"title":"ジュリアで有限差分を使用する方法"},{"categories":"줄리아","contents":"概要 ジュリアでは、数値解析的な補間のためにInterpolations.jlパッケージを使用する1。ジュリアで変数の値を出力する際に使用する補間法と混同しないように注意しよう。\nコード Interpolate() julia\u0026gt; y = rand(10)\r10-element Vector{Float64}:\r0.8993801321974316\r0.12988982511901515\r0.49781160399025925\r0.22555299914088356\r0.4848674643768577\r0.6089318286915111\r0.10444895196527337\r0.5921775799940143\r0.2149546302906653\r0.32749334953170317\rjulia\u0026gt; f = interpolate(y, BSpline(Linear()));\rjulia\u0026gt; f(1.2)\r0.7454820707817483\rjulia\u0026gt; f(0.1)\rERROR: BoundsError: attempt to access 10-element interpolate(::Vector{Float64}, BSpline(Linear())) with element type Float64 at index [0.1] 基本的には上記のようにデータを渡して、補間関数 f=$f$ 自体をリターンしてもらって使用することができる。例では、10個の点が与えられ、1番目(0.899)と2番目(0.129)の間の1.2あたりにある値(0.745)がどのように補間されるかを示している。具体的にどの方法を使用するかは、公式ドキュメントのAPIセクションを参照しよう2。\nlinear_interpolation() x = 1:10\rf_lin = linear_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_lin.(1:0.01:10), label = \u0026#34;Linear\u0026#34;) cubic_spline_interpolation() f_cub = cubic_spline_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_cub.(1:0.01:10), label = \u0026#34;Cubic\u0026#34;) constant_interpolation() f_con = constant_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_con.(1:0.01:10), label = \u0026#34;Constant\u0026#34;) 全体のコード using Interpolations, Plots\ry = rand(10)\rf = interpolate(y, BSpline(Linear()));\rf(1.2)\rf(0.1)\rx = 1:10\rf_lin = linear_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_lin.(1:0.01:10), label = \u0026#34;Linear\u0026#34;)\rf_cub = cubic_spline_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_cub.(1:0.01:10), label = \u0026#34;Cubic\u0026#34;)\rf_con = constant_interpolation(x, y);\rscatter(x, y, label = \u0026#34;Data\u0026#34;); plot!(1:0.01:10, f_con.(1:0.01:10), label = \u0026#34;Constant\u0026#34;) 環境 OS: Windows julia: v1.8.5 https://github.com/JuliaMath/Interpolations.jl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttp://juliamath.github.io/Interpolations.jl/stable/api/#Public-API\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2566,"permalink":"https://freshrimpsushi.github.io/jp/posts/2566/","tags":null,"title":"ジュリアでの数値解析的補間"},{"categories":"줄리아","contents":"概要 Juliaでは、差分を計算するためにdiff()関数が提供されている1。circshift()関数も使って簡単に書けるけど、端点の処理などがちょっと面倒に感じるから、知ってるとずっと楽になる。RやMatlabのdiff()関数とほぼ同じ方法で使えるが、これらと違い2次差分（差分を二回とること）などは特に実装されていない。\nコード 基本的な使い方 julia\u0026gt; x = rand(0:9, 12)\r12-element Vector{Int64}:\r3\r1\r9\r7\r1\r0\r6\r5\r3\r2\r9\r9 例えば上記のような配列がある場合、ただdiff()を適用して、前の要素と後ろの要素の差を計算した結果を得ることができる。結果から配列のサイズが正確に1だけ縮小されたのを確認できる。\njulia\u0026gt; diff(x)\r11-element Vector{Int64}:\r-2\r8\r-2\r-6\r-1\r6\r-1\r-2\r-1\r7\r0 多次元配列 julia\u0026gt; X = reshape(x, 3, :)\r3×4 Matrix{Int64}:\r3 7 6 2\r1 1 5 9\r9 0 3 9\rjulia\u0026gt; diff(X)\rERROR: UndefKeywordError: keyword argument dims not assigned\rStacktrace:\r[1] diff(a::Matrix{Int64})\r@ Base .\\multidimensional.jl:997\r[2] top-level scope\r@ c:\\Users\\rmsms\\OneDrive\\lab\\DataDrivenModel\\REPL.jl:7 例えば、上記のような多次元配列がある場合、ただdiff()を適用するとエラーになる。これは、配列を押す方向が与えられていないためであり、次のようにdimsを引数にしてどの次元で差分を計算するかを決める必要がある。1次元の場合と同じように、差分をとった方向で長さが1ずつ短くなることに注意。\njulia\u0026gt; diff(X, dims = 1)\r2×4 Matrix{Int64}:\r-2 -6 -1 7\r8 -1 -2 0\rjulia\u0026gt; diff(X, dims = 2)\r3×3 Matrix{Int64}:\r4 -1 -4\r0 4 4\r-9 3 6 全コード x = rand(0:9, 12)\rdiff(x)\rX = reshape(x, 3, :)\rdiff(X)\rdiff(X, dims = 1)\rdiff(X, dims = 2) 環境 OS: Windows julia: v1.8.5 https://docs.julialang.org/en/v1/base/arrays/#Base.diff\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2564,"permalink":"https://freshrimpsushi.github.io/jp/posts/2564/","tags":null,"title":"ジュリアで配列の差分を計算する方法"},{"categories":"줄리아","contents":"概要 実は、Juliaではネイティブに円形配列Circular Arrayをサポートしていないが、要素を円形にCircularlyシフトしてくれるcircshift()関数を提供していて、事実上それを使うことができる1。自分で書くのはそれほど難しくないが、知っていればわざわざ書かなくても良い。この関数はマットラボのcircshift()とほぼ同じ方法で使える。\nコード この関数は、配列を平行移動する方法のポストでも紹介された。\n基本的な使い方 julia\u0026gt; circshift(1:4, 1)\r4-element Vector{Int64}:\r4\r1\r2\r3\rjulia\u0026gt; circshift(1:4, -1)\r4-element Vector{Int64}:\r2\r3\r4\r1 circshift()は基本的に二番目の引数に整数を入れて、要素をシフトする。上の例では、正の整数で後ろ(下)にシフトし、負の整数で前(上)にシフトすることが確認できる。\n多次元配列 julia\u0026gt; ca = reshape(1:20, 5, :)\r5×4 reshape(::UnitRange{Int64}, 5, 4) with eltype Int64:\r1 6 11 16\r2 7 12 17\r3 8 13 18\r4 9 14 19\r5 10 15 20 上のような多次元配列が与えられた場合、次のように同じ次元のタプルを与えて、各次元をどれだけシフトするかを指定する。\njulia\u0026gt; circshift(ca, (0,1))\r5×4 Matrix{Int64}:\r16 1 6 11\r17 2 7 12\r18 3 8 13\r19 4 9 14\r20 5 10 15\rjulia\u0026gt; circshift(ca, (-1,0))\r5×4 Matrix{Int64}:\r2 7 12 17\r3 8 13 18\r4 9 14 19\r5 10 15 20\r1 6 11 16\rjulia\u0026gt; circshift(ca, (-1,1))\r5×4 Matrix{Int64}:\r17 2 7 12\r18 3 8 13\r19 4 9 14\r20 5 10 15\r16 1 6 11 全体のコード circshift(1:4, 1)\rcircshift(1:4, -1)\rca = reshape(1:20, 5, :)\rcircshift(ca, (0,1))\rcircshift(ca, (-1,0))\rcircshift(ca, (-1,1)) 環境 OS: Windows julia: v1.8.5 https://docs.julialang.org/en/v1/base/arrays/#Base.circshift\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2562,"permalink":"https://freshrimpsushi.github.io/jp/posts/2562/","tags":null,"title":"ジュリアで円形配列を使う方法"},{"categories":"줄리아","contents":"コード 1 長々と説明する必要はなく、文字通りマーカースタイルとラインスタイルが実際にどう見えるかを示す。\nlinesytle [:auto, :solid, :dash, :dot, :dashdot, :dashdotdot] の中から一つ選ぶ。\nshape [:none, :auto, :circle, :rect, :star5, :diamond, :hexagon, :cross, :xcross, :utriangle, :dtriangle, :rtriangle, :ltriangle, :pentagon, :heptagon, :octagon, :star4, :star6, :star7, :star8, :vline, :hline, :+, :x] の中から一つ選ぶ。\n全体のコード using Plots\rlines = [:auto, :solid, :dash, :dot, :dashdot, :dashdotdot]\rplt = plot(grid = :none, showaxis = :false, legend = :outerright)\rfor k in 1:6\rplot!(plt, [0, 1], [-k, -k], lw = 2, color = :black, linestyle = lines[k], label = string(lines[k]))\rend\rplt\rshapes = [:none, :auto, :circle, :rect, :star5, :diamond, :hexagon, :cross, :xcross, :utriangle, :dtriangle, :rtriangle, :ltriangle, :pentagon, :heptagon, :octagon, :star4, :star6, :star7, :star8, :vline, :hline, :+, :x]\rplot(grid = :none, showaxis = :false, xlims = (-0.1,4), size = (600, 600))\rscatter!((0:23) .% 4, -(0:23) .÷ 4, shape = shapes, color = :black, legend = :none, markersize = 10)\rannotate!(.15 .+ ((0:23) .% 4), -(0:23) .÷ 4, text.(shapes, :left)) 環境 OS: Windows julia: v1.8.5 Plots v1.38.5 https://docs.juliaplots.org/latest/generated/attributes_series/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2560,"permalink":"https://freshrimpsushi.github.io/jp/posts/2560/","tags":null,"title":"ジュリアでのマーカーとラインスタイルのリスト"},{"categories":"줄리아","contents":"コード Juliaの散布図に回帰直線を入れる方法は、オプションでsmooth = trueを使うことだ。\nusing Plots\rx = rand(100)\rscatter(x, 2x .+ 0.1randn(100), smooth = true)\rsavefig(\u0026#34;plot.svg\u0026#34;) 環境 OS: Windows julia: v1.8.3 Plots v1.38.5 ","id":2558,"permalink":"https://freshrimpsushi.github.io/jp/posts/2558/","tags":null,"title":"ジュリアプロットで回帰直線を描く方法"},{"categories":"줄리아","contents":"概要 Juliaで頻繁に使われるsplatの...の用途について、オプショナル引数を伝える方法を説明する。基本的に、どんなオプションにどんな引数を入れるかを事前に名前付きタプルの形で決めた後、そのタプルにsplatオペレーターを適用する方式で使う。\nコード 複数の関数に伝える args1 = (; dims = 1)\n上の名前付きタプルargs1はdimsというオプショナル引数がある全ての関数に共通して使うことができる。次の例でsum()とminimum()は全く異なる関数だが、共にdimsを持っているため、関数の種類に関係なく適用された。\njulia\u0026gt; sum(rand(100,100); args1...)\r1×100 Matrix{Float64}:\r47.0704 45.7637 44.4513 48.2325 50.5745 51.9176 … 49.9548 47.6825 50.7284 50.0861 50.0168 50.5116\rjulia\u0026gt; minimum(rand(100,100); args1...)\r1×100 Matrix{Float64}:\r0.00702003 0.0163299 0.00665818 0.0174564 0.00589048 … 0.002967 0.00460205 0.0116248 0.0114521 0.0698425 複数の引数を伝える args2 = (; dims = 2, rev = true)\n上の名前付きタプルargs2はdimsに加えてrevというオプショナル引数を含んでいる。次の例でsort()関数は入力データに関係なく二つのオプションをよく反映して計算結果を返した。\njulia\u0026gt; sort(rand(0:9, 3,3); args2...)\r3×3 Matrix{Int64}:\r9 4 4\r6 5 2\r8 0 0\rjulia\u0026gt; sort(rand(3,3); args2...)\r3×3 Matrix{Float64}:\r0.438682 0.211154 0.108741\r0.72113 0.445214 0.00910109\r0.971441 0.666732 0.0227372 全体コード args1 = (; dims = 1)\rsum(rand(100,100); args1...)\rminimum(rand(100,100); args1...)\rargs2 = (; dims = 2, rev = true)\rsort(rand(0:9, 3,3); args2...)\rsort(rand(3,3); args2...) 環境 OS: Windows julia: v1.8.3 ","id":2554,"permalink":"https://freshrimpsushi.github.io/jp/posts/2554/","tags":null,"title":"Juliaスプラットオペレーターを通じたオプション引数の渡し方のヒント"},{"categories":"줄리아","contents":"概要 Juliaで...はスプラット・オペレーターと呼ばれ、関数を使用したり、配列を定義する際に便利に使われる1。このオペレーターはJuliaに限定されているわけではないが、他の言語に比べて直感的に定義されており、学びやすく覚えやすいところが突出している。個人的な経験だと、...を使うようになるとJuliaプログラミングに関する何かしらの気づきを得る気がする。\nコード 関数入力 基本的に...は配列やジェネレータの後に付けて使い、そのまま前にあるコンテナ/イテレータの要素を全部展開して出す。\njulia\u0026gt; min([1,2,3,4,5,6,7,8,9,10])\rERROR: MethodError: no method matching min(::Vector{Int64})\rjulia\u0026gt; min(1,2,3,4,5,6,7,8,9,10)\r1 例えばJuliaのmin()関数はリデュースとして働くから、そのまま配列を渡すのではなく、複数の数を直接引数として渡さなければならない。もちろん、配列が大きくなるほど手で全部解いて書くのは難しくなり、配列の要素を展開して入れることができるように...を使うことができる。\njulia\u0026gt; min(1:10)\rERROR: MethodError: no method matching min(::UnitRange{Int64})\rjulia\u0026gt; min((1:10)...)\r1 もちろん、実際は配列に使えるminimum()関数があるから、わざわざこうする必要はない。\n配列定義 julia\u0026gt; [(1:10)]\r1-element Vector{UnitRange{Int64}}:\r1:10 上で定義された配列はユニットレンジの配列なので、直接要素にアクセスするのが少し面倒になる。時間があれば、ただ単に1から10までの数字を書き込んだらいいけど、スプラットオペレータを使えば、次のように簡単に定義できる。\njulia\u0026gt; [(1:10)...]\r10-element Vector{Int64}:\r1\r2\r3\r4\r5\r6\r7\r8\r9\r10 もちろん、collect()関数のような代替手段もあるが、知っておくときれいでセンスがある表現だという点が魅力的だ。ただし、速度の面ではあまり推奨されていないので、必要以上に...を使わないようにしよう。\njulia\u0026gt; [eachrow(rand(3,3))...]\r3-element Vector{SubArray{Float64, 1, Matrix{Float64}, Tuple{Int64, Base.Slice{Base.OneTo{Int64}}}, true}}:\r[0.6695368164913422, 0.69695509795356, 0.12084083239135612]\r[0.6833475867141307, 0.5368141950494666, 0.7877252857572066]\r[0.2810163716135018, 0.04317485597011517, 0.44214186775440534] ...が面白くなるのは、上のようにジェネレータを一度通るときだ。eachrow()は行列を行単位に切ったベクトルのジェネレータをリターンし、スプラットオペレータを通じて、その各ベクトルを配列表記[]に入れて、ベクトルのベクトルを作ったものだ。\n全コード min([1,2,3,4,5,6,7,8,9,10])\rmin(1,2,3,4,5,6,7,8,9,10)\rmin(1:10)\rmin((1:10)...)\r[(1:10)]\r[(1:10)...]\r[eachrow(rand(3,3))...] 環境 OS: Windows julia: v1.8.3 https://docs.julialang.org/en/v1/base/base/#\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2552,"permalink":"https://freshrimpsushi.github.io/jp/posts/2552/","tags":null,"title":"ジュリアのスプラットオペレータ"},{"categories":"줄리아","contents":"概要 他のプログラム言語がそうであるように、ジュリアでは英語をASCIIコードASCII Codeで書き、漢字、韓国語などをユニコードUnicodeで書く。問題は、他の言語たちと違って、この文字列たちを扱うことがかなり厄介であることだけど、これは性能上の理由から意図されたものであるため1、不便でも我慢して使うしかない。\nコード julia\u0026gt; str1 = \u0026#34;English\u0026#34;\r\u0026#34;English\u0026#34;\rjulia\u0026gt; str2 = \u0026#34;日本語\u0026#34;\r\u0026#34;日本語\u0026#34;\rjulia\u0026gt; str3 = \u0026#34;한국어\u0026#34;\r\u0026#34;한국어\u0026#34; 例えば、上のような文字列たちが与えられているとしよう。\njulia\u0026gt; str1[2:end]\r\u0026#34;nglish\u0026#34; str1の場合は何も特別なことがない英語の文字列で、ASCIIコードであるため、上のように普通の配列にアクセスするようにスライシングが可能である。\njulia\u0026gt; str2[2:end]\rERROR: StringIndexError: invalid index [2], valid nearby indices [1]=\u0026gt;\u0026#39;日\u0026#39;, [4]=\u0026gt;\u0026#39;本\u0026#39;\rStacktrace:\r[1] string_index_err(s::String, i::Int64)\r@ Base .\\strings\\string.jl:12\r[2] getindex(s::String, r::UnitRange{Int64})\r@ Base .\\strings\\string.jl:266\r[3] top-level scope\r@ c:\\Users\\rmsms\\OneDrive\\lab\\population_dynamics\\REPL.jl:6 しかし、str2は漢字があるためにユニコードで書かれており、上のようにインデックスエラーを発生させる。エラーメッセージを見ると、2番目の文字のインデックスは2ではなく4であることが推測でき、実際に4からインデキシングをすると、意図された通りにスライシングされる。\njulia\u0026gt; str2[4:end]\r\u0026#34;本語\u0026#34; これは以下のように韓国語にも同様に適用される。同じユニコードであるため、違う理由がない。\njulia\u0026gt; str3[4:end]\r\u0026#34;국어\u0026#34;\rjulia\u0026gt; str3[6]\rERROR: StringIndexError: invalid index [6], valid nearby indices [4]=\u0026gt;\u0026#39;국\u0026#39;, [7]=\u0026gt;\u0026#39;어\u0026#39;\rStacktrace:\r[1] string_index_err(s::String, i::Int64)\r@ Base .\\strings\\string.jl:12\r[2] getindex_continued(s::String, i::Int64, u::UInt32)\r@ Base .\\strings\\string.jl:237\r[3] getindex(s::String, i::Int64)\r@ Base .\\strings\\string.jl:230\r[4] top-level scope\r@ c:\\Users\\rmsms\\OneDrive\\lab\\population_dynamics\\REPL.jl:9\rjulia\u0026gt; str3[7]\r\u0026#39;어\u0026#39;: Unicode U+C5B4 (category Lo: Letter, other) トリック julia\u0026gt; String(collect(str3)[2:3])\r\u0026#34;국어\u0026#34; まあまあ楽に使う方法としては、上のようにcollect()で文字の配列に解いてスライシングし、また文字列にまとめる方法がある。\n環境 OS: Windows julia: v1.8.3 https://discourse.julialang.org/t/weird-string-slicing-in-korean/92252/2\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2550,"permalink":"https://freshrimpsushi.github.io/jp/posts/2550/","tags":null,"title":"ジュリアでユニコード文字列の一部だけをスライスする方法"},{"categories":"줄리아","contents":"概要 ジュリアのStatsPlotsパッケージでは、図を描く時に@dfマクロを通して、繰り返されるデータフレーム名を省略することができる1。マクロを使う文法は、データフレームXのa列を使う場合、@df X というようにどのデータフレームを使うか指定した後、直ぐに続くスコープでaをシンボルの:aとして引数に渡し、plot (:a)という風に書くことだ。コードで要約すると、@df X plot(:a)と書く。\nコード 以下は、アイリスデータのSepalLengthとSepalWidthで描いた散布図だ。\n次のコードscatter(iris.SepalLength, iris.SepalWidth)と@df iris scatter(:SepalLength, :SepalWidth)は同じである。\nusing RDatasets\riris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;)\rusing StatsPlots\rscatter(iris.SepalLength, iris.SepalWidth)\r@df iris scatter(:SepalLength, :SepalWidth) 環境 OS: Windows julia: v1.8.3 StatsPlots v0.15.4 https://github.com/JuliaPlots/StatsPlots.jl#original-author-thomas-breloff-tbreloff-maintained-by-the-juliaplots-members\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2548,"permalink":"https://freshrimpsushi.github.io/jp/posts/2548/","tags":null,"title":"Julia StatsPlotsでデータフレーム名を省略するマクロ@df"},{"categories":"줄리아","contents":"概要 他のファイルにある関数を使えるようにするために、ジュリアコード自体を実行するinclude()関数を紹介する。マットラボでは、同じディレクトリ内にあれば自動的に関数を見つけてくれるため、このプロセスを難しく考える人もいる。ちなみに、ちゃんとモジュール化してエクスポートする方法があるが1、難しくて複雑なので、機能が急に必要な初心者にはお勧めしない。パッケージを自分で作るか、プログラムの規模が扱えないほど大きくなった後にモジュール化を学んでも遅くはない。\nガイド 上記のように、foo/bar.jlファイルにあるbaz()関数をmain.jlから実行したいとしよう。スクリーンショットで確認できるように、モジュールのようなものを別に使用せず、ただ普通にジュリアコードを書けばいい。\nそして、include()でパスを指定して実行した結果は、次のようになる。\ninclude()の実行結果で23が表示された理由は、bar.jlファイルの最下部にy = 23という値の割り当てがあったからだ。見ての通り、関数だけでなく変数も移せるし、ファイル自体を実行する方式なので、データのロードやログ出力も全部できる。\n環境 OS: Windows julia: v1.8.3 https://docs.julialang.org/en/v1/manual/modules/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2544,"permalink":"https://freshrimpsushi.github.io/jp/posts/2544/","tags":null,"title":"ジュリアで他のファイルに定義された関数の使用方法"},{"categories":"줄리아","contents":"コード using Plots\rx, y = rand(100), rand(100) 上記のようなデータが与えられたとしよう。データが連続かカテゴリカルかによって、図の形や描く方法が異なる。\n連続型 scatter(marker_z=) z = x + y\rscatter(x, y, marker_z = z) カテゴリカル scatter(group=) 1 team = rand(\u0026#39;A\u0026#39;:\u0026#39;C\u0026#39;, 100)\rscatter(x, y, group = team) 環境 OS: Windows julia: v1.8.3 https://stackoverflow.com/a/60846501/12285249\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2537,"permalink":"https://freshrimpsushi.github.io/jp/posts/2537/","tags":null,"title":"ジュリア集合でマーカーに色をつける方法"},{"categories":"통계적분석","contents":"モデル オーディナリークリギング 空間データ分析で、ランダムフィールド $\\mathbf{Y} = \\left( Y \\left( s_{1} \\right) , \\cdots , Y \\left( s_{n} \\right) \\right)$ の平均 $\\mu \\in \\mathbb{R}$ と共分散行列 $\\Sigma \\in \\mathbb{R}^{n \\times n}$ が多変量正規分布に従う $\\varepsilon \\sim N_{n} \\left( \\mathbf{0} , \\Sigma \\right)$ ことについて、次のモデル $$ \\mathbf{Y} = \\mu \\mathbf{1} + \\varepsilon $$ を通じて、新しいサイトSite $s_{0}$ の $Y \\left( s_{0} \\right)$ を推定した値をオーディナリークリギング推定値Ordinary Kriging Estimateと呼ぶ。このようにモデルを立てて推定する行為自体をクリギングとも呼ぶ。\n$\\mathbf{1} = (1 , \\cdots , 1)$ は全ての成分が $1$ の1ベクトルだ。 $N_{n} \\left( \\mathbf{0} , \\Sigma \\right)$ は多変量正規分布を意味する。 説明 語源 クリギングKrigingは、ダニエル・G・クリージDaine G. Krigeという巨匠の名前がそのまま動詞化されたものである。通常の統計学で予測、予想、適合、推定などと使う表現はもちろん、「空白の空間」の値を埋めるという意味で補間技術のように説明する場合もかなりあるが、これら全ての説明を短くしてクリギングするという一般動詞になったと考えられる。\n狭義の応用数学、コンピュータアルゴリズム、機械学習技術などと区別されるクリギングの特徴は、とにかく統計学らしくその平均（点推定量）だけでなくその分散まで考慮することである。想像してみてほしいが、各地点で分散が高い場所のクリギング推定値は同様に分散が大きく、分散が低い場所同士の地点では分散が低いだろう。これはあるデータの観測所の位置を選定するのにも使われるが、例えば微粒子の濃度を測定するとしたら、微粒子をどのように測定するかが気になるのではなく、その測定がどれだけ正確か―つまり、測定値の分散が最も高い場所を選定するようなアプローチがある。\n依存性 $$ \\mathbf{Y} = \\mu \\mathbf{1} + \\varepsilon \\qquad , \\text{where } \\varepsilon \\sim N_{n} \\left( 0, \\Sigma \\right) $$ モデルの数式を見ると、回帰分析や時系列分析と異なり $\\varepsilon$ こそが我々の関心事である。$\\Sigma$ が対角行列、つまり観測値ごとの依存性がなければ、そもそも空間的な構造がないという意味であり、わざわざクリギングをする理由がない。実際の分析ではこの $\\Sigma$ はセミバリオグラムのモデルを通じて次のように決定される。 $$ \\Sigma = \\sigma^{2} H \\left( \\phi \\right) + \\tau^{2} I $$ ここで $\\tau^{2}$ はナゲット効果分散（理論と異なり実際のデータで距離に関係なく基本的に見られる共分散性）であり、$I$ は単位行列である。\n一般化 次のように他の独立変数に対して一般化モデルを使用するクリギングをユニバーサルクリギングと呼ぶ。 $$ \\mathbf{Y} = \\mathbf{X} \\beta + \\varepsilon $$\n公式 ランダムフィールド $\\left\\{ Y (s_{k}) \\right\\}_{k=1}^{n}$ が固有的定常空間過程であるとし、新たに予測したい地点を $s_{0}$ とする。バリオグラム $2 \\gamma$ に対して行列 $\\Gamma \\in \\mathbb{R}^{n \\times n}$ を $\\left( \\Gamma \\right)_{ij} := \\gamma \\left( s_{i} - s_{j} \\right)$ のように定義し、ベクトル $\\gamma_{0} \\in \\mathbb{R}^{n}$ を $\\left( \\gamma_{0} \\right)_{i} := \\left( \\gamma \\left( s_{0} - s_{i} \\right) \\right)$ のように定義する。あるベクトル $l = \\left( l_{1} , \\cdots , l_{n} \\right)$ に対して $Y \\left( s_{0} \\right)$ の最良線形不偏予測量BLUP, Best Linear Unbiased Predictorは $l$ と $\\mathbf{Y}$ の内積であり、 $$ l^{T} \\mathbf{Y} = \\begin{bmatrix} l_{1} \u0026amp; \\cdots \u0026amp; l_{n} \\end{bmatrix} \\begin{bmatrix} Y \\left( s_{1} \\right)\\\\ Y \\left( s_{2} \\right)\\\\ \\vdots\\\\ Y \\left( s_{n} \\right) \\end{bmatrix} = \\sum_{k=1}^{n} l_{k} Y \\left( s_{k} \\right) $$ ベクトル $l$ は具体的に次のように求められる。 $$ l = \\Gamma^{-1} \\left( \\gamma_{0} + {{ 1 - \\mathbf{1}^T \\Gamma^{-1} \\gamma_{0} } \\over { \\mathbf{1}^T \\Gamma^{-1} \\mathbf{1} }} \\mathbf{1} \\right) $$\n導出 1 具体的にクリギングがどのように行われるか、数式で調べるプロセスである。この公式の仮定にガウス過程という仮定まで追加されれば、オーディナリークリギングになる。\nパート1. 最適化問題\nある定数 $l_{1} , \\cdots , l_{n} , \\delta_{0} \\in \\mathbb{R}$ に対して新しい $Y \\left( s_{0} \\right)$ を既存データの線形結合 $$ \\hat{y} \\left( s_{0} \\right) = l_{1} y_{1} + \\cdots + l_{n} y_{n} + \\delta_{0} $$ で予測したいとする。これはつまり、目的関数 $$ E \\left[ Y \\left( s_{0} \\right) - \\left( \\sum_{k} l_{k} Y \\left( s_{k} \\right) + \\delta_{0} \\right) \\right]^{2} $$ を最小化する最適解 $l_{1} , \\cdots , l_{n} , \\delta_{0}$ を見つけることになる。\n固有的定常性の定義: ユークリッド空間の固定された部分集合 $D \\subset \\mathbb{R}^{r}$ で、確率変数 $Y(s) : \\Omega \\to \\mathbb{R}^{1}$ の集合である空間過程 $\\left\\{ Y(s) \\right\\}_{s \\in D}$ と方向ベクトル $\\mathbf{h} \\in \\mathbb{R}^{r}$ を考える。具体的に $n \\in \\mathbb{N}$ 個のサイトSiteを $\\left\\{ s_{1} , \\cdots , s_{n} \\right\\} \\subset D$ のように表し、$Y(s)$ はすべての $s \\in D$ に対して分散が存在すると仮定する。$\\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right]$ の平均が $0$ でありながら分散が唯一 $\\mathbf{h}$ にのみ依存する場合、$\\left\\{ Y \\left( s_{k} \\right) \\right\\}$ が固有的定常性Intrinsic Stationarityを持つと言われる。 $$ \\begin{align*} E \\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right] =\u0026amp; 0 \\\\ \\text{Var} \\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right] =\u0026amp; 2 \\gamma ( \\mathbf{h} ) \\end{align*} $$\nここで $\\left\\{ Y \\left( s_{k} \\right) \\right\\}_{k=1}^{n}$ が固有的定常性を持つ場合、$\\sum_{k} l_{k} = 1$ という制約条件を置くことにより、 $$ \\begin{align*} \u0026amp; E \\left[ Y \\left( s_{0} \\right) - \\left( \\sum_{k} l_{k} Y \\left( s_{k} \\right) \\right) \\right] \\\\ =\u0026amp; E \\left[ \\sum_{k} l_{k} Y \\left( s_{0} \\right) - \\left( \\sum_{k} l_{k} Y \\left( s_{k} \\right) \\right) \\right] \\\\ =\u0026amp; \\sum_{k} l_{k} E \\left[ Y \\left( s_{0} \\right) - Y \\left( s_{k} \\right) \\right] \\\\ =\u0026amp; 0 \\end{align*} $$ となるようにできる。これにより、我々が最小化するべき目的関数は $\\delta_{0}$ が外れた次の形になる。 $$ E \\left[ Y \\left( s_{0} \\right) - \\sum_{k} l_{k} Y \\left( s_{k} \\right) \\right]^{2} + \\delta_{0}^{2} $$ ここで $\\delta_{0}^{2}$ は予測と関係ない。実際にもモデルが $\\mathbf{Y} = \\mu \\mathbf{1} + \\varepsilon$ であれば、$\\delta_{0}$ は $\\mu$ に該当し、$\\delta_{0} = 0$ として $$ E \\left[ Y \\left( s_{0} \\right) - \\sum_{k} l_{k} Y \\left( s_{k} \\right) \\right]^{2} $$ としても問題ない。今、$a_{0} = 1$ とし、$a_{k} = - l_{k}$ とすると、 $$ E \\left[ Y \\left( s_{0} \\right) - \\sum_{k=1}^{n} l_{k} Y \\left( s_{k} \\right) \\right]^{2} = E \\left[ \\sum_{k=0}^{n} a_{k} Y \\left( s_{k} \\right) \\right]^{2} $$ となるため、我々は次の最適化問題をラグランジュ乗数法で解くことになる。 $$ \\begin{matrix} \\text{Minimize} \u0026amp; \\displaystyle E \\left[ \\sum_{k=0}^{n} a_{k} Y \\left( s_{k} \\right) \\right]^{2} \\\\ \\text{subject to} \u0026amp; \\displaystyle \\sum_{k=0}^{n} a_{k} = 0 \\end{matrix} $$\nパート2. セミバリオグラム $\\gamma$\n今、$E \\left[ \\sum_{k=0}^{n} a_{k} Y \\left( s_{k} \\right) \\right]^{2}$ を「我々がデータから計算できると仮定できる」セミバリオグラムに依存した形で表してみよう。\nセミバリオグラムの定義: ユークリッド空間の固定された部分集合 $D \\subset \\mathbb{R}^{r}$ で、確率変数 $Y(s) : \\Omega \\to \\mathbb{R}^{1}$ の集合である空間過程 $\\left\\{ Y(s) \\right\\}_{s \\in D}$ と方向ベクトル $\\mathbf{h} \\in \\mathbb{R}^{r}$ を考える。具体的に $n \\in \\mathbb{N}$ 個のサイトを $\\left\\{ s_{1} , \\cdots , s_{n} \\right\\} \\subset D$ のように表し、$Y(s)$ はすべての $s \\in D$ に対して分散が存在すると仮定する。次のように定義される $2 \\gamma ( \\mathbf{h} )$ をバリオグラムVariogramと呼ぶ。 $$ 2 \\gamma ( \\mathbf{h} ) := E \\left[ Y \\left( s + \\mathbf{h} \\right) - Y(s) \\right]^{2} $$ 特にバリオグラムの半分 $\\gamma ( \\mathbf{h} )$ をセミバリオグラムSemivariogramと呼ぶ。\n$\\gamma \\left( s_{i} - s_{j} \\right)$ を2つのサイト $s_{i}, s_{j}$ の間の方向ベクトルによるセミバリオグラムとしよう。$\\sum_{0=1}^{n} a_{k} = 0$ を満たす任意の集合 $\\left\\{ a_{k} : k = 1 , \\cdots , n \\right\\} \\subset \\mathbb{R}$ に対して次が成立する。 $$ \\sum_{i} \\sum_{j} a_{i} a_{j} \\gamma \\left( s_{i} - s_{j} \\right) = - E \\left[ \\sum_{i} a_{i} Y \\left( s_{i} \\right) \\right]^{2} $$ これは次の展開 $$ \\begin{align*} \u0026amp; \\sum_{i} \\sum_{j} a_{i} a_{j} \\gamma \\left( s_{i} - s_{j} \\right) \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\sum_{i} \\sum_{j} a_{i} a_{j} \\text{Var} \\left[ Y \\left( s_{i} \\right) - Y \\left( s_{j} \\right) \\right] \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\sum_{i} \\sum_{j} a_{i} a_{j} E \\left[ Y \\left( s_{i} \\right) - Y \\left( s_{j} \\right) \\right]^{2} \\\\ =\u0026amp; {{ 1 } \\over { 2 }} \\sum_{i} \\sum_{j} a_{i} a_{j} E \\left( \\left[ Y \\left( s_{i} \\right) \\right]^{2} - 2 Y \\left( s_{i} \\right) Y \\left( s_{j} \\right) + \\left[ Y \\left( s_{j} \\right) \\right]^{2} \\right) \\\\ =\u0026amp; - \\sum_{i} \\sum_{j} a_{i} a_{j} E \\left( Y \\left( s_{i} \\right) Y \\left( s_{j} \\right) \\right) \u0026amp; \\because \\text{cases of } i = j \\\\ =\u0026amp; - E \\sum_{i} \\sum_{j} a_{i} a_{j} Y \\left( s_{i} \\right) Y \\left( s_{j} \\right) \\\\ =\u0026amp; - E \\sum_{i} a_{i} Y \\left( s_{i} \\right) \\sum_{j} a_{j} Y \\left( s_{j} \\right) \\\\ =\u0026amp; - E \\left[ \\sum_{i} a_{i} Y \\left( s_{i} \\right) \\right]^{2} \\end{align*} $$ で確認できる。今、$\\gamma_{ij} = \\gamma \\left( s_{i} - s_{j} \\right)$ とし、$\\gamma_{0j} = \\gamma \\left( s_{0} - s_{j} \\right)$ とすれば、我々の目的関数は $$ \\begin{align*} \u0026amp; E \\left[ \\sum_{i} a_{i} Y \\left( s_{i} \\right) \\right]^{2} \\\\ =\u0026amp; - \\sum_{i} \\sum_{j} a_{i} a_{j} \\gamma \\left( s_{i} - s_{j} \\right) \\\\ =\u0026amp; - \\sum_{i} \\sum_{j} l_{i} l_{j} \\gamma_{ij} + 2 \\sum_{i} l_{i} \\gamma_{0i} \\end{align*} $$ のように表され、ラグランジュ乗数法によって制約条件 $\\sum_{i} l_{i} = 1$ にラグランジュ乗数Lagrange Multiplierを掛けて引くことで、 $$ \\ - \\sum_{i} \\sum_{j} l_{i} l_{j} \\gamma_{ij} + 2 \\sum_{i} l_{i} \\gamma_{0i} - \\lambda \\sum_{i} l_{i} $$ を得る。したがって、$l_{i}$ に対して偏微分して $$ \\ - \\sum_{j} l_{j} \\gamma_{ij} + \\gamma_{0i} - \\lambda = 0 $$ となる時、目的関数を最小化できることがわかる。\nパート3. 最適解\nここで、具体的な最適解の公式を導出する。行列 $\\Gamma \\in \\mathbb{R}^{n \\times n}$ の $(i,j)$ 成分を ▷\neq74◁ とし、つまり $\\left( \\Gamma \\right)_{ij} := \\gamma_{ij}$ とし、ベクトル $\\gamma_{0} \\in \\mathbb{R}^{n}$ を $\\left( \\gamma_{0} \\right)_{i} := \\gamma_{0i}$ のように定義しよう。係数のベクトルも同様に $l := \\left( l_{1} , \\cdots , l_{n} \\right) \\in \\mathbb{R}^{n}$ とすると、パート2で得た式は次のような行列/ベクトル形式で表すことができる。 $$ \\begin{align*} \u0026amp; - \\sum_{j} l_{j} \\gamma_{ij} + \\gamma_{0i} - \\lambda = 0 \\\\ \\implies \u0026amp; - \\Gamma l + \\gamma_{0} - \\lambda \\mathbf{1} = 0 \\\\ \\implies \u0026amp; \\Gamma l + \\lambda \\mathbf{1} = \\gamma_{0} \\end{align*} $$ 一方、制約条件で $$ \\sum_{i} l_{i} = 1 \\iff \\begin{bmatrix} 1 \u0026amp; \\cdots \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} l_{1} \\\\ \\vdots \\\\ l_{n} \\end{bmatrix} = 1 \\iff \\mathbf{1}^{T} l = 1 $$ となるので、$\\mathbf{1}^{T} l = 1$ も得られる。ここで、$\\mathbf{x}^{T}$ は $\\mathbf{x}$ の転置を表す。まず単独で $\\lambda$ は $$ \\begin{align*} 1 =\u0026amp; \\mathbf{1}^T l \\\\ =\u0026amp; \\mathbf{1}^T \\Gamma^{-1} \\Gamma l \\\\ =\u0026amp; \\mathbf{1}^T \\Gamma^{-1} \\left( \\gamma_{0} - \\lambda \\mathbf{1} \\right) \\\\ =\u0026amp; \\mathbf{1}^T \\Gamma^{-1} \\gamma_{0} - \\lambda \\mathbf{1}^T \\Gamma^{-1} \\mathbf{1} \\end{align*} $$ となり、整理すると $$ \\ - \\lambda = {{ 1 - \\mathbf{1}^T \\Gamma^{-1} \\gamma_{0} } \\over { \\mathbf{1}^T \\Gamma^{-1} \\mathbf{1} }} $$ のように表すことができる。今、$l$ はほぼ求めたも同然である。 $$ \\begin{align*} \u0026amp; \\Gamma l + \\lambda \\mathbf{1} = \\gamma_{0} \\\\ \\implies \u0026amp; \\Gamma l = \\gamma_{0} - \\lambda \\mathbf{1} \\\\ \\implies \u0026amp; \\Gamma l = \\gamma_{0} + {{ 1 - \\mathbf{1}^T \\Gamma^{-1} \\gamma_{0} } \\over { \\mathbf{1}^T \\Gamma^{-1} \\mathbf{1} }} \\mathbf{1} \\\\ \\implies \u0026amp; l = \\Gamma^{-1} \\left( \\gamma_{0} + {{ 1 - \\mathbf{1}^T \\Gamma^{-1} \\gamma_{0} } \\over { \\mathbf{1}^T \\Gamma^{-1} \\mathbf{1} }} \\mathbf{1} \\right) \\end{align*} $$\n■\nBanerjee. (2015). Hierarchical Modeling and Analysis for Spatial Data(2nd Edition): p25, 40~41.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":2521,"permalink":"https://freshrimpsushi.github.io/jp/posts/2521/","tags":null,"title":"空間データ分析におけるクリギングとは？"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","id":null,"permalink":"https://freshrimpsushi.github.io/jp/search/","tags":null,"title":"Search Results"}]