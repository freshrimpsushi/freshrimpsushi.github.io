<!doctype html><html class=blog lang=jp><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><link rel=preload href=https://freshrimpsushi.github.io/jp/css/style.css rel=preload as=style onload='this.rel="stylesheet"'><link rel=preload href=https://freshrimpsushi.github.io/jp/css/comment.css rel=preload as=style onload='this.rel="stylesheet"'><link rel=icon href=https://freshrimpsushi.github.io/ko/logo/favicon.ico><meta name=msapplication-TileColor content="#FFFFFF"><meta name=msapplication-TileImage content="logo/basic.png"><meta name=NaverBot content="All"><meta name=NaverBot content="index,follow"><meta name=Yeti content="All"><meta name=Yeti content="index,follow"><meta name=google-site-verification content="KYAokS7-6C5YuXOjatJQsiK1T0O8x4YncYFIF4tneYI"><meta name=naver-site-verification content="e5651d6f97899061897203413efc84994f04bbba"><link rel=alternate type=application/rss+xml title=生エビ寿司屋 href=https://freshrimpsushi.github.io/jp/index.xml><title>機械学習における強化学習とは？</title></head><meta name=title content="機械学習における強化学習とは？"><meta name=description content="국내 최대의 수학, 물리학, 통계학 블로그"><meta property="og:title" content="機械学習における強化学習とは？"><meta property="og:description" content><meta property="og:image" content="https://freshrimpsushi.github.io/jp/logo/basic.png/"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"j_","name":"機械学習における強化学習とは？","headline":"機械学習における強化学習とは？","alternativeHeadline":"","description":"定義 強化学習とは、エージェントが環境と相互作用して累積報酬を最大化するポリシーを見つけることができるようにすることである。 説明1 強化学習を構","inLanguage":"jp","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/freshrimpsushi.github.io\/jp\/posts\/3029\/"},"author":{"@type":"Person","name":"전기현","url":"https://math.stackexchange.com/users/459895/ryu-dae-sick"},"creator":{"@type":"Person","name":"전기현"},"accountablePerson":{"@type":"Person","name":"전기현"},"copyrightHolder":"生エビ寿司屋","copyrightYear":"2021","dateCreated":"2021-02-04T00:00:00.00Z","datePublished":"2021-02-04T00:00:00.00Z","dateModified":"2021-02-04T00:00:00.00Z","publisher":{"@type":"Organization","name":"生エビ寿司屋","url":"https://freshrimpsushi.github.io/jp/","logo":{"@type":"ImageObject","url":"https:\/\/freshrimpsushi.github.io\/jp\/logo\/basic.png","width":"32","height":"32"}},"image":"https://freshrimpsushi.github.io/jp/logo/basic.png","url":"https:\/\/freshrimpsushi.github.io\/jp\/posts\/3029\/","wordCount":"4291","genre":[],"keywords":[]}</script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/mhchem.min.js integrity=sha384-F2ptQFZqNJuqfGGl28mIXyQ5kXH48spn7rcoS0Y9psqIKAcZPLd1NzwFlm/bl1mH crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>function renderKaTex(e){renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],trust:!0,trust:e=>["\\htmlId","\\href","\\includegraphics"].includes(e.command),macros:{"\\eqref":"(\\text{#1})","\\ref":"\\href{###1}{\\text{#1}}","\\label":"\\htmlId{#1}{}","\\sech":"\\operatorname{sech}","\\csch":"\\operatorname{csch}","\\sgn":"\\operatorname{sgn}","\\sign":"\\operatorname{sign}","\\sinc":"\\operatorname{sinc}","\\diag":"\\operatorname{diag}","\\diam":"\\operatorname{diam}","\\trace":"\\operatorname{trace}","\\Tr":"\\operatorname{Tr}","\\tr":"\\operatorname{tr}","\\re":"\\operatorname{Re}","\\im":"\\operatorname{Im}","\\Var":"\\operatorname{Var}","\\Poi":"\\operatorname{Poi}","\\Cov":"\\operatorname{Cov}","\\span":"\\operatorname{span}","\\supp":"\\operatorname{supp}","\\rank":"\\operatorname{rank}","\\nullity":"\\operatorname{nullity}","\\ad":"\\operatorname{ad}","\\Ric":"\\operatorname{Ric}","\\i":"\\mathrm{i}","\\d":"\\mathrm{d}","\\cR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2202/2615581243_1645770096.5863.png}","\\acR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.4371.png}","\\bcR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2202/2615581243_1645770096.5899.png}","\\abcR":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.0596.png}","\\crH":"\\includegraphics[height=0.8em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680622958.7632.png}","\\smallcrH":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680622958.7632.png}","\\acrH":"\\includegraphics[height=0.8em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.8386.png}","\\smallacrH":"\\includegraphics[height=0.6em]{https://freshrimpsushi.com/community/data/editor/2304/2175452104_1680634690.8386.png}"},throwOnError:!1})}</script><body class=main><header><a href=https://freshrimpsushi.github.io/jp/ rel=home><p style=text-align:center;font-size:1rem;color:#000><img src=https://freshrimpsushi.github.io/jp/logo/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D.png style=height:80px alt=logo></p></a></header><form method=get action=/jp/search style=border:1px;text-align:center><div class=field><input type=text id=searchtext placeholder=🔍︎ class=input_text name=s style=background-color:#eee;text-align:center;width:200px;font-size:1.5rem;border:1px;border-radius:5px;padding-top:5px;padding-bottom:5px;margin-bottom:1.5rem></div></form><aside style=text-align:center;margin-bottom:1rem><a href=https://freshrimpsushi.github.io/ko//posts/3029/>한국어</a> |
<a href=https://freshrimpsushi.github.io/en//posts/3029/>English</a> |
<a href=https://freshrimpsushi.github.io/jp//posts/3029/>日本語</a></aside><div class=wrapper><div class=content><div class=content-box><title>機械学習における強化学習とは？</title>
<a href=https://freshrimpsushi.github.io/jp/categories/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/ style=background-color:rgba(0,0,0,.8);color:orange;border-radius:10px;padding:5px>📂機械学習</a><h1>機械学習における強化学習とは？</h1><aside><div class=innerheader><div class=innertoc><b>目次</b><nav id=TableOfContents><ul><li><a href=#定義>定義</a></li><li><a href=#説明1>説明</a></li><li><a href=#強化学習の問題グリッドモデル>強化学習の問題：グリッドモデル</a><ul><li><a href=#エージェント>エージェント</a></li><li><a href=#状態>状態</a></li><li><a href=#行動>行動</a></li><li><a href=#方針>方針</a></li><li><a href=#報酬>報酬</a></li><li><a href=#環境>環境</a></li><li><a href=#エピソード>エピソード</a></li></ul></li></ul></nav></div></div></aside><h2 id=定義>定義</h2><p>強化学習とは、<strong>エージェント</strong>が<strong>環境</strong>と相互作用して<strong>累積報酬</strong>を最大化する<strong>ポリシー</strong>を見つけることができるようにすることである。</p><h2 id=説明1>説明<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></h2><p>強化学習を構成する要素は次のとおりである。</p><ul><li><strong>エージェント<sup>agent</sup></strong>: 与えられた状態において、ポリシーに従って行動を決定する。</li><li><strong>ステート<sup>state, 状態</sup></strong>: エージェントが置かれている状況を指す。</li><li><strong>アクション<sup>action, 行動</sup></strong>: エージェントが与えられた状態で選ぶことができる選択肢を指す。</li><li><strong>ポリシー<sup>policy, 方針</sup></strong>: エージェントが与えられた状態で行動を決定する戦略を指す。</li><li><strong>リワード<sup>reward, 報酬</sup></strong>: エージェントが与えられた状態で選んだ行動によって得られる点数を指す。エージェントが達成すべき目標と見なすことができる。</li><li><strong>環境<sup>environment</sup></strong>: エージェントが与えられた状態でどのような行動を決定すれば、MDPに従って次の状態とそれに伴う報酬を決定するか。</li><li><strong>エピソード<sup>episode</sup></strong>: エージェントと環境の相互作用が始まった時から終わるまでを指す。</li></ul><p>これをさまざまな状況に例えると次のようになる。</p><table><thead><tr><th style=text-align:center>強化学習</th><th style=text-align:center>試験勉強</th><th style=text-align:center>囲碁</th></tr></thead><tbody><tr><td style=text-align:center>エージェント</td><td style=text-align:center>学生</td><td style=text-align:center>囲碁の棋士</td></tr><tr><td style=text-align:center>ステート</td><td style=text-align:center>試験まで残り日数</td><td style=text-align:center>碁盤</td></tr><tr><td style=text-align:center>アクション</td><td style=text-align:center>勉強、飲酒、ゲームなど</td><td style=text-align:center>着手</td></tr><tr><td style=text-align:center>ポリシー</td><td style=text-align:center>日付別勉強計画</td><td style=text-align:center>戦略</td></tr><tr><td style=text-align:center>リワード</td><td style=text-align:center>試験点数</td><td style=text-align:center>勝敗</td></tr><tr><td style=text-align:center>エピソード</td><td style=text-align:center>試験期間</td><td style=text-align:center>一局</td></tr></tbody></table><h2 id=強化学習の問題グリッドモデル>強化学習の問題：グリッドモデル</h2><p>強化学習を説明するための代表的な例としてグリッドワールド<sup>grid world</sup>がある。これから次のグリッドモデルを例に各要素を具体的に説明する。一度に上下左右の4方向のうち一つに一マスずつ動けるロボットが下記のような$4 \times 4$のグリッドで動く場合を考えてみよう。スタート地点は$\boxed{\ 2\ }$から$\boxed{15}$まで任意に決められ、ロボットが$\fcolorbox{black}{darkgray}{\ 1\ }$または$\fcolorbox{black}{darkgray}{16}$まで最短距離で行くことが目標とする。</p><h3 id=エージェント>エージェント</h3><p>強化学習におけるエージェントは学習する主体として説明されるが、実際には存在しない。後述する他の概念が確率変数などで定義されるのに対し、エージェントには明確な数学的定義がない。したがって、強化学習に関する理論的な勉強はエージェントという対象がなくても可能であり、実際にそうである。強化学習理論において本質的にエージェントを意味するのはポリシーである。しかし直感的には、学習する対象があると考える方が便利なため、「エージェントが行動する」「エージェントの状態が変わった」といった表現を用いる。エージェントは単にコンピュータシミュレーション（特にゲーム）においてキャラクターのように<strong>学習しているように見えるもの</strong>に過ぎない。たとえば、グリッドモデルではエージェントが下の右側の図のように移動するのは、単純に状態の列挙で表すこともできる。
$$
\boxed{\ 3\ } \to \boxed{\ 2\ } \to \fcolorbox{black}{darkgray}{\ 1\ }
$$
$3, 2, 1$を順番に<code>print</code>するだけでよい。強化学習の最終的に私たちが得たいのは本質的にポリシーであるため、エージェントというものを定義しなくても学習することができる。一言で言えば、エージェントは<b>ポリシーの視覚化（実現化）</b>であると言える。</p><p>もちろん、上記の話は理論やコンピュータシミュレーションでの話であり、自動運転のような実際の応用では、ポリシーに従って実際に動くドローンや自動車が必要である。この場合、ドローンや自動車などのロボットや機械がエージェントとなり、それがなければポリシーの学習は不可能である。</p><h3 id=状態>状態</h3><p><img src=1.PNG#center alt=1.PNG></p><p><strong>状態</strong><sup>state</sup>は<a href=../1433>確率変数</a>であり、stateの頭文字をとって$S$と表記する。エピソードは時間に沿って順次進行するため、インデックスとして$t$を使用する。したがって、タイムステップが$t$のときのステート関数を$S_{t}$と表記する。初期ステートは通常$t=0$で表される。まとめると、$S_{t}$は時間が$t$のとき、各グリッドに対して次のような関数値を与える関数である。</p><p>$$
S_{t} \left( \boxed{ N } \right) = n,\quad 1\le n \le 16
$$</p><p>このとき、可能なすべての状態値（状態関数の関数値）の集合を$\mathcal{S}\subset \mathbb{R}$と表記し、その要素を$s$と表記する。</p><p>$$
\mathcal{S} = \left\{ s_{1}, s_{2},\dots \right\}
$$</p><p>それでは上記の格子モデルに対する状態関数は次のようになります。</p><p>$$
S_{t} : \left\{ \fcolorbox{black}{darkgray}{\ 1\ } , \boxed{\ 2\ }, \dots, \boxed{15}, \fcolorbox{black}{darkgray}{16} \right\} \to \mathcal{S} \\
S_{t} \left( \boxed{\ n\ } \right) = s_{n} = n,\quad 1\le n \le 16
$$</p><p>それでは時間が$t$のときの状態値が$s_{6}$から次のタイムステップで状態値が$s_{10}$に変わる<a href=../1431>確率</a>は次のようになります。</p><p>$$
P \left( S_{t+1} = s_{10} | S_{t} = s_{6} \right)
$$</p><p>到達した瞬間にエピソードが終了する状態を<strong>ターミナルステート</strong><sup>terminal state</sup>と呼びます。上記の格子モデルではターミナルステートは$\fcolorbox{black}{darkgray}{1}, \fcolorbox{black}{darkgray}{16}$です。</p><h3 id=行動>行動</h3><p><strong>行動</strong><sup>action</sup>とはエージェントが現在の状態で取ることができる選択肢のことであり、これもまた確率変数です。actionの頭文字を取って$A_{t}$と表記します。上記の格子モデルの例では、$\boxed{2}$ ~ $\boxed{15}$の各々で上下左右を選択することができます。可能な全ての行動値（行動関数の関数値）の集合を$\mathcal{A}\subset \mathbb{R}$と表記し、その要素を$a$と表記します。</p><p>$$
\mathcal{A} = \left\{ a_{1}, a_{2}, \dots \right\}
$$</p><p>それではタイムステップ$t$での行動関数は次のようになります。</p><p>$$
A_{t} : \left\{ \uparrow, \rightarrow, \downarrow, \leftarrow \right\} \to \mathcal{A} \\
\begin{cases}
A_{t}(\uparrow) = a_{1} \\
A_{t}(\rightarrow) = a_{2} \\
A_{t}(\downarrow) = a_{3} \\
A_{t}(\leftarrow) = a_{4}
\end{cases}
$$</p><p>エージェントは与えられた状態で確率に従って行動を決定します。例えばタイムステップが$t$のときの状態値が$s_{6}$で行動$a_{1}$を選択した確率は次のようになります。</p><p>$$
P(A_{t} = a_{1} | S_{t} = s_{6})
$$</p><h3 id=方針>方針</h3><p><strong>方針</strong><sup>policy</sup>とは状態$s$で行動$a$を決定する確率を全ての$s$と$a$に対して明記したものを言い、$\pi$で表記します。ゲームや戦争に例えると戦略です。格子モデルの例で行動を決定する確率が$\dfrac{1}{4}$で全て同じだとすると、方針$\pi$は次のようになります。</p><p>$$
\pi
\begin{cases}
P(a_{1} | s_{2}) = \dfrac{1}{4} \\
P(a_{2} | s_{2}) = \dfrac{1}{4} \\
P(a_{3} | s_{2}) = \dfrac{1}{4} \\
\vdots \\
P(a_{2} | s_{15}) = \dfrac{1}{4} \\
P(a_{3} | s_{15}) = \dfrac{1}{4} \\
P(a_{4} | s_{15}) = \dfrac{1}{4}
\end{cases} \quad \text{or} \quad
\pi : \mathcal{S} \times \mathcal{A} \to [0,1]
$$</p><p>もちろんこれは最適化された方針ではありません。簡単に$\boxed{2}$の場合だけ考えても、上に行くと格子の外に出てしまうため、上に行く確率自体が全く無い方がより良い方針です。したがって、下の図で$\pi_{1}$よりも$\pi_{2}$がより良い方針だと言えます。</p><p><img src=2.PNG#center alt=2.PNG></p><p>強化学習アルゴリズムの目標は最適な方針を見つけることです。では、最適な方針をどのように見つけるかというと、方針の良さを評価する<strong>価値関数</strong><sup>value function</sup>を通じて見つけることができます。</p><h3 id=報酬>報酬</h3><p><strong>報酬</strong><sup>reward</sup>とは、与えられた状態でエージェントが選択した行動に対して実数をマッピングする関数であり、rewardの頭文字を取って$R_{t}$と表記します。全ての報酬値（報酬関数の関数値）の集合を$\mathcal{R} \subset \mathbb{R}$と表記し、その要素を$r$と表記します。</p><p>$$
\mathcal{R} = \left\{ r_{1}, r_{2}, \dots \right\} \\
R_{t} = \mathcal{S} \times \mathcal{A} \to \mathcal{R}
$$</p><p>報酬は一回のタイムステップごとに一回ずつ受け取り、一回のエピソードで受け取った総報酬、つまり蓄積された報酬が最も大きくなるような方針を見つけることが強化学習の究極的な目標です。</p><p>では、なぜ各タイムステップの報酬よりも蓄積された報酬が大きくなるようにするのか疑問に思うかもしれません。これは試験勉強に例えると簡単に理解できます。試験期間中に毎晩勉強する代わりにお酒を飲んだり遊んだりゲームをした場合、当面は勉強するよりも楽しいでしょう。しかし、蓄積された報酬、つまり試験の成績は散々なものになります。したがって、今は勉強することが疲れて大変だとしても、将来の大きな報酬のために勉強する方が良いと判断し、試験勉強をするわけです。</p><p>報酬は人が設定するハイパーパラメータです。したがって、エージェントが行うべき仕事に応じて適切に設定する必要があります。例えば、格子モデルの例で格子が迷路であり、エージェントが迷路を脱出するロボットである場合、一マス移動するごとに$-1$の報酬、ターミナルステートに到達した場合は$+10$の報酬を与えるなどの設定ができます。格子が公園であり、エージェントがペットの散歩をするロボットである場合、一マス移動するごとに$0$の報酬、ターミナルステートに到達した場合は$+10$の報酬を与えるなどの設定ができます。</p><h3 id=環境>環境</h3><p><strong>環境</strong><sup>environment</sup>とはエージェントが与えられた状態で選択した行動に応じて次の状態と報酬を決定する関数、すなわち$f : (s,a) \mapsto (s^{\prime},r)$です。したがって、常に現実にぴったりと当てはまる比喩を見つけるのは難しいです。</p><p>タイムステップが$t$のときの状態を$s_{t}$、$s_{t}$で選択した行動を$a_{t}$とします。これにより、環境が決定した次の状態を$s_{t+1}$、報酬を$r_{t+1}$とすると次のように表されます。</p><p>$$
f(s_{t}, a_{t}) = (s_{t+1}, r_{t+1})
$$</p><p>格子モデルの例について具体的に説明すると、エージェントが$\boxed{7}$で$\uparrow$を選択し、環境が次の状態$\boxed{3}$と報酬$-1$を決定した場合は、次のような数式で</p><p>表されます。</p><p>$$
f(s_{7}, a_{1}) = (s_{3}, -1)
$$</p><p>エージェントが行動を決定する戦略を<strong>方針</strong>と呼ぶならば、環境が次の状態と報酬を決定することを<strong>MDP</strong><sup>markov decision process, マルコフ決定プロセス</sup>と言います。エージェントと環境の相互作用を図で表すと次のようになります。</p><p><img src=3.PNG#center alt=3.PNG></p><h3 id=エピソード>エピソード</h3><p>エージェントと環境が相互作用しながら決定された状態、行動、報酬の数列を<strong>経路</strong><sup>trajectory, 軌跡</sup>または<strong>履歴</strong><sup>history</sup>と言います。経路が有限の場合を<strong>episode task</strong>と言います。上で例に挙げた試験期間、囲碁、格子モデルもこれに該当します。</p><p>$$
s_{0}, a_{0}, s_{1}, r_{1}, a_{1}, s_{2}, r_{2}, \dots, a_{T-1}, s_{T}, r_{T} \\
\text{or} \\
(s_{0},) \overset{a_{0}}{\to} (s_{1}, r_{1}) \overset{a_{1}}{\to} (s_{2}, r_{2}) \overset{a_{2}}{\to} \cdots \overset{a_{T-1}}{\to} (s_{T}, r_{T})
$$</p><p>経路が無限の場合を<strong>continuing task</strong>と言います。ただし、非常に長い時間にわたって続くエピソードは無限の場合とみなされることもあります。</p><p>$$
s_{0}, a_{0}, s_{1}, r_{1}, a_{1}, s_{2}, r_{2}, \dots, a_{t-1}, s_{t}, r_{t}, a_{t}, s_{t+1}, r_{t+1},\dots \\
\text{or} \\
(s_{0},) \overset{a_{0}}{\to} (s_{1}, r_{1}) \overset{a_{1}}{\to} (s_{2}, r_{2}) \overset{a_{2}}{\to} \cdots \overset{a_{t-1}}{\to} (s_{t}, r_{t}) \overset{a_{t}}{\to} (s_{t+1}, r_{t+1}) \overset{a_{t+1}}{\to} \cdots
$$</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>オ・イルソク, 機械学習(MACHINE LEARNING). 2017, p466-480&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div><aside style=text-align:right>2021-02-04&emsp;
전기현&emsp;
<a href=../1377>🎲 3029</a></aside><script>document.addEventListener("DOMContentLoaded",()=>{const s=document.querySelector(".content-box");s&&renderKaTex(s);const o=document.querySelector(".tex");o&&renderKaTex(o);var n,r,i="機械学習における強化学習とは？",c="https://freshrimpsushi.github.io/jp/posts/3029/",t=new Array,a={title:i,link:c},e=JSON.parse(localStorage.getItem("latelyViewPostList"));if(e==""||e==null||e==null||e==0||e==NaN)t.push(a),localStorage.setItem("latelyViewPostList",JSON.stringify(t));else{for(r=e.length,n=0;n<r;n++)if(i==e[n].title){e.splice(n,1);break}t=e,t.push(a),t.length>5&&t.shift(),localStorage.setItem("latelyViewPostList",JSON.stringify(t))}})</script></div><aside><h2>コメント</h2><div class=area-reply><div class=list-reply></div></div><div class=write-box><div class="write-author-info box-account"><input type=hidden name=cmt_idx>
<input class=ai-author type=text placeholder=Name>
<input class=ai-password type=password maxlength=20 placeholder=Password></div><div class=write-content><textarea class=ai-content value placeholder=日本語でも構いませんが、できれば英語でお願いします style=ime-mode:active></textarea></div><button class=write-button onclick=write_comment() aria-label=send><i class='fa-solid fa-paper-plane'></i></button><aside class=tex>コメントにも $\TeX$ が適用されます。</aside></div></aside><script>let commentRows=[];const listReply=document.querySelector(".list-reply");document.addEventListener("DOMContentLoaded",()=>{get_all_comment()});function get_all_comment(){fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(e=>{const t="https://freshrimpsushi.com/blog/ajax/comment.php?action=getAllComment";return postFetch(t,{board_idx:"3029",user_ip:e})}).then(e=>e.json()).then(e=>{e.ok&&(commentRows=e.rows,render_comment(commentRows))}).catch(e=>console.error(e))}function render_comment(e){const n=["류대식","전기현","ㅇㅇ","질문"],s=["류대식","전기현"];render_blank();let t="";e.map(e=>{t+=`<div id="comment${e.cmt_idx}" class="parents">`,t+=`<div class="content-info">`;let o="";e.cmt_cnt>125?o="🥇":e.cmt_cnt>25?o="🥈":e.cmt_cnt>5&&(o="🥉"),n.includes(e.author)&&(o="");let i="";e.ip_address==null?i="(-)":(ipParts=e.ip_address.split("."),i=`(${ipParts[0]}.${ipParts[1]})`),s.includes(e.author)&&(i=""),t+=`<div class="list-author">${o} ${e.author} ${i}</div>`,t+=`<sup class="list-date">${e.datetime.slice(2,-3)}</sup>`,t+=`<div class="list-button">`,t+=`<div class="list-update-button" onclick="update_comment('${e.cmt_idx}', 'parents');"><i class="fa-solid fa-pen-to-square"></i></div>`,t+=`<div class="list-delete-button" onclick="delete_comment('${e.cmt_idx}', 'parents');"><i class="fa-solid fa-trash"></i></div>`,t+=`</div>`,t+=`</div>`,e.approve||(t+=`<div style="text-align: right;"><i class="fa-solid fa-hourglass"></i> 管理者の承認を待っています</div>`);let a=e.content;a=a.replace(/\n/g,"<br />"),t+=`<div class="content-text">${a} <span class="re-comment-button" onclick="re_comment('${e.cmt_idx}');"><i class="fa-solid fa-reply" style="cursor: pointer;"></i></span></div>`,t+=`<div class="re-comment"></div>`,t+=`</div>`,e.child.map(o=>{let c="child";e.ip_address&&o.ip_address===e.ip_address&&(c="parentself"),t+=`<div id="comment${o.cmt_idx}" class="${c}">`,t+=`<div class="content-info">`;let i="";o.cmt_cnt>5?i="🥉":o.cmt_cnt>25?i="🥈":o.cmt_cnt>125&&(i="🥇"),n.includes(o.author)&&(i="");let a="";o.ip_address==null?a="(-)":a=`(${o.ip_address})`,s.includes(o.author)&&(a=""),t+=`<div class="list-author">${i} ${o.author} ${a}</div>`,t+=`<sup class="list-date">${o.datetime.slice(2,-3)}</sup>`,t+=`<div class="list-button">`,t+=`<div class="list-update-button" onclick="update_comment('${o.cmt_idx}', 'child');"><i class="fa-solid fa-pen-to-square"></i></div>`,t+=`<div class="list-delete-button" onclick="delete_comment('${o.cmt_idx}', 'child');"><i class="fa-solid fa-trash"></i></div>`,t+=`</div>`,t+=`</div>`,o.approve||(t+=`<div style="text-align: right;"><i class="fa-solid fa-hourglass"></i> 管理者の承認を待っています</div>`);let r=o.content;r=r.replace(/\n/g,"<br />"),t+=`<div class="content-text">${r} <span class="re-comment-button" onclick="re_comment('${o.cmt_idx}', '${e.cmt_idx}');"><i class="fa-solid fa-reply" style="cursor: pointer;"></i></span></div>`,t+=`<div class="re-comment"></div>`,t+=`</div>`})}),listReply.innerHTML=t,renderKaTex(listReply),location.href.indexOf("#")!=-1&&(location.href=location.href.substr(location.href.indexOf("#")))}function render_blank(){listReply.innerHTML=""}function write_comment(){const e=document.querySelector(".ai-author"),t=document.querySelector(".ai-password"),n=document.querySelector(".ai-content"),s=e.value,o=t.value,i=n.value;if(s===""||o===""||i===""){alert("빈칸을 채워주세요");return}const a="3029",r="",c="機械学習における強化学習とは？";fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(e=>{const t="https://freshrimpsushi.com/blog/ajax/comment.php?action=writeComment";return postFetch(t,{board_idx:a,board_slug:r,board_title:c,author:s.replace(/\+/g,"%2B"),password:o,content:i.replace(/\+/g,"%2B"),user_ip:e})}).then(e=>e.json()).then(s=>{s.ok?(commentRows.push(s.row),render_comment(commentRows),e.value="",t.value="",n.value=""):s.status===606?(Swal.fire(`Warning!
지속적인 도배 시도시
IP가 차단될 수 있습니다.`),e.value="",t.value="",n.value=""):s.status===607&&(alert(`지나친 댓글 도배를 확인하여
접근을 차단합니다.`),window.location.href="https://google.com")}).catch(e=>console.error(e))}async function update_comment(e,t){const{value:n}=await Swal.fire({title:"댓글 수정",input:"password",inputPlaceholder:"글 작성 시 입력했던 패스워드를 입력하세요",inputAttributes:{maxlength:20,autocapitalize:"off",autocorrect:"off"}});if(n){const s="https://freshrimpsushi.com/blog/ajax/comment.php?action=checkPassword";postFetch(s,{cmt_idx:e,password:n}).then(e=>e.json()).then(s=>{if(s.ok){const s=document.querySelector(`#comment${e}`),o="https://freshrimpsushi.com/blog/ajax/comment.php?action=getComment";postFetch(o,{cmt_idx:e}).then(e=>e.json()).then(o=>{if(o.ok){const a=o.row,r=a.author,c=a.content;let i="";i+=`<div class="update-author-info">`,i+=`<input class="update-author" type="text" value="${r}" placeholder="이름" />`,i+=`<input class="update-password" type="password" value="${n}" placeholder="비밀번호" disabled />`,i+="</div>",i+=`<textarea class="update-content" value="" style="IME-MODE:active;">${c}</textarea>`,i+=`<input class="update_comment-button" type="submit" value="수정" onclick="update_comment_click(${e}, '${t}')" />`,i+=`<input class="update_comment-button" type="submit" value="취소" onclick="cancel_click(${e})" />`,s.innerHTML=i}}).catch(e=>console.error(e))}else Swal.fire("패스워드 일치 오류")}).catch(e=>console.error(e))}}async function delete_comment(e,t){const{value:n}=await Swal.fire({title:"댓글 삭제",text:"삭제하면 되돌릴 수 없습니다.",input:"password",icon:"warning",showCancelButton:!0,confirmButtonColor:"#3085d6",cancelButtonColor:"#d33",inputPlaceholder:"글 작성 시 입력했던 패스워드를 입력하세요",inputAttributes:{maxlength:20,autocapitalize:"off",autocorrect:"off"}});if(n){const s="https://freshrimpsushi.com/blog/ajax/comment.php?action=checkPassword";postFetch(s,{cmt_idx:e,password:n}).then(e=>e.json()).then(n=>{if(n.ok){const n="https://freshrimpsushi.com/blog/ajax/comment.php?action=deleteComment";postFetch(n,{cmt_idx:e}).then(e=>e.json()).then(n=>{Swal.fire("삭제되었습니다."),t==="parents"?commentRows=commentRows.filter(t=>t.cmt_idx!==e):t==="child"&&commentRows.map(t=>{t.child=t.child.filter(t=>t.cmt_idx!==e),t.child_cnt=t.child.length}),render_comment(commentRows)}).catch(e=>console.error(e))}else Swal.fire("패스워드 일치 오류")}).catch(e=>console.error(e))}}function update_comment_click(e,t){const i=document.querySelector(`#comment${e} .update-author`),a=document.querySelector(`#comment${e} .update-password`),r=document.querySelector(`#comment${e} .update-content`),n=i.value,s=a.value,o=r.value;(n===""||s===""||o==="")&&alert("빈칸을 채워주세요");const c="https://freshrimpsushi.com/blog/ajax/comment.php?action=updateComment";postFetch(c,{cmt_idx:e,author:n.replace(/\+/g,"%2B"),password:s,content:o.replace(/\+/g,"%2B")}).then(e=>e.json()).then(n=>{n.ok&&(Swal.fire("수정되었습니다."),t==="parents"?commentRows.map(t=>{t.cmt_idx==e&&(t.author=n.author,t.content=n.content)}):t==="child"&&commentRows.map(t=>{t.cmt_idx==n.parent_cmt_idx&&t.child.map(t=>{t.cmt_idx==e&&(t.author=n.author,t.content=n.content)})}),render_comment(commentRows),recentRows.map(t=>{t.cmt_idx==e&&(t.author=n.author,t.content=n.content)}),render_recent_comment(recentRows))}).catch(e=>console.error(e))}function cancel_click(){render_comment(commentRows)}function re_comment(e,t){const s=document.querySelector(`#comment${e} .re-comment`);let n="";n+='<div class="re-comment-box">',n+='<div class="re-comment-author-info">',n+='<input class="re-comment-author" type="text" value="" placeholder="이름" />',n+='<input class="re-comment-password" type="password" value="" placeholder="비밀번호" />',n+="</div>",n+='<textarea class="re-comment-content" placeholder="내용" value="" style="IME-MODE:active;"></textarea>',t===void 0?n+=`<button class="write-button" onclick="re_comment_click(${e})" value="send"><i class='fa-solid fa-paper-plane'></i></button>`:n+=`<button class="write-button" onclick="re_comment_click(${e}, ${t})" value="send"><i class='fa-solid fa-paper-plane'></i></button>`,n+=`<button class="write-button" onclick="cancel_click(${e})" value="close"><i class='fa-solid fa-solid fa-ban'></i></button>`,n+="</div>",s.innerHTML=n}function re_comment_click(e,t){const n=document.querySelector(`#comment${e} .re-comment-author`),s=document.querySelector(`#comment${e} .re-comment-password`),o=document.querySelector(`#comment${e} .re-comment-content`),i=n.value,a=s.value,r=o.value;if(i===""||a===""||r===""){alert("빈칸을 채워주세요");return}const c="3029",l="",d="機械学習における強化学習とは？";fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(n=>{const s="https://freshrimpsushi.com/blog/ajax/comment.php?action=writeReComment";return postFetch(s,{board_idx:c,board_slug:l,board_title:d,cmt_idx:t===void 0?e:t,author:i.replace(/\+/g,"%2B"),password:a,content:r.replace(/\+/g,"%2B"),user_ip:n})}).then(e=>e.json()).then(i=>{i.ok?(commentRows.map(n=>{t===void 0?n.cmt_idx==e&&n.child.push(i.row):n.cmt_idx==t&&n.child.push(i.row)}),render_comment(commentRows)):i.status==606?(Swal.fire(`Warning!
지속적인 도배 시도시
IP가 차단될 수 있습니다.`),n.value="",s.value="",o.value=""):i.status===607&&(alert(`지나친 댓글 도배를 확인하여
접근을 차단합니다.`),window.location.href="https://google.com")}).catch(e=>console.error(e))}</script></div><aside class=sidebar><aside><style>.reddot a:link{color:#67d10f;text-shadow:0 0 10px #d3d3d3;font-weight:700}.reddot a:visited{color:#f5f5f5}</style><p class=reddot style=text-align:center;margin:0><a href=https://freshrimpsushi.github.io/jp/posts/2707/>夏の特選おまかせ<br>「想像上の数」</a></p></aside><br><div class=category></div><div style=display:flex>● をクリックして、興味ある分野だけ強調。<div class=resetmute><button class="sb-btn btnReset" style=border-radius:5px;border:0>reset</button>
<button class="sb-btn btnMute" style=border-radius:5px;border:0>mute</button></div></div><script defer>const color={green:"#33cc33",yellow:"#ffcc00",red:"#ff3333",black:"#000000"},categoryRows=[[{idx:1,name:"함수",color:color.green,show:"関数",size:"146"},{idx:2,name:"보조정리",color:color.green,show:"レンマ",size:"55"},{idx:3,name:"미분적분학",color:color.green,show:"微分積分学",size:"45"},{idx:4,name:"행렬대수",color:color.green,show:"行列代数",size:"117"}],[{idx:1,name:"정수론",color:color.green,show:"整数論",size:"90"},{idx:2,name:"집합론",color:color.green,show:"集合論",size:"49"},{idx:3,name:"그래프이론",color:color.green,show:"グラフ理論",size:"65"},{idx:4,name:"선형대수",color:color.green,show:"線形代数",size:"97"},{idx:5,name:"해석개론",color:color.green,show:"解析学",size:"84"},{idx:6,name:"추상대수",color:color.green,show:"抽象代数",size:"105"},{idx:7,name:"위상수학",color:color.green,show:"位相幾何学",size:"64"},{idx:8,name:"기하학",color:color.green,show:"幾何学",size:"167"}],[{idx:1,name:"다변수벡터해석",color:color.green,show:"ベクトル分析",size:"36"},{idx:2,name:"복소해석",color:color.green,show:"複素解析",size:"71"},{idx:3,name:"측도론",color:color.green,show:"測度論",size:"53"},{idx:4,name:"푸리에해석",color:color.green,show:"フーリエ解析",size:"54"},{idx:5,name:"초함수론",color:color.green,show:"シュワルツ超函数",size:"22"},{idx:6,name:"단층촬영",color:color.green,show:"トモグラフィ",size:"20"}],[{idx:1,name:"거리공간",color:color.green,show:"距離空間",size:"38"},{idx:2,name:"바나흐공간",color:color.green,show:"バナッハ空間",size:"38"},{idx:3,name:"힐베르트공간",color:color.green,show:"ヒルベルト空間",size:"31"},{idx:4,name:"르벡공간",color:color.green,show:"ルベーグ空間",size:"33"}],[{idx:1,name:"상미분방정식",color:color.green,show:"微分方程式",size:"58"},{idx:2,name:"편미분방정식",color:color.green,show:"偏微分方程式",size:"60"},{idx:3,name:"확률미분방정식",color:color.green,show:"確率微分方程式",size:"26"}],[{idx:1,name:"줄리아",color:color.green,show:"ジュリア",size:"231"},{idx:2,name:"알고리즘",color:color.green,show:"アルゴリズム",size:"28"},{idx:3,name:"수치해석",color:color.green,show:"数値解析",size:"63"},{idx:4,name:"최적화이론",color:color.green,show:"最適化理論",size:"37"},{idx:5,name:"머신러닝",color:color.green,show:"機械学習",size:"114"},{idx:6,name:"프로그래밍",color:color.yellow,show:"プログラミング",size:"116"},{idx:7,name:"세이버메트릭스",color:color.green,show:"セイバーメトリクス",size:"231",size:"13"}],[{idx:1,name:"물리학",color:color.green,show:"物理学",size:"29"},{idx:2,name:"수리물리",color:color.green,show:"数理物理学",size:"78"},{idx:3,name:"고전역학",color:color.green,show:"古典力学",size:"48"},{idx:4,name:"전자기학",color:color.green,show:"電磁気学",size:"51"},{idx:5,name:"양자역학",color:color.green,show:"量子力学",size:"57"},{idx:6,name:"열물리학",color:color.green,show:"熱物理学",size:"29"}],[{idx:1,name:"R",color:color.green,show:"R",size:"54"},{idx:2,name:"데이터확보",color:color.green,show:"データ確保",size:"29"},{idx:3,name:"데이터과학",color:color.green,show:"データサイエンス",size:"41"},{idx:4,name:"통계적검정",color:color.green,show:"統計的検定",size:"33"},{idx:5,name:"통계적분석",color:color.green,show:"統計的分析",size:"76"},{idx:6,name:"수리통계학",color:color.green,show:"数理統計学",size:"123"},{idx:7,name:"확률분포론",color:color.green,show:"確率分布論",size:"84"},{idx:8,name:"확률론",color:color.green,show:"確率論",size:"80"},{idx:9,name:"위상데이터분석",color:color.green,show:"位相データ分析",size:"40"}],[{idx:1,name:"논문작성",color:color.red,show:"論文作成",size:"63"},{idx:2,name:"생새우초밥지",color:color.black,show:"生エビ寿司誌",size:"7"}]];document.addEventListener("DOMContentLoaded",()=>{const e=JSON.parse(localStorage.getItem("blindList"));(e==""||e==null||e==null||e==0||e==NaN)&&localStorage.setItem("blindList",null),render_category(categoryRows),blind_category(e);const t=document.querySelector(".btnReset");t.addEventListener("click",()=>{let e=new Array;localStorage.setItem("blindList",JSON.stringify(e)),blind_category(e)});const n=document.querySelector(".btnMute");n.addEventListener("click",()=>{let e=new Array;for(let t=0;t<categoryRows.length;t++)categoryRows[t].map(n=>{const s={mainIdx:t,subIdx:n.idx};e.push(s)});localStorage.setItem("blindList",JSON.stringify(e)),blind_category(e)})});function render_category(e){const n=document.querySelector(".category");let t="";for(let n=0;n<e.length;n++)e[n].map(e=>{t+=`<span id="cate${n}-${e.idx}" class="cate etewsert">`,t+=`<span onclick="check_blind(${n}, ${e.idx});" style="cursor: pointer; color: ${e.color}">●</span>`,t+=`<a href="https://freshrimpsushi.github.io/jp/categories/${e.name.toLowerCase()}/">`,t+=` ${e.show} (${e.size})</a>`,t+="</span>",screen.width>954?t+="<br>":t+=" "}),t+="<hr>";n.innerHTML=t}function check_blind(e,t){const o=document.querySelector(`#cate${e}-${t}`);let n=new Array;const i={mainIdx:e,subIdx:t};let s=JSON.parse(localStorage.getItem("blindList"));if(s==""||s==null||s==null||s==0||s==NaN)n.push(i),localStorage.setItem("blindList",JSON.stringify(n)),o.className+=" blind";else{n=s;let a=null;for(let s=0;s<n.length;s++)if(n[s].mainIdx==e&&n[s].subIdx==t){a=n.filter(n=>n.mainIdx!=e||n.subIdx!=t);break}a===null?(n.push(i),localStorage.setItem("blindList",JSON.stringify(n)),o.className+=" blind"):(localStorage.setItem("blindList",JSON.stringify(a)),o.className="cate")}}function blind_category(e){const t=document.querySelectorAll(".cate");t.forEach(e=>{e.className="cate"}),e!==null&&e.map(e=>{const t=document.querySelector(`#cate${e.mainIdx}-${e.subIdx}`);t.className+=" blind"})}</script><br><br><b>最近見たポスト</b><div class=lately-viewed-list style=padding-left:4px></div><script defer>document.addEventListener("DOMContentLoaded",()=>{const e=JSON.parse(localStorage.getItem("latelyViewPostList")),n=document.querySelector(".lately-viewed-list");let t="";if(e==""||e==null||e==null||e==0||e==NaN)localStorage.setItem("latelyViewPostList",null),t+='<div class="lv-list">',t+=" · 열람한 포스트가 없습니다.",t+="</div>",n.innerHTML=t;else{for(let n=e.length-1;n>=0;n--)t+='<div class="lv-list">',t+=`<a href="${e[n].link}"> · ${e[n].title}</a>`,t+="</div>";n.innerHTML=t}})</script><script defer>document.addEventListener("DOMContentLoaded",()=>{const s=document.querySelector(".content-box");s&&renderKaTex(s);const o=document.querySelector(".tex");o&&renderKaTex(o);var n,r,i="機械学習における強化学習とは？",c="https://freshrimpsushi.github.io/jp/posts/3029/",t=new Array,a={title:i,link:c},e=JSON.parse(localStorage.getItem("latelyViewPostList"));if(e==""||e==null||e==null||e==0||e==NaN)t.push(a),localStorage.setItem("latelyViewPostList",JSON.stringify(t));else{for(r=e.length,n=0;n<r;n++)if(i==e[n].title){e.splice(n,1);break}t=e,t.push(a),t.length>5&&t.shift(),localStorage.setItem("latelyViewPostList",JSON.stringify(t))}})</script><br><b>最新コメント</b><div class=current-reply></div><script defer>const url="https://freshrimpsushi.com/blog/ajax/recent_comment.php?action=getCurrentComment";let recentRows=[];fetch(url).then(e=>e.json()).then(e=>{e.ok&&(recentRows=e.rows,render_recent_comment(recentRows))}).catch(e=>console.error(e));function render_recent_comment(e){const n=document.querySelector(".current-reply");let t="";e.map(e=>{let n="https://freshrimpsushi.github.io/jp/";e.board_idx>-1?n+=`posts/${e.board_idx}#comment${e.cmt_idx}`:e.board_idx==-1?n+=`#comment${e.cmt_idx}`:n+=`categories/${e.board_title}#comment${e.cmt_idx}`,t+='<div class="current-reply-list">',t+=`<a href="${n}">`,t+=`<b> - ${e.author}</b>: `,t+=`${e.content}`,t+=`</a>`,t+=`</div>`}),n.innerHTML=t,renderKaTex(n)}</script><br></aside></div><footer><aside><div><p id=mirror-link style=text-align:center><a style=cursor:text href=http://localhost:1313//jp/posts/3029/>© 生エビ寿司屋 / Powered by 류대식, 전기현</a><br>Contact:
<img src=https://freshrimpsushi.github.io/jp/logo/gmail.png width=12px alt=mail> freshrimpsushi@gmail.com
<a href=https://freshrimpsushi.github.io/jp/index.xml><img src=https://freshrimpsushi.github.io/jp/logo/RSS.png width=12px alt=RSS> RSS</a></p></div><script type=text/javascript>var goIndex=function(){var e=document.getElementsByName("idx")[0].value,t="https://freshrimpsushi.github.io/jp/posts/"+e;location.replace(t)};document.addEventListener("keydown",function(e){const n=document.getElementById("navigator");if(e.altKey&&e.ctrlKey&&e.key==="l"){e.preventDefault();var t=document.querySelector("#mirror-link a");t?t.click():console.log("거울 링크를 찾지 못했습니다.")}})</script></aside></footer></body><script async src="https://www.googletagmanager.com/gtag/js?id=G-NLV8Y9PRK1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NLV8Y9PRK1")</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4751085325232621" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/sweetalert2@11></script><script src=https://freshrimpsushi.github.io/jp/js/fontawsome.min.js></script><script src=https://freshrimpsushi.github.io/jp/js/common.js></script><script>document.addEventListener("DOMContentLoaded",()=>{fetch("https://api64.ipify.org?format=json").then(e=>e.json()).then(e=>e.ip).then(e=>{const t="https://freshrimpsushi.com/blog/ajax/ip_checker.php";return postFetch(t,{user_ip:e})}).then(e=>e.json()).then(e=>{e.ok||(alert(`차단된 IP입니다.
Contact:
freshrimpsushi@gmail.com`),window.location.href="https://google.com")}).catch(e=>console.error(e))})</script></html>